<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – PCI</title><link>https://desistdaydream.github.io/tags/PCI/</link><description>Recent content in PCI on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/tags/PCI/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Linux 网络设备</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/Linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/Linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/if_arp.h">GitHub 项目，torvalds/linux - include/uapi/linux/if_arp.h&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.iana.org/assignments/arp-parameters/arp-parameters.xhtml#arp-parameters-2">IANA，Address Resolution Protocol (ARP) Parameters，Hardware Types&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stackoverflow.com/questions/18598283/the-meaning-of-the-sys-class-net-interface-type-value">https://stackoverflow.com/questions/18598283/the-meaning-of-the-sys-class-net-interface-type-value&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 网络设备归属于 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/PCI.md">PCI&lt;/a> 总线类型。&lt;/p>
&lt;h1 id="关联文件">关联文件&lt;a class="td-heading-self-link" href="#%e5%85%b3%e8%81%94%e6%96%87%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="sysfs-中的网络设备信息">sysfs 中的网络设备信息&lt;a class="td-heading-self-link" href="#sysfs-%e4%b8%ad%e7%9a%84%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e4%bf%a1%e6%81%af" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>每个网络设备，都会在 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Filesystem/%E7%89%B9%E6%AE%8A%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/sysfs.md">sysfs&lt;/a> 中注册（主要是与 PCI 相关），有一系列文件用来描述或定义这些网络设备。&lt;/p>
&lt;p>在 &lt;code>/sys/class/net/${NetDeviceName}/&lt;/code> 目录下可以找到已在内核注册的关于网络设备的信息&lt;/p>
&lt;blockquote>
&lt;p>Note: &lt;code>${NetDeviceName}&lt;/code> 是指向 &lt;code>/sys/devices/pciXXX/XXX/.../XXX/${NetDeviceName}/&lt;/code> 的 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Filesystem/%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/Symbolic%20link.md">Symbolic link&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>./type&lt;/strong> # 网络设备的类型。文件内容是数字。从 &lt;a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/if_arp.h">GitHub 项目，torvalds/linux - include/uapi/linux/if_arp.h&lt;/a> 文件中找到数字对应的设备类型表和该设备的定义（e.g. 1 表示 ARPHRD_ETHER），这个 C 的头文件将网络设备分为如下几大块&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ARP 协议硬件定义&lt;/strong> # &lt;a href="https://desistdaydream.github.io/docs/4.%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/ARP%20%E4%B8%8E%20NDP.md">ARP&lt;/a> 的 RFC 标准中，定义了这些，并且 IANA 中也维护了这些注册信息。&lt;/li>
&lt;li>&lt;strong>非 ARP 硬件的虚拟网络设备&lt;/strong> # Linux 自身实现的一些虚拟网络设备&lt;/li>
&lt;li>&lt;strong>TODO&lt;/strong>: 其他信息待整理&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>./flags&lt;/strong> # 网络设备的 Flags(标志)。常用来描述设备的状态和基本功能。从 &lt;a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/if.h">GitHub 项目，torvalds/linux - include/uapi/linux/if.sh&lt;/a> 文件中找到这些 Flags 的含义&lt;/p>
&lt;ul>
&lt;li>Notes: &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux%20%E7%AE%A1%E7%90%86/Linux%20%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/Iproute%20%E5%B7%A5%E5%85%B7%E5%8C%85/ip/ip.md">ip&lt;/a> 工具下的 link 和 address 子命令通过 show 显示的网络设备信息中，第三部分由 &lt;code>&amp;lt; &amp;gt;&lt;/code> 包裹起来的就是网络设备的 Flags&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>./device/&lt;/strong> # &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/PCI.md#PCI%20%E8%AE%BE%E5%A4%87%E8%B5%84%E6%BA%90%E4%BF%A1%E6%81%AF">PCI 设备资源信息&lt;/a>（包括设备供应商、设备类别、etc.），该目录是 &lt;code>/sys/devices/pciXXXX:XX/.../XXX&lt;/code> 下的 PCI 相关目录的软链接，可以从 PCI 文章中查看各文件的含义。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./uevent&lt;/strong> # 用户空间事件，物理机中该文件中包含 网络设备的驱动与 PCI 信息。
&lt;ul>
&lt;li>PCI_SLOT_NAME # 网络设备所在的总线信息，与 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux%20%E7%AE%A1%E7%90%86/Linux%20%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/ethtool.md">ethtool&lt;/a> 命令的 -i 选项输出的 bus-info 信息相同；与 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux%20%E7%AE%A1%E7%90%86/Linux%20%E7%A1%AC%E4%BB%B6%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/lspci.md">lspci&lt;/a> 的第一列信息相同；与 &lt;code>lshw -C net -businfo&lt;/code> 的第一列信息相同
&lt;ul>
&lt;li>Notes: 虚拟机中，该文件没有 PCI_SLOT_NAME 的信息。&lt;/li>
&lt;li>&lt;a href="https://stackoverflow.com/questions/78497110/how-to-get-bus-info-in-a-generic-way">https://stackoverflow.com/questions/78497110/how-to-get-bus-info-in-a-generic-way&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://askubuntu.com/questions/654820/how-to-find-pci-address-of-an-ethernet-interface">https://askubuntu.com/questions/654820/how-to-find-pci-address-of-an-ethernet-interface&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stackoverflow.com/questions/73650069/how-to-use-ethtool-drvinfo-to-collect-driver-information-for-a-network-interface">https://stackoverflow.com/questions/73650069/how-to-use-ethtool-drvinfo-to-collect-driver-information-for-a-network-interface&lt;/a>&lt;/li>
&lt;li>具体解释详见下文 &lt;a href="#%E9%80%9A%E8%BF%87%20PCI%20%E8%AF%86%E5%88%AB%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87">通过 PCI 识别网络设备&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>TODO: 其他信息待整理&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># lshw -C net -businfo | grep I350&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pci@0000:61:00.0 eno1 network I350 Gigabit Network Connection
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pci@0000:61:00.1 eno2 network I350 Gigabit Network Connection
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># lspci | grep I350&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>61:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>rev 01&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>61:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>rev 01&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ethtool -i eno1 | grep bus-info&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bus-info: 0000:61:00.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># cat /sys/class/net/eno1/device/uevent | grep PCI_SLOT_NAME&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">PCI_SLOT_NAME&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>0000:61:00.0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="网卡驱动">网卡驱动&lt;a class="td-heading-self-link" href="#%e7%bd%91%e5%8d%a1%e9%a9%b1%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>可以在 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/Driver.md#PCI">Driver&lt;/a> 的 PCI 部分找到 Linux 是如何管理网卡驱动的&lt;/p>
&lt;h1 id="通过-pci-识别网络设备">通过 PCI 识别网络设备&lt;a class="td-heading-self-link" href="#%e9%80%9a%e8%bf%87-pci-%e8%af%86%e5%88%ab%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>下面示例中的 16 进制数值的 PCI 信息如何解读可以参考 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/PCI.md">PCI&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">for&lt;/span> i in &lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>realpath /sys/bus/pci/devices/*&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span> &lt;span style="color:#204a87;font-weight:bold">do&lt;/span> cat &lt;span style="color:#000">$i&lt;/span>/class &lt;span style="color:#000;font-weight:bold">|&lt;/span> grep 0x0200&lt;span style="color:#000;font-weight:bold">;&lt;/span> &lt;span style="color:#204a87;font-weight:bold">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 下面这个命令可以显示筛选出来的 0x0200 对应的文件名&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>realpath /sys/bus/pci/devices/* &lt;span style="color:#000;font-weight:bold">|&lt;/span> xargs -I &lt;span style="color:#ce5c00;font-weight:bold">{}&lt;/span> sh -c &lt;span style="color:#4e9a06">&amp;#39;grep &amp;#34;0x0200&amp;#34; {}/class 2&amp;gt;/dev/null | sed &amp;#34;s|^|{}/class:|&amp;#34;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>命令原理见下面的描述，我们首先观察物理机和虚拟机的一些 class 信息&lt;/p>
&lt;p>物理机&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ls -l /sys/class/net/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>总用量 &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> root root &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 6月 &lt;span style="color:#0000cf;font-weight:bold">14&lt;/span> 16:55 eno3 -&amp;gt; ../../devices/pci0000:00/0000:00:1c.3/0000:01:00.0/net/eno3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># cat /sys/devices/pci0000:00/0000:00:1c.3/class&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0x060400
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># cat /sys/devices/pci0000:00/0000:00:1c.3/0000:01:00.0/class&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0x020000
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>虚拟机&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ls -l /sys/class/net/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> root root &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> May &lt;span style="color:#0000cf;font-weight:bold">13&lt;/span> 23:23 eth0 -&amp;gt; ../../devices/pci0000:00/0000:00:03.0/virtio0/net/eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># cat /sys/devices/pci0000:00/0000:00:03.0/class&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0x020000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># cat /sys/devices/pci0000:00/0000:00:03.0/virtio0/class&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat: &lt;span style="color:#4e9a06">&amp;#39;/sys/devices/pci0000:00/0000:00:03.0/virtio0/class&amp;#39;&lt;/span>: No such file or directory
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>从 &lt;a href="https://github.com/torvalds/linux/blob/master/include/linux/pci_ids.h">https://github.com/torvalds/linux/blob/master/include/linux/pci_ids.h&lt;/a> 查询到 class 文件中 ID 的含义。可以看到，虚拟机的 PCI 设备类型是 0x020000(PCI_CLASS_NETWORK_ETHERNET)，而物理机的是 0x060400(PCI_CLASS_BRIDGE_PCI)。&lt;/p>
&lt;p>在物理机上，这个 PCI 就相当于一个有多个网口的网卡（i.e. PCI_CLASS_BRIDGE_PCI）&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>物理机&lt;/th>
&lt;th>虚拟机&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>路径结构&lt;/td>
&lt;td>../../devices/pci0000:00/0000:00:1c.3/0000:01:00.0/net/eno3&lt;/td>
&lt;td>../../devices/pci0000:00/0000:00:03.0/virtio0/net/eth0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>./${DOMAIN:BUS:SOLT.FUNC}/class&lt;/code> 文件的值&lt;/td>
&lt;td>0x060400(i.e. PCI_CLASS_BRIDGE_PCI)&lt;/td>
&lt;td>0x020000(i.e. PCI_CLASS_NETWORK_ETHERNET)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>./${DOMAIN:BUS:SOLT.FUNC}/&lt;/code> 下的目录&lt;/td>
&lt;td>0000:01:00.0/&lt;/td>
&lt;td>virtio0/&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>./${DOMAIN:BUS:SOLT.FUNC}/${PCI_ID}/class&lt;/code> 文件的值&lt;/td>
&lt;td>0x020000(i.e. PCI_CLASS_NETWORK_ETHERNET)&lt;/td>
&lt;td>无该文件&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，虚拟机的整体结构也是跟物理机类似的，只不过 PCI 类型直接就是 PCI_CLASS_NETWORK_ETHERNET，导致没有下级 PCI ID（只是一个名为 virtio0 的目录），也就是说，如果想要通过 PCI 找到网卡，通常目录结构都是 &lt;code>/sys/devices/pci${DOMAIN:BUS}/${DOMAIN:BUS:SOLT.FUNC}/${PCI_ID}/net/${NETWORK_DEVICE_NAME}&lt;/code>&lt;/p>
&lt;p>&lt;font color="#ff0000">总的来说，就是从 /sys/devices/ 目录逐级查找值为 0x02 开头的 class 文件，那么这个文件所在目录就可以当作该网口（网路设备）的 PCI 地址。&lt;/font>&lt;/p>
&lt;p>Tips: 如果再深入查一下的话，可以从物理机看到这样的场景&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ls /sys/devices/pci0000:00/0000:00:1c.3/0000:01:00.{0,1}/net&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;/sys/devices/pci0000:00/0000:00:1c.3/0000:01:00.0/net&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>eno3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;/sys/devices/pci0000:00/0000:00:1c.3/0000:01:00.1/net&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>eno4
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>网卡上具体网口的 PCI Addr 在网卡 PCI Addr 下各有一个，也就是 PCI Addr 中的 FUNC，一个 0 一个 1。其中 0000:01:00.{0,1} 若是在虚拟机中看的话，则是 virtio{0,1} 这两个 PCI Addr。&lt;/p>
&lt;h1 id="虚拟网络设备">虚拟网络设备&lt;a class="td-heading-self-link" href="#%e8%99%9a%e6%8b%9f%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#">https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Virtualization/Network%20Virtualization/Network%20Virtualization.md">Network Virtualization&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 具有丰富的虚拟网络功能，可用作托管 VM 和容器以及云环境的基础。在这篇文章中，我将简要介绍所有常用的虚拟网络接口类型。没有代码分析，只简单介绍了接口及其在 Linux 上的使用。任何有网络背景的人都可能对这篇博文感兴趣。可以使用命令 ip link help 获取接口列表。&lt;/p>
&lt;p>这篇文章涵盖了以下常用网络设备和一些容易相互混淆的网络设备：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#bridge">Bridge&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#bonded">Bond&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#team">Team device&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vlan">VLAN (Virtual LAN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vxlan">VXLAN (Virtual eXtensible Local Area Network)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#macvlan">MACVLAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ipvlan">IPVLAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#macvtap">MACVTAP/IPVTAP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#macsec">MACsec (Media Access Control Security)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#veth">VETH (Virtual Ethernet)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vcan">VCAN (Virtual CAN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vxcan">VXCAN (Virtual CAN tunnel)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ipoib">IPOIB (IP-over-InfiniBand)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#nlmon">NLMON (NetLink MONitor)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#dummy">Dummy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ifb">IFB (Intermediate Functional Block)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#netdevsim">netdevsim&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bridge">Bridge&lt;a class="td-heading-self-link" href="#bridge" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Bridge 网络设备的行为类似于网络交换机。它在连接到它的网络设备之间转发数据包。通常用于在 路由器、网关、虚拟机、网络名称空间之间转发数据包。同时 Bridge 设备还支持 STP、VLAN 过滤和组播侦听。&lt;/p>
&lt;p>当我们想要在 虚拟机、容器、宿主机 之间建立通信时，Bridge 设备是必不可少的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495381-a7c7e048-4783-45a6-a1d1-c44526401132.png" alt="image.png">&lt;/p>
&lt;p>下面是一个是创建 Brdige 并连接其他网络设备的示例：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ip link add br0 &lt;span style="color:#204a87">type&lt;/span> bridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link &lt;span style="color:#204a87">set&lt;/span> eth0 master br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link &lt;span style="color:#204a87">set&lt;/span> tap1 master br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link &lt;span style="color:#204a87">set&lt;/span> tap2 master br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link &lt;span style="color:#204a87">set&lt;/span> veth1 master br0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的例子将会创建一个名为 &lt;code>br0&lt;/code> 的 Bridge 设备，并将两个 TAP 设备和一个 VETH 设备作为其从属设备。&lt;/p>
&lt;h2 id="bond">Bond&lt;a class="td-heading-self-link" href="#bond" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The Linux bonding driver provides a method for aggregating multiple network interfaces into a single logical &amp;ldquo;bonded&amp;rdquo; interface. The behavior of the bonded interface depends on the mode; generally speaking, modes provide either hot standby or load balancing services.&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495394-4aaea4bd-34d9-4987-9873-38af16247d1b.png" alt="https://developers.redhat.com/blog/wp-content/uploads/2018/10/bond.png">&lt;/p>
&lt;p>Use a bonded interface when you want to increase your link speed or do a failover on your server.
Here&amp;rsquo;s how to create a bonded interface:
ip link add bond1 type bond miimon 100 mode active-backup ip link set eth0 master bond1 ip link set eth1 master bond1
This creates a bonded interface named bond1 with mode active-backup. For other modes, please see the &lt;a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt">kernel documentation&lt;/a>.&lt;/p>
&lt;h2 id="team">Team&lt;a class="td-heading-self-link" href="#team" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Similar a bonded interface, the purpose of a team device is to provide a mechanism to group multiple NICs (ports) into one logical one (teamdev) at the L2 layer.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/team.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495378-cdc041c4-6321-49b7-b532-63f2cded7392.png" alt="image.png">&lt;/a>
The main thing to realize is that a team device is not trying to replicate or mimic a bonded interface. What it does is to solve the same problem using a different approach, using, for example, a lockless (RCU) TX/RX path and modular design.
But there are also some functional differences between a bonded interface and a team. For example, a team supports LACP load-balancing, NS/NA (IPV6) link monitoring, D-Bus interface, etc., which are absent in bonding. For further details about the differences between bonding and team, see &lt;a href="https://github.com/jpirko/libteam/wiki/Bonding-vs.-Team-features">Bonding vs. Team features&lt;/a>.
Use a team when you want to use some features that bonding doesn&amp;rsquo;t provide.
Here&amp;rsquo;s how to create a team:
# teamd -o -n -U -d -t team0 -c &amp;lsquo;{&amp;ldquo;runner&amp;rdquo;: {&amp;ldquo;name&amp;rdquo;: &amp;ldquo;activebackup&amp;rdquo;},&amp;ldquo;link_watch&amp;rdquo;: {&amp;ldquo;name&amp;rdquo;: &amp;ldquo;ethtool&amp;rdquo;}}&amp;rsquo; # ip link set eth0 down # ip link set eth1 down # teamdctl team0 port add eth0 # teamdctl team0 port add eth1
This creates a team interface named team0 with mode active-backup, and it adds eth0 and eth1 as team0&amp;rsquo;s sub-interfaces.
A new driver called &lt;a href="https://www.kernel.org/doc/html/latest/networking/net_failover.html">net_failover&lt;/a> has been added to Linux recently. It&amp;rsquo;s another failover master net device for virtualization and manages a primary (&lt;a href="https://wiki.libvirt.org/page/Networking#PCI_Passthrough_of_host_network_devices">passthru/VF [Virtual Function]&lt;/a> device) slave net device and a standby (the original paravirtual interface) slave net device.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/net_failover.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495395-cb9efff7-7595-4750-b0a0-18f0ef15f765.png" alt="image.png">&lt;/a>&lt;/p>
&lt;h2 id="vlan">VLAN&lt;a class="td-heading-self-link" href="#vlan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>A VLAN, aka virtual LAN, separates broadcast domains by adding tags to network packets. VLANs allow network administrators to group hosts under the same switch or between different switches.
The VLAN header looks like:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495406-ba20c202-7aec-4f2d-8e4a-718c8db481ad.png" alt="image.png">&lt;/a>
Use a VLAN when you want to separate subnet in VMs, namespaces, or hosts.
Here&amp;rsquo;s how to create a VLAN:
# ip link add link eth0 name eth0.2 type vlan id 2 # ip link add link eth0 name eth0.3 type vlan id 3
This adds VLAN 2 with name eth0.2 and VLAN 3 with name eth0.3. The topology looks like this:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496104-953046d4-0fac-4ca7-908d-45785c3a097d.png" alt="image.png">&lt;/a>
&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: When configuring a VLAN, you need to make sure the switch connected to the host is able to handle VLAN tags, for example, by setting the switch port to trunk mode.&lt;/p>
&lt;h2 id="vxlan">VXLAN&lt;a class="td-heading-self-link" href="#vxlan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>VXLAN (Virtual eXtensible Local Area Network) is a tunneling protocol designed to solve the problem of limited VLAN IDs (4,096) in IEEE 802.1q. It is described by &lt;a href="https://tools.ietf.org/html/rfc7348">IETF RFC 7348&lt;/a>.
With a 24-bit segment ID, aka VXLAN Network Identifier (VNI), VXLAN allows up to 2^24 (16,777,216) virtual LANs, which is 4,096 times the VLAN capacity.
VXLAN encapsulates Layer 2 frames with a VXLAN header into a UDP-IP packet, which looks like this:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vxlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496161-ef228755-9102-427a-b3fa-f9d8f5cae248.png" alt="image.png">&lt;/a>
VXLAN is typically deployed in data centers on virtualized hosts, which may be spread across multiple racks.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vxlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496231-8fa6bbd6-045f-43f3-a2f9-58039a973086.png" alt="image.png">&lt;/a>
Here&amp;rsquo;s how to use VXLAN:
# ip link add vx0 type vxlan id 100 local 1.1.1.1 remote 2.2.2.2 dev eth0 dstport 4789
For reference, you can read the &lt;a href="https://www.kernel.org/doc/Documentation/networking/vxlan.txt">VXLAN kernel documentation&lt;/a> or &lt;a href="https://vincent.bernat.ch/en/blog/2017-vxlan-linux">this VXLAN introduction&lt;/a>.&lt;/p>
&lt;h2 id="macvlan">MACVLAN&lt;a class="td-heading-self-link" href="#macvlan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>With VLAN, you can create multiple interfaces on top of a single one and filter packages based on a VLAN tag. With MACVLAN, you can create multiple interfaces with different Layer 2 (that is, Ethernet MAC) addresses on top of a single one.
Before MACVLAN, if you wanted to connect to physical network from a VM or namespace, you would have needed to create TAP/VETH devices and attach one side to a bridge and attach a physical interface to the bridge on the host at the same time, as shown below.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/br_ns.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496373-bba2ae90-3c29-497b-9cf2-9707146ab063.png" alt="image.png">&lt;/a>
Now, with MACVLAN, you can bind a physical interface that is associated with a MACVLAN directly to namespaces, without the need for a bridge.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496402-8b2910b5-780c-4a7b-885d-e1ae96f70a20.png" alt="image.png">&lt;/a>
There are five MACVLAN types:
1. Private: doesn&amp;rsquo;t allow communication between MACVLAN instances on the same physical interface, even if the external switch supports hairpin mode.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496672-ece58d6f-86b8-4bb6-bfdc-4877583cf2a8.png" alt="image.png">&lt;/a>
2. VEPA: data from one MACVLAN instance to the other on the same physical interface is transmitted over the physical interface. Either the attached switch needs to support hairpin mode or there must be a TCP/IP router forwarding the packets in order to allow communication.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_02.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497664-fcf059dc-8d97-4d85-83dc-79e35d8a0502.png" alt="image.png">&lt;/a>
3. Bridge: all endpoints are directly connected to each other with a simple bridge via the physical interface.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_03.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497742-be699130-38ef-45a2-a657-a7d90bcbbe81.png" alt="image.png">&lt;/a>
4. Passthru: allows a single VM to be connected directly to the physical interface.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_04.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497676-73e85ca2-4f2a-47b9-b14b-1d5fb34c877b.png" alt="image.png">&lt;/a>
5. Source: the source mode is used to filter traffic based on a list of allowed source MAC addresses to create MAC-based VLAN associations. Please see the &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?id=79cf79abce71">commit message&lt;/a>.
The type is chosen according to different needs. Bridge mode is the most commonly used.
Use a MACVLAN when you want to connect directly to a physical network from containers.
Here&amp;rsquo;s how to set up a MACVLAN:
# ip link add macvlan1 link eth0 type macvlan mode bridge # ip link add macvlan2 link eth0 type macvlan mode bridge # ip netns add net1 # ip netns add net2 # ip link set macvlan1 netns net1 # ip link set macvlan2 netns net2
This creates two new MACVLAN devices in bridge mode and assigns these two devices to two different namespaces.&lt;/p>
&lt;h2 id="ipvlan">IPVLAN&lt;a class="td-heading-self-link" href="#ipvlan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>IPVLAN is similar to MACVLAN with the difference being that the endpoints have the same MAC address.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/ipvlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497707-c70596b7-5689-41be-a5b3-94b69d978090.png" alt="image.png">&lt;/a>
IPVLAN supports L2 and L3 mode. IPVLAN L2 mode acts like a MACVLAN in bridge mode. The parent interface looks like a bridge or switch.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/ipvlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497736-65194dad-9ecb-4e91-af55-b16fae98721f.png" alt="image.png">&lt;/a>
In IPVLAN L3 mode, the parent interface acts like a router and packets are routed between endpoints, which gives better scalability.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/ipvlan_02.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498348-d23366af-ed6b-46f0-8bcc-4997dacca461.png" alt="image.png">&lt;/a>
Regarding when to use an IPVLAN, the &lt;a href="https://www.kernel.org/doc/Documentation/networking/ipvlan.txt">IPVLAN kernel documentation&lt;/a> says that MACVLAN and IPVLAN &amp;ldquo;are very similar in many regards and the specific use case could very well define which device to choose. if one of the following situations defines your use case then you can choose to use ipvlan -
(a) The Linux host that is connected to the external switch / router has policy configured that allows only one mac per port.
(b) No of virtual devices created on a master exceed the mac capacity and puts the NIC in promiscuous mode and degraded performance is a concern.
(c) If the slave device is to be put into the hostile / untrusted network namespace where L2 on the slave could be changed / misused.&amp;rdquo;
Here&amp;rsquo;s how to set up an IPVLAN instance:
# ip netns add ns0 # ip link add name ipvl0 link eth0 type ipvlan mode l2 # ip link set dev ipvl0 netns ns0
This creates an IPVLAN device named ipvl0 with mode L2, assigned to namespace ns0.&lt;/p>
&lt;h2 id="macvtapipvtap">MACVTAP/IPVTAP&lt;a class="td-heading-self-link" href="#macvtapipvtap" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>MACVTAP/IPVTAP is a new device driver meant to simplify virtualized bridged networking. When a MACVTAP/IPVTAP instance is created on top of a physical interface, the kernel also creates a character device/dev/tapX to be used just like a &lt;a href="https://en.wikipedia.org/wiki/TUN/TAP">TUN/TAP&lt;/a> device, which can be directly used by KVM/QEMU.
With MACVTAP/IPVTAP, you can replace the combination of TUN/TAP and bridge drivers with a single module:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvtap.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498407-337b9bb8-cbee-4990-8283-55a3f5c36e66.png" alt="image.png">&lt;/a>
Typically, MACVLAN/IPVLAN is used to make both the guest and the host show up directly on the switch to which the host is connected. The difference between MACVTAP and IPVTAP is same as with MACVLAN/IPVLAN.
Here&amp;rsquo;s how to create a MACVTAP instance:
# ip link add link eth0 name macvtap0 type macvtap&lt;/p>
&lt;h2 id="macsec">MACsec&lt;a class="td-heading-self-link" href="#macsec" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>MACsec (Media Access Control Security) is an IEEE standard for security in wired Ethernet LANs. Similar to IPsec, as a layer 2 specification, MACsec can protect not only IP traffic but also ARP, neighbor discovery, and DHCP. The MACsec headers look like this:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macsec_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498628-bcdf3cf6-08f5-4bd3-bac8-ef3412bea0d3.png" alt="image.png">&lt;/a>
The main use case for MACsec is to secure all messages on a standard LAN including ARP, NS, and DHCP messages.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macsec.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498641-f72e7987-0e5b-438c-959d-6d0397b337d6.png" alt="image.png">&lt;/a>
Here&amp;rsquo;s how to set up a MACsec configuration:
# ip link add macsec0 link eth1 type macsec
&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: This only adds a MACsec device called macsec0 on interface eth1. For more detailed configurations, please see the &amp;ldquo;Configuration example&amp;rdquo; section in this &lt;a href="https://developers.redhat.com/blog/2016/10/14/macsec-a-different-solution-to-encrypt-network-traffic/">MACsec introduction by Sabrina Dubroca&lt;/a>.&lt;/p>
&lt;h2 id="veth">VETH&lt;a class="td-heading-self-link" href="#veth" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The VETH (virtual Ethernet) device is a local Ethernet tunnel. Devices are created in pairs, as shown in the diagram below.
Packets transmitted on one device in the pair are immediately received on the other device. When either device is down, the link state of the pair is down.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/veth.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498671-cf9ff7ff-0ee0-469e-adff-620edf2a237a.png" alt="image.png">&lt;/a>
Use a VETH configuration when namespaces need to communicate to the main host namespace or between each other.
Here&amp;rsquo;s how to set up a VETH configuration:
# ip netns add net1 # ip netns add net2 # ip link add veth1 netns net1 type veth peer name veth2 netns net2
This creates two namespaces, net1 and net2, and a pair of VETH devices, and it assigns veth1 to namespace net1 and veth2 to namespace net2. These two namespaces are connected with this VETH pair. Assign a pair of IP addresses, and you can ping and communicate between the two namespaces.&lt;/p>
&lt;h2 id="vcan">VCAN&lt;a class="td-heading-self-link" href="#vcan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Similar to the network loopback devices, the VCAN (virtual CAN) driver offers a virtual local CAN (Controller Area Network) interface, so users can send/receive CAN messages via a VCAN interface. CAN is mostly used in the automotive field nowadays.
For more CAN protocol information, please refer to the &lt;a href="https://www.kernel.org/doc/Documentation/networking/can.txt">kernel CAN documentation&lt;/a>.
Use a VCAN when you want to test a CAN protocol implementation on the local host.
Here&amp;rsquo;s how to create a VCAN:
# ip link add dev vcan1 type vcan&lt;/p>
&lt;h2 id="vxcan">VXCAN&lt;a class="td-heading-self-link" href="#vxcan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Similar to the VETH driver, a VXCAN (Virtual CAN tunnel) implements a local CAN traffic tunnel between two VCAN network devices. When you create a VXCAN instance, two VXCAN devices are created as a pair. When one end receives the packet, the packet appears on the device&amp;rsquo;s pair and vice versa. VXCAN can be used for cross-namespace communication.
Use a VXCAN configuration when you want to send CAN message across namespaces.
Here&amp;rsquo;s how to set up a VXCAN instance:
# ip netns add net1 # ip netns add net2 # ip link add vxcan1 netns net1 type vxcan peer name vxcan2 netns net2
&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: VXCAN is not yet supported in Red Hat Enterprise Linux.&lt;/p>
&lt;h2 id="ipoib">IPOIB&lt;a class="td-heading-self-link" href="#ipoib" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>An IPOIB device supports the IP-over-InfiniBand protocol. This transports IP packets over InfiniBand (IB) so you can use your IB device as a fast NIC.
The IPoIB driver supports two modes of operation: datagram and connected. In datagram mode, the IB UD (Unreliable Datagram) transport is used. In connected mode, the IB RC (Reliable Connected) transport is used. The connected mode takes advantage of the connected nature of the IB transport and allows an MTU up to the maximal IP packet size of 64K.
For more details, please see the &lt;a href="https://www.kernel.org/doc/Documentation/infiniband/ipoib.txt">IPOIB kernel documentation&lt;/a>.
Use an IPOIB device when you have an IB device and want to communicate with a remote host via IP.
Here&amp;rsquo;s how to create an IPOIB device:
# ip link add ib0 name ipoib0 type ipoib pkey IB_PKEY mode connected&lt;/p>
&lt;h2 id="nlmon">NLMON&lt;a class="td-heading-self-link" href="#nlmon" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>NLMON is a Netlink monitor device.
Use an NLMON device when you want to monitor system Netlink messages.
Here&amp;rsquo;s how to create an NLMON device:
# ip link add nlmon0 type nlmon # ip link set nlmon0 up # tcpdump -i nlmon0 -w nlmsg.pcap
This creates an NLMON device named nlmon0 and sets it up. Use a packet sniffer (for example, tcpdump) to capture Netlink messages. Recent versions of Wireshark feature decoding of Netlink messages.&lt;/p>
&lt;h2 id="dummy">Dummy&lt;a class="td-heading-self-link" href="#dummy" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>dummy 是一个网络接口，是完全虚拟的，就像 loopback 设备。dummy 设备的目的是提供一种网络设备，用以路由数据包，而无需实际传输他们&lt;/p>
&lt;p>使用 dummy 设备使不活动的 SLIP(串行线路 Internet 协议) 地址看起来像本地程序的真实地址。 现在，dummy 设备主要用于测试和调试，同时，在 Kubernetes 中，Flannel 在 ipvs 模式下，也会创建一个名为 kube-ipvs0 的网络设备来路由数据包。&lt;/p>
&lt;p>dummy 设备的创建方式如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ip link add dummy1 &lt;span style="color:#204a87">type&lt;/span> dummy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip addr add 1.1.1.1/24 dev dummy1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link &lt;span style="color:#204a87">set&lt;/span> dummy1 up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ifb">IFB&lt;a class="td-heading-self-link" href="#ifb" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The IFB (Intermediate Functional Block) driver supplies a device that allows the concentration of traffic from several sources and the shaping incoming traffic instead of dropping it.
Use an IFB interface when you want to queue and shape incoming traffic.
Here&amp;rsquo;s how to create an IFB interface:
# ip link add ifb0 type ifb # ip link set ifb0 up # tc qdisc add dev ifb0 root sfq # tc qdisc add dev eth0 handle ffff: ingress # tc filter add dev eth0 parent ffff: u32 match u32 0 0 action mirred egress redirect dev ifb0
This creates an IFB device named ifb0 and replaces the root qdisc scheduler with SFQ (Stochastic Fairness Queueing), which is a classless queueing scheduler. Then it adds an ingress qdisc scheduler on eth0 and redirects all ingress traffic to ifb0.
For more IFB qdisc use cases, please refer to this &lt;a href="https://wiki.linuxfoundation.org/networking/ifb">Linux Foundation wiki on IFB&lt;/a>.&lt;/p>
&lt;h2 id="additional-resources">Additional resources&lt;a class="td-heading-self-link" href="#additional-resources" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/tag/virtual-networking/">Virtual networking articles&lt;/a> on the Red Hat Developer blog&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/">Dynamic IP Address Management in Open Virtual Network (OVN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/03/23/non-root-open-vswitch-rhel/">Non-root Open vSwitch in Red Hat Enterprise Linux&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/tag/open-vswitch/">Open vSwitch articles&lt;/a> on the Red hat Developer Blog&lt;/li>
&lt;/ul>
&lt;h2 id="netdevsim-interface">netdevsim interface&lt;a class="td-heading-self-link" href="#netdevsim-interface" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>netdevsim is a simulated networking device which is used for testing various networking APIs. At this time it is particularly focused on testing hardware
offloading, tc/XDP BPF and SR-IOV.
A netdevsim device can be created as follows
# ip link add dev sim0 type netdevsim # ip link set dev sim0 up
To enable tc offload:
# ethtool -K sim0 hw-tc-offload on
To load XDP BPF or tc BPF programs:
# ip link set dev sim0 xdpoffload obj prog.o
To add VFs for SR-IOV testing:
# echo 3 &amp;gt; /sys/class/net/sim0/device/sriov&lt;em>numvfs # ip link set sim0 vf 0 mac
To change the vf numbers, you need to disable them completely first:
# echo 0 &amp;gt; /sys/class/net/sim0/device/sriov_numvfs # echo 5 &amp;gt; /sys/class/net/sim0/device/sriov_numvfs
Note: netdevsim is not compiled in RHEL by default
_Last updated: September 11, 2019&lt;/em>&lt;/p>
&lt;h1 id="隧道网络设备">隧道网络设备&lt;a class="td-heading-self-link" href="#%e9%9a%a7%e9%81%93%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#">https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 支持多种隧道，但新用户可能会对它们的差异感到困惑，并不确定哪一种最适合给定的用例。在本文中，我将简要介绍 Linux 内核中常用的隧道接口。没有代码分析，只简单介绍了接口及其在 Linux 上的使用。任何有网络背景的人都可能对这些信息感兴趣。可以通过发出 iproute2 命令 ip link help 获得隧道接口列表以及特定隧道配置的帮助。&lt;/p>
&lt;p>这篇文章涵盖了以下常用接口：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#ipip">IPIP Tunnel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#sit">SIT Tunnel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#ip6tnl">ip6tnl Tunnel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#vti">VTI and VTI6&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#gre">GRE and GRETAP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#ip6gre">IP6GRE and IP6GRETAP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#fou">FOU&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#gue">GUE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#geneve">GENEVE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#erspan">ERSPAN and IP6ERSPAN&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="ipip-tunnel">IPIP Tunnel&lt;a class="td-heading-self-link" href="#ipip-tunnel" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>IPIP tunnel, just as the name suggests, is an IP over IP tunnel, defined in &lt;a href="https://tools.ietf.org/html/rfc2003">RFC 2003&lt;/a>. The IPIP tunnel header looks like:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541929-3c498a49-7406-4a02-ae97-da31bd386c6b.png" alt="image.png">&lt;/p>
&lt;p>It&amp;rsquo;s typically used to connect two internal IPv4 subnets through public IPv4 internet. It has the lowest overhead but can only transmit IPv4 unicast traffic. That means you &lt;strong>cannot&lt;/strong> send multicast via IPIP tunnel.
IPIP tunnel supports both IP over IP and MPLS over IP.
&lt;strong>Note&lt;/strong>: When the ipip module is loaded, or an IPIP device is created for the first time, the Linux kernel will create a tunl0 default device in each namespace, with attributes local=any and remote=any. When receiving IPIP protocol packets, the kernel will forward them to tunl0 as a fallback device if it can&amp;rsquo;t find another device whose local/remote attributes match their source or destination address more closely.
Here is how to create an IPIP tunnel:
On Server A: # ip link add name ipip0 type ipip local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR # ip link set ipip0 up # ip addr add INTERNAL_IPV4_ADDR/24 dev ipip0 Add a remote internal subnet route if the endpoints don&amp;rsquo;t belong to the same subnet # ip route add REMOTE_INTERNAL_SUBNET/24 dev ipip0 On Server B: # ip link add name ipip0 type ipip local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR # ip link set ipip0 up # ip addr add INTERNAL_IPV4_ADDR/24 dev ipip0 # ip route add REMOTE_INTERNAL_SUBNET/24 dev ipip0
Note: Please replace LOCAL_IPv4_ADDR, REMOTE_IPv4_ADDR, INTERNAL_IPV4_ADDR, REMOTE_INTERNAL_SUBNET to the addresses based on your testing environment. The same with following example configs.&lt;/p>
&lt;h2 id="sit-tunnel">SIT Tunnel&lt;a class="td-heading-self-link" href="#sit-tunnel" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>SIT stands for Simple Internet Transition. The main purpose is to interconnect isolated IPv6 networks, located in global IPv4 internet.
Initially, it only had an IPv6 over IPv4 tunneling mode. After years of development, however, it acquired support for several different modes, such as ipip (the same with IPIP tunnel), ip6ip, mplsip, and any. Mode any is used to accept both IP and IPv6 traffic, which may prove useful in some deployments. SIT tunnel also supports &lt;a href="https://www.ietf.org/rfc/rfc4214.txt">ISATA&lt;/a>, and here is a &lt;a href="http://www.litech.org/isatap">usage example&lt;/a>.
The SIT tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541941-c151c630-2925-4a09-aa89-845588e3e5b3.png" alt="image.png">
When the sit module is loaded, the Linux kernel will create a default device, named sit0.
Here is how to create a SIT tunnel:
On Server A: # ip link add name sit1 type sit local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR mode any # ip link set sit1 up # ip addr add INTERNAL_IPV4_ADDR/24 dev sit1
Then, perform the same steps on the remote side.&lt;/p>
&lt;h2 id="ip6tnl-tunnel">ip6tnl Tunnel&lt;a class="td-heading-self-link" href="#ip6tnl-tunnel" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>ip6tnl is an IPv4/IPv6 over IPv6 tunnel interface, which looks like an IPv6 version of the SIT tunnel. The tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541939-55d26347-12f2-4dab-b439-a20720e2714e.png" alt="image.png">
ip6tnl supports modes ip6ip6, ipip6, any. Mode ipip6 is IPv4 over IPv6, and mode ip6ip6 is IPv6 over IPv6, and mode any supports both IPv4/IPv6 over IPv6.
When the ip6tnl module is loaded, the Linux kernel will create a default device, named ip6tnl0.
Here is how to create an ip6tnl tunnel:
# ip link add name ipip6 type ip6tnl local LOCAL_IPv6_ADDR remote REMOTE_IPv6_ADDR mode any&lt;/p>
&lt;h2 id="vti-and-vti6">VTI and VTI6&lt;a class="td-heading-self-link" href="#vti-and-vti6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Virtual Tunnel Interface (VTI) on Linux is similar to Cisco&amp;rsquo;s VTI and Juniper&amp;rsquo;s implementation of secure tunnel (st.xx).
This particular tunneling driver implements IP encapsulations, which can be used with xfrm to give the notion of a secure tunnel and then use kernel routing on top.
In general, VTI tunnels operate in almost the same way as ipip or sit tunnels, except that they add a fwmark and IPsec encapsulation/decapsulation.
VTI6 is the IPv6 equivalent of VTI.
Here is how to create a VTI tunnel:
# ip link add name vti1 type vti key VTI_KEY local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR # ip link set vti1 up # ip addr add LOCAL_VIRTUAL_ADDR/24 dev vti1 # ip xfrm state add src LOCAL_IPv4_ADDR dst REMOTE_IPv4_ADDR spi SPI PROTO ALGR mode tunnel # ip xfrm state add src REMOTE_IPv4_ADDR dst LOCAL_IPv4_ADDR spi SPI PROTO ALGR mode tunnel # ip xfrm policy add dir in tmpl src REMOTE_IPv4_ADDR dst LOCAL_IPv4_ADDR PROTO mode tunnel mark VTI_KEY # ip xfrm policy add dir out tmpl src LOCAL_IPv4_ADDR dst REMOTE_IPv4_ADDR PROTO mode tunnel mark VTI_KEY
You can also configure IPsec via &lt;a href="https://libreswan.org/wiki/Route-based_VPN_using_VTI">libreswan&lt;/a> or &lt;a href="https://wiki.strongswan.org/projects/strongswan/wiki/RouteBasedVPN">strongSwan&lt;/a>.&lt;/p>
&lt;h2 id="gre-and-gretap">GRE and GRETAP&lt;a class="td-heading-self-link" href="#gre-and-gretap" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Generic Routing Encapsulation, also known as GRE, is defined in &lt;a href="https://tools.ietf.org/html/rfc2784">RFC 2784&lt;/a>
GRE tunneling adds an additional GRE header between the inside and outside IP headers. In theory, GRE could encapsulate any Layer 3 protocol with a valid Ethernet type, unlike IPIP, which can only encapsulate IP. The GRE header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541950-7306c06b-3438-4a0c-98f1-1dfac809d7a8.png" alt="image.png">
Note that you can transport multicast traffic and IPv6 through a GRE tunnel.
When the gre module is loaded, the Linux kernel will create a default device, named gre0.
Here is how to create a GRE tunnel:
# ip link add name gre1 type gre local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR [seq] key KEY
While GRE tunnels operate at OSI Layer 3, GRETAP works at OSI Layer 2, which means there is an Ethernet header in the inner header.
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541978-5b18c4ce-a484-4882-90a4-1341aab450fd.png" alt="image.png">
Here is how to create a GRETAP tunnel:
# ip link add name gretap1 type gretap local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR&lt;/p>
&lt;h2 id="ip6gre-and-ip6gretap">IP6GRE and IP6GRETAP&lt;a class="td-heading-self-link" href="#ip6gre-and-ip6gretap" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>IP6GRE is the IPv6 equivalent of GRE, which allows us to encapsulate any Layer 3 protocol over IPv6. The tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543192-8bbd04d7-88b1-407e-b183-27e70ac3142e.png" alt="image.png">
IP6GRETAP, just like GRETAP, has an Ethernet header in the inner header:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543437-bc1c8e95-4dee-449b-a31d-61154a0bbd85.png" alt="image.png">
Here is how to create a GRE tunnel:
# ip link add name gre1 type ip6gre local LOCAL_IPv6_ADDR remote REMOTE_IPv6_ADDR # ip link add name gretap1 type ip6gretap local LOCAL_IPv6_ADDR remote REMOTE_IPv6_ADDR&lt;/p>
&lt;h2 id="fou">FOU&lt;a class="td-heading-self-link" href="#fou" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Tunneling can happen at multiple levels in the networking stack. IPIP, SIT, GRE tunnels are at the IP level, while FOU (foo over UDP) is UDP-level tunneling.
There are some advantages of using UDP tunneling as UDP works with existing HW infrastructure, like &lt;a href="https://en.wikipedia.org/wiki/Network_interface_controller#RSS">RSS&lt;/a> in NICs, &lt;a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP&lt;/a> in switches, and checksum offload. The developer&amp;rsquo;s &lt;a href="https://lwn.net/Articles/614433/">patch set&lt;/a> shows significant performance increases for the SIT and IPIP protocols.
Currently, the FOU tunnel supports encapsulation protocol based on IPIP, SIT, GRE. An example FOU header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543253-f5dddca7-c7be-42cd-8f26-7f17a6b45499.png" alt="image.png">
Here is how to create a FOU tunnel:
# ip fou add port 5555 ipproto 4 # ip link add name tun1 type ipip remote 192.168.1.1 local 192.168.1.2 ttl 225 encap fou encap-sport auto encap-dport 5555
The first command configured a FOU receive port for IPIP bound to 5555; for GRE, you need to set ipproto 47. The second command set up a new IPIP virtual interface (tun1) configured for FOU encapsulation, with dest port 5555.
&lt;strong>NOTE&lt;/strong>: FOU is not supported in Red Hat Enterprise Linux.&lt;/p>
&lt;h2 id="gue">GUE&lt;a class="td-heading-self-link" href="#gue" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;a href="https://tools.ietf.org/html/draft-ietf-intarea-gue">Generic UDP Encapsulation&lt;/a> (GUE) is another kind of UDP tunneling. The difference between FOU and GUE is that GUE has its own encapsulation header, which contains the protocol info and other data.
Currently, GUE tunnel supports inner IPIP, SIT, GRE encapsulation. An example GUE header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543283-20ce12c4-edf1-4a2b-8888-42d3bdb4d08e.png" alt="image.png">
Here is how to create a GUE tunnel:
# ip fou add port 5555 gue # ip link add name tun1 type ipip remote 192.168.1.1 local 192.168.1.2 ttl 225 encap gue encap-sport auto encap-dport 5555
This will set up a GUE receive port for IPIP bound to 5555, and an IPIP tunnel configured for GUE encapsulation.
&lt;strong>NOTE&lt;/strong>: GUE is not supported in Red Hat Enterprise Linux.&lt;/p>
&lt;h2 id="geneve">GENEVE&lt;a class="td-heading-self-link" href="#geneve" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Generic Network Virtualization Encapsulation (GENEVE) supports all of the capabilities of VXLAN, NVGRE, and STT and was designed to overcome their perceived limitations. Many believe GENEVE could eventually replace these earlier formats entirely. The tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543437-55251107-feaf-4448-9a65-9b726a1fa1bb.png" alt="image.png">
which looks very similar to &lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/#vxlan">VXLAN&lt;/a>. The main difference is that the GENEVE header is flexible. It&amp;rsquo;s very easy to add new features by extending the header with a new Type-Length-Value (TLV) field. For more details, you can see the latest &lt;a href="https://tools.ietf.org/html/draft-ietf-nvo3-geneve-08">geneve ietf draft&lt;/a> or refer to this &lt;a href="https://www.redhat.com/en/blog/what-geneve">What is GENEVE?&lt;/a> article.
&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/networking_with_open_virtual_network/open_virtual_network_ovn">Open Virtual Network (OVN)&lt;/a> uses GENEVE as default encapsulation. Here is how to create a GENEVE tunnel:
# ip link add name geneve0 type geneve id VNI remote REMOTE_IPv4_ADDR&lt;/p>
&lt;h2 id="erspan-and-ip6erspan">ERSPAN and IP6ERSPAN&lt;a class="td-heading-self-link" href="#erspan-and-ip6erspan" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Encapsulated Remote Switched Port Analyzer (ERSPAN) uses GRE encapsulation to extend the basic port mirroring capability from Layer 2 to Layer 3, which allows the mirrored traffic to be sent through a routable IP network. The ERSPAN header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493544476-8decf814-c9a0-4c61-9005-e7626ca9ae4a.png" alt="image.png">
The ERSPAN tunnel allows a Linux host to act as an ERSPAN traffic source and send the ERSPAN mirrored traffic to either a remote host or to an ERSPAN destination, which receives and parses the ERSPAN packets generated from Cisco or other ERSPAN-capable switches. This setup could be used to analyze, diagnose, and detect malicious traffic.
Linux currently supports most features of two ERSPAN versions: v1 (type II) and v2 (type III).
Here is how to create an ERSPAN tunnel:
# ip link add dev erspan1 type erspan local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR seq key KEY erspan_ver 1 erspan IDX or # ip link add dev erspan1 type erspan local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR seq key KEY erspan_ver 2 erspan_dir DIRECTION erspan_hwid HWID Add tc filter to monitor traffic # tc qdisc add dev MONITOR_DEV handle ffff: ingress # tc filter add dev MONITOR_DEV parent ffff: matchall skip_hw action mirred egress mirror dev erspan1&lt;/p>
&lt;h2 id="summary">Summary&lt;a class="td-heading-self-link" href="#summary" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Here is a summary of all the tunnels we introduced.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Tunnel/Link Type&lt;/th>
&lt;th>Outer Header&lt;/th>
&lt;th>Encapsulate Header&lt;/th>
&lt;th>Inner Header&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ipip&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>None&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sit&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>None&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6tnl&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>None&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vti&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>IPsec&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vti6&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>IPsec&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gre&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gretap&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>Ether + IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6gre&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6gretap&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>Ether + IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>fou&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;td>UDP&lt;/td>
&lt;td>IPv4/IPv6/GRE&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gue&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;td>UDP + GUE&lt;/td>
&lt;td>IPv4/IPv6/GRE&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>geneve&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;td>UDP + Geneve&lt;/td>
&lt;td>Ether + IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>erspan&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>GRE + ERSPAN&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6erspan&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>GRE + ERSPAN&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Note&lt;/strong>: All configurations in this tutorial are volatile and won’t survive to a server reboot. If you want to make the configuration persistent across reboots, please consider using a networking configuration daemon, such as &lt;a href="https://developer.gnome.org/NetworkManager/stable/">NetworkManager&lt;/a>, or distribution-specific mechanisms.
_Also read: _&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/">Introduction to Linux interfaces for virtual networking&lt;/a>
&lt;em>Last updated: October 18, 2019&lt;/em>&lt;/p></description></item><item><title>Docs: Driver</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/Driver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/Driver/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 中的 &lt;strong>Driver(驱动)&lt;/strong> 管理。&lt;/p>
&lt;p>部分 BUS_TYPE 可以在 &lt;code>/sys/bus/${BUS_TYPE}/drivers/&lt;/code> 可以找到各类型总线下的驱动。e.g. &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Hardware/PCI.md">PCI&lt;/a> 设备、USB 设备、etc. 。&lt;/p>
&lt;blockquote>
&lt;p>Tip: 并不是所有的 BUS_TYPE 都有驱动，e.g. &lt;code>/sys/bus/memory/drivers/&lt;/code> 目录是空的&lt;/p>
&lt;/blockquote>
&lt;h1 id="pci">PCI&lt;a class="td-heading-self-link" href="#pci" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;code>/sys/bus/pci/drivers/${DRIVER}/&lt;/code> 目录是某个具体驱动下关联的设备的 PCI Addr 以及驱动相关操作&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./${PCI_ADDR}&lt;/strong> # 使用了本驱动的设备的 PCI Addr 软链接，指向 &lt;code>/sys/devices/pciXXX/XXX/...&lt;/code> 某个目录&lt;/li>
&lt;li>&lt;strong>./bind&lt;/strong> # 向该文件写入 PCI Addr 将会让设备与内核绑定&lt;/li>
&lt;li>&lt;strong>./unbind&lt;/strong> # 向该文件写入 PCI Addr 将会让设备从内核解绑&lt;/li>
&lt;/ul>
&lt;h1 id="最佳实践">最佳实践&lt;a class="td-heading-self-link" href="#%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="网卡的绑定与解绑">网卡的绑定与解绑&lt;a class="td-heading-self-link" href="#%e7%bd%91%e5%8d%a1%e7%9a%84%e7%bb%91%e5%ae%9a%e4%b8%8e%e8%a7%a3%e7%bb%91" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>我这里用网卡演示绑定和解绑的过程&lt;/p>
&lt;p>可以通过 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux%20%E7%AE%A1%E7%90%86/Linux%20%E7%A1%AC%E4%BB%B6%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/lspci.md">lspci&lt;/a> 命令查看该设备在内核中可以使用的驱动。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># lspci -s 0000:21:00.1 -v&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>21:00.1 Ethernet controller: Intel Corporation Ethernet Controller X710 &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> 10GbE SFP+ &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>rev 02&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Subsystem: Intel Corporation Ethernet Converged Network Adapter X710
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......略
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Kernel driver in use: i40e
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Kernel modules: i40e
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Kernel driver in use 表示当前设备使用的驱动；Kernel modules 表示当前设备使用的内核模块在。我们想要为某个 PCI 设备加载驱动时，可以参考 Kernel modules 的值。&lt;/p>
&lt;p>比如现在我们可以在 i40e 驱动下看到 0000:21:00.1 设备&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ls /sys/bus/pci/drivers/i40e/ | grep 0000:21:00.1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>利用 unbind 可以将该 PCI 设备（i.e. 网卡）从内核解绑&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">echo&lt;/span> -n &lt;span style="color:#4e9a06">&amp;#34;0000:21:00.1&amp;#34;&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> sudo tee /sys/bus/pci/drivers/i40e/unbind
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>解绑后，&lt;code>lspci -s 0000:21:00.1 -v&lt;/code> 命令不会显示 Kernel driver in use 这行内容&lt;/p>
&lt;/blockquote>
&lt;p>利用 bind 可以将该 PCI 设备（i.e. 网卡）绑定到内核的指定驱动上&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">echo&lt;/span> -n &lt;span style="color:#4e9a06">&amp;#34;0000:21:00.1&amp;#34;&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> sudo tee /sys/bus/pci/drivers/i40e/unbind
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>[!Note]
在解绑时，若该 PCI 设备使用的不是这个驱动，则会报错&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># echo -n &amp;#34;0000:21:00.1&amp;#34; | sudo tee /sys/bus/pci/drivers/igb/unbind&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0000:21:00.1tee: /sys/bus/pci/drivers/igb/unbind: No such device
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># echo -n &amp;#34;0000:21:00.1&amp;#34; | sudo tee /sys/bus/pci/drivers/i40e/unbind&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0000:21:00.1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以看到，0000:21:00.1 设备是 i40e 驱动，如果对 igb 解绑，则会提示 No such device&lt;/p>
&lt;p>同样的，若该 PCI 设备不支持某个驱动，在绑定时也会提示 No such device&lt;/p>
&lt;/blockquote>
&lt;p>通过 dmesg 过滤 PCI 地址，也可以看到在启动时，系统是如何为网卡分配 IP，以及如何为网卡命名的&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> ~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># dmesg | grep 0000:0c:00.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.352192&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>8086:1533&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#204a87">type&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">00&lt;/span> class 0x020000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.352219&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: reg 0x10: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>mem 0xc7200000-0xc72fffff&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.352247&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: reg 0x18: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>io 0x2000-0x201f&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.352262&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: reg 0x1c: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>mem 0xc7300000-0xc7303fff&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.352305&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: reg 0x30: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>mem 0xc7100000-0xc71fffff pref&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.352419&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: PME# supported from D0 D3hot D3cold
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 1.927697&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> pci 0000:0c:00.0: Adding to iommu group &lt;span style="color:#0000cf;font-weight:bold">86&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 从这里开始就不是 pci 了，而是 igb 了，说明该 PCI 设备的驱动使用了 igb&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 2.864926&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0: added PHC on eth1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 2.864928&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0: Intel&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>R&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> Gigabit Ethernet Network Connection
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 2.864930&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0: eth1: &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>PCIe:2.5Gb/s:Width x1&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> d8:cb:8a:fa:dc:7f
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 2.864975&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0: eth1: PBA No: 000200-000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 2.864976&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0: Using MSI-X interrupts. &lt;span style="color:#0000cf;font-weight:bold">4&lt;/span> rx queue&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, &lt;span style="color:#0000cf;font-weight:bold">4&lt;/span> tx queue&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 3.381286&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0 enp12s0: renamed from eth1 &lt;span style="color:#8f5902;font-style:italic"># 将网络设备重命名为 enp12s0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span> 32.790007&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> igb 0000:0c:00.0 enp12s0: igb: enp12s0 NIC Link is Up &lt;span style="color:#0000cf;font-weight:bold">1000&lt;/span> Mbps Full Duplex, Flow Control: RX
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>