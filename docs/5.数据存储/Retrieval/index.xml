<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Retrieval on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/</link><description>Recent content in Retrieval on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/index.xml" rel="self" type="application/rss+xml"/><item><title>Hash</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Hashing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Hashing/</guid><description>概述 参考：
Wiki 类别，Hashing Wiki, Hash function Hashing 是一种实现数据 Retrieval(检索) 的算法，有多种 Hashing 算法，比如
Consistent hashing Hash table 参考：
Wiki, Hash table Hash table(哈希表) 也称为 hash map(哈希映射) 或 hash set(哈希集)，是一种实现关联数组的数据结构，也称为 dictionary(字典)，它是一种将键映射到值的抽象数据类型。哈希表使用哈希函数来计算索引（也称为哈希码）到桶或槽数组中，从中可以找到所需的值。在查找过程中，对键进行哈希处理，生成的哈希值指示相应值的存储位置。
理想情况下，哈希函数会将每个键分配给一个唯一的存储桶，但大多数哈希表设计都采用不完善的哈希函数，这可能会导致哈希冲突，即哈希函数为多个键生成相同的索引。此类冲突通常以某种方式进行调节</description></item><item><title>一致性哈希算法 consistent hashing</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Consistent-hashing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Consistent-hashing/</guid><description>概述 参考：
Wiki, Consistent hashing 朱双印 blog，白话解析：一致性哈希算法 consistent hashing 在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。
场景描述 假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0 号、1 号、2 号，现在，有 3 万张图片需要缓存，我们希望这些图片被均匀的缓存到这 3 台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存 1 万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将 3 万张图片平均的缓存在 3 台服务器上，可以满足我们的要求吗？可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历 3 台缓存服务器，从 3 万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接收的，也就失去了缓存的意义，缓存的目的就是 提高速度，改善用户体验，减轻后端服务器压力，如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？原始的做法是对缓存项的键进行哈希，将 hash 后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的 key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。
hash（图片名称）% N
因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有 3 台服务器，使用哈希后的结果对 3 求余，那么余数一定是 0、1 或者 2，没错，正好与我们之前的服务器编号相同，如果求余的结果为 0， 我们就把当前图片名称对应的图片缓存在 0 号服务器上，如果余数为 1，就把当前图片名对应的图片缓存在 1 号服务器上，如果余数为 2，同理，那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将 3 万张图片随机的分布到 3 台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为 HASH 算法或者取模算法，取模算法的过程可以用下图表示。
但是，使用上述 HASH 算法进行缓存时，会出现一些缺陷，试想一下，如果 3 台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两台缓存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由 3 台变成了 4 台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来 3 台服务器时所在的服务器编号不同，因为除数由 3 变为了 4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设 3 台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从 3 台变为 2 台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了 缓存雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述 HASH 算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。</description></item><item><title>搜索引擎简介</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</guid><description>基础概念 搜索引擎
索引链+搜索组件，用户给搜索组件提供关键字，然后进行搜索
索引链： 原始内容-获取-创建文档-文档分析-创建索引-索引 搜索组件 UI-构建查询-运行查询-读取结果-UI 开源索引链：Lucene，自己不获取内容，不提供前段界面，ElasticSearch 基于 Lucene
Lucene ElasticSearch：简称 ES ES 是基于 Lucene 实现的开源、分布式、Restful 的全文本搜索引擎；并且还是一个分布式实时存储文档，其中每个文档的每个 field 均是被索引的数据，且可被搜索；也是一个实时分析功能的分布式搜索引擎，能够扩展至数以百计的节点实时处理 PB 级的数据
组件 基本组件 索引(index) # 文档容器。含有相同属性的文档集合，索引名必须使用小写字母 类型(type) # 索引内部的逻辑分区。一个索引内部可以定义一个或多个类型，文档必须属于一个类型 文档(document) # 文档是 Lucene 索引和搜索的原子单位，它包含一个或多个域，是域的容器：基于 JSON 格式表示，每个域的组成部分(一个名字，一个或多个值，拥有多个值得域通常称为多值域)。可以被索引的基础数据单位 映射(mapping) # 原始内容存储为文档之前需要事先进行分析，如切词、过滤掉某些词等；映射就是用于定义此分析机制该如何实现；此外，ES 为映射提供了诸如将域中的内容排序等功能 集群组件 Cluster # ES 的集群标识为集群名称：默认为“elasticsearch”，各个节点就是靠此名字来决定加入到哪个集群中。一个节点只能属于一个集群 Node # 运行了单个 ES 实例的主机即为节点，用于存储数据、参与集群索引及搜索操作。节点的标识靠节点名 shard # 分片，每个索引都会切割为多个分片，每一个 shard 都是一个完整且独立的索引，可以理解为分词，把一段文字分成多个单词。shard 有两种类型 primary shard # 主 shard，ES 默认可为其将其分割为 5 个主 shard，用户也可自定义。 replica shard # 备份 shard，每个主 shard 的备份，用户数据冗余以及查询时的负载均衡,shard 的备份数量可以自定义 数据获取组件： solr,nutch,grub,Apeture 等， 基本组件的形象比喻：</description></item><item><title>索引为什么能提高查询性能....</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD/</guid><description>原文：公众号 - 小林coding
前言 昨天，有个女孩子问我提高数据库查询性能有什么立竿见影的好方法？
这简直是一道送分题，我自豪且略带鄙夷的说，当然是加「索引」了。
她又不紧不慢的问，索引为什么就能提高查询性能。
这还用问，索引就像一本书的目录，用目录查当然很快。
她失望地摇了摇头，你说的只是一个类比，可为什么通过目录就能提高查询速度呢。
唉，对啊，通过书目可以快速查询，这只是一个现象，真正原因到底是什么呢。
那女孩看着诧异且表情僵硬的我，满意而又意味深长的笑笑：原来你这个男程序员也不会，看来我还得靠自己研究了。
哎，熬夜又要憔悴了我这该死的美貌。
来自同行的羞辱，是可忍孰不可忍？！
于是，我踏上了数据库索引学习的不归路，原来数据库索引使用了一种叫 B+ 树的古老数据结构，当然也有 Hash 等类型，暂且不说，可 B+ 树 这是个什么妖魔鬼怪呢？
下面就来浅尝辄止的扒一扒树的前世今生。
正文 二叉树 由 n（ n &amp;gt; 0）个有限节点组成一个具有层次关系的集合，看起来就像一个倒挂的树，因此称这样的数据结构为树。
一个节点的子节点个数叫做度，通俗的讲就是树叉的个数。树中最大的度叫做树的度，也叫做阶。一个 2 阶树最多有 2 个子节点即最多有 2 叉，因此这样的树称为二叉树，二叉树是树家族中最简单的树。
两个叉的树就是二叉树，可这除了用来按一定结构存放数据外，跟查询性能好像也没关系，不会又是一个没用的噱头吧。
二分查找 听说二叉树的原始威力来源于一种叫做二分查找的算法。
相传在鹦鹉的原始社会，存在着森严的等级制度，每只鸟必须按高矮顺序分出等级和尊卑。
那么问题来了，如下图，怎样才能找出最高、最矮、中等高的那些鹦鹉呢、以及指定高度的那只呢?
第一种方法: 扫描法
一个一个依次测量，完毕后所有的问题都迎刃而解。
这种一个一个依次全部测量的方法叫做扫描，他的缺点很明显，最高和最矮，需要全部测量完毕才能知晓。
而对于指定高度，最好的情况是第一次就找到；最坏的情况是最后一次才找到，时间复杂度为 n，也就是说从 13 个鹦鹉中找到指定身高的那只，最坏的情况是查 13 次。
第二种方法：二分法
13 个鹦鹉全部听令，按从矮到高列队，向左看齐，报数。
报数字 1 的就是最矮的，报数字 13 的就是最高的，报数字 7 的就是中等身高的那只。
最好和最坏的情况都是一次找到。而查询性能一下子提高 13 倍，我的个乖乖，无论多个只鹦鹉，时间复杂度都是 1，好可怕。
问题：我不服，你这是偷换概念，有本事对比一个查找指定高度鹦鹉的性能。
因为鹦鹉们已经按高矮排好了队，所以指定高度的鹦鹉，要么是站中间那个只，要么就是在它的左边或右边的那群里。
如果是中间那个，一次就找到，如果不是只需要从中间左边或右边那一半中找，再在这一半中找中间那只，对比身高。
以此类推，每次都把查询的范围减半，时间复杂度log2(n)。
那么 log2(13) 就是 4，最坏的情况也才 4 次，时间复杂度确实不是 1 了，但好像也不糟，简化如下：</description></item></channel></rss>