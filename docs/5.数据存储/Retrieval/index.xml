<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – Retrieval</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/</link><description>Recent content in Retrieval on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Hash</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Hashing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Hashing/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Category:Hashing">Wiki 类别，Hashing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Hash_function">Wiki，Hash function&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Hashing 是一种实现数据 Retrieval(检索) 的算法，有多种 Hashing 算法，比如&lt;/p>
&lt;ul>
&lt;li>&lt;a href="docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Consistent%20hashing.md">Consistent hashing&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="hash-table">Hash table&lt;a class="td-heading-self-link" href="#hash-table" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Hash_table">Wiki，Hash table&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Hash table(哈希表)&lt;/strong> 也称为 hash map(哈希映射) 或 hash set(哈希集)，是一种实现关联数组的数据结构，也称为 dictionary(字典)，它是一种将键映射到值的抽象数据类型。哈希表使用哈希函数来计算索引（也称为哈希码）到桶或槽数组中，从中可以找到所需的值。在查找过程中，对键进行哈希处理，生成的哈希值指示相应值的存储位置。&lt;/p>
&lt;p>理想情况下，哈希函数会将每个键分配给一个唯一的存储桶，但大多数哈希表设计都采用不完善的哈希函数，这可能会导致哈希冲突，即哈希函数为多个键生成相同的索引。此类冲突通常以某种方式进行调节&lt;/p></description></item><item><title>Docs: 一致性哈希算法 consistent hashing</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Consistent-hashing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/Consistent-hashing/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Consistent_hashing">Wiki，Consistent hashing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.zsythink.net/archives/1182">朱双印 blog，白话解析：一致性哈希算法 consistent hashing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。&lt;/p>
&lt;h1 id="场景描述">场景描述&lt;a class="td-heading-self-link" href="#%e5%9c%ba%e6%99%af%e6%8f%8f%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0 号、1 号、2 号，现在，有 3 万张图片需要缓存，我们希望这些图片被均匀的缓存到这 3 台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存 1 万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将 3 万张图片平均的缓存在 3 台服务器上，可以满足我们的要求吗？可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历 3 台缓存服务器，从 3 万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接收的，也就失去了缓存的意义，缓存的目的就是 提高速度，改善用户体验，减轻后端服务器压力，如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？原始的做法是对缓存项的键进行哈希，将 hash 后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的 key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。&lt;/p>
&lt;p>&lt;strong>hash（图片名称）% N&lt;/strong>&lt;/p>
&lt;p>因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有 3 台服务器，使用哈希后的结果对 3 求余，那么余数一定是 0、1 或者 2，没错，正好与我们之前的服务器编号相同，如果求余的结果为 0， 我们就把当前图片名称对应的图片缓存在 0 号服务器上，如果余数为 1，就把当前图片名对应的图片缓存在 1 号服务器上，如果余数为 2，同理，那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将 3 万张图片随机的分布到 3 台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为 HASH 算法或者取模算法，取模算法的过程可以用下图表示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202555-a60e8446-0c60-472c-84a4-e164dc32c2eb.jpeg" alt="">&lt;/p>
&lt;p>但是，使用上述 HASH 算法进行缓存时，会出现一些缺陷，试想一下，如果 3 台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两台缓存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由 3 台变成了 4 台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来 3 台服务器时所在的服务器编号不同，因为除数由 3 变为了 4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设 3 台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从 3 台变为 2 台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了 缓存雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述 HASH 算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。&lt;/p>
&lt;p>我们来回顾一下使用上述算法会出现的问题：&lt;/p>
&lt;p>问题 1：当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。&lt;/p>
&lt;p>问题 2：当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，怎样才能尽量减少受影响的缓存呢？&lt;/p>
&lt;p>其实，上面两个问题是一个问题，那么，一致性哈希算法能够解决上述问题吗？我们现在就来了解一下一致性哈希算法。&lt;/p>
&lt;h1 id="一致性哈希算法的基本概念">一致性哈希算法的基本概念&lt;a class="td-heading-self-link" href="#%e4%b8%80%e8%87%b4%e6%80%a7%e5%93%88%e5%b8%8c%e7%ae%97%e6%b3%95%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对 $2^{32}$ 取模，什么意思呢？我们慢慢聊。&lt;/p>
&lt;p>首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 $2^{32}$ 个点组成的圆，示意图如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202586-97ba41e2-bd8d-4bcb-ab83-7b07b1876d76.jpeg" alt="">&lt;/p>
&lt;p>圆环的正上方的点代表 0，0 点右侧的第一个点代表 1，以此类推，2、3、4、5、6……直到 2^32-1,也就是说 0 点左侧的第一个点代表 2^32-1&lt;/p>
&lt;p>我们把这个由 2 的 32 次方个点组成的圆环称为 hash 环。&lt;/p>
&lt;p>那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有 3 台缓存服务器，服务器 A、服务器 B、服务器 C，那么，在生产环境中，这三台服务器肯定有自己的 IP 地址，我们使用它们各自的 IP 地址进行哈希计算，使用哈希后的结果对 2^32 取模，可以使用如下公式示意。&lt;/p>
&lt;p>&lt;strong>hash（服务器 A 的 IP 地址） % 2^32&lt;/strong>&lt;/p>
&lt;p>通过上述公式算出的结果一定是一个 0 到 2^32-1 之间的一个整数，我们就用算出的这个整数，代表服务器 A，既然这个整数肯定处于 0 到 2^32-1 之间，那么，上图中的 hash 环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器 A，那么，服务器 A 就可以映射到这个环上，用下图示意：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202585-14af29fb-df19-4b5d-a9a8-54b069ac66cb.jpeg" alt="">&lt;/p>
&lt;p>同理，服务器 B 与服务器 C 也可以通过相同的方法映射到上图中的 hash 环中：&lt;/p>
&lt;p>&lt;strong>hash（服务器 B 的 IP 地址） % 2^32&lt;/strong>&lt;/p>
&lt;p>&lt;strong>hash（服务器 C 的 IP 地址） % 2^32&lt;/strong>&lt;/p>
&lt;p>通过上述方法，可以将服务器 B 与服务器 C 映射到上图中的 hash 环上，示意图如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202603-15e04eaa-c965-4854-84fa-b342412db159.jpeg" alt="">&lt;/p>
&lt;p>假设 3 台服务器映射到 hash 环上以后如上图所示（当然，这是理想的情况，我们慢慢聊）。&lt;/p>
&lt;p>好了，到目前为止，我们已经把缓存服务器与 hash 环联系在了一起，我们通过上述方法，把缓存服务器映射到了 hash 环上，那么使用同样的方法，我们也可以将需要缓存的对象映射到 hash 环上。&lt;/p>
&lt;p>假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的 key，那么我们使用如下公式可以将图片映射到上图中的 hash 环上。&lt;/p>
&lt;p>&lt;strong>hash（图片名称） % 2^32&lt;/strong>&lt;/p>
&lt;p>映射后的示意图如下，下图中的橘黄色圆形表示图片：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202605-8c3c063d-9556-434e-b6fa-cbeed21e5a3c.jpeg" alt="">&lt;/p>
&lt;p>好了，现在服务器与图片都被映射到了 hash 环上，那么上图中的这个图片到底应该被缓存到哪一台服务器上呢？上图中的图片将会被缓存到服务器 A 上，为什么呢？因为从图片的位置开始，沿顺时针方向遇到的第一个服务器 就是 A 服务器，所以，上图中的图片将会被缓存到服务器 A 上，如下图所示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202623-dbd970a7-40c9-4f77-bf0a-af258f3bf6a5.jpeg" alt="">&lt;/p>
&lt;p>没错，一致性哈希算法就是通过这种方法，判断一个对象应该被缓存到哪台服务器上的，将缓存服务器与被缓存对象都映射到 hash 环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器，由于被缓存对象与服务器 hash 后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。/font&amp;gt;&lt;/p>
&lt;p>刚才的示例只使用了一张图片进行演示，假设有四张图片需要缓存，示意图如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202612-68280120-c095-4f3a-80fe-8f42cf7e71f8.jpeg" alt="">&lt;/p>
&lt;p>1 号、2 号图片将会被缓存到服务器 A 上，3 号图片将会被缓存到服务器 B 上，4 号图片将会被缓存到服务器 C 上。&lt;/p>
&lt;h1 id="一致性哈希算法的优点">一致性哈希算法的优点&lt;a class="td-heading-self-link" href="#%e4%b8%80%e8%87%b4%e6%80%a7%e5%93%88%e5%b8%8c%e7%ae%97%e6%b3%95%e7%9a%84%e4%bc%98%e7%82%b9" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>经过上述描述，我想兄弟你应该已经明白了一致性哈希算法的原理了，但是话说回来，一致性哈希算法能够解决之前出现的问题吗，我们说过，如果简单的对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，那么使用一致性哈希算法，能够避免这个问题吗？我们来模拟一遍，即可得到答案。&lt;/p>
&lt;p>假设，服务器 B 出现了故障，我们现在需要将服务器 B 移除，那么，我们将上图中的服务器 B 从 hash 环上移除即可，移除服务器 B 以后示意图如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202653-039f84ad-a042-419b-ae51-5e181853f80d.jpeg" alt="">&lt;/p>
&lt;p>在服务器 B 未移除时，图片 3 应该被缓存到服务器 B 中，可是当服务器 B 移除以后，按照之前描述的一致性哈希算法的规则，图片 3 应该被缓存到服务器 C 中，因为从图片 3 的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器 C，也就是说，如果服务器 B 出现故障被移除时，图片 3 的缓存位置会发生改变。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202621-d4a2c0fd-f096-4e13-9839-7f50dfe7b430.jpeg" alt="">&lt;/p>
&lt;p>但是，图片 4 仍然会被缓存到服务器 C 中，图片 1 与图片 2 仍然会被缓存到服务器 A 中，这与服务器 B 移除之前并没有任何区别，这就是一致性哈希算法的优点，如果使用之前的 hash 算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分缓存会失效，前端的缓存仍然能分担整个系统的压力，而不至于所有压力都在同一时间集中到后端服务器上。&lt;/p>
&lt;p>这就是一致性哈希算法所体现出的优点。&lt;/p>
&lt;hr>
&lt;h1 id="hash-环的偏斜">hash 环的偏斜&lt;a class="td-heading-self-link" href="#hash-%e7%8e%af%e7%9a%84%e5%81%8f%e6%96%9c" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>在介绍一致性哈希的概念时，我们理想化的将 3 台服务器均匀的映射到了 hash 环上，如下图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202636-2bc6be5c-d885-49aa-a228-3c7a6d13efe6.jpeg" alt="">&lt;/p>
&lt;p>但是，理想很丰满，现实很骨感，我们想象的与实际情况往往不一样。&lt;/p>
&lt;p>在实际的映射中，服务器可能会被映射成如下模样。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202670-68b20340-9bdd-4ef1-955a-4bfa4a413343.jpeg" alt="">&lt;/p>
&lt;p>聪明如你一定想到了，如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上，如下图所示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202641-e7413fb2-fab4-46fa-9bf7-ca879555b484.jpeg" alt="">&lt;/p>
&lt;p>上图中，1 号、2 号、3 号、4 号、6 号图片均被缓存在了服务器 A 上，只有 5 号图片被缓存在了服务器 B 上，服务器 C 上甚至没有缓存任何图片，如果出现上图中的情况，A、B、C 三台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器 A 出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引起系统的崩溃，上图中的情况则被称之为 hash 环的偏斜，那么，我们应该怎样防止 hash 环的偏斜呢？一致性 hash 算法中使用 “虚拟节点” 解决了这个问题，我们继续聊。&lt;/p>
&lt;h1 id="虚拟节点">虚拟节点&lt;a class="td-heading-self-link" href="#%e8%99%9a%e6%8b%9f%e8%8a%82%e7%82%b9" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>话接上文，由于我们只有 3 台服务器，当我们把服务器映射到 hash 环上的时候，很有可能出现 hash 环偏斜的情况，当 hash 环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，聪明如你一定已经想到了，如果想要均衡的将缓存分布到 3 台服务器上，最好能让这 3 台服务器尽量多的、均匀的出现在 hash 环上，但是，真实的服务器资源只有 3 台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为”虚拟节点”。加入虚拟节点以后的 hash 环如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202654-66fcca21-b967-41c3-8ebf-aa501e1f51f9.jpeg" alt="">&lt;/p>
&lt;p>“虚拟节点”是”实际节点”（实际的物理服务器）在 hash 环上的复制品,一个实际节点可以对应多个虚拟节点。&lt;/p>
&lt;p>从上图可以看出，A、B、C 三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上图中，1 号、3 号图片被缓存在服务器 A 中，5 号、4 号图片被缓存在服务器 B 中，6 号、2 号图片被缓存在服务器 C 中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小 hash 环偏斜所带来的影响，虚拟节点越多，hash 环上的节点就越多，缓存被均匀分布的概率就越大。&lt;/p></description></item><item><title>Docs: 搜索引擎简介</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</guid><description>
&lt;h1 id="基础概念">基础概念&lt;a class="td-heading-self-link" href="#%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>搜索引擎&lt;/p>
&lt;p>索引链+搜索组件，用户给搜索组件提供关键字，然后进行搜索&lt;/p>
&lt;ul>
&lt;li>索引链：&lt;/li>
&lt;li>原始内容-获取-创建文档-文档分析-创建索引-索引&lt;/li>
&lt;li>搜索组件&lt;/li>
&lt;li>UI-构建查询-运行查询-读取结果-UI&lt;/li>
&lt;/ul>
&lt;p>开源索引链：Lucene，自己不获取内容，不提供前段界面，ElasticSearch 基于 Lucene&lt;/p>
&lt;h2 id="lucene">Lucene&lt;a class="td-heading-self-link" href="#lucene" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h2 id="elasticsearch简称-es">ElasticSearch：简称 ES&lt;a class="td-heading-self-link" href="#elasticsearch%e7%ae%80%e7%a7%b0-es" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>ES 是基于 Lucene 实现的开源、分布式、Restful 的全文本搜索引擎；并且还是一个分布式实时存储文档，其中每个文档的每个 field 均是被索引的数据，且可被搜索；也是一个实时分析功能的分布式搜索引擎，能够扩展至数以百计的节点实时处理 PB 级的数据&lt;/p>
&lt;h3 id="组件">组件&lt;a class="td-heading-self-link" href="#%e7%bb%84%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>基本组件
&lt;ul>
&lt;li>索引(index) # 文档容器。含有相同属性的文档集合，索引名必须使用小写字母&lt;/li>
&lt;li>类型(type) # 索引内部的逻辑分区。一个索引内部可以定义一个或多个类型，文档必须属于一个类型&lt;/li>
&lt;li>文档(document) # 文档是 Lucene 索引和搜索的原子单位，它包含一个或多个域，是域的容器：基于 JSON 格式表示，每个域的组成部分(一个名字，一个或多个值，拥有多个值得域通常称为多值域)。可以被索引的基础数据单位&lt;/li>
&lt;li>映射(mapping) # 原始内容存储为文档之前需要事先进行分析，如切词、过滤掉某些词等；映射就是用于定义此分析机制该如何实现；此外，ES 为映射提供了诸如将域中的内容排序等功能&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>集群组件
&lt;ul>
&lt;li>Cluster # ES 的集群标识为集群名称：默认为“elasticsearch”，各个节点就是靠此名字来决定加入到哪个集群中。一个节点只能属于一个集群&lt;/li>
&lt;li>Node # 运行了单个 ES 实例的主机即为节点，用于存储数据、参与集群索引及搜索操作。节点的标识靠节点名&lt;/li>
&lt;li>shard # 分片，每个索引都会切割为多个分片，每一个 shard 都是一个完整且独立的索引，可以理解为分词，把一段文字分成多个单词。shard 有两种类型&lt;/li>
&lt;li>primary shard # 主 shard，ES 默认可为其将其分割为 5 个主 shard，用户也可自定义。&lt;/li>
&lt;li>replica shard # 备份 shard，每个主 shard 的备份，用户数据冗余以及查询时的负载均衡,shard 的备份数量可以自定义&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>数据获取组件：
&lt;ul>
&lt;li>solr,nutch,grub,Apeture 等，&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>基本组件的形象比喻：&lt;/p>
&lt;ul>
&lt;li>百货大楼里有各式各样的商品，例如书籍、笔、水果等。书籍可以根据内容划分成不同种类，如科技类、教育类、悬疑推理等。悬疑推理类的小说中比较有名气的有《福尔摩斯探案集》、《白夜行》等。&lt;/li>
&lt;li>百货大楼 &amp;ndash;&amp;gt; ElasticSearch 数据库&lt;/li>
&lt;li>书籍 &amp;ndash;&amp;gt; 索引&lt;/li>
&lt;li>悬疑推理 &amp;ndash;&amp;gt; 类型&lt;/li>
&lt;li>白夜行 &amp;ndash;&amp;gt; 文档&lt;/li>
&lt;/ul>
&lt;h3 id="es-的工作过程">ES 的工作过程&lt;a class="td-heading-self-link" href="#es-%e7%9a%84%e5%b7%a5%e4%bd%9c%e8%bf%87%e7%a8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>启动时，通过多播(默认)或单播方式在 9300/tcp 查找同一集群中的其它节点，并与之建立通信；建立后，通过集群名称来进行判定&lt;/li>
&lt;li>集群不用单独部署管理节点，所有节点会选举出一个主节点负责管理一整个集群状态，以及在集群范围内决定各 shards 的分布方式；从用户角度看，每个节点均可接收并响应用户的各类请求。&lt;/li>
&lt;li>集群有三种状态，green、red、yellow，当状态不可用时会自动启动修复模式&lt;/li>
&lt;/ul></description></item><item><title>Docs: 索引为什么能提高查询性能....</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Retrieval/%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD/</guid><description>
&lt;p>原文：公众号 - 小林coding&lt;/p>
&lt;h2 id="前言">前言&lt;a class="td-heading-self-link" href="#%e5%89%8d%e8%a8%80" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>昨天，有个女孩子问我提高数据库查询性能有什么立竿见影的好方法？&lt;/p>
&lt;p>这简直是一道送分题，我自豪且略带鄙夷的说，当然是加「索引」了。&lt;/p>
&lt;p>她又不紧不慢的问，索引为什么就能提高查询性能。&lt;/p>
&lt;p>这还用问，索引就像一本书的目录，用目录查当然很快。&lt;/p>
&lt;p>她失望地摇了摇头，你说的只是一个类比，可&lt;strong>为什么通过目录就能提高查询速度呢&lt;/strong>。&lt;/p>
&lt;p>唉，对啊，通过书目可以快速查询，这只是一个现象，真正原因到底是什么呢。&lt;/p>
&lt;p>那女孩看着诧异且表情僵硬的我，满意而又意味深长的笑笑：原来你这个男程序员也不会，看来我还得靠自己研究了。&lt;/p>
&lt;p>哎，熬夜又要憔悴了我这该死的美貌。&lt;/p>
&lt;p>来自同行的羞辱，是可忍孰不可忍？！&lt;/p>
&lt;p>于是，我踏上了数据库索引学习的不归路，原来数据库索引使用了一种叫 B+ 树的古老数据结构，当然也有 Hash 等类型，暂且不说，可 B+ 树 这是个什么妖魔鬼怪呢？&lt;/p>
&lt;p>下面就来浅尝辄止的扒一扒树的前世今生。&lt;/p>
&lt;hr>
&lt;h2 id="正文">正文&lt;a class="td-heading-self-link" href="#%e6%ad%a3%e6%96%87" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="二叉树">二叉树&lt;a class="td-heading-self-link" href="#%e4%ba%8c%e5%8f%89%e6%a0%91" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>由 n（ n &amp;gt; 0）个有限节点组成一个具有层次关系的集合，看起来就像一个倒挂的树，因此称这样的数据结构为树。&lt;/p>
&lt;p>一个节点的子节点个数叫做度，通俗的讲就是树叉的个数。树中最大的度叫做树的度，也叫做阶。一个 2 阶树最多有 2 个子节点即最多有 2 叉，因此这样的树称为&lt;strong>二叉树&lt;/strong>，二叉树是树家族中最简单的树。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791571-a448ab7a-a724-4b7a-854d-b833da5d831e.jpeg" alt="">&lt;/p>
&lt;p>两个叉的树就是二叉树，可这除了用来按一定结构存放数据外，跟查询性能好像也没关系，不会又是一个没用的噱头吧。&lt;/p>
&lt;hr>
&lt;h3 id="二分查找">二分查找&lt;a class="td-heading-self-link" href="#%e4%ba%8c%e5%88%86%e6%9f%a5%e6%89%be" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>听说二叉树的原始威力来源于一种叫做二分查找的算法。&lt;/p>
&lt;p>相传在鹦鹉的原始社会，存在着森严的等级制度，每只鸟必须按高矮顺序分出等级和尊卑。&lt;/p>
&lt;p>那么问题来了，如下图，怎样才能找出&lt;strong>最高&lt;/strong>、&lt;strong>最矮&lt;/strong>、&lt;strong>中等高&lt;/strong>的那些鹦鹉呢、以及&lt;strong>指定高度&lt;/strong>的那只呢?&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791613-b27de62c-39c8-4d66-9060-039a36d50607.jpeg" alt="">&lt;/p>
&lt;p>第一种方法: 扫描法&lt;/p>
&lt;p>一个一个依次测量，完毕后所有的问题都迎刃而解。&lt;/p>
&lt;p>这种一个一个依次全部测量的方法叫做扫描，他的缺点很明显，最高和最矮，需要全部测量完毕才能知晓。&lt;/p>
&lt;p>而对于指定高度，最好的情况是第一次就找到；最坏的情况是最后一次才找到，时间复杂度为 n，也就是说从 13 个鹦鹉中找到指定身高的那只，最坏的情况是查 13 次。&lt;/p>
&lt;p>第二种方法：二分法&lt;/p>
&lt;p>13 个鹦鹉全部听令，按从矮到高列队，向左看齐，报数。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791617-908b3fbe-74b4-4709-afa7-2b11d4052a5b.jpeg" alt="">&lt;/p>
&lt;p>报数字 1 的就是最矮的，报数字 13 的就是最高的，报数字 7 的就是中等身高的那只。&lt;/p>
&lt;p>最好和最坏的情况都是一次找到。而查询性能一下子提高 13 倍，我的个乖乖，无论多个只鹦鹉，时间复杂度都是 1，好可怕。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>问题&lt;/strong>：我不服，你这是偷换概念，有本事对比一个查找指定高度鹦鹉的性能。&lt;/p>
&lt;/blockquote>
&lt;p>因为鹦鹉们已经按高矮排好了队，所以指定高度的鹦鹉，要么是站中间那个只，要么就是在它的左边或右边的那群里。&lt;/p>
&lt;p>如果是中间那个，一次就找到，如果不是只需要从中间左边或右边那一半中找，再在这一半中找中间那只，对比身高。&lt;/p>
&lt;p>以此类推，每次都把查询的范围减半，时间复杂度&lt;code>log2(n)&lt;/code>。&lt;/p>
&lt;p>那么 log2(13) 就是 4，最坏的情况也才 4 次，时间复杂度确实不是 1 了，但好像也不糟，简化如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791626-ac4b3212-7eb2-418f-97b4-c8ca4feb3081.jpeg" alt="">&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>问题&lt;/strong>：如果按高矮排队，仍然需要一个一个比较，跟扫描有什么区别，那还不如直接扫描呢？&lt;/p>
&lt;/blockquote>
&lt;p>事实确实如此,单纯的一次查询，先排序，再二分查找，不见得比扫描快，甚至还不如。&lt;/p>
&lt;p>但是，在数据的世界，大部分数据一生会被查询无数次，如果只在数据降生的时候排一次序，往后余生，是不是就可以直接用二分查找，这似乎就是传说的读多写少，以及对应的复用。&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>查找快&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>缺点&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>必须有序，需要提前排序&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每次查找都需要不断计算中间位置&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="二分查找树">二分查找树&lt;a class="td-heading-self-link" href="#%e4%ba%8c%e5%88%86%e6%9f%a5%e6%89%be%e6%a0%91" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果一组数据不会或不常变更，那么他们的位置也基本不变。可是每次查询都需要重新计算中间位置是一种浪费，而浪费可耻。&lt;/p>
&lt;p>我们能不能把所有中间节点组织起来，每次使用时，直接取中间节点?&lt;/p>
&lt;p>请看下图，找到所有单次二分查找的中间节点，把他们连起来，并用手提起最中间的那个节点，就是一棵二分查找树。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791615-d57285a4-ec89-4930-ad46-489369e3d498.gif" alt="">&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：二分查找树就是通过数据结构的方式实现了二分查找算法，通过存储中间节点的数据，弥补了二分查找每次都要计算中间位置的缺点。&lt;/p>
&lt;hr>
&lt;h3 id="平衡二叉树">平衡二叉树:&lt;a class="td-heading-self-link" href="#%e5%b9%b3%e8%a1%a1%e4%ba%8c%e5%8f%89%e6%a0%91" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果二分查找树不断进行修改，比如删除某些节点，经过一段时间后，最早那个中间节点的数据（根），很可能就不在中间了。&lt;/p>
&lt;p>中间位置就像一个天平的支点，如果他不在中间了，那么整个天平就会失衡，失衡的世界就会坍塌成不伦不类的瘸树，甚至是降维成一个链表或者数组。&lt;/p>
&lt;p>二分查找算法的关键在于有序和中间节点，而二分查找树的关键是中间节点的维护，如果维护的节点已经不在中间了，那么它就失去了意义。&lt;/p>
&lt;p>所以必须保证「二分查找树」是一个正确的树，一个根节点在中心的树，一个左右子树层级（高度）基本相等（高度相差不超过 1）的树，一个平衡的树。&lt;/p>
&lt;p>平衡二叉树中最常见的就是红黑树：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791611-76893fa7-eb50-41db-8478-5fc241a07fbe.jpeg" alt="">&lt;/p>
&lt;p>红黑树规定了一系列节点颜色规则，以及对应的左旋和右旋操作来保证颜色规则，从而达到树的平衡性。&lt;/p>
&lt;p>看到这花里胡哨的颜色以及复杂的规则，让人第一眼就望而却步，但所有的这些，也不过是为了保证二叉树的平衡性，由于维持平衡的操作太过麻烦，无法用一句话简单概括，只好用一堆人鬼难分的规则和步骤来实现，只要按着这些步骤就一定能实现二叉树的平衡。&lt;/p>
&lt;p>&lt;em>平衡二叉树 = 二分查找树 + 平衡（左右高度相差不超过 1 ）&lt;/em>&lt;/p>
&lt;p>平衡二叉树并未提高二分查找树的性能，它只是保正树不会被二向箔（多次增删改）打击降维成链表或不对称的残缺树，永远维持平衡。&lt;/p>
&lt;p>另外，不仅仅是二叉树，其他种类的树，也是需要有序和平衡，才能发挥最大的威力。&lt;/p>
&lt;hr>
&lt;h3 id="多叉树之-b-tree">多叉树之 B-tree&lt;a class="td-heading-self-link" href="#%e5%a4%9a%e5%8f%89%e6%a0%91%e4%b9%8b-b-tree" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>两个叉的树就能折半查询，理论可以提高一倍性能，那么多个叉是不是能提高更多倍性能？&lt;/p>
&lt;p>如下图的 3 阶（叉）树（所有数据仅用于演示，非真实分布）&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791626-104c0a58-a5d7-491d-8589-0f512b873106.jpeg" alt="">&lt;/p>
&lt;p>每个节点维护两个数据，并指向最多 3 个子节点。如图 3 个子节点的数据分别为：小于 17， 17 ~ 35 ，大于 35。&lt;/p>
&lt;p>假设，从上图中查找 10 这个数，步骤如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>找到根节点，对比 10 与 17 和 35 的大小，发现 10 &amp;lt; 17 在左子节点，也就是第 2 层节点；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从根节点的指针，找到左子节点，对比 10 与 8 和 12 的大小，发现 8 &amp;lt; 10 &amp;lt; 12，数据在当前节点的中间子节点，也就是第 3 层节点；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过上步节点的指针，找到中间子节点（第 3 层节点），对比 10 与 9 和 10 的大小，发现 9 &amp;lt; 10 == 10，因此找到当前节点的第二数即为结果。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>加上忽略的 12 个数据，从 26 个数据中查找一个数字 10，仅仅用了 log3(26)≈ 3 次，而如果用平衡二叉树,则需要 log2(26)≈ 5 次，事实证明，多叉树确实可以再次提高查找性能。&lt;/p>
&lt;p>多叉树是在二分查找树的基础上，增加单个节点的数据存储数量，同时增加了树的子节点数，一次计算可以把查找范围缩小更多。&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：二叉平衡树的基础上，使加载一次节点，可以加载更多路径数据，同时把查询范围缩减到更小。&lt;/p>
&lt;p>&lt;strong>复杂节点&lt;/strong>:&lt;/p>
&lt;p>至此，我们列举的数据都是孤零零的单个数字。试想，你手里已经有一个数据 10，为什么还要费力吧唧的再从一堆数据中找到这个 10，自己找自己？这不是有病吗？&lt;/p>
&lt;p>单个数字只能活在演示中，现实的世界要复杂的多，我们来看一个接近真实场景的案例。&lt;/p>
&lt;p>现有一个以年龄为索引的 3 阶树，存储了一批用户信息，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791623-f94d684d-bf9f-404d-90ea-216ed91bedb2.jpeg" alt="">&lt;/p>
&lt;p>数字为用户的年龄，其它为与树排序查找无关的业务数据，像这种索引数据与树排序查找无关的业务一起维护在节点的平衡多叉（阶）树称为 B- 树（ B 树）。&lt;/p>
&lt;p>&lt;strong>缺点&lt;/strong>：业务数据的大小可能远远超过了索引数据的大小，每次为了查找对比计算，需要把数据加载到内存以及 CPU 高速缓存中时，都要把索引数据和无关的业务数据全部查出来。本来一次就可以把所有索引数据加载进来，现在却要多次才能加载完。如果所对比的节点不是所查的数据，那么这些加载进内存的业务数据就毫无用处，全部抛弃。&lt;/p>
&lt;hr>
&lt;h3 id="磁盘-io">磁盘 I/O&lt;a class="td-heading-self-link" href="#%e7%a3%81%e7%9b%98-io" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>计算机的功能主要为：计算、存储和网络。而用于计算的数据以及计算后的结果很大一部分都需要存储起来，以备后续再次使用。向磁盘中存储和读取的过程叫磁盘 I/O。磁盘的读取方式和速度会严重影响到整个业务的计算性能。&lt;/p>
&lt;p>下面我们简单了解一下磁盘是如何工作的。&lt;/p>
&lt;p>磁盘大概长这个样子：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791643-edc82136-2780-4205-828b-07b66aad27ad.jpeg" alt="">&lt;/p>
&lt;p>磁盘主要由磁盘盘片、传动手臂、读写磁头和马达组成。&lt;/p>
&lt;p>为了存储容量,主轴像穿糖葫芦一样把多个磁盘片组成一个阵列。通过马达驱动主轴转动以及传动手臂移动，使读写磁头在磁盘片上读写数据。大概如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791647-8aba4e01-69c5-44e0-8f71-698da35373ce.jpeg" alt="">&lt;/p>
&lt;p>磁盘片由很多半径不等的同心圆组成，这些圆被称为磁道，数据就是写在这些磁道上。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791655-573a6baf-f037-408b-b066-e0e77c08e680.jpeg" alt="">&lt;/p>
&lt;p>每个磁道又划分成块称为扇区。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791667-89cc8abb-23e3-454d-9b8c-bab61790b0e9.jpeg" alt="">&lt;/p>
&lt;p>如果磁盘是一记事本，那么一张磁盘片就是本子的一页纸，而主轴就是本子的装订线；磁道就是纸页的行，而扇区可以看作是很宽的列。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791661-9c41f891-0818-4b44-b26c-927a33b5c7e7.jpeg" alt="">&lt;/p>
&lt;p>如果在磁盘中存储一首诗,想象中大概这个样子。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791698-850f52dd-d6cf-4479-9de1-6c6185b62ee9.jpeg" alt="">&lt;/p>
&lt;p>磁盘的读 I/O 操作,需要找到数据所在的磁盘片，以及对应的磁道和扇区。这些操作类似于从一本书中找到数据所在的页，行，列。&lt;/p>
&lt;p>因为每个磁盘片都对应一个磁头，所以性能的关键就在于找行和列，即寻道和磁盘旋转。寻道即通过磁头找到数据所在的磁道，相当于换行到数据所在行。由于磁头只能水平移动，即只能换行寻道，无法在指定磁道上移动，因此需要磁盘高速旋转移动到指定扇区，类似写春联时，笔不动，纸动。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791666-feb53a3d-7950-46f5-8ff8-be9ab19e1d6c.gif" alt="">&lt;/p>
&lt;p>综上所述，磁盘的读写是通过机械运动来定位数据所在位置，而 cpu 是通过电信号进行数字运算。粗略的认为，机械查询数据，与光速处理数据的性能完全不是在一个量级，总之一句话就是&lt;strong>磁盘处理太慢太慢了&lt;/strong>。&lt;/p>
&lt;p>虽然磁盘处理数据太慢了，但是它是目前相对廉价且稳定的存储设备，所以又不能舍弃不用，但大致可以通过以下方法进行优化。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>尽量减少 I/O 次数，比如可以使用缓存；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每次 I/O 尽量获取更多的数据；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每次 I/O 尽量获取有用的数据，当然相应的也间接减少总 I/O 次数；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="多叉树之-btree">多叉树之 B+tree&lt;a class="td-heading-self-link" href="#%e5%a4%9a%e5%8f%89%e6%a0%91%e4%b9%8b-btree" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>做为数据库的索引，无论用什么样的数据结构维护，这些数据最终都会存储到磁盘中。&lt;/p>
&lt;p>鉴于磁盘 I/O 的性能问题，以及每次 I/O 获取数据量上限所限，提高索引本身 I/O 的方法最好是，减少 I/O 次数和每次获取有用的数据。&lt;/p>
&lt;p>B-tree 已经大大改进了树家族的性能，它把多个数据集中存储在一个节点中，本身就可能减少了 I/O 次数或者寻道次数。&lt;/p>
&lt;p>但是仍然有一个致命的缺陷，那就是它的索引数据与业务绑定在一块，而业务数据的大小很有可能远远超过了索引数据，这会大大减小一次 I/O 有用数据的获取，间接的增加 I/O 次数去获取有用的索引数据。&lt;/p>
&lt;p>因为业务数据才是我们查询最终的目的，但是它又是在「二分」查找中途过程无用的数据，因此，如果只把业务数据存储在最终查询到的那个节点是不是就可以了？&lt;/p>
&lt;p>理想很丰满，现实很骨瘦如柴，谁知道哪个节点就是最终要查询的节点呢？&lt;/p>
&lt;p>B+tree 横空出世，&lt;strong>B+ 树就是为了拆分索引数据与业务数据的平衡多叉树&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791690-25dbb78a-2067-404c-82b4-89d75ba4dd21.jpeg" alt="">&lt;/p>
&lt;p>B+ 树中，非叶子节点只保存索引数据，叶子节点保存索引数据与业务数据。这样即保证了叶子节点的简约干净，数据量大大减小，又保证了最终能查到对应的业务数。既提高了单次 I/O 数据的有效性，又减少了 I/O 次数，还实现了业务。&lt;/p>
&lt;p>但是，在数据中索引与数据是分离的，不像示例那样的？&lt;/p>
&lt;p>如图：我们只需要把真实的业务数据，换成数据所在地址就可以了，此时，业务数据所在的地址在 B+ 树中充当业务数据。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wl73g8/1616132791672-c0127e82-a3f6-40ed-8c46-b068661de9ae.jpeg" alt="">&lt;/p>
&lt;hr>
&lt;h3 id="总结">总结&lt;a class="td-heading-self-link" href="#%e6%80%bb%e7%bb%93" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>数据存储在磁盘（ SSD 跟 CPU 性能也不在一个量级），而磁盘处理数据很慢；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>提高磁盘性能主要通过减少 I/O 次数，以及单次 I/O 有效数据量；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>索引通过多阶（一个节点保存多个数据，指向多个子节点）使树的结构更矮胖，从而减少 I/O 次数；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>索引通过 B+ 树，把业务数据与索引数据分离，来提高单次 I/O 有效数据量，从而减少 I/O 次数；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>索引通过树数据的有序和「二分查找」（多阶树可以假设为多分查找），大大缩小查询范围；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>索引针对的是单个字段或部分字段，数据量本身比一条记录的数据量要少的多，这样即使通过扫描的方式查询索引也比扫描数据库表本身快的多；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="知识扩展">知识扩展&lt;a class="td-heading-self-link" href="#%e7%9f%a5%e8%af%86%e6%89%a9%e5%b1%95" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>树的结构最大的优点就是查询性能高，因此所有需要提高查询性能的都可以考虑树。&lt;/p>
&lt;p>而现实中也确实有这样的例子，比如：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>HashMap 中的数据冲突时，链表转化成红黑树；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据库索引使用的 B+ 树；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>搜索引擎倒排索引使用的字典树；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>以上只是浅尝辄止、点到为止的描述了数据库使用 B+ 树索引为什么能提高查询性能原因及简单过程。&lt;/p>
&lt;p>并没有深入各种数据结构的细节，也未提及其它索引类型和索引的具体存储格式，目的仅仅是，为了让大家对索引有一个感性的认识。&lt;/p></description></item></channel></rss>