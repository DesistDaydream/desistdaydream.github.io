<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed Storage(分布式存储) on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/</link><description>Recent content in Distributed Storage(分布式存储) on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/index.xml" rel="self" type="application/rss+xml"/><item><title>分布式存储</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/</guid><description>概述 参考：
Wiki, GFS Distributed Storage(分布式存储) 最早可追溯到 Google File System(谷歌文件系统，GFS)，GFS 是由 Google 开发，旨在使用大型低廉的商用硬件集群提供高效、可靠的数据访问。在不可追溯的年月，Google 发布了这种文件系统的论文，其中就有公司，基于这个论文所描述的设计架构，使用 Java 实现了 HDFS，也就是红极一时的 Hadoop 所使用的文件系统。但是随着时代的发展，Hadoop 臃肿的设计，并不适用于云原生的环境而被逐渐淘汰了，但是基于 GFS 的设计理念，一直延续至今。
分布式存储首先需要解决的就是文件路由问题，在一个分布式存储中，数据分散到各个节点，客户端想要读取时，如何快速得找到数据所在位置呢？这就需要一个元数据服务器，来记录数据存放位置。但是数据存储位置的规则，一直是分布式存储的热门话题之一。一般来说，系统中所有角色（Clients、Servers）需要有一个统一的数据寻址算法 Locator，满足：Locator(ID) -&amp;gt; [Device_1, Device_2, Device_3, ...]
其中输入 ID 是数据的唯一标识符，输出 Device 列表是一系列存储设备（多设备冗余以达到多份数据保护或切分提高并发等效果）。早期的直观方案是维护一张全局的 Key-Value 表，任何角色操作数据时查询该表即可。显然，随着数据量的增多和集群规模的扩大，要在整个系统中维护这么一张不断扩大的表变得越来越困难。Ceph 的 CRUSH(Controlled Replication Under Scalable Hashing) 算法即为解决此问题而生，她仅需要一份描述集群物理架构的信息和预定义的规则（均包含在CRUSH map中），便可实现确定数据存储位置的功能。
分布式基础学习 详见：集群与分布式
所谓分布式，在这里，很狭义的指代以 Google 的三驾马车，GFS、Map/Reduce、BigTable 为框架核心的分布式存储和计算系统。通常如我一样初学的人，会以 Google 这几份经典的论文作为开端的。它们勾勒出了分布式存储和计算的一个基本蓝图，已可窥见其几分风韵，但终究还是由于缺少一些实现的代码和示例，色彩有些斑驳，缺少了点感性。幸好我们还有 Open Source，还有 Hadoop。Hadoop 是一个基于 Java 实现的，开源的，分布式存储和计算的项目。作为这个领域最富盛名的开源项目之一，它的使用者也是大牌如云，包括了 Yahoo，Amazon，Facebook 等等（好吧，还可能有校内，不过这真的没啥分量&amp;hellip;）。Hadoop 本身，实现的是分布式的文件系统 HDFS，和分布式的计算（Map/Reduce）框架，此外，它还不是一个人在战斗，Hadoop 包含一系列扩展项目，包括了分布式文件数据库 HBase（对应 Google 的 BigTable），分布式协同服务 ZooKeeper（对应 Google 的 Chubby），等等。。。</description></item></channel></rss>