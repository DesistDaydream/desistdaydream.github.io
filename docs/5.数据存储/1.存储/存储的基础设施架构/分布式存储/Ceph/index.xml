<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – Ceph</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/</link><description>Recent content in Ceph on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Ceph</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ceph.io/">官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/">官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Ceph_(software)">Wiki，Ceph&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.csdn.net/younger_china/article/details/73410727">https://blog.csdn.net/younger_china/article/details/73410727&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Ceph 是一个开源的分布式存储系统，可以提供 对象存储、快存储、文件存储 能力。是一个 Software Defined Storage(软件定义存储) 的代表性产品。&lt;/p>
&lt;p>一个 Ceph 存储集群至少需要 Ceph Monitor、Ceph Manager、Ceph OSD 这三个组件；如果要运行 Ceph 文件系统客户端，则也需要 Ceph MDS。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Monitor&lt;/strong> # &lt;strong>Ceph Monitor(Ceph 监视器，简称 ceph-mon)&lt;/strong> 负责维护集群状态的映射关系。通常至少需要 3 个 ceph-mon 以实现高可用，多节点使用 Paxos 算法达成共识。
&lt;ul>
&lt;li>可以这么说，Ceph 集群就是指 ceph-mon 集群。ceph-mon 负责维护的集群状态，就是用来提供存储服务的。&lt;/li>
&lt;li>ceph-mon 映射、ceph-mgr 映射、ceph-osd 映射、ceph-mds 映射、ceph-crush 映射。这些映射是 Ceph 守护进程相互协调所需的关键集群状态，说白了，就是&lt;strong>映射关系&lt;/strong>。
&lt;ul>
&lt;li>这里的映射，英文用的是 Map，其实也有地图的意思，就是表示这个集群有多少个 ceph-mon、有多少个 ceph-mgr 等等，还有底层对象属于哪个 PG，等等等等，这些东西构成了一副 Ceph 的运行图。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ceph-mon 还负责管理守护进程和客户端之间的身份验证。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Manager&lt;/strong> # &lt;strong>Ceph Manager(Ceph 管理器，简称 ceph-mgr)&lt;/strong> 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、性能、系统负载等。通常至少需要 2 个 ceph-mgr 以实现高可用。
&lt;ul>
&lt;li>ceph-mgr 可以提供 Web 管理页面、关于 Ceph 集群的 Prometheus 格式的监控指标&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>OSD Daemon&lt;/strong> # &lt;strong>Ceph OSD Daemon(Ceph OSD 守护进程，简称 ceph-osd)&lt;/strong> 负责向 OSD 读写数据、处理数据复制、恢复、重新平衡，并通过检查其他 ceph-osd 的心跳向 ceph-mon 和 ceph-mgr 提供一些监控信息。通常至少需要 3 个 ceph-osd 以实现高科用。
&lt;ul>
&lt;li>&lt;strong>Object Storage Device(对象存储设备，简称 OSD)&lt;/strong> 是一个物理或逻辑上的存储单元(比如一块硬盘)，这是 Ceph 得以运行的最基本的存储单元。
&lt;ul>
&lt;li>有的时候，人们容易把 OSD 理解为 Ceph OSD Daemon，这俩是有本质区别的。因为在最早的时候，OSD 有两种含义，一种是 &lt;code>Object Storage Device&lt;/code> 另一种是 &lt;code>Object Storage Daemon&lt;/code>。由于这种称呼的模糊性，后来就将 Object Storage daemon 扩展为 OSD Daemon。OSD 则仅仅代表 Object Storage Device。只不过运行 OSD Daemon 的程序名称，依然沿用了 osd 的名字。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>注意，为了让每一个 OSD 都可以被单独使用并管理，所以每个 OSD 都有一个对应的 ceph-osd 进程来管理。一般情况，Ceph 集群中每个节点，除了系统盘做 Raid 以外，其他硬盘都会单独作为 OSD 使用，且一个节点会有大量磁盘来对应 OSD。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>MDS&lt;/strong> # &lt;strong>Ceph Metadata Server(Ceph 元数据服务器，简称 ceph-mds)&lt;/strong> 代表 Ceph 文件系统元数据。ceph-mds 允许 POSIX 文件系统用户执行基本命令(比如 ls、find 等)，而不会给 Ceph 集群带来巨大负担。
&lt;ul>
&lt;li>注意，Ceph 提供的 块存储 和 对象存储 功能并不使用 ceph-mds。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构">架构&lt;a class="td-heading-self-link" href="#%e6%9e%b6%e6%9e%84" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/architecture/">官方文档，架构&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sakrws/1630769971104-82bcc0c6-1dbd-4c47-b986-3e5b8321aac0.png" alt="image.png">&lt;/p>
&lt;p>&lt;strong>其实 Ceph 本身就是一个对象存储&lt;/strong>，基于&lt;strong>RADOS&lt;/strong> 实现，并通过 Ceph Client 为上层应用提供了通用的 块存储、文件存储、对象存储 的调用接口。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>RADOS&lt;/strong> # &lt;strong>Reliable Autonomic Distributed Object Store(可靠的、自动化的分布式对象存储，简称 RADOS)&lt;/strong> 是一种由多个主机组成、由 CRUSH 算法实现数据路由的，分布式对象存储系统。是 Ceph 的底层存储系统。
&lt;ul>
&lt;li>OSD 是组成 RADOS 的基本存储单元。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Ceph Client&lt;/strong> # &lt;strong>Ceph 客户端&lt;/strong>。是可以访问 Ceph 存储集群(即 RADOS) 的 Ceph 组件的集合。
&lt;ul>
&lt;li>&lt;strong>LIBRADOS&lt;/strong> # &lt;strong>Library RADOS(RADOS 库，简称 librados)&lt;/strong>。应用程序可以调用 librados 以直接访问 RADOS。当我们使用 Ceph 时，Ceph 实际上是调用 librados 的 API(这是一个 rpc 接口)，将提交的文件切分为固定大小的数据，存放到 RADOS 中。
&lt;ul>
&lt;li>同时，我们自己也可以使用 librados 开发出类似 ceph-rgw、ceph-rbd 这种应用程序以实现个性化需求。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>RADOSGW&lt;/strong> # &lt;strong>RADOS Gateway(RADOS 网关，简称 radosgw)&lt;/strong>。使用 librados 实现的应用程序，可以提供兼容 S3 和 Swift 对象存储的接口&lt;/li>
&lt;li>&lt;strong>RBD&lt;/strong> # &lt;strong>RADOS Block Device(RADOS 块设备，简称 RBD)&lt;/strong>。使用 librados 实现的应用程序，为 Linux 内核 和 QEMU/KVM 提供一个可靠且完全分布式的块存储设备。&lt;/li>
&lt;li>&lt;strong>CEPH FS&lt;/strong> # &lt;strong>Ceph File System(Ceph 文件系统，简称 CFS)&lt;/strong>。直接使用 RADOS 实现一个符合 POSIX 的分布式文件系统，带有 Linux 内核客户端并支持 FUSE，可以直接挂载使用。甚至可以进一步抽象，实现 NFS 功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ceph-数据写入流程">Ceph 数据写入流程&lt;a class="td-heading-self-link" href="#ceph-%e6%95%b0%e6%8d%ae%e5%86%99%e5%85%a5%e6%b5%81%e7%a8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Ceph 集群从 Ceph 的客户端接收到的数据后，将会切分为一个或多个固定大小的 &lt;strong>RADOS Object(RADOS 对象)&lt;/strong>。Ceph 使用 &lt;strong>Controlled Replication Under Scalable Hashing(简称 CRUSH)&lt;/strong> 算法计算出 RADOS 对象应该放在哪个 &lt;strong>Placement Group(归置组，简称 PG)&lt;/strong>，并进一步计算出，应该由哪个 ceph-osd 来处理这个 PG 并将 PG 存储到指定的 OSD 中。ceph-osd 会通过存储驱动器处理 RADOS 对象的 读、写 和 复制操作。&lt;/p>
&lt;blockquote>
&lt;p>注意：当创建完 Ceph 集群后，会有一个默认的 Pool，Pool 是用来对 PG 进行分组的，且 PG 必须属于一个组，不可独立存在。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sakrws/1630834243384-b650e1e5-1c84-4846-bdc5-9180a361fb09.png" alt="image.png">&lt;/p>
&lt;p>RADOS 对象有如下几个部分组成&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Object Identify(对象标识符，简称 OID)&lt;/strong> # OID 在整个 Ceph 集群中是唯一。&lt;/li>
&lt;li>&lt;strong>Binary Data(二进制数据数据)&lt;/strong> # 对象的数据&lt;/li>
&lt;li>&lt;strong>Metadata(元数据)&lt;/strong> # 元数据的语义完全取决于 Ceph 客户端。例如，CephFS 使用元数据来存储文件属性，如文件所有者、创建日期、上次修改日期等。&lt;/li>
&lt;/ul>
&lt;p>ceph-osd 将数据作为对象存储在平坦的命名空间中 (例如，没有目录层次结构)。对象具有标识符，二进制数据和由一组名称/值对组成的元数据。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sakrws/1630808425695-75766062-7570-47f0-9ae4-916c7819d113.png" alt="image.png">&lt;/p>
&lt;h1 id="rados">RADOS&lt;a class="td-heading-self-link" href="#rados" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>与传统分布式存储不同，传统分布式存储中的 NameNode 极易形成性能瓶颈。基于此，RADOS 设计了一种新的方式来快速找到对象数据。RADOS 中并不需要 NameNode 来存储每个对象的元数据，RADOS 中的对象，都是通过 &lt;strong>Controlled Replication Under Scalable Hashing(简称 CRUSH)&lt;/strong> 算法来快速定位的。&lt;/p>
&lt;h2 id="bluestore">bluestore&lt;a class="td-heading-self-link" href="#bluestore" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>这是 Ceph 所管理的 OSD 的文件系统类型&lt;/p>
&lt;h1 id="ceph-的存储能力">Ceph 的存储能力&lt;a class="td-heading-self-link" href="#ceph-%e7%9a%84%e5%ad%98%e5%82%a8%e8%83%bd%e5%8a%9b" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="块存储">块存储&lt;a class="td-heading-self-link" href="#%e5%9d%97%e5%ad%98%e5%82%a8" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Ceph 通过 RDB 提供块存储能力&lt;/p>
&lt;h2 id="文件存储">文件存储&lt;a class="td-heading-self-link" href="#%e6%96%87%e4%bb%b6%e5%ad%98%e5%82%a8" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Ceph 通过 CEPHFS 提供文件存储能力&lt;/p>
&lt;h2 id="对象存储">对象存储&lt;a class="td-heading-self-link" href="#%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>RADOS Gateway 简称 radosgw，Ceph 通过 radosgw 程序，可以对外提供标准的 S3 或 swift 接口，以实现主流对象存储功能。很多时候，radosgw 程序运行的进程称为 ceph-rgw&lt;/p>
&lt;h1 id="ceph-manager">Ceph Manager&lt;a class="td-heading-self-link" href="#ceph-manager" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/mgr/">官方文档,Ceph 管理器&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Ceph Manager 是通过一个名为 ceph-mgr 的二进制程序以守护进程运行的管理器。ceph-mgr 可以向外部监控和管理系统提供额外的监控和接口。&lt;/p>
&lt;p>ceph-mgr 曾经是 ceph-mon 的一部分，自 luinous(12.x) 版本依赖，ceph-mgr 独立出来，成为 Ceph 集群的必选组件。&lt;/p>
&lt;h2 id="dashboard-模块">Dashboard 模块&lt;a class="td-heading-self-link" href="#dashboard-%e6%a8%a1%e5%9d%97" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Dashboard 模块是一个内置的基于 Web 的 Ceph 管理和监控程序，通过它可以检查和管理 Ceph 集群中的各个方面和资源。默认监听 &lt;code>8443&lt;/code> 端口&lt;/p>
&lt;p>在 Dashboard 模块中，提供了一组用于管理集群的 RESTful 风格的 API 接口。这组 API 位于 &lt;code>/api&lt;/code> 路径下。详见《[API](&amp;lt;/docs/5.数据存储/1.存储/存储的基础设施架构/Distributed%20Storage(分布式存储)/Ceph/API.md&amp;raquo;)》章节&lt;/p>
&lt;h2 id="prometheus-模块">Prometheus 模块&lt;a class="td-heading-self-link" href="#prometheus-%e6%a8%a1%e5%9d%97" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>启动 Prometheus 模块后，ceph-mgr 默认在 &lt;code>9283&lt;/code> 端口上暴露 Prometheus 格式的监控指标。&lt;/p>
&lt;h1 id="ceph-radosgw">Ceph RADOSGW&lt;a class="td-heading-self-link" href="#ceph-radosgw" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>默认监听 &lt;code>7480&lt;/code> 端口&lt;/p></description></item><item><title>Docs: API</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/API/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/API/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="ceph-restful-api">Ceph RESTful API&lt;a class="td-heading-self-link" href="#ceph-restful-api" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/mgr/ceph_api/">官方文档，Ceph 管理器守护进程-Ceph RESTful API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ceph/ceph/blob/master/src/pybind/mgr/dashboard/openapi.yaml">GitHub，ceph/ceph/src/pybind/mgr/dashboard/openapi.yaml&lt;/a>(该 API 的 openapi 文件)&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 Dashboard 模块中，提供了一组用于管理集群的 RESTful 风格的 API 接口。这组 API 默认位于 &lt;code>https://localhost:8443/api&lt;/code> 路径下&lt;/p>
&lt;p>在 &lt;code>/docs&lt;/code> 端点下，可以查看 OpenAPI 格式的信息&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/fl1wmh/1630938180924-97cb2959-3cf0-48bf-b312-57be88e9471d.png" alt="image.png">
在 &lt;code>/dpcs/api.json&lt;/code> 端点可以获取 openapi 格式的 API 信息。&lt;/p>
&lt;h2 id="apiauth">/api/auth&lt;a class="td-heading-self-link" href="#apiauth" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>/api/auth&lt;/code> 接口获取 Token&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -X POST &lt;span style="color:#4e9a06">&amp;#34;https://example.com:8443/api/auth&amp;#34;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#34;Accept: application/vnd.ceph.api.v1.0+json&amp;#34;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#34;Content-Type: application/json&amp;#34;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -d &lt;span style="color:#4e9a06">&amp;#39;{&amp;#34;username&amp;#34;: &amp;lt;username&amp;gt;, &amp;#34;password&amp;#34;: &amp;lt;password&amp;gt;}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>获取 Token 后，其他接口，都可以使用该 Token 进行认证，比如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -H &lt;span style="color:#4e9a06">&amp;#34;Authorization: Bearer &lt;/span>&lt;span style="color:#000">$TOKEN&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;&lt;/span> ......
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="apiauthcheck">/api/auth/check&lt;a class="td-heading-self-link" href="#apiauthcheck" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>/api/auth/check&lt;/code> 接口可以检查 Token。通常还可以作为对 API 的健康检查接口。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -k -XPOST &lt;span style="color:#4e9a06">&amp;#39;https://example.com:8443/api/auth/check&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;accept: */*&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;Content-Type: application/json&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -d &lt;span style="color:#4e9a06">&amp;#34;{\&amp;#34;token\&amp;#34;: \&amp;#34;&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">TOKEN&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>&lt;span style="color:#4e9a06">\&amp;#34;}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Ceph 部署</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph-%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph-%E9%83%A8%E7%BD%B2/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/cephadm/">官方文档, Cephadm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="以-pacific-版本为例">以 pacific 版本为例&lt;a class="td-heading-self-link" href="#%e4%bb%a5-pacific-%e7%89%88%e6%9c%ac%e4%b8%ba%e4%be%8b" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="安装-cephadm">安装 cephadm&lt;a class="td-heading-self-link" href="#%e5%ae%89%e8%a3%85-cephadm" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>在所有节点安装 cephadm，这是一个 python 程序，当通过第一个节点让其他节点加入集群时，会调用待加入节点中的 cephadm 程序。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod +x cephadm
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./cephadm add-repo --release pacific
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./cephadm install
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cephadm install ceph-common
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="引导第一个节点">引导第一个节点&lt;a class="td-heading-self-link" href="#%e5%bc%95%e5%af%bc%e7%ac%ac%e4%b8%80%e4%b8%aa%e8%8a%82%e7%82%b9" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cephadm bootstrap --mon-ip 192.168.1.201
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="配置-ceph-cli">配置 ceph CLI&lt;a class="td-heading-self-link" href="#%e9%85%8d%e7%bd%ae-ceph-cli" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cephadm install ceph-common
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="其他">其他&lt;a class="td-heading-self-link" href="#%e5%85%b6%e4%bb%96" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 开启遥测，发送数据给官方&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph telemetry on
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="添加其他节点">添加其他节点&lt;a class="td-heading-self-link" href="#%e6%b7%bb%e5%8a%a0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 添加认证信息&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.1.202
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.1.203
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 添加节点&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch host add hw-cloud-xngy-ecs-test-0002 192.168.1.202
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch host add hw-cloud-xngy-ecs-test-0003 192.168.1.203
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 为节点添加 _admin 标签&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch host label add hw-cloud-xngy-ecs-test-0002 _admin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch host label add hw-cloud-xngy-ecs-test-0003 _admin
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>节点添加完成后，在 1 和 2 上活动 ceph-mgr，1，2，3 上启动了 ceph-mon 和 ceph-crash&lt;/p>
&lt;h2 id="添加存储设备">添加存储设备&lt;a class="td-heading-self-link" href="#%e6%b7%bb%e5%8a%a0%e5%ad%98%e5%82%a8%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>注意：如下所示，当一块磁盘具有 GPT 分区表时，是无法作为 Ceph 的 OSD 使用&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@hw-cloud-xngy-ecs-test-0001:~# ceph orch device ls --wide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Hostname Path Type Transport RPM Vendor Model Serial Size Health Ident Fault Available Reject Reasons
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hw-cloud-xngy-ecs-test-0001 /dev/vdb hdd Unknown Unknown 0x1af4 N/A 4afb2ab1-9244-45bf-a 107G Unknown N/A N/A No Has GPT headers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hw-cloud-xngy-ecs-test-0002 /dev/vdb hdd Unknown Unknown 0x1af4 N/A 74321443-d05c-4803-9 107G Unknown N/A N/A No Has GPT headers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hw-cloud-xngy-ecs-test-0003 /dev/vdb hdd Unknown Unknown 0x1af4 N/A f9c0ddbb-7ede-4958-8 107G Unknown N/A N/A No Has GPT headers
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>执行命令 &lt;code>parted /dev/vdb mklabel msdos&lt;/code> 删除 GPT 分区表，即可。也可以使用 &lt;code>sgdisk&lt;/code> 命令进行磁盘清理。&lt;/p>
&lt;p>清理完成后，开始在所有节点上添加 OSD&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ceph orch apply osd --all-available-devices
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="基本部署完成">基本部署完成&lt;a class="td-heading-self-link" href="#%e5%9f%ba%e6%9c%ac%e9%83%a8%e7%bd%b2%e5%ae%8c%e6%88%90" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>组成了一个三节点的 Ceph 集群效果如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sx1zt0/1630850693982-c0ecf1f3-1f37-4f61-8b7c-84c1447ac04f.png" alt="image.png">&lt;/p>
&lt;p>Ceph 集群中，除了监控套件以外，有 3 个 ceph-mon、2 个 ceph-mgr、3 个 ceph-crash、6 个 ceph-osd。从服务角度看，当前有三个服务：mon、mgr、osd，使用 &lt;code>ceph -s&lt;/code> 命令查看：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ceph -s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: 24750534-0e45-11ec-9849-7bf16e3e2cb9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> health: HEALTH_OK
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mon: &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span> daemons, quorum hw-cloud-xngy-ecs-test-0001,hw-cloud-xngy-ecs-test-0002,hw-cloud-xngy-ecs-test-0003 &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>age 10h&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mgr: hw-cloud-xngy-ecs-test-0001.afnavu&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>active, since 10h&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, standbys: hw-cloud-xngy-ecs-test-0002.jucqwq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> osd: &lt;span style="color:#0000cf;font-weight:bold">6&lt;/span> osds: &lt;span style="color:#0000cf;font-weight:bold">6&lt;/span> up &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>since 10h&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, &lt;span style="color:#0000cf;font-weight:bold">6&lt;/span> in &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>since 10h&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pools: &lt;span style="color:#0000cf;font-weight:bold">7&lt;/span> pools, &lt;span style="color:#0000cf;font-weight:bold">145&lt;/span> pgs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> objects: &lt;span style="color:#0000cf;font-weight:bold">253&lt;/span> objects, &lt;span style="color:#0000cf;font-weight:bold">10&lt;/span> KiB
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usage: &lt;span style="color:#0000cf;font-weight:bold">343&lt;/span> MiB used, &lt;span style="color:#0000cf;font-weight:bold">600&lt;/span> GiB / &lt;span style="color:#0000cf;font-weight:bold">600&lt;/span> GiB avail
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pgs: &lt;span style="color:#0000cf;font-weight:bold">145&lt;/span> active+clean
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="添加-rgw-服务">添加 RGW 服务&lt;a class="td-heading-self-link" href="#%e6%b7%bb%e5%8a%a0-rgw-%e6%9c%8d%e5%8a%a1" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 为集群中两个节点添加标签，以准备后续将 radosgw 部署到具有 rgw 标签的节点上&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch host label add hw-cloud-xngy-ecs-test-0001 rgw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch host label add hw-cloud-xngy-ecs-test-0001 rgw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 部署 rgw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch apply rgw foo &lt;span style="color:#4e9a06">&amp;#39;--placement=label:rgw count-per-host:2&amp;#39;&lt;/span> --port&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>此时，节点 1 和节点 2 上，每个节点都有运行有两个 ceph-rgw 实例，可以对外提供服务&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># docker ps -a | grep rgw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>9ad5fe363abd ceph/ceph &lt;span style="color:#4e9a06">&amp;#34;/usr/bin/radosgw -n…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">24&lt;/span> seconds ago Up &lt;span style="color:#0000cf;font-weight:bold">24&lt;/span> seconds ceph-24750534-0e45-11ec-9849-7bf16e3e2cb9-rgw.foo.hw-cloud-xngy-ecs-test-0001.rqcvxl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>519d6a07f001 ceph/ceph &lt;span style="color:#4e9a06">&amp;#34;/usr/bin/radosgw -n…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">26&lt;/span> seconds ago Up &lt;span style="color:#0000cf;font-weight:bold">26&lt;/span> seconds ceph-24750534-0e45-11ec-9849-7bf16e3e2cb9-rgw.foo.hw-cloud-xngy-ecs-test-0001.hsjpqq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@hw-cloud-xngy-ecs-test-0001:~# ss -ntlp &lt;span style="color:#000;font-weight:bold">|&lt;/span> grep gw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> 0.0.0.0:8000 0.0.0.0:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>16598,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>57&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> 0.0.0.0:8001 0.0.0.0:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>17447,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>57&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:8000 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>16598,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>58&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:8001 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>17447,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>58&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@hw-cloud-xngy-ecs-test-0002:~# docker ps -a &lt;span style="color:#000;font-weight:bold">|&lt;/span> grep rgw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>1ab21ec662de ceph/ceph &lt;span style="color:#4e9a06">&amp;#34;/usr/bin/radosgw -n…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">23&lt;/span> seconds ago Up &lt;span style="color:#0000cf;font-weight:bold">23&lt;/span> seconds ceph-24750534-0e45-11ec-9849-7bf16e3e2cb9-rgw.foo.hw-cloud-xngy-ecs-test-0002.zsrkkp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10ad804e541d ceph/ceph &lt;span style="color:#4e9a06">&amp;#34;/usr/bin/radosgw -n…&amp;#34;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">25&lt;/span> seconds ago Up &lt;span style="color:#0000cf;font-weight:bold">25&lt;/span> seconds ceph-24750534-0e45-11ec-9849-7bf16e3e2cb9-rgw.foo.hw-cloud-xngy-ecs-test-0002.giyyjf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@hw-cloud-xngy-ecs-test-0002:~# ss -ntlp &lt;span style="color:#000;font-weight:bold">|&lt;/span> grep gw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> 0.0.0.0:8000 0.0.0.0:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>14294,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>57&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> 0.0.0.0:8001 0.0.0.0:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>15152,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>57&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:8000 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>14294,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>58&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LISTEN &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">128&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:8001 &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>::&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>:* users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;radosgw&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>15152,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>58&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>当访问这些端口时，显示如下内容，则说明已经可以提供 S3 服务了&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@hw-cloud-xngy-ecs-test-0001:~# curl 192.168.1.201:8000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;?xml &lt;span style="color:#000">version&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;1.0&amp;#34;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;UTF-8&amp;#34;&lt;/span>?&amp;gt;&amp;lt;ListAllMyBucketsResult &lt;span style="color:#000">xmlns&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;http://s3.amazonaws.com/doc/2006-03-01/&amp;#34;&lt;/span>&amp;gt;&amp;lt;Owner&amp;gt;&amp;lt;ID&amp;gt;anonymous&amp;lt;/ID&amp;gt;&amp;lt;DisplayName&amp;gt;&amp;lt;/DisplayName&amp;gt;&amp;lt;/Owner&amp;gt;&amp;lt;Buckets&amp;gt;&amp;lt;/Buckets&amp;gt;&amp;lt;/ListAllMyBucketsResult&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>创建一个系统用户并记录下 ak 与 sk。通过这个用户的信息，可以使用 blemmenes/radosgw_usage_exporter 导出对象存储监控指标&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@hw-cloud-xngy-ecs-test-0001:~# radosgw-admin user create --uid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>lichenhao --display-name&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>lichenhao --system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;user_id&amp;#34;&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;lichenhao&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;display_name&amp;#34;&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;lichenhao&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;keys&amp;#34;&lt;/span>: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;user&amp;#34;&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;lichenhao&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;access_key&amp;#34;&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;4O23LGQI3UAUKSSO50UK&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;secret_key&amp;#34;&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;JQLul4q2r2qo1vyOLpQ4FVUnh3LWfiNyuiZHQDT6&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a href="https://docs.ceph.com/en/pacific/mgr/dashboard/#enabling-the-object-gateway-management-frontend">启动对象网关的管理前端&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>radosgw-admin user info --uid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>lichenhao &lt;span style="color:#000;font-weight:bold">|&lt;/span> jq .keys&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>.access_key &amp;gt; ak
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>radosgw-admin user info --uid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>lichenhao &lt;span style="color:#000;font-weight:bold">|&lt;/span> jq .keys&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>.secret_key &amp;gt; sk
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph dashboard set-rgw-api-access-key -i ak
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph dashboard set-rgw-api-secret-key -i sk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>此时从 Ceph 的 Web 页面中，可以从 Object Gateway 标签中看到内容了：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sx1zt0/1630856235488-d2d2e334-a0a9-4d41-aa17-06522f30d11a.png" alt="image.png">&lt;/p>
&lt;p>使用 192.168.1.202:8000 作为 Endpoint，以及 lichenhao 用户的 ak、sk，可以通过 S3 Brower 访问 Ceph 提供的对象存储。注意：由于此时没有开启 SSL，所以 S3 Brower 也要关闭 SSL。&lt;/p>
&lt;h2 id="添加监控服务可选">添加监控服务(可选)&lt;a class="td-heading-self-link" href="#%e6%b7%bb%e5%8a%a0%e7%9b%91%e6%8e%a7%e6%9c%8d%e5%8a%a1%e5%8f%af%e9%80%89" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ceph orch apply node-exporter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch apply alertmanager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch apply prometheus
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ceph orch apply grafana
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sx1zt0/1630835511543-fb85907a-97d5-4f99-80d5-2214a0236810.png" alt="image.png">&lt;/p>
&lt;h1 id="其他-1">其他&lt;a class="td-heading-self-link" href="#%e5%85%b6%e4%bb%96-1" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run --rm --network host --name ceph-rgw-exporter &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>blemmenes/radosgw_usage_exporter:latest &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-H 172.38.30.2:7480 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-a F52JL32RD8NWI78XT3A9 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-s jjs3uAIGJYFMyprkHov6D85D1YGSo0HowisHZmJl &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-p &lt;span style="color:#0000cf;font-weight:bold">9243&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run --rm --network host --name ceph-rgw-exporter &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>blemmenes/radosgw_usage_exporter:latest &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-H 192.168.1.202:8000 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-a 4O23LGQI3UAUKSSO50UK &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-s JQLul4q2r2qo1vyOLpQ4FVUnh3LWfiNyuiZHQDT6 &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span>-p &lt;span style="color:#0000cf;font-weight:bold">9243&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>异常&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sx1zt0/1630750629528-40ac128e-4c7c-4ccf-9aa4-d64741aae089.png" alt="image.png">&lt;/p>
&lt;p>正常&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sx1zt0/1630835261055-137daaea-90de-4045-a62f-5a0e28077860.png" alt="image.png">&lt;/p></description></item><item><title>Docs: Ceph 故障排查笔记</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5%E7%AC%94%E8%AE%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5%E7%AC%94%E8%AE%B0/</guid><description>
&lt;p>参考：&lt;a href="https://mp.weixin.qq.com/s/k5-gkm78KXmty7F2qerZ6g">公众号,云原声实验室-Ceph 故障排查笔记 | 万字经验总结&lt;/a>&lt;/p>
&lt;h3 id="ceph-osd-异常无法正常启动">Ceph OSD 异常无法正常启动&lt;a class="td-heading-self-link" href="#ceph-osd-%e5%bc%82%e5%b8%b8%e6%97%a0%e6%b3%95%e6%ad%a3%e5%b8%b8%e5%90%af%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>当某个 OSD 无法正常启动时：&lt;/p>
&lt;pre>&lt;code>$ ceph -s
cluster:
id: b313ec26-5aa0-4db2-9fb5-a38b207471ee
health: HEALTH_WARN
Degraded data redundancy: 177597/532791 objects degraded (33.333%), 212 pgs degraded, 212 pgs undersized
application not enabled on 3 pool(s)
mon master003 is low on available space
1/3 mons down, quorum master002,master003
services:
mon: 3 daemons, quorum master002,master003, out of quorum: master001
mgr: master003(active), standbys: master002
mds: kubernetes-1/1/1 up {0=master002=up:active}, 1 up:standby
osd: 2 osds: 2 up, 2 in
data:
pools: 5 pools, 212 pgs
objects: 177.6 k objects, 141 GiB
usage: 297 GiB used, 2.8 TiB / 3.0 TiB avail
pgs: 177597/532791 objects degraded (33.333%)
212 active+undersized+degraded
io:
client: 170 B/s rd, 127 KiB/s wr, 0 op/s rd, 5 op/s wr
&lt;/code>&lt;/pre>
&lt;p>查看状态信息：&lt;/p>
&lt;pre>&lt;code>$ ceph health detail
HEALTH_WARN Degraded data redundancy: 177615/532845 objects degraded (33.333%), 212 pgs degraded, 212 pgs undersized; application not enabled on 3 pool(s); mon master003 is low on available space
PG_DEGRADED Degraded data redundancy: 177615/532845 objects degraded (33.333%), 212 pgs degraded, 212 pgs undersized
pg 1.15 is active+undersized+degraded, acting [1,2]
pg 1.2e is stuck undersized for 12701595.129535, current state active+undersized+degraded, last acting [1,2]
pg 1.2f is stuck undersized for 12701595.110228, current state active+undersized+degraded, last acting [2,1]
pg 1.30 is stuck undersized for 12701595.128371, current state active+undersized+degraded, last acting [1,2]
pg 1.31 is stuck undersized for 12701595.129981, current state active+undersized+degraded, last acting [1,2]
pg 1.32 is stuck undersized for 12701595.122298, current state active+undersized+degraded, last acting [2,1]
pg 1.33 is stuck undersized for 12701595.129509, current state active+undersized+degraded, last acting [2,1]
pg 1.34 is stuck undersized for 12701595.116494, current state active+undersized+degraded, last acting [2,1]
pg 1.35 is stuck undersized for 12701595.132276, current state active+undersized+degraded, last acting [2,1]
pg 1.36 is stuck undersized for 12701595.131601, current state active+undersized+degraded, last acting [1,2]
pg 1.37 is stuck undersized for 12701595.126213, current state active+undersized+degraded, last acting [1,2]
pg 1.38 is stuck undersized for 12701595.119082, current state active+undersized+degraded, last acting [2,1]
pg 1.39 is stuck undersized for 12701595.127812, current state active+undersized+degraded, last acting [1,2]
pg 1.3a is stuck undersized for 12701595.117611, current state active+undersized+degraded, last acting [2,1]
pg 1.3b is stuck undersized for 12701595.125454, current state active+undersized+degraded, last acting [2,1]
pg 1.3c is stuck undersized for 12701595.131540, current state active+undersized+degraded, last acting [1,2]
pg 1.3d is stuck undersized for 12701595.130465, current state active+undersized+degraded, last acting [1,2]
pg 1.3e is stuck undersized for 12701595.120532, current state active+undersized+degraded, last acting [2,1]
pg 1.3f is stuck undersized for 12701595.129921, current state active+undersized+degraded, last acting [1,2]
pg 1.40 is stuck undersized for 12701595.115146, current state active+undersized+degraded, last acting [2,1]
pg 1.41 is stuck undersized for 12701595.132582, current state active+undersized+degraded, last acting [1,2]
pg 1.42 is stuck undersized for 12701595.122272, current state active+undersized+degraded, last acting [2,1]
pg 1.43 is stuck undersized for 12701595.132359, current state active+undersized+degraded, last acting [1,2]
pg 1.44 is stuck undersized for 12701595.129082, current state active+undersized+degraded, last acting [2,1]
pg 1.45 is stuck undersized for 12701595.118952, current state active+undersized+degraded, last acting [2,1]
pg 1.46 is stuck undersized for 12701595.129618, current state active+undersized+degraded, last acting [1,2]
pg 1.47 is stuck undersized for 12701595.112277, current state active+undersized+degraded, last acting [2,1]
pg 1.48 is stuck undersized for 12701595.131721, current state active+undersized+degraded, last acting [1,2]
pg 1.49 is stuck undersized for 12701595.130365, current state active+undersized+degraded, last acting [1,2]
pg 1.4a is stuck undersized for 12701595.126070, current state active+undersized+degraded, last acting [1,2]
pg 1.4b is stuck undersized for 12701595.113785, current state active+undersized+degraded, last acting [2,1]
pg 1.4c is stuck undersized for 12701595.129074, current state active+undersized+degraded, last acting [1,2]
pg 1.4d is stuck undersized for 12701595.115487, current state active+undersized+degraded, last acting [2,1]
pg 1.4e is stuck undersized for 12701595.131307, current state active+undersized+degraded, last acting [1,2]
pg 1.4f is stuck undersized for 12701595.132162, current state active+undersized+degraded, last acting [2,1]
pg 1.50 is stuck undersized for 12701595.129346, current state active+undersized+degraded, last acting [2,1]
pg 1.51 is stuck undersized for 12701595.131897, current state active+undersized+degraded, last acting [1,2]
pg 1.52 is stuck undersized for 12701595.126480, current state active+undersized+degraded, last acting [2,1]
pg 1.53 is stuck undersized for 12701595.116500, current state active+undersized+degraded, last acting [2,1]
pg 1.54 is stuck undersized for 12701595.122930, current state active+undersized+degraded, last acting [2,1]
pg 1.55 is stuck undersized for 12701595.116566, current state active+undersized+degraded, last acting [2,1]
pg 1.56 is stuck undersized for 12701595.130017, current state active+undersized+degraded, last acting [1,2]
pg 1.57 is stuck undersized for 12701595.129217, current state active+undersized+degraded, last acting [1,2]
pg 1.58 is stuck undersized for 12701595.124121, current state active+undersized+degraded, last acting [2,1]
pg 1.59 is stuck undersized for 12701595.127802, current state active+undersized+degraded, last acting [1,2]
pg 1.5a is stuck undersized for 12701595.131028, current state active+undersized+degraded, last acting [1,2]
pg 1.5b is stuck undersized for 12701595.114646, current state active+undersized+degraded, last acting [2,1]
pg 1.5c is stuck undersized for 12701595.109604, current state active+undersized+degraded, last acting [2,1]
pg 1.5d is stuck undersized for 12701595.126384, current state active+undersized+degraded, last acting [2,1]
pg 1.5e is stuck undersized for 12701595.129456, current state active+undersized+degraded, last acting [1,2]
pg 1.5f is stuck undersized for 12701595.126573, current state active+undersized+degraded, last acting [2,1]
POOL_APP_NOT_ENABLED application not enabled on 3 pool(s)
application not enabled on pool 'nextcloud'
application not enabled on pool 'gitlab-ops'
application not enabled on pool 'kafka-ops'
use 'ceph osd pool application enable &amp;lt;pool-name&amp;gt; &amp;lt;app-name&amp;gt;', where &amp;lt;app-name&amp;gt; is 'cephfs', 'rbd', 'rgw', or freeform for custom applications.
MON_DISK_LOW mon master003 is low on available space
mon.master003 has 22% avail
&lt;/code>&lt;/pre>
&lt;p>并且通过 log 也无法完全定位问题时，可以通过如下方式解决。&lt;/p>
&lt;h3 id="删除-osd-重新加载">删除 osd 重新加载&lt;a class="td-heading-self-link" href="#%e5%88%a0%e9%99%a4-osd-%e9%87%8d%e6%96%b0%e5%8a%a0%e8%bd%bd" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>删除当前的 osd 重新让其进行加载，此方式适合于异常重启后的操作。
首先删除这个 osd：&lt;/p>
&lt;pre>&lt;code>$ ceph osd out osd.0
$ systemctl stop ceph-osd@0
$ ceph osd crush remove osd.0
$ ceph auth del osd.0
$ ceph osd rm 0
&lt;/code>&lt;/pre>
&lt;p>重新加载 osd：&lt;/p>
&lt;pre>&lt;code>$ ceph osd create 0
$ ceph auth add osd.0 osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-0/keyring
$ ceph osd crush add 0 1.0 host=master001
$ systemctl start ceph-osd@0
&lt;/code>&lt;/pre>
&lt;h3 id="清除当前-osd-所有数据重新添加">清除当前 osd 所有数据重新添加&lt;a class="td-heading-self-link" href="#%e6%b8%85%e9%99%a4%e5%bd%93%e5%89%8d-osd-%e6%89%80%e6%9c%89%e6%95%b0%e6%8d%ae%e9%87%8d%e6%96%b0%e6%b7%bb%e5%8a%a0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>删除当前 osd 的所有数据，并且重新加载 osd，此操作一定要保证有冗余可用的 osd，否则会造成整个 osd 数据损坏。
删除当前 osd：&lt;/p>
&lt;pre>&lt;code>$ ceph osd out osd.0
$ systemctl stop ceph-osd@0
$ ceph osd crush remove osd.0
$ ceph auth del osd.0
$ ceph osd rm 0
&lt;/code>&lt;/pre>
&lt;p>卸载：&lt;/p>
&lt;pre>&lt;code>$ umount -l /var/lib/ceph/osd/ceph-0
&lt;/code>&lt;/pre>
&lt;p>清空磁盘数据：&lt;/p>
&lt;pre>&lt;code>$ wipefs -af /dev/mapper/VolGroup-lv_data1
$ ceph-volume lvm zap /dev/mapper/VolGroup-lv_data1
&lt;/code>&lt;/pre>
&lt;p>重新添加 osd：&lt;/p>
&lt;pre>&lt;code>$ ceph-deploy --overwrite-conf osd create master001 --data /dev/mapper/VolGroup-lv_data1
&lt;/code>&lt;/pre>
&lt;h3 id="删除当前节点所有服务">删除当前节点所有服务&lt;a class="td-heading-self-link" href="#%e5%88%a0%e9%99%a4%e5%bd%93%e5%89%8d%e8%8a%82%e7%82%b9%e6%89%80%e6%9c%89%e6%9c%8d%e5%8a%a1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>删除当前节点的所有服务，让其重新加载数据：&lt;/p>
&lt;pre>&lt;code>$ ceph-deploy purge master001
$ ceph-deploy purgedata master001
&lt;/code>&lt;/pre>
&lt;p>创建数据目录：&lt;/p>
&lt;pre>&lt;code>$ rm -rf /var/lib/ceph
$ mkdir -p /var/lib/ceph
$ mkdir -p /var/lib/ceph/osd/ceph-0
$ chown ceph:ceph /var/lib/ceph
&lt;/code>&lt;/pre>
&lt;p>然后安装 ceph：&lt;/p>
&lt;pre>&lt;code>$ ceph-deploy install master001
&lt;/code>&lt;/pre>
&lt;p>同步配置：&lt;/p>
&lt;pre>&lt;code>$ ceph-deploy --overwrite-conf admin master001
&lt;/code>&lt;/pre>
&lt;p>添加 osd：&lt;/p>
&lt;pre>&lt;code>$ ceph-deploy osd create master001 --data /dev/mapper/VolGroup-lv_data1
&lt;/code>&lt;/pre>
&lt;h3 id="查看当前系统-ceph-服务状态">查看当前系统 ceph 服务状态&lt;a class="td-heading-self-link" href="#%e6%9f%a5%e7%9c%8b%e5%bd%93%e5%89%8d%e7%b3%bb%e7%bb%9f-ceph-%e6%9c%8d%e5%8a%a1%e7%8a%b6%e6%80%81" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>查看当前系统 ceph 服务状态&lt;/p>
&lt;pre>&lt;code>$ systemctl list-units |grep ceph
&lt;/code>&lt;/pre>
&lt;h3 id="重启当前系统-ceph-服务">重启当前系统 ceph 服务&lt;a class="td-heading-self-link" href="#%e9%87%8d%e5%90%af%e5%bd%93%e5%89%8d%e7%b3%bb%e7%bb%9f-ceph-%e6%9c%8d%e5%8a%a1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>重启当前系统 ceph 服务&lt;/p>
&lt;pre>&lt;code>$ systemctl restart ceph*.service ceph*.target
&lt;/code>&lt;/pre>
&lt;h3 id="初始化-ceph-volume">初始化 ceph-volume&lt;a class="td-heading-self-link" href="#%e5%88%9d%e5%a7%8b%e5%8c%96-ceph-volume" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>初始化 ceph-volume&lt;/p>
&lt;pre>&lt;code>$ ceph-volume lvm activate --bluestore --all
&lt;/code>&lt;/pre>
&lt;h3 id="修改-client-keyring-和修复">修改 Client keyring 和修复&lt;a class="td-heading-self-link" href="#%e4%bf%ae%e6%94%b9-client-keyring-%e5%92%8c%e4%bf%ae%e5%a4%8d" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>修改 Client keyring 和修复，首先通过 ceph 命令进行查看：
然后把内容复制到：&lt;/p>
&lt;pre>&lt;code>$ cat /var/lib/ceph/osd/ceph-0/keyring
[osd.0]
key = AQCzhrpeLRK+MhAAbjAgSsE7O81Q+8h8OwA92A==
&lt;/code>&lt;/pre>
&lt;h3 id="pool-开启-enabled">Pool 开启 enabled&lt;a class="td-heading-self-link" href="#pool-%e5%bc%80%e5%90%af-enabled" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>pool 的 enabled 开启：&lt;/p>
&lt;pre>&lt;code>$ ceph -s
cluster:
id: b313ec26-5aa0-4db2-9fb5-a38b207471ee
health: HEALTH_WARN
application not enabled on 3 pool(s)
$ ceph health detail
HEALTH_WARN application not enabled on 3 pool(s); mon master003 is low on available space
POOL_APP_NOT_ENABLED application not enabled on 3 pool(s)
application not enabled on pool 'nextcloud'
application not enabled on pool 'gitlab-ops'
application not enabled on pool 'kafka-ops'
use 'ceph osd pool application enable &amp;lt;pool-name&amp;gt; &amp;lt;app-name&amp;gt;', where &amp;lt;app-name&amp;gt; is 'cephfs', 'rbd', 'rgw', or freeform for custom applications.
MON_DISK_LOW mon master003 is low on available space
mon.master003 has 24% avail
&lt;/code>&lt;/pre>
&lt;p>执行 enabled：&lt;/p>
&lt;pre>&lt;code>$ ceph osd pool application enable nextcloud rbd
$ ceph osd pool application enable gitlab-ops rbd
$ ceph osd pool application enable kafka-ops rbd
&lt;/code>&lt;/pre>
&lt;h3 id="rbd-无法删除">Rbd 无法删除&lt;a class="td-heading-self-link" href="#rbd-%e6%97%a0%e6%b3%95%e5%88%a0%e9%99%a4" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>rbd 无法删除，错误如下：&lt;/p>
&lt;pre>&lt;code>$ rbd rm nextcloud/mysql
2020-05-13 16:27:46.155 7f024bfff700 -1 librbd::image::RemoveRequest: 0x557a7af027a0 check_image_watchers: image has watchers - not removing
Removing image: 0% complete...failed.
rbd: error: image still has watchers
This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 30s for the crashed client to timeout.
$ rbd info nextcloud/mysql
rbd image 'mysql':
size 40 GiB in 10240 objects
order 22 (4 MiB objects)
id: 17e006b8b4567
block_name_prefix: rbd_data.17e006b8b4567
format: 2
features: layering
op_features:
flags:
create_timestamp: Tue Oct 15 10:47:34 2019
&lt;/code>&lt;/pre>
&lt;p>查看当前 rbd 状态：&lt;/p>
&lt;pre>&lt;code>$ rbd status nextcloud/mysql
Watchers:
watcher=10.100.21.95:0/115493307 client.67866 cookie=7
&lt;/code>&lt;/pre>
&lt;p>发现有节点正在挂载，登入到相应机器进行查看：&lt;/p>
&lt;pre>&lt;code>$ rbd showmapped
id pool image snap device
...
3 nextcloud mysql - /dev/rbd3
&lt;/code>&lt;/pre>
&lt;p>取消映射：&lt;/p>
&lt;pre>&lt;code>$ rbd unmap nextcloud/mysql
&lt;/code>&lt;/pre>
&lt;p>重新执行删除操作即可：&lt;/p>
&lt;pre>&lt;code>$ rbd rm nextcloud/mysql
Removing image: 100% complete...done.
&lt;/code>&lt;/pre>
&lt;p>暴力解决方案，直接对其添加黑名单，忽略挂载节点：&lt;/p>
&lt;pre>&lt;code>$ ceph osd blacklist add 10.100.21.95:0/115493307
$ rbd rm nextcloud/mysql
&lt;/code>&lt;/pre>
&lt;h3 id="osd-延迟">OSD 延迟&lt;a class="td-heading-self-link" href="#osd-%e5%bb%b6%e8%bf%9f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>查看是否有 osd 延迟：&lt;/p>
&lt;pre>&lt;code>$ ceph osd perf
osd commit_latency(ms) apply_latency(ms)
2 0 0
1 0 0
0 0 0
&lt;/code>&lt;/pre>
&lt;h3 id="碎片整理">碎片整理&lt;a class="td-heading-self-link" href="#%e7%a2%8e%e7%89%87%e6%95%b4%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>查看碎片：&lt;/p>
&lt;pre>&lt;code>$ xfs_db -c frag -r /dev/mapper/VolGroup-lv_data1
&lt;/code>&lt;/pre>
&lt;p>整理碎片：&lt;/p>
&lt;h3 id="查看通电时长">查看通电时长&lt;a class="td-heading-self-link" href="#%e6%9f%a5%e7%9c%8b%e9%80%9a%e7%94%b5%e6%97%b6%e9%95%bf" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>查看磁盘通电时长：&lt;/p>
&lt;pre>&lt;code>$ smartctl -A /dev/mapper/VolGroup-lv_data1
&lt;/code>&lt;/pre>
&lt;h3 id="修改副本数量">修改副本数量&lt;a class="td-heading-self-link" href="#%e4%bf%ae%e6%94%b9%e5%89%af%e6%9c%ac%e6%95%b0%e9%87%8f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>修改副本数量：&lt;/p>
&lt;pre>&lt;code>$ ceph osd pool set fs_data2 min_size 1
$ ceph osd pool set fs_data2 size 2
&lt;/code>&lt;/pre>
&lt;h3 id="添加--删除-pool">添加 / 删除 pool&lt;a class="td-heading-self-link" href="#%e6%b7%bb%e5%8a%a0--%e5%88%a0%e9%99%a4-pool" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>添加 / 删除 pool：&lt;/p>
&lt;pre>&lt;code>$ ceph fs add_data_pool fs fs_data2
$ ceph fs rm_data_pool fs fs_data2
&lt;/code>&lt;/pre>
&lt;h3 id="osd-数据均衡分布">osd 数据均衡分布&lt;a class="td-heading-self-link" href="#osd-%e6%95%b0%e6%8d%ae%e5%9d%87%e8%a1%a1%e5%88%86%e5%b8%83" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>osd 数据均衡分布：&lt;/p>
&lt;pre>&lt;code>$ ceph balancer status
$ ceph balancer on
$ ceph balancer mode crush-compat
&lt;/code>&lt;/pre>
&lt;h3 id="mds-无法查询">mds 无法查询&lt;a class="td-heading-self-link" href="#mds-%e6%97%a0%e6%b3%95%e6%9f%a5%e8%af%a2" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>mds 无法查询:&lt;/p>
&lt;pre>&lt;code>$ ceph fs status
Error EINVAL: Traceback (most recent call last):
File &amp;quot;/usr/lib64/ceph/mgr/status/module.py&amp;quot;, line 311, in handle_command
return self.handle_fs_status(cmd)
File &amp;quot;/usr/lib64/ceph/mgr/status/module.py&amp;quot;, line 177, in handle_fs_status
mds_versions[metadata.get('ceph_version', &amp;quot;unknown&amp;quot;)].append(info['name'])
AttributeError: 'NoneType' object has no attribute 'get'
$ ceph mds metadata
[
{
&amp;quot;name&amp;quot;: &amp;quot;BJ-YZ-CEPH-94-54&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;BJ-YZ-CEPH-94-53&amp;quot;,
&amp;quot;addr&amp;quot;: &amp;quot;10.100.94.53:6825/4233274463&amp;quot;,
&amp;quot;arch&amp;quot;: &amp;quot;x86_64&amp;quot;,
&amp;quot;ceph_release&amp;quot;: &amp;quot;mimic&amp;quot;,
&amp;quot;ceph_version&amp;quot;: &amp;quot;ceph version 13.2.10 (564bdc4ae87418a232fc901524470e1a0f76d641) mimic (stable)&amp;quot;,
&amp;quot;ceph_version_short&amp;quot;: &amp;quot;13.2.10&amp;quot;,
&amp;quot;cpu&amp;quot;: &amp;quot;Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz&amp;quot;,
&amp;quot;distro&amp;quot;: &amp;quot;centos&amp;quot;,
&amp;quot;distro_description&amp;quot;: &amp;quot;CentOS Linux 7 (Core)&amp;quot;,
&amp;quot;distro_version&amp;quot;: &amp;quot;7&amp;quot;,
&amp;quot;hostname&amp;quot;: &amp;quot;BJ-YZ-CEPH-94-53&amp;quot;,
&amp;quot;kernel_description&amp;quot;: &amp;quot;#1 SMP Sat Dec 10 18:16:05 EST 2016&amp;quot;,
&amp;quot;kernel_version&amp;quot;: &amp;quot;4.4.38-1.el7.elrepo.x86_64&amp;quot;,
&amp;quot;mem_swap_kb&amp;quot;: &amp;quot;67108860&amp;quot;,
&amp;quot;mem_total_kb&amp;quot;: &amp;quot;131914936&amp;quot;,
&amp;quot;os&amp;quot;: &amp;quot;Linux&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;BJ-YZ-CEPH-94-52&amp;quot;,
&amp;quot;addr&amp;quot;: &amp;quot;10.100.94.52:6800/3956121270&amp;quot;,
&amp;quot;arch&amp;quot;: &amp;quot;x86_64&amp;quot;,
&amp;quot;ceph_release&amp;quot;: &amp;quot;mimic&amp;quot;,
&amp;quot;ceph_version&amp;quot;: &amp;quot;ceph version 13.2.10 (564bdc4ae87418a232fc901524470e1a0f76d641) mimic (stable)&amp;quot;,
&amp;quot;ceph_version_short&amp;quot;: &amp;quot;13.2.10&amp;quot;,
&amp;quot;cpu&amp;quot;: &amp;quot;Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz&amp;quot;,
&amp;quot;distro&amp;quot;: &amp;quot;centos&amp;quot;,
&amp;quot;distro_description&amp;quot;: &amp;quot;CentOS Linux 7 (Core)&amp;quot;,
&amp;quot;distro_version&amp;quot;: &amp;quot;7&amp;quot;,
&amp;quot;hostname&amp;quot;: &amp;quot;BJ-YZ-CEPH-94-52&amp;quot;,
&amp;quot;kernel_description&amp;quot;: &amp;quot;#1 SMP Sat Dec 10 18:16:05 EST 2016&amp;quot;,
&amp;quot;kernel_version&amp;quot;: &amp;quot;4.4.38-1.el7.elrepo.x86_64&amp;quot;,
&amp;quot;mem_swap_kb&amp;quot;: &amp;quot;67108860&amp;quot;,
&amp;quot;mem_total_kb&amp;quot;: &amp;quot;131914936&amp;quot;,
&amp;quot;os&amp;quot;: &amp;quot;Linux&amp;quot;
}
]
&lt;/code>&lt;/pre>
&lt;p>重启 mds 解决。&lt;/p>
&lt;h3 id="cephfs-显示状态正常但无法写入数据">cephfs 显示状态正常但无法写入数据&lt;a class="td-heading-self-link" href="#cephfs-%e6%98%be%e7%a4%ba%e7%8a%b6%e6%80%81%e6%ad%a3%e5%b8%b8%e4%bd%86%e6%97%a0%e6%b3%95%e5%86%99%e5%85%a5%e6%95%b0%e6%8d%ae" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>cephfs 显示正常无法使用，一般是有异常 client 导致的，首先查找 mds 是否存在链接，尝试删除链接解决：&lt;/p>
&lt;pre>&lt;code>$ ceph tell mds.BJ-YZ-CEPH-94-52 session ls
$ ceph tell mds.BJ-YZ-CEPH-94-52 session evict id=834283
&lt;/code>&lt;/pre>
&lt;p>每一个 mds 的 id 号不通用，不能跨节点删除。&lt;/p>
&lt;h3 id="fs-增加-mds">fs 增加 mds&lt;a class="td-heading-self-link" href="#fs-%e5%a2%9e%e5%8a%a0-mds" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>fs 增加 mds:&lt;/p>
&lt;pre>&lt;code>$ ceph fs set fs max_mds 2
&lt;/code>&lt;/pre>
&lt;h3 id="mon-时区异常">mon 时区异常&lt;a class="td-heading-self-link" href="#mon-%e6%97%b6%e5%8c%ba%e5%bc%82%e5%b8%b8" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>mon 因为时区有部分异常导致报错如下：&lt;/p>
&lt;pre>&lt;code>$ ceph -s
cluster:
id: 2f77b028-ed2a-4010-9b79-90fd3052afc6
health: HEALTH_WARN
9 slow ops, oldest one blocked for 211643 sec, daemons [mon.BJ-YZ-CEPH-94-53,mon.BJ-YZ-CEPH-94-54] have slow ops.
services:
mon: 3 daemons, quorum BJ-YZ-CEPH-94-52,BJ-YZ-CEPH-94-53,BJ-YZ-CEPH-94-54
mgr: BJ-YZ-CEPH-94-52(active), standbys: BJ-YZ-CEPH-94-54, BJ-YZ-CEPH-94-53
mds: fs-2/2/2 up {0=BJ-YZ-CEPH-94-52=up:active,1=BJ-YZ-CEPH-94-53=up:active}, 1 up:standby-replay
osd: 36 osds: 36 up, 36 in
data:
pools: 7 pools, 1152 pgs
objects: 37.66 M objects, 67 TiB
usage: 136 TiB used, 126 TiB / 262 TiB avail
pgs: 1148 active+clean
4 active+clean+scrubbing+deep
io:
client: 13 KiB/s rd, 27 MiB/s wr, 2 op/s rd, 19 op/s wr
&lt;/code>&lt;/pre>
&lt;p>配置 npt sever：&lt;/p>
&lt;pre>&lt;code>$ systemctl status ntpd
$ systemctl start ntpd
&lt;/code>&lt;/pre>
&lt;p>重启异常的 mon.targe 解决：&lt;/p>
&lt;pre>&lt;code>$ systemctl status ceph-mon.target
$ systemctl restart ceph-mon.target
&lt;/code>&lt;/pre>
&lt;h3 id="1-mdss-report-slow-requests">1 MDSs report slow requests&lt;a class="td-heading-self-link" href="#1-mdss-report-slow-requests" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>报错如下：&lt;/p>
&lt;pre>&lt;code>$ ceph -s
cluster:
id: b313ec26-5aa0-4db2-9fb5-a38b207471ee
health: HEALTH_WARN
1 MDSs report slow requests
Reduced data availability: 38 pgs inactive
Degraded data redundancy: 122006/1192166 objects degraded (10.234%), 102 pgs degraded, 116 pgs undersized
101 slow ops, oldest one blocked for 81045 sec, daemons [osd.1,osd.2] have slow ops.
&lt;/code>&lt;/pre>
&lt;p>重启 mon 即可解决：&lt;/p>
&lt;pre>&lt;code>$ systemctl restart ceph-mon.target
&lt;/code>&lt;/pre>
&lt;p>如果无法解决需要重启 mds 解决：&lt;/p>
&lt;pre>&lt;code>$ systemctl restart ceph-mds@${HOSTNAME}
&lt;/code>&lt;/pre>
&lt;h3 id="reduced-data-availability-38-pgs-inactive">Reduced data availability: 38 pgs inactive&lt;a class="td-heading-self-link" href="#reduced-data-availability-38-pgs-inactive" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>报错如下：&lt;strong>&lt;a href="https://zhuanlan.zhihu.com/p/74323736">https://zhuanlan.zhihu.com/p/74323736&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ ceph -s
cluster:
id: b313ec26-5aa0-4db2-9fb5-a38b207471ee
health: HEALTH_WARN
1 MDSs report slow requests
Reduced data availability: 38 pgs inactive
145 slow ops, oldest one blocked for 184238 sec, daemons [osd.1,osd.2] have slow ops.
services:
mon: 3 daemons, quorum master001,master002,master003
mgr: master001(active), standbys: master002, master003
mds: kubernetes-2/2/2 up {0=master001=up:active,1=master002=up:active}, 1 up:standby
osd: 3 osds: 3 up, 3 in
rgw: 1 daemon active
data:
pools: 9 pools, 244 pgs
objects: 535.1 k objects, 177 GiB
usage: 470 GiB used, 4.1 TiB / 4.6 TiB avail
pgs: 15.574% pgs unknown
206 active+clean
38 unknown
io:
client: 35 KiB/s wr, 0 op/s rd, 2 op/s wr
&lt;/code>&lt;/pre>
&lt;p>此问题属于 pg 丢失数据并且无法自动回复造成的。解决办法是清除 pg 数据让其自动修复，但这样可能会造成数据丢失（如果 size 为 1 则肯定丢失数据）
首先查看异常的 pg：
然后执行 query 查看信息：&lt;/p>
&lt;pre>&lt;code>$ ceph pg 1.6e query
Error ENOENT: i don't have pgid 1.6e
&lt;/code>&lt;/pre>
&lt;p>上述无法查到 pg，通过如下命令查看异常的 pg：&lt;/p>
&lt;pre>&lt;code>$ ceph pg dump_stuck unclean
ok
PG_STAT STATE UP UP_PRIMARY ACTING ACTING_PRIMARY
1.74 unknown [] -1 [] -1
1.70 unknown [] -1 [] -1
1.6a unknown [] -1 [] -1
1.2d unknown [] -1 [] -1
1.20 unknown [] -1 [] -1
1.1e unknown [] -1 [] -1
1.1c unknown [] -1 [] -1
1.17 unknown [] -1 [] -1
1.9 unknown [] -1 [] -1
1.29 unknown [] -1 [] -1
1.56 unknown [] -1 [] -1
1.72 unknown [] -1 [] -1
1.45 unknown [] -1 [] -1
1.4e unknown [] -1 [] -1
1.46 unknown [] -1 [] -1
1.22 unknown [] -1 [] -1
1.53 unknown [] -1 [] -1
1.59 unknown [] -1 [] -1
1.24 unknown [] -1 [] -1
1.55 unknown [] -1 [] -1
1.3f unknown [] -1 [] -1
1.38 unknown [] -1 [] -1
1.a unknown [] -1 [] -1
1.7 unknown [] -1 [] -1
1.34 unknown [] -1 [] -1
1.64 unknown [] -1 [] -1
1.6 unknown [] -1 [] -1
1.32 unknown [] -1 [] -1
1.4 unknown [] -1 [] -1
1.2e unknown [] -1 [] -1
1.31 unknown [] -1 [] -1
1.5e unknown [] -1 [] -1
1.0 unknown [] -1 [] -1
1.42 unknown [] -1 [] -1
1.15 unknown [] -1 [] -1
1.6e unknown [] -1 [] -1
1.41 unknown [] -1 [] -1
1.10 unknown [] -1 [] -1
&lt;/code>&lt;/pre>
&lt;p>执行如下命令强制清除 pg 的数据：&lt;strong>&lt;a href="https://docs.ceph.com/docs/mimic/rados/troubleshooting/troubleshooting-pg/">https://docs.ceph.com/docs/mimic/rados/troubleshooting/troubleshooting-pg/&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ ceph osd force-create-pg 1.74 --yes-i-really-mean-it
# 批量执行
# ceph pg dump_stuck unclean|awk '{print $1}'|xargs -i ceph osd force-create-pg {} --yes-i-really-mean-it
&lt;/code>&lt;/pre>
&lt;p>执行完成后即可恢复。&lt;/p>
&lt;h3 id="1-clients-failing-to-respond-to-capability-release">1 clients failing to respond to capability release&lt;a class="td-heading-self-link" href="#1-clients-failing-to-respond-to-capability-release" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>报错如下：&lt;/p>
&lt;pre>&lt;code>$ ceph health detail
HEALTH_WARN 1 clients failing to respond to capability release
MDS_CLIENT_LATE_RELEASE 1 clients failing to respond to capability release
mdsmaster001(mds.0): Client master003.k8s.shileizcc-ops.com: failing to respond to capability release client_id: 284951
&lt;/code>&lt;/pre>
&lt;p>清除次 ID 即可：&lt;strong>&lt;a href="https://blog.csdn.net/zuoyang1990/article/details/98530070">https://blog.csdn.net/zuoyang1990/article/details/98530070&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ ceph daemon mds.master003 session ls|grep 284951
$ ceph tell mds.master003 session evict id=284951
&lt;/code>&lt;/pre>
&lt;p>如果报错如下：&lt;/p>
&lt;pre>&lt;code>$ ceph tell mds.master003 session evict id=284951
2020-08-13 10:45:03.869 7f271b7fe700 0 client.306366 ms_handle_reset on 10.100.21.95:6800/1646216103
2020-08-13 10:45:03.881 7f2730ff9700 0 client.316415 ms_handle_reset on 10.100.21.95:6800/1646216103
Error EAGAIN: MDS is replaying log
&lt;/code>&lt;/pre>
&lt;p>需要到 mds.0 节点执行，否则无法找到次 client。&lt;/p>
&lt;h3 id="内核优化">内核优化&lt;a class="td-heading-self-link" href="#%e5%86%85%e6%a0%b8%e4%bc%98%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>内核优化：&lt;strong>&lt;a href="https://blog.csdn.net/fuzhongfaya/article/details/80932766">https://blog.csdn.net/fuzhongfaya/article/details/80932766&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;8192&amp;quot; &amp;gt; /sys/block/sda/queue/read_ahead_kb
$ echo &amp;quot;vm.swappiness = 0&amp;quot; | tee -a /etc/sysctl.conf
$ sysctl -p
$ echo &amp;quot;deadline&amp;quot; &amp;gt; /sys/block/sd[x]/queue/scheduler
# ssd
# echo &amp;quot;noop&amp;quot; &amp;gt; /sys/block/sd[x]/queue/scheduler
&lt;/code>&lt;/pre>
&lt;p>swap 最好是直接关闭，配置内存参数在一定程度上不会生效。
配置文件
40 核心 128 GB 配置文件：&lt;/p>
&lt;pre>&lt;code>[global]
fsid = 2f77b028-ed2a-4010-9b79-90fd3052afc6
mon_initial_members = BJ-YZ-CEPH-94-52, BJ-YZ-CEPH-94-53, BJ-YZ-CEPH-94-54
mon_host = 10.100.94.52,10.100.94.53,10.100.94.54
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
public network = 10.100.94.0/24
cluster network = 10.100.94.0/24
[mon.a]
host = BJ-YZ-CEPH-94-52
mon addr = 10.100.94.52:6789
[mon.b]
host = BJ-YZ-CEPH-94-53
mon addr = 10.100.94.53:6789
[mon.c]
host = BJ-YZ-CEPH-94-54
mon addr = 10.100.94.54:6789
[mon]
mon data = /var/lib/ceph/mon/ceph-$id
# monitor 间的 clock drift，默认值 0.05
mon clock drift allowed = 1
# 向 monitor 报告 down 的最小 OSD 数，默认值 1
mon osd min down reporters = 1
# 标记一个OSD状态为down和out之前ceph等待的秒数，默认值300
mon osd down out interval = 600
mon_allow_pool_delete = true
[osd]
# osd 数据路径
osd data = /var/lib/ceph/osd/ceph-$id
# 默认 pool pg,pgp 数量
osd pool default pg num = 1200
osd pool default pgp num = 1200
# osd 的 journal 写日志时的大小默认 5120
osd journal size = 20000
# 格式化文件系统类型
osd mkfs type = xfs
# 格式化文件系统时附加参数
osd mkfs options xfs = -f
# 为 XATTRS 使用 object map，EXT4 文件系统时使用，XFS 或者 btrf 也可以使用，默认 false
filestore xattr use omap = true
# 从日志到数据盘最小同步间隔(seconds)，默认值 0.1
filestore min sync interval = 10
# 从日志到数据盘最大同步间隔(seconds)，默认值 5
filestore max sync interval = 15
# 数据盘最大接受的操作数，默认值 500
filestore queue max ops = 25000
# 数据盘能够 commit 的最大字节数(bytes)，默认值 100
filestore queue max bytes = 10485760
# 数据盘能够 commit 的操作数，500
filestore queue committing max ops = 5000
# 数据盘能够 commit 的最大字节数(bytes)，默认值 100
filestore queue committing max bytes = 10485760000
# 前一个子目录分裂成子目录中的文件的最大数量，默认值 2
filestore split multiple = 8
# 前一个子类目录中的文件合并到父类的最小数量，默认值10
filestore merge threshold = 40
# 对象文件句柄缓存大小，默认值 128
filestore fd cache size = 1024
# 并发文件系统操作数，默认值 2
filestore op threads = 32
# journal 一次性写入的最大字节数(bytes)，默认值 1048560
journal max write bytes = 1073714824
# journal一次性写入的最大记录数，默认值 100
journal max write entries = 10000
# journal一次性最大在队列中的操作数，默认值 50
journal queue max ops = 50000
# journal一次性最大在队列中的字节数(bytes)，默认值 33554432
journal queue max bytes = 10485760000
# # OSD一次可写入的最大值(MB), 默认 90
osd max write size = 512
# 客户端允许在内存中的最大数据(bytes), 默认值100
osd client message size cap = 2147483648
# 在 Deep Scrub 时候允许读取的字节数(bytes), 默认值524288
osd deep scrub stride = 1310720
# 并发文件系统操作数, 默认值 2
osd op threads = 32
# OSD 密集型操作例如恢复和 Scrubbing 时的线程, 默认值1
osd disk threads = 10
# 保留 OSD Map 的缓存(MB), 默认 500
osd map cache size = 10240
# OSD 进程在内存中的 OSD Map 缓存(MB), 默认 50
osd map cache bl size = 1280
# 默认值rw,noatime,inode64, Ceph OSD xfs Mount选项
osd mount options xfs = &amp;quot;rw,noexec,nodev,noatime,nodiratime,nobarrier&amp;quot;
# 恢复操作优先级，取值 1-63，值越高占用资源越高, 默认值 10
osd recovery op priority = 20
# 同一时间内活跃的恢复请求数, 默认值 15
osd recovery max active = 15
# 一个 OSD 允许的最大 backfills 数, 默认值 10
osd max backfills = 10
# 开启严格队列降级操作
osd op queue cut off = high
osd_deep_scrub_large_omap_object_key_threshold = 800000
osd_deep_scrub_large_omap_object_value_sum_threshold = 10737418240
[mds]
# mds 缓存大小设置 60GB
mds cache memory limit = 62212254726
# 超时时间默认 60 秒
mds_revoke_cap_timeout = 360
mds log max segments = 51200
mds log max expiring = 51200
mds_beacon_grace = 300
# 对目录碎片大小的硬限制 默认 100000
# https://docs.ceph.com/docs/master/cephfs/dirfrags/
mds_bal_fragment_size_max = 500000
## 官方配置 https://ceph.readthedocs.io/en/latest/cephfs/mds-config-ref/
[client]
# RBD缓存, 默认 true
rbd cache = true
# RBD缓存大小(bytes), 默认 335544320（320M）
rbd cache size = 268435456
# 缓存为 write-back 时允许的最大 dirty 字节数(bytes)，如果为0，使用 write-through，默认值为 25165824
rbd cache max dirty = 134217728
# 在被刷新到存储盘前 dirty 数据存在缓存的时间(seconds), 默认值为 1
rbd cache max dirty age = 5
client_try_dentry_invalidate = false
[mgr]
mgr modules = dashboard
# 华为云调优指南 https://support.huaweicloud.com/tngg-kunpengsdss/kunpengcephobject_05_0008.html
# https://poph163.com/2020/02/18/ceph-crushmap%E4%B8%8E%E8%B0%83%E4%BC%98/
&lt;/code>&lt;/pre>
&lt;h3 id="full-osd">full osd&lt;a class="td-heading-self-link" href="#full-osd" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>full osd 每个 osd 已经写满上限:&lt;strong>&lt;a href="https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space">https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ ceph osd dump | grep full_ratio
full_ratio 0.95
backfillfull_ratio 0.9
nearfull_ratio 0.85
&lt;/code>&lt;/pre>
&lt;p>集群状态:&lt;/p>
&lt;pre>&lt;code>$ ceph -s
cluster:
id: 2f77b028-ed2a-4010-9b79-90fd3052afc6
health: HEALTH_ERR
2 backfillfull osd(s)
1 full osd(s)
2 nearfull osd(s)
7 pool(s) full
&lt;/code>&lt;/pre>
&lt;p>执行 osd 磁盘状态时，如果已经有超过 95% 使用率时则会报错 full osd 则会造成 cluster 无法正常使用：&lt;/p>
&lt;pre>&lt;code>$ ceph osd df
ID CLASS WEIGHT REWEIGHT SIZE USE DATA OMAP META AVAIL %USE VAR PGS
0 hdd 7.27689 1.00000 7.3 TiB 4.7 TiB 4.7 TiB 918 MiB 9.1 GiB 2.5 TiB 65.15 0.84 68
1 hdd 7.27689 1.00000 7.3 TiB 6.1 TiB 6.1 TiB 327 MiB 11 GiB 1.2 TiB 84.07 1.09 67
2 hdd 7.27689 1.00000 7.3 TiB 4.3 TiB 4.3 TiB 924 MiB 8.4 GiB 2.9 TiB 59.70 0.77 67
3 hdd 7.27689 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 807 MiB 9.8 GiB 2.1 TiB 70.57 0.91 66
4 hdd 7.27689 1.00000 7.3 TiB 6.7 TiB 6.7 TiB 770 MiB 13 GiB 583 GiB 92.18 1.19 66
5 hdd 7.27689 1.00000 7.3 TiB 5.5 TiB 5.5 TiB 623 MiB 10 GiB 1.8 TiB 75.87 0.98 66
6 hdd 7.27689 1.00000 7.3 TiB 5.7 TiB 5.7 TiB 602 MiB 11 GiB 1.6 TiB 78.67 1.02 64
7 hdd 7.27689 1.00000 7.3 TiB 5.3 TiB 5.3 TiB 1.1 GiB 10 GiB 1.9 TiB 73.35 0.95 65
8 hdd 7.27689 1.00000 7.3 TiB 5.9 TiB 5.9 TiB 498 MiB 11 GiB 1.4 TiB 81.29 1.05 68
9 hdd 7.27689 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 1.1 GiB 9.8 GiB 2.1 TiB 70.59 0.91 65
10 hdd 7.27689 1.00000 7.3 TiB 6.3 TiB 6.3 TiB 297 MiB 12 GiB 985 GiB 86.78 1.12 61
11 hdd 7.27689 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 923 MiB 9.7 GiB 2.1 TiB 70.56 0.91 67
12 hdd 7.27689 1.00000 7.3 TiB 5.9 TiB 5.9 TiB 203 MiB 11 GiB 1.4 TiB 81.39 1.05 65
13 hdd 7.27689 1.00000 7.3 TiB 5.3 TiB 5.3 TiB 799 MiB 10 GiB 1.9 TiB 73.29 0.95 66
14 hdd 7.27689 1.00000 7.3 TiB 4.9 TiB 4.9 TiB 873 MiB 9.4 GiB 2.3 TiB 67.77 0.88 71
15 hdd 0.29999 1.00000 7.3 TiB 6.9 TiB 6.9 TiB 191 MiB 13 GiB 387 GiB 94.81 1.23 39
16 hdd 7.27689 1.00000 7.3 TiB 5.5 TiB 5.5 TiB 548 MiB 11 GiB 1.8 TiB 75.91 0.98 69
17 hdd 7.27689 1.00000 7.3 TiB 6.7 TiB 6.7 TiB 806 MiB 13 GiB 581 GiB 92.20 1.20 66
18 hdd 7.27689 1.00000 7.3 TiB 4.5 TiB 4.5 TiB 1.4 GiB 8.5 GiB 2.7 TiB 62.43 0.81 66
19 hdd 7.27689 1.00000 7.3 TiB 5.3 TiB 5.3 TiB 1.4 GiB 10 GiB 1.9 TiB 73.28 0.95 65
20 hdd 7.27689 1.00000 7.3 TiB 5.5 TiB 5.5 TiB 705 MiB 11 GiB 1.8 TiB 75.91 0.98 64
21 hdd 7.27689 1.00000 7.3 TiB 6.1 TiB 6.1 TiB 911 MiB 11 GiB 1.2 TiB 84.11 1.09 62
22 hdd 7.27689 1.00000 7.3 TiB 6.1 TiB 6.1 TiB 301 MiB 11 GiB 1.2 TiB 84.03 1.09 66
23 hdd 7.27689 1.00000 7.3 TiB 5.5 TiB 5.5 TiB 401 MiB 9.8 GiB 1.7 TiB 75.96 0.98 67
24 hdd 7.27689 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 1.3 GiB 9.6 GiB 2.1 TiB 70.58 0.91 63
25 hdd 7.27689 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 1.1 GiB 9.7 GiB 2.1 TiB 70.56 0.91 65
26 hdd 7.27689 1.00000 7.3 TiB 5.3 TiB 5.3 TiB 730 MiB 10 GiB 1.9 TiB 73.32 0.95 68
27 hdd 7.27689 1.00000 7.3 TiB 6.1 TiB 6.1 TiB 818 MiB 12 GiB 1.2 TiB 84.08 1.09 62
28 hdd 7.27689 1.00000 7.3 TiB 4.9 TiB 4.9 TiB 587 MiB 9.3 GiB 2.3 TiB 67.84 0.88 68
29 hdd 7.27689 1.00000 7.3 TiB 6.1 TiB 6.1 TiB 215 MiB 11 GiB 1.2 TiB 84.09 1.09 66
30 hdd 7.27689 1.00000 7.3 TiB 6.1 TiB 6.1 TiB 690 MiB 12 GiB 1.2 TiB 84.15 1.09 64
31 hdd 7.27689 1.00000 7.3 TiB 5.5 TiB 5.5 TiB 1020 MiB 10 GiB 1.8 TiB 75.94 0.98 64
32 hdd 7.27689 1.00000 7.3 TiB 6.5 TiB 6.5 TiB 616 MiB 12 GiB 786 GiB 89.45 1.16 66
33 hdd 7.27689 1.00000 7.3 TiB 4.9 TiB 4.9 TiB 622 MiB 8.9 GiB 2.3 TiB 67.84 0.88 66
34 hdd 7.27689 1.00000 7.3 TiB 5.7 TiB 5.7 TiB 102 MiB 11 GiB 1.6 TiB 78.56 1.02 65
35 hdd 7.27689 1.00000 7.3 TiB 5.9 TiB 5.9 TiB 723 MiB 11 GiB 1.4 TiB 81.31 1.05 63
TOTAL 262 TiB 202 TiB 202 TiB 25 GiB 381 GiB 60 TiB 77.15
&lt;/code>&lt;/pre>
&lt;p>可以手动修改权重解决:&lt;/p>
&lt;pre>&lt;code>$ ceph osd crush reweight osd.4 0.3
&lt;/code>&lt;/pre>
&lt;h3 id="pg-均衡">pg 均衡&lt;a class="td-heading-self-link" href="#pg-%e5%9d%87%e8%a1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>pg 在默认分配有不合理的地方。&lt;strong>&lt;a href="https://cloud.tencent.com/developer/article/1664655">https://cloud.tencent.com/developer/article/1664655&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ ceph osd df tree | awk '/osd\./{print $NF&amp;quot; &amp;quot;$(NF-1)&amp;quot; &amp;quot;$(NF-3) }'
osd.0 89 71.20
osd.1 38 94.80
osd.2 92 68.44
osd.3 92 72.36
osd.4 28 76.86
osd.5 64 81.37
osd.6 62 87.90
osd.7 89 78.78
osd.8 52 86.18
osd.9 89 75.44
osd.10 37 96.33
osd.11 102 75.26
osd.12 33 91.41
osd.13 34 95.98
osd.14 59 84.97
osd.15 20 70.92
osd.16 113 89.46
osd.17 30 77.12
osd.18 124 77.11
osd.19 44 95.23
osd.20 65 84.63
osd.21 98 96.71
osd.22 34 95.93
osd.23 62 84.56
osd.24 110 76.63
osd.25 64 82.32
osd.26 59 88.26
osd.27 38 95.83
osd.28 105 79.19
osd.29 36 94.94
osd.30 94 90.79
osd.31 91 81.74
osd.32 12 42.44
osd.33 94 81.32
osd.34 46 86.51
osd.35 37 92.68
&lt;/code>&lt;/pre>
&lt;p>reweight-by-pg 按归置组分布情况调整 OSD 的权重:&lt;/p>
&lt;pre>&lt;code>$ ceph osd reweight-by-pg
moved 0 / 2336 (0%)
avg 64.8889
stddev 58.677 -&amp;gt; 58.677 (expected baseline 7.9427)
min osd.1 with 0 -&amp;gt; 0 pgs (0 -&amp;gt; 0 * mean)
max osd.18 with 168 -&amp;gt; 168 pgs (2.58904 -&amp;gt; 2.58904 * mean)
oload 120
max_change 0.05
max_change_osds 4
average_utilization 18.2677
overload_utilization 21.9212
osd.19 weight 1.0000 -&amp;gt; 0.9500
osd.1 weight 1.0000 -&amp;gt; 0.9500
osd.27 weight 1.0000 -&amp;gt; 0.9500
osd.10 weight 1.0000 -&amp;gt; 0.9500
&lt;/code>&lt;/pre>
&lt;p>reweight-by-utilization 按利用率调整 OSD 的权重:&lt;/p>
&lt;pre>&lt;code>$ ceph osd reweight-by-pg
moved 0 / 2336 (0%)
avg 64.8889
stddev 58.677 -&amp;gt; 58.677 (expected baseline 7.9427)
min osd.1 with 0 -&amp;gt; 0 pgs (0 -&amp;gt; 0 * mean)
max osd.18 with 168 -&amp;gt; 168 pgs (2.58904 -&amp;gt; 2.58904 * mean)
oload 120
max_change 0.05
max_change_osds 4
average_utilization 18.2677
overload_utilization 21.9212
osd.19 weight 1.0000 -&amp;gt; 0.9500
osd.1 weight 1.0000 -&amp;gt; 0.9500
osd.27 weight 1.0000 -&amp;gt; 0.9500
osd.10 weight 1.0000 -&amp;gt; 0.9500
&lt;/code>&lt;/pre>
&lt;p>调整写入权重：&lt;/p>
&lt;pre>&lt;code>$ ceph osd reweight osd.35 0.001
&lt;/code>&lt;/pre>
&lt;p>查看当前 osd 信息：&lt;/p>
&lt;pre>&lt;code>$ ceph osd df
ID CLASS WEIGHT REWEIGHT SIZE USE DATA OMAP META AVAIL %USE VAR PGS
0 hdd 7.27689 1.00000 7.3 TiB 5.2 TiB 5.2 TiB 1.0 GiB 9.4 GiB 2.0 TiB 71.96 0.86 39
1 hdd 0.00999 0.90002 7.3 TiB 6.9 TiB 6.9 TiB 604 MiB 12 GiB 382 GiB 94.88 1.13 37
2 hdd 7.27689 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 1.2 GiB 8.8 GiB 2.2 TiB 69.55 0.83 34
3 hdd 7.27689 1.00000 7.3 TiB 5.3 TiB 5.3 TiB 812 MiB 9.9 GiB 2.0 TiB 73.15 0.87 34
4 hdd 0.29999 1.00000 7.3 TiB 5.6 TiB 5.6 TiB 185 MiB 12 GiB 1.7 TiB 77.01 0.92 26
5 hdd 3.00000 1.00000 7.3 TiB 6.0 TiB 5.9 TiB 443 MiB 11 GiB 1.3 TiB 81.90 0.98 36
6 hdd 3.00000 1.00000 7.3 TiB 6.5 TiB 6.5 TiB 499 MiB 11 GiB 809 GiB 89.14 1.06 38
7 hdd 7.27689 1.00000 7.3 TiB 5.8 TiB 5.8 TiB 1.2 GiB 11 GiB 1.4 TiB 80.10 0.96 43
8 hdd 3.00000 1.00000 7.3 TiB 6.3 TiB 6.3 TiB 502 MiB 11 GiB 992 GiB 86.69 1.03 36
9 hdd 7.27689 1.00000 7.3 TiB 5.6 TiB 5.6 TiB 1.5 GiB 9.8 GiB 1.7 TiB 76.57 0.91 42
10 hdd 0.00999 0.00099 7.3 TiB 7.0 TiB 7.0 TiB 295 MiB 12 GiB 267 GiB 96.41 1.15 37
11 hdd 7.27689 1.00000 7.3 TiB 5.5 TiB 5.5 TiB 1.2 GiB 9.8 GiB 1.7 TiB 76.13 0.91 37
12 hdd 0.00999 1.00000 7.3 TiB 6.7 TiB 6.6 TiB 95 MiB 12 GiB 635 GiB 91.48 1.09 32
13 hdd 0.00999 1.00000 7.3 TiB 7.0 TiB 7.0 TiB 584 MiB 12 GiB 315 GiB 95.78 1.14 34
14 hdd 3.00000 1.00000 7.3 TiB 6.2 TiB 6.2 TiB 974 MiB 11 GiB 1.0 TiB 85.86 1.02 40
15 hdd 0.00999 1.00000 7.3 TiB 5.1 TiB 5.1 TiB 116 KiB 10 GiB 2.2 TiB 70.43 0.84 20
16 hdd 7.27689 1.00000 7.3 TiB 6.6 TiB 6.6 TiB 1.2 GiB 11 GiB 697 GiB 90.64 1.08 43
17 hdd 0.29999 1.00000 7.3 TiB 5.6 TiB 5.6 TiB 40 KiB 12 GiB 1.7 TiB 76.75 0.92 26
18 hdd 7.27689 1.00000 7.3 TiB 5.7 TiB 5.7 TiB 1.9 GiB 9.3 GiB 1.6 TiB 78.01 0.93 53
19 hdd 0.00999 0.00099 7.3 TiB 6.9 TiB 6.9 TiB 1.5 GiB 13 GiB 371 GiB 95.02 1.13 40
20 hdd 3.00000 1.00000 7.3 TiB 6.2 TiB 6.2 TiB 744 MiB 12 GiB 1.0 TiB 85.86 1.02 37
21 hdd 7.27689 0.00099 7.3 TiB 7.0 TiB 7.0 TiB 913 MiB 12 GiB 239 GiB 96.79 1.15 40
22 hdd 0.00999 0.00099 7.3 TiB 7.0 TiB 7.0 TiB 283 MiB 12 GiB 298 GiB 96.00 1.14 34
23 hdd 3.00000 1.00000 7.3 TiB 6.2 TiB 6.2 TiB 515 MiB 11 GiB 1.1 TiB 85.30 1.02 35
24 hdd 7.27689 1.00000 7.3 TiB 5.6 TiB 5.6 TiB 1.4 GiB 9.8 GiB 1.6 TiB 77.63 0.93 42
25 hdd 3.00000 1.00000 7.3 TiB 6.0 TiB 6.0 TiB 1.2 GiB 10 GiB 1.3 TiB 82.66 0.99 40
26 hdd 2.00000 1.00000 7.3 TiB 6.5 TiB 6.5 TiB 737 MiB 11 GiB 823 GiB 88.95 1.06 36
27 hdd 0.00999 0.00099 7.3 TiB 7.0 TiB 6.9 TiB 822 MiB 12 GiB 327 GiB 95.61 1.14 37
28 hdd 7.27689 1.00000 7.3 TiB 5.8 TiB 5.8 TiB 859 MiB 10 GiB 1.4 TiB 80.23 0.96 40
29 hdd 0.00999 0.00099 7.3 TiB 6.9 TiB 6.9 TiB 215 MiB 12 GiB 371 GiB 95.02 1.13 36
30 hdd 7.27689 1.00000 7.3 TiB 6.7 TiB 6.7 TiB 1.0 GiB 12 GiB 607 GiB 91.85 1.10 47
31 hdd 7.27689 1.00000 7.3 TiB 6.0 TiB 6.0 TiB 1.2 GiB 10 GiB 1.3 TiB 82.81 0.99 41
32 hdd 0.29999 1.00000 7.3 TiB 3.0 TiB 3.0 TiB 32 KiB 7.1 GiB 4.3 TiB 41.47 0.49 10
33 hdd 7.27689 1.00000 7.3 TiB 6.0 TiB 6.0 TiB 827 MiB 9.7 GiB 1.3 TiB 82.06 0.98 41
34 hdd 2.00000 1.00000 7.3 TiB 6.3 TiB 6.3 TiB 308 MiB 11 GiB 976 GiB 86.90 1.04 33
35 hdd 0.00999 0.00099 7.3 TiB 6.7 TiB 6.7 TiB 613 MiB 12 GiB 540 GiB 92.75 1.11 36
TOTAL 262 TiB 220 TiB 219 TiB 27 GiB 391 GiB 42 TiB 83.87
MIN/MAX VAR: 0.49/1.15 STDDEV: 10.62
&lt;/code>&lt;/pre>
&lt;p>删除 Cephfs
关闭所有 mds 服务, 需要登入服务器手动关闭:&lt;/p>
&lt;pre>&lt;code>$ systemctl stop ceph-mds@${HOSTNAME}
&lt;/code>&lt;/pre>
&lt;p>删除所需 fs:&lt;/p>
&lt;pre>&lt;code>$ ceph fs ls
$ ceph fs rm data --yes-i-really-mean-it
&lt;/code>&lt;/pre>
&lt;p>SSD 使用
查看当前 OSD 状态: (相关文档:&lt;strong>&lt;a href="https://blog.csdn.net/kozazyh/article/details/79904219">https://blog.csdn.net/kozazyh/article/details/79904219&lt;/a>&lt;/strong>)&lt;/p>
&lt;pre>&lt;code>$ ceph osd crush class ls
[
&amp;quot;ssd&amp;quot;
]
&lt;/code>&lt;/pre>
&lt;p>如果使用的 SSD 标识错误，请自定义修改，命令如下, 移除 osd 1 ~ 3 的标识:&lt;/p>
&lt;pre>&lt;code>$ for i in 0 1 2;do ceph osd crush rm-device-class osd.$i;done
&lt;/code>&lt;/pre>
&lt;p>设置 1 ~ 3 标识为 ssd：&lt;/p>
&lt;pre>&lt;code>$ for i in 0 1 2;do ceph osd crush set-device-class ssd osd.$i;done
&lt;/code>&lt;/pre>
&lt;p>创建一个 crush rule:&lt;/p>
&lt;pre>&lt;code>$ ceph osd crush rule create-replicated rule-ssd default host ssd
$ ceph osd crush rule ls
&lt;/code>&lt;/pre>
&lt;p>然后创建 pool 时附带 rule 的名称：&lt;/p>
&lt;pre>&lt;code>$ ceph osd pool create fs_data 96 rule-ssd
$ ceph osd pool create fs_metadata 16 rule-ssd
$ ceph fs new fs fs_data fs_metadata
&lt;/code>&lt;/pre>
&lt;h3 id="crushmap-查看">crushmap 查看&lt;a class="td-heading-self-link" href="#crushmap-%e6%9f%a5%e7%9c%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>执行命令如下:&lt;/p>
&lt;pre>&lt;code>$ ceph osd getcrushmap -o crushmap
$ crushtool -d crushmap -o crushmap
$ cat crushmap
&lt;/code>&lt;/pre>
&lt;h3 id="3-monitors-have-not-enabled-msgr2">3 monitors have not enabled msgr2&lt;a class="td-heading-self-link" href="#3-monitors-have-not-enabled-msgr2" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>解决如下：&lt;/p>
&lt;pre>&lt;code>$ ceph mon enable-msgr2
&lt;/code>&lt;/pre>
&lt;h3 id="2-daemons-have-recently-crashed">2 daemons have recently crashed&lt;a class="td-heading-self-link" href="#2-daemons-have-recently-crashed" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>解决如下：&lt;strong>&lt;a href="https://blog.csdn.net/QTM_Gitee/article/details/106004435">https://blog.csdn.net/QTM_Gitee/article/details/106004435&lt;/a>&lt;/strong>&lt;/p>
&lt;pre>&lt;code>$ ceph crash ls
$ ceph crash archive-all
&lt;/code>&lt;/pre>
&lt;h3 id="脚注">脚注&lt;a class="td-heading-self-link" href="#%e8%84%9a%e6%b3%a8" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>[1]
&lt;a href="https://zhuanlan.zhihu.com/p/74323736:">https://zhuanlan.zhihu.com/p/74323736:&lt;/a> &lt;em>&lt;a href="https://zhuanlan.zhihu.com/p/74323736">https://zhuanlan.zhihu.com/p/74323736&lt;/a>&lt;/em>
[2]
&lt;a href="https://docs.ceph.com/docs/mimic/rados/troubleshooting/troubleshooting-pg/:">https://docs.ceph.com/docs/mimic/rados/troubleshooting/troubleshooting-pg/:&lt;/a> &lt;em>&lt;a href="https://docs.ceph.com/docs/mimic/rados/troubleshooting/troubleshooting-pg/">https://docs.ceph.com/docs/mimic/rados/troubleshooting/troubleshooting-pg/&lt;/a>&lt;/em>
[3]
&lt;a href="https://blog.csdn.net/zuoyang1990/article/details/98530070:">https://blog.csdn.net/zuoyang1990/article/details/98530070:&lt;/a> &lt;em>&lt;a href="https://blog.csdn.net/zuoyang1990/article/details/98530070">https://blog.csdn.net/zuoyang1990/article/details/98530070&lt;/a>&lt;/em>
[4]
&lt;a href="https://blog.csdn.net/fuzhongfaya/article/details/80932766:">https://blog.csdn.net/fuzhongfaya/article/details/80932766:&lt;/a> &lt;em>&lt;a href="https://blog.csdn.net/fuzhongfaya/article/details/80932766">https://blog.csdn.net/fuzhongfaya/article/details/80932766&lt;/a>&lt;/em>
[5]
&lt;a href="https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space:">https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space:&lt;/a> &lt;em>&lt;a href="https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space">https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space&lt;/a>&lt;/em>
[6]
&lt;a href="https://cloud.tencent.com/developer/article/1664655:">https://cloud.tencent.com/developer/article/1664655:&lt;/a> &lt;em>&lt;a href="https://cloud.tencent.com/developer/article/1664655">https://cloud.tencent.com/developer/article/1664655&lt;/a>&lt;/em>
[7]
&lt;a href="https://blog.csdn.net/kozazyh/article/details/79904219:">https://blog.csdn.net/kozazyh/article/details/79904219:&lt;/a> &lt;em>&lt;a href="https://blog.csdn.net/kozazyh/article/details/79904219">https://blog.csdn.net/kozazyh/article/details/79904219&lt;/a>&lt;/em>
[8]
&lt;a href="https://blog.csdn.net/QTM_Gitee/article/details/106004435:">https://blog.csdn.net/QTM_Gitee/article/details/106004435:&lt;/a> &lt;em>&lt;a href="https://blog.csdn.net/QTM_Gitee/article/details/106004435">https://blog.csdn.net/QTM_Gitee/article/details/106004435&lt;/a>&lt;/em>&lt;/p></description></item><item><title>Docs: Ceph 命令行工具</title><link>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/Ceph-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/rados/man/">官方文档,Ceph 存储集群-手册页&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.ceph.com/en/latest/man/8/radosgw-admin/#">官方文档,Ceph 对象网关-radosgw-admin 手册页&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="ceph--ceph-管理工具">ceph # Ceph 管理工具&lt;a class="td-heading-self-link" href="#ceph--ceph-%e7%ae%a1%e7%90%86%e5%b7%a5%e5%85%b7" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>一个 Python 实现的脚本工具，用于手动部署和似乎 Ceph 集群。通过很多的子命令，允许部署 ceph-mon、ceph-osd、PG、ceph-mds 等，并可以对集群整体进行维护和管理。&lt;/p>
&lt;h2 id="orch">orch&lt;a class="td-heading-self-link" href="#orch" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Orchestrator(编排器，简称 orch)&lt;/p>
&lt;h3 id="syntax语法">Syntax(语法)&lt;a class="td-heading-self-link" href="#syntax%e8%af%ad%e6%b3%95" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>COMMAND&lt;/p>
&lt;ul>
&lt;li>&lt;strong>host&lt;/strong> # 对集群中的节点进行管理
&lt;ul>
&lt;li>&lt;strong>add &amp;lt;HOSTNAME&amp;gt; [ADDR] [LABELs&amp;hellip;] [&amp;ndash;maintenance]&lt;/strong> # 向集群中添加一个节点&lt;/li>
&lt;li>&lt;strong>label add &amp;lt;HOSTNAME&amp;gt; &amp;lt;LABEL&amp;gt;&lt;/strong> # 为节点添加一个标签&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>ls&lt;/strong> # 列出 Orch 已知的服务&lt;/li>
&lt;li>**rm &amp;lt;ServiceName&amp;gt; **# 移除一个服务&lt;/li>
&lt;/ul>
&lt;p>EXAMPLE&lt;/p>
&lt;h1 id="radosgw-admin--rados-网关的用户管理工具">radosgw-admin # RADOS 网关的用户管理工具&lt;a class="td-heading-self-link" href="#radosgw-admin--rados-%e7%bd%91%e5%85%b3%e7%9a%84%e7%94%a8%e6%88%b7%e7%ae%a1%e7%90%86%e5%b7%a5%e5%85%b7" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>radosgw-admin 是一个 RADOS 网关用户的管理工具。可以增删改查用户。该工具通过非常多的子命令进行管理，并且每个子命令可用的选项也各不相同，Ceph 官方对这个工具的提示做的非常不好，子命令需要带的选项并不提示，只能自己尝试~~~&lt;/p>
&lt;h2 id="user">user&lt;a class="td-heading-self-link" href="#user" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="syntax语法-1">Syntax(语法)&lt;a class="td-heading-self-link" href="#syntax%e8%af%ad%e6%b3%95-1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>radosgw-admin user COMMAND [OPTIONS]&lt;/strong>&lt;/p>
&lt;p>COMMAND&lt;/p>
&lt;ul>
&lt;li>&lt;strong>user create &amp;ndash;display-name=&amp;lt;STRING&amp;gt; &amp;ndash;uid=&amp;lt;STRING&amp;gt;&lt;/strong> # 创建一个新用户&lt;/li>
&lt;li>&lt;strong>user info [&amp;ndash;uid=&amp;lt;STRING&amp;gt; | &amp;ndash;access-key=&amp;lt;STRING&amp;gt;]&lt;/strong> # 显示一个用户的信息，包括其子用户和密钥。通过 uid 或 ak 指定要显示的用户。&lt;/li>
&lt;li>&lt;strong>user list&lt;/strong> # 列出所有用户&lt;/li>
&lt;li>&lt;strong>user modify &amp;ndash;uid=&amp;lt;STRING&amp;gt;&lt;/strong> # 修改指定的用户&lt;/li>
&lt;/ul>
&lt;p>OPTIONS&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&amp;ndash;admin&lt;/strong> # 为指定用户设定 admin 标志&lt;/li>
&lt;li>&lt;strong>&amp;ndash;display-name=STRING&lt;/strong> # 为指定用户设定对外显示的名称&lt;/li>
&lt;li>&lt;strong>&amp;ndash;email=STRING&lt;/strong> # 为用户设定邮箱&lt;/li>
&lt;li>&lt;strong>&amp;ndash;uid=STRING&lt;/strong> # 指定用户的 ID。在执行绝大部分与用户相关的命令时，都需要指定该选项，以确定操作的用户。比如 查看用户信息、查看属于指定用户的桶的信息 等等等等&lt;/li>
&lt;li>&lt;strong>&amp;ndash;system&lt;/strong> # 为指定用户设定 system 标志&lt;/li>
&lt;/ul>
&lt;h3 id="example">EXAMPLE&lt;a class="td-heading-self-link" href="#example" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>创建一个名为 lichenhao 的用户，并添加 system 标志&lt;/p>
&lt;ul>
&lt;li>&lt;strong>radosgw-admin user create &amp;ndash;uid=lichenhao &amp;ndash;display-name=lichenhao &amp;ndash;system&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="bucket">bucket&lt;a class="td-heading-self-link" href="#bucket" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="syntax语法-2">Syntax(语法)&lt;a class="td-heading-self-link" href="#syntax%e8%af%ad%e6%b3%95-2" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>radosgw-admin bucket COMMAND [OPTIONS]&lt;/strong>&lt;/p>
&lt;p>COMMAND&lt;/p>
&lt;ul>
&lt;li>&lt;strong>bucket stats [OPTIONS]&lt;/strong> # 显示桶的统计信息。可以通过选项指定用户下的桶或指定的桶。&lt;/li>
&lt;/ul>
&lt;p>OPTIONS&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&amp;ndash;bucket=STRING&lt;/strong> # 指定桶的名称。可以被 quota 子命令使用。&lt;/li>
&lt;li>&lt;strong>&amp;ndash;uid=STRING&lt;/strong> # 指定用户的 ID。查看桶信息时，将会显示该用户下所有桶的信息。&lt;/li>
&lt;/ul></description></item></channel></rss>