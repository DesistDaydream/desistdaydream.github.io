<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Memory on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/</link><description>Recent content in Memory on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/index.xml" rel="self" type="application/rss+xml"/><item><title>Memory</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory/</guid><description>概述 参考：
Linux Kernel 文档，子系统 - 内存管理文档 Linux Kernel 文档，管理员指南 - 内存管理 《操作系统导论》 公众号-小林coding，真棒！ 20 张图揭开内存管理的迷雾，瞬间豁然开朗 该文是对《操作系统导论》一书中内存部分的提炼与总结。 公众号-码农的荒岛求生，神秘！申请内存时底层发生了什么？(malloc 简介) Linux Memory Management Subsystem(Linux 内存管理子系统) 负责管理系统中的内存。这包括 virtual memory(虚拟内存) 和 demand paging(请求分页) 的实现、Linux Kernel内部结构和用户空间程序的内存分配、将文件映射到进程地址空间、etc.
Linux 内存管理是一个非常复杂系统，有很多可以配置的设置，这些设置可以通过 proc 修改，还可以使用 sysctl 程序进行查询和调整，具体参数详见 Kernel 参数中的 vm(内存相关参数)。
基础概念 Linux 中的内存管理是一个经过多年发展的复杂系统，包含越来越多的功能来支持从 微控制器(nommu) 到 超级计算机 的各种系统，下面是一些常见的内存管理中的概念和术语
没有 Memory management unit(内存管理单元，简称 MMU) 的系统的内存管理称为 nommu，它绝对值得一个专门的文档。然而，随着时代的发展，nommu 的系统或设备已经基本不存在了，这里我们假设 MMU 可用并且 CPU 可以将虚拟地址转换为物理地址。
Virtual Memory(虚拟内存) Huge Pages(大页) # Huge Pages 是一种解决 Page table 过大、TLB 未命中过多、etc.</description></item><item><title>Memory 的分配</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory-%E7%9A%84%E5%88%86%E9%85%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory-%E7%9A%84%E5%88%86%E9%85%8D/</guid><description>概述 参考：
公众号-小林coding，我做了个实验！ Linux 进程的内存分布长什么样？ 在 Linux 操作系统中，虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：
通过这张图你可以看到，用户空间内存从低到高分别是 6 种不同的内存段：
通过这里可以看出：
32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间； 64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。 再来说说，内核空间与用户空间的区别：
进程在用户态时，只能访问用户空间内存； 只有进入内核态后，才可以访问内核空间的内存； 虽然每个进程都各自有独立的虚拟内存，但是每个虚拟内存中的内核地址，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。
接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。
我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：
程序文件段，包括二进制可执行代码； 已初始化数据段，包括静态常量； 未初始化数据段，包括未初始化的静态变量； 堆段，包括动态分配的内存，从低地址开始向上增长； 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关 ）； 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。当然系统也提供了参数，以便我们自定义大小； 在这 6 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。
malloc 是如何分配内存的？ 实际上，malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。
malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。
方式一：通过 brk() 系统调用从堆分配内存 方式二：通过 mmap() 系统调用在文件映射区域分配内存； 方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：
方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：</description></item><item><title>Memory 的缓存</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory-%E7%9A%84%E7%BC%93%E5%AD%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory-%E7%9A%84%E7%BC%93%E5%AD%98/</guid><description>概述 参考：
linux 中每个程序启动之后都会占用内存，一般情况下是不会把内存全部占满。那么空闲的这部分内存用来干什么呢？~
Linux 会充分利用这些空闲的内存，设计思想是：内存空闲还不如拿来多缓存一些数据，等下次程序再次访问这些数据速度就快了，而如果程序要使用内存而系统中内存又不足时，这时不是使用交换分区，而是快速回收部分 cached，将它们留给用户程序使用。
比如说：当使用该程序时，就会从硬盘中读取该程序的内容，这一部分读取的内容会加载到内存中(caceh 中)，因为内存比硬盘的读写速度更快，所以当下次再使用该程序需要读取该程序的内容时，直接从内存就能读取了。而 buff 中的数据一般是程序运行中产生的(比如玩游戏的存档)，当程序结束之前，需要把再 buff 中的数据写入到硬盘中以便永久保存(要不再运行这游戏，不就没存档了么~哈哈)。
内存管理做了很多精心的设计，除了对 dentry 进行缓存（用于 VFS，加速文件路径名到 inode 的转换），还采取了两种主要 Cache 方式：Buffer Cache 和 Page Cache，目的就是为了提升磁盘 IO 的性能。从低速的块设备(e.g.硬盘)上读取数据会暂时保存在内存中，即使数据在当时已经不再需要了，但在应用程序下一次访问该数据时，它可以从内存中直接读取，从而绕开低速的块设备，从而提高系统的整体性能。
因此，可以看出，buffers/cached 真是百益而无一害，真正的坏处可能让用户产生一种错觉——Linux 耗内存！其实不然，Linux 并没有吃掉你的内存，只要还未使用到交换分区，你的内存所剩无几时，你应该感到庆幸，因为 Linux 缓存了大量的数据，也许某一次应用程序读取数据时，就会用到这些已经缓存了的数据！
Buffer 与 Cache 在 Memory 管理工具 的 free 命令中，可以看到的 buff 与 cache 是从虚拟文件系统 /proc/meminfo 中获取的数据
buff # 内核缓冲器用到的内存，对应的是 /proc/meminfo 中 Buffers 的值 Buffers 是对原始磁盘块的临时存储，也就是来缓存从磁盘块读写的数据，通常不会特别大(20MB 左右)。 这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写，等等。 cache # 内核 PageCache(页缓存) 和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和 Cached 是从磁盘读写文件时的页缓存，也就是用来缓存从文件读写的数据。 读取过一个文件后会缓存，下次访问过这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。 或者在向文件写入数据时，先将数据写入到内存中，然后系统后台再逐步将数据从内存写入到磁盘中。 SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。 实验示例 写文件时会用到 Cache 缓存数据，写磁盘时则会用到 Buffer 缓存数据</description></item><item><title>Huge Pages</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Huge-Pages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Huge-Pages/</guid><description>概述 参考:
Linux Kernel 文档，管理员指南 - 内存管理 - 概念 - Huge pages Wiki, Huge pages Huge Pages(大页) 是指每个 Page 的容量都远超默认大小(4 KiB)的 Page。比如 2 MiB、1 GiB、etc. 都是常见的大页内存空间中的每页容量。Huge Pages Memory 则是指每个 Page 的容量都超过 4 KiB 的内存的统称。
在 x86 架构上，可以使用 第二级 和 第三级 页表中的条目来映射 2MiB 甚至 1GiB 的 Page。
HugePages 可以减少页表开销、减轻 TLB 压力并提高 TLB 的命中率、减轻内存数据查询压力、避免使用 Swap 降低性能。但是前提是保证使用大页的程序可以完善得利用大页，否则就会造成内存的极大浪费。
[!Notes] 为什么已经分页了还要用大页？ 如果一个程序（比如数据库），把大量数据加载到内存中，这时候其查询的数据量一定远超 TLB 的容量，这必然会导致 TLB 的未命中急速上升，严重影响性能。还有很多其他的方面就不一一举例了。
所以大页并不是所有程序都适用的，而是针对特定场景，需要处理大量数据，亲自管理内存的程序，才要配置大页。比如 DPDK 处理流量也需要使用大页的内存空间
Linux Kernel 中有两种机制可以实现 物理内存 与 Huge Pages 的映射</description></item><item><title>Memory Info</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory-Info/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memory-Info/</guid><description>概述 参考：
/proc/meminfo 参考：
Manual(手册)，proc_meminfo(5) GitHub 项目，torvalds/linux - Documentation/filesystems/proc.rst#meminfo RedHat 官方给的解释 该文件报告有关系统上 Memory 使用情况的统计信息。 free 命令使用该文件来报告系统上的可用内存和已使用内存（物理内存和交换内存）以及内核使用的共享内存和缓冲区的数量。该文件是以 : 符号分割的 Key/Value pair(键/值对) 格式。可用参数及其详解如下：
MemTotal 总可用 Memory。即.物理 RAM 减去一些保留的 bits 和内核二进制代码所用的量
MemFree 空闲的 Memory。LowFree 与 HighFree 两个参数的值的和
MemAvailable 可用的 Memory。估算值，估计有多少内存可用于启动新的应用程序
Buffers 与 Cached 详见：《Memory 的缓存》
Active 最近使用过的 Memory。除非必要，否则通常不会回收。
Inactive 最近使用比较收的 Memory。这些内存会被优先回收。
Slab 内核数据结构缓存。dentry、inode_cache 等
SReclaimable Slab Reclaimable。Slab 的一部分，可以被 reclaimed(回收)。例如 dentry、inode 的缓存等等。
SUnreclaim Slab UnReclaim。Slab 的一部分，不可以被 reclaimed(回收)。即使内存有压力也无法回收
CommitLimit 提交限制。当前可以分配的内存上限。只有当 [/proc/sys/vm/overcommit_memory](net(网络相关参数).md Kernel/Kernel 参数/net(网络相关参数).md) 的参数值为 2 的时候，该限制才生效。这个上限是指当程序向系统申请内存时，如果申请的内存加上现在已经分配的内存，超过了 commitlimit 的值，则该申请将会失败。</description></item><item><title>NUMA</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/NUMA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/NUMA/</guid><description>概述 参考：
Linux Kernel 文档，管理员指南 - 内存管理 - 概念 - Nodes Linux Kernel 文档，内存管理 - NUMA Linux Kernel 文档，管理员指南 - 内存管理 - NUMA 内存策略 Wiki, Non-uniform memory access Non-uniform memory access(非均匀内存访问，简称 NUMA) 是一种用于多处理结构的计算机的内存设计，其中内存访问时间取决于相对于处理器的内存位置。在 NUMA 下，处理器可以比非本地内存（另一个处理器的本地内存或处理器之间共享的内存）更快地访问自己的本地内存。 NUMA 的优势仅限于特定的工作负载，特别是在数据通常与某些任务或用户密切相关的服务器上。
Linux 将系统的硬件资源按照抽象的 Nodes(节点) 概念进行划分。Linux 将 Nodes 映射到硬件平台的物理 Cells(单元) 上，抽象出某些架构的一些细节。与物理单元一样，软件 Nodes 可能包含 0 个或多个 CPU、内存和/或 IO 总线。而且，与对更远程单元的访问相比，对“较近” Node 上的存储器的存储器访问通常会经历更快的访问时间和更高的有效带宽。</description></item><item><title>Memroy 的 Over Commit 与 OOM</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memroy-%E7%9A%84-Over-Commit-%E4%B8%8E-OOM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/Memroy-%E7%9A%84-Over-Commit-%E4%B8%8E-OOM/</guid><description>概述 over commit memory 机制与 out of memory 机制
over-commit memory 机制 Linux 内核根据应用程序的要求分配内存，通常来说应用程序分配了内存但是并没有实际全部使用，为了提高性能，这部分没用的内存可以留作它用，这部分内存是属于每个进程的，内核直接回收利用的话比较麻烦，所以内核采用一种过度分配内存（over-commit memory）的办法来间接利用这部分 “空闲” 的内存，提高整体内存的使用效率。一般来说这样做没有问题，但当大多数应用程序都消耗完自己的内存的时候麻烦就来了，因为这些应用程序的内存需求加起来超出了物理内存（包括 swap）的容量，内核（OOM killer）必须杀掉一些进程才能腾出空间保障系统正常运行。用银行的例子来讲可能更容易懂一些，部分人取钱的时候银行不怕，银行有足够的存款应付，当全国人民（或者绝大多数）都取钱而且每个人都想把自己钱取完的时候银行的麻烦就来了，银行实际上是没有这么多钱给大家取的。
out of memory 机制(OOM) 某时刻应用程序大量请求内存导致系统内存不足造成的，这通常会触发 Linux 内核里的 Out of Memory (OOM) killer，OOM killer 会杀掉某个进程以腾出内存留给系统用，不致于让系统立刻崩溃
内核检测到系统内存不足、挑选并杀掉某个进程的过程可以参考内核源代码 linux/mm/oom_kill.c，当系统内存不足的时候，out_of_memory() 被触发，然后调用 select_bad_process() 选择一个 “bad” 进程杀掉，如何判断和选择一个 “bad” 进程呢，总不能随机选吧？挑选的过程由 oom_badness() 决定，挑选的算法和想法都很简单很朴实：最 bad 的那个进程就是那个最占用内存的进程。
OOM 触发后的 Message 信息 Nov 24 19:52:22 dr-2 kernel: dsm_sa_datamgrd invoked oom-killer: gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0 Nov 24 19:52:22 dr-2 kernel: dsm_sa_datamgrd cpuset=/ mems_allowed=0-1 Nov 24 19:52:22 dr-2 kernel: Pid: 4917, comm: dsm_sa_datamgrd Not tainted 2.</description></item><item><title>mmap可以让程序员解锁哪些骚操作？</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/mmap%E5%8F%AF%E4%BB%A5%E8%AE%A9%E7%A8%8B%E5%BA%8F%E5%91%98%E8%A7%A3%E9%94%81%E5%93%AA%E4%BA%9B%E9%AA%9A%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/mmap%E5%8F%AF%E4%BB%A5%E8%AE%A9%E7%A8%8B%E5%BA%8F%E5%91%98%E8%A7%A3%E9%94%81%E5%93%AA%E4%BA%9B%E9%AA%9A%E6%93%8D%E4%BD%9C/</guid><description>mmap 可以让程序员解锁哪些骚操作？
大家好，我是小风哥！
今天这篇文章带你讲解下稍显神秘的 mmap 到底是怎么一回事。
简单的与麻烦的
用代码读写内存对程序员来说是非常方便非常自然的，但用代码读写磁盘对程序员来说就不那么方便不那么自然了。
回想一下，你在代码中读写内存有多简单：
定义一个数组：
int a[100]; a[0] = 2; 看到了吧，这时你就在写内存，甚至你可能在写这段代码时下意识里都没有去想读内存这件事。
再想想你是怎样读磁盘文件的？
char buf[1024]; int fd = open(&amp;#34;/filepath/abc.txt&amp;#34;); read(fd, buf, 1024); 看到了吧，读写磁盘文件其实是一件很麻烦的事情，你需要 open 一个文件，意思是告诉操作系统 “Hey，操作系统，我要开始读 abc.txt 这个文件了，把这个文件的所有信息准备好，然后给我一个代号”。这个代号就是所谓的文件描述符，拿到文件描述符后你才能继续接下来的读写操作。
为什么麻烦
现在你应该看到了，操作磁盘文件要比操作内存复杂很多，根本原因就在于寻址方式不同。
对内存来说我们可以直接按照字节粒度去寻址，但对磁盘上保存的文件来说则不是这样的，磁盘上保存的文件是按照块 (block) 的粒度来寻址的，因此你必须先把磁盘中的文件读取到内存中，然后再按照字节粒度来操作文件内容。
你可能会想既然直接操作内存很简单，那么我们有没有办法像读写内存那样去直接读写磁盘文件呢？
答案是肯定的。
要开脑洞了
对于像我们这样在用户态编程的程序员来说，内存在我们眼里就是一段连续的空间。啊哈，巧了，磁盘上保存的文件在程序员眼里也存放在一段连续的空间中（有的同学可能会说文件其实是在磁盘上离散存放的，请注意，我们在这里只从文件使用者的角度来讲）。
那么这两段空间有没有办法关联起来呢？
答案是肯定的，怎么关联呢？
答案就是。。。。。。你猜对了吗？答案是通过虚拟内存。
关于虚拟内存我们已经讲解过很多次了，虚拟内存就是假的地址空间，是进程看到的幻象，其目的是让每个进程都认为自己独占内存，关于虚拟内存完整的详细讲解请参考博主的深入理解操作系统，关注公众号码农的荒岛求生并回复操作系统即可。
既然进程看到地址空间是假的那么一切都好办了。
既然是假的，那么就有做手脚的操作空间，怎么做手脚呢？
从普通程序员眼里看文件不是保存在一段连续的磁盘空间上吗？我们可以直接把这段空间映射到进程的内存中，就像这样：
假设文件长度是 100 字节，我们把该文件映射到了进程的内存中，地址是从 600 ~ 800，那么当你直接读写 600 ~ 800 这段内存时，实际上就是在直接操作磁盘文件。
这一切是怎么做到呢？
魔术师操作系统
原来这一切背后的功劳是操作系统。
当我们首次读取 600~800 这段地址空间时，操作系统会检测的这一操作，因为此时这段内存中什么内容都还没有，此时操作系统自己读取磁盘文件填充到这段内存空间中，此后程序就可以像读内存一样直接读取磁盘内容了。
写操作也很简单，用户程序依然可以直接修改这块内存，此后操作系统会在背后将修改内容写回磁盘。
现在你应该看到了，其实采用 mmap 这种方法磁盘依然还是按照块的粒度来寻址的，只不过在操作系统的一番骚操作下对于用户态的程序来说 “看起来” 我们能像读写内存那样直接读写磁盘文件了，从按块粒度寻址到按照字节粒度寻址，这中间的差异就是操作系统来填补的。</description></item><item><title>堆区与栈区</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/%E5%A0%86%E5%8C%BA%E4%B8%8E%E6%A0%88%E5%8C%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/%E5%A0%86%E5%8C%BA%E4%B8%8E%E6%A0%88%E5%8C%BA/</guid><description>Heap(堆) 与 Stack(栈) 参考：
StackOverflow 公众号-码农的荒岛求生，程序员应如何理解内存：中篇 堆与栈的本质是什么 在编程语言中，堆区和栈区本质上都是内存，因此二者在本质上没有任何区别，只不过这两块内存的使用方式是不一样的。
在数据结构与算法中，我们也有堆和栈的概念，但那里指的不是内存，而是两种数据结构。
你可能会想，我们为什么要费尽心力的提出堆和栈这两个概念呢？之所以需要区分两种内存用法，根源在于：内存是有限的。
如果计算机内存是无限的，那么我们根本就不用这么麻烦的给内存划分两个区域，在其中的一个区域中这样使用内存，另一区域那样使用内存，这些都是不需要的。即使在今天 PC 内存普遍都在 8G、16G，这依然是不够的，因此我们需要合理的来安排内存的使用，堆和栈就是为达到这一目的而采用的技术。
你会发现栈其实是一种非常巧妙的内存使用方法。函数调用完成后，函数运行过程中占用的内存就会被释放掉，这样，只要程序员代码写的合理 (栈帧不至于过大)，那我们程序就可以一直运行下去，而不会出现内存不足的现象。程序员在栈区不需要担心内存分配释放问题，因为这一切都是自动进行的。而如果程序员想自己控制内存，那么可以选择在堆上进行内存分配。因此这里提供了两种选择，一种是 “自动的”，一种是 “手动的”，目的都是在合理使用内存的同时提供给程序员最大的灵活性。
堆和栈是计算机科学中很优秀的设计思想，这种设计思想充分的体现了计算机如何合理且灵活的使用有限资源。
堆区和栈区对 C/C++ 程序员来说就是实实在在的内存，而对于 Java、Python 等语言的程序员来说又该如何理解内存呢?
Rust 中的 Stack 与 Heap 参考：
英文 非官方中文 在很多语言中，你并不需要经常考虑到栈与堆。不过在像 Rust 这样的系统编程语言中，值是位于栈上还是堆上在更大程度上影响了语言的行为以及为何必须做出这样的抉择。我们会在本章的稍后部分描述所有权与栈和堆相关的内容，所以这里只是一个用来预热的简要解释。
栈和堆都是代码在运行时可供使用的内存，但是它们的结构不同。栈以放入值的顺序存储值并以相反顺序取出值。这也被称作 后进先出（last in, first out）。想象一下一叠盘子：当增加更多盘子时，把它们放在盘子堆的顶部，当需要盘子时，也从顶部拿走。不能从中间也不能从底部增加或拿走盘子！增加数据叫做 进栈（pushing onto the stack），而移出数据叫做 出栈（popping off the stack）。
栈中的所有数据都必须占用已知且固定的大小。在编译时大小未知或大小可能变化的数据，要改为存储在堆上。堆是缺乏组织的：当向堆放入数据时，你要请求一定大小的空间。操作系统在堆的某处找到一块足够大的空位，把它标记为已使用，并返回一个表示该位置地址的 指针（pointer）。这个过程称作 在堆上分配内存（allocating on the heap），有时简称为 “分配”（allocating）。将数据推入栈中并不被认为是分配。因为指针的大小是已知并且固定的，你可以将指针存储在栈上，不过当需要实际数据时，必须访问指针。
想象一下去餐馆就座吃饭。当进入时，你说明有几个人，餐馆员工会找到一个够大的空桌子并领你们过去。如果有人来迟了，他们也可以通过询问来找到你们坐在哪。
入栈比在堆上分配内存要快，因为（入栈时）操作系统无需为存储新数据去搜索内存空间；其位置总是在栈顶。相比之下，在堆上分配内存则需要更多的工作，这是因为操作系统必须首先找到一块足够存放数据的内存空间，并接着做一些记录为下一次分配做准备。
访问堆上的数据比访问栈上的数据慢，因为必须通过指针来访问。现代处理器在内存中跳转越少就越快（缓存）。继续类比，假设有一个服务员在餐厅里处理多个桌子的点菜。在一个桌子报完所有菜后再移动到下一个桌子是最有效率的。从桌子 A 听一个菜，接着桌子 B 听一个菜，然后再桌子 A，然后再桌子 B 这样的流程会更加缓慢。出于同样原因，处理器在处理的数据彼此较近的时候（比如在栈上）比较远的时候（比如可能在堆上）能更好的工作。在堆上分配大量的空间也可能消耗时间。
当你的代码调用一个函数时，传递给函数的值（包括可能指向堆上数据的指针）和函数的局部变量被压入栈中。当函数结束时，这些值被移出栈。
跟踪哪部分代码正在使用堆上的哪些数据，最大限度的减少堆上的重复数据的数量，以及清理堆上不再使用的数据确保不会耗尽空间，这些问题正是所有权系统要处理的。一旦理解了所有权，你就不需要经常考虑栈和堆了，不过明白了所有权的存在就是为了管理堆数据，能够帮助解释为什么所有权要以这种方式工作。
知乎文章 原文连接: https://zhuanlan.zhihu.com/p/58191270 - 已失效</description></item><item><title>分享Linux内存占用几个案例</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/%E5%88%86%E4%BA%ABLinux%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E5%87%A0%E4%B8%AA%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Memory/%E5%88%86%E4%BA%ABLinux%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E5%87%A0%E4%B8%AA%E6%A1%88%E4%BE%8B/</guid><description>案例一 问题 最近一台 CentOS 服务器，发现内存无端损失了许多，free 和 ps 统计的结果相差十几个 G，非常奇怪，后来 Google 了许久才搞明白。
分析 1、linux 系统内存消耗主要有三个地方：
进程 slab pagecacge 用 free 命令查看到的是系统整体的内容使用情况，而使用 ps 和 top 看到的内存使用情况都是以进程维度来看的，因此看不到 slabcache 和 pagecache 的内存占用信息。
2、判断应用程序是否有内存泄露问题，只根据进程的内存使用或机器的内存变化来判定都不太准确，如果单凭进程的内存变化可能会疏忽一些小对象的内存泄露问题。
同时对于机器的内存的使用也要做是否合理的判断。对于不同语言的应用都有相应的神器可以辅助定位内存泄露问题，同时结合 linux 内存的监控工具进行分析， 除了 top，free还有 pmap，/proc/meminfo 和 /slabinfo，slaptop等。
3、通过这个问题，有一点比较重要的是，在使用监控工具进行监控和分析时，对其值的具体含义还是要了解的很清楚，否则会造成误判，使问题变得更加复杂。
4、此外 page cache，dentries和inodes cache，系统是会自动回收的。
可以通过以下几种方式加速其回收，不过实际并不需要这么做。
手工清除内存 缓存
echo 1 &amp;gt; /proc/sys/vm/drop_caches 清除 page cache echo 2 &amp;gt; /proc/sys/vm/drop_caches 清除 denries 和 inodes echo 3 &amp;gt; /proc/sys/vm/drop_caches 清除 page cache ，dentries 及 inodes 调整 vm.</description></item></channel></rss>