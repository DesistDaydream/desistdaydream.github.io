<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>BPF 流量控制机制 on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/</link><description>Recent content in BPF 流量控制机制 on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/index.xml" rel="self" type="application/rss+xml"/><item><title>BPF 流量控制机制</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/</guid><description>概述 参考：
Kernel 网络官方文档：LInux Socket Filtering aka Berkeley Packet Filter</description></item><item><title>【BPF网络篇系列-2】容器网络延时之 ipvs 定时器篇 | 深入浅出 eBPF</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF%E7%BD%91%E7%BB%9C%E7%AF%87%E7%B3%BB%E5%88%97-2%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E4%B9%8B-ipvs-%E5%AE%9A%E6%97%B6%E5%99%A8%E7%AF%87-_-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-eBPF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF%E7%BD%91%E7%BB%9C%E7%AF%87%E7%B3%BB%E5%88%97-2%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E4%B9%8B-ipvs-%E5%AE%9A%E6%97%B6%E5%99%A8%E7%AF%87-_-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-eBPF/</guid><description>1. 前言 趣头条的容器化已经开展了一年有余，累计完成了近 1000 个服务的容器化工作，微服务集群的规模也达到了千台以上的规模。随着容器化服务数量和集群规模的不断增大，除了常规的 API Server 参数优化、Scheduler 优化等常规优化外，近期我们还碰到了 kubernetes 底层负载均衡 ipvs 模块导致的网络抖动问题，在此把整个问题的分析、排查和解决的思路进行总结，希望能为有类似问题场景解决提供一种思路。
涉及到的 k8s 集群和机器操作系统版本如下：
k8s 阿里云 ACK 14.8 版本，网络模型为 CNI 插件 terway 中的 terway-eniip 模式； 操作系统为 CentOS 7.7.1908，内核版本为 3.10.0-1062.9.1.el7.x86_64； 2. 网络抖动问题 在容器集群中新部署的服务 A，在测试初期发现通过服务注册发现访问下游服务 B（在同一个容器集群） 调用延时 999 线偶发抖动，测试 QPS 比较小，从业务监控上看起来比较明显，最大的延时可以达到 200 ms。
图 2-1 服务调用延时
服务间的访问通过 gRPC 接口访问，节点发现基于 consul 的服务注册发现。通过在服务 A 容器内的抓包分析和排查，经过了以下分析和排查：
服务 B 部分异常注册节点，排除异常节点后抖动情况依然存在； HTTP 接口延时测试， 抖动情况没有改善； 服务 A 在 VM（ECS）上部署测试，抖动情况没有改善； 经过上述的对比测试，我们逐步把范围缩小至服务 B 所在的主机上的底层网络抖动。
经过多次 ping 包测试，我们寻找到了某台主机 A 与 主机 B 两者之间的 ping 延时抖动与服务调用延时抖动规律比较一致，由于 ping 包 的分析比 gRPC 的分析更加简单直接，因此我们将目标转移至底层网络的 ping 包测试的轨道上。</description></item><item><title>10.2.深入理解 Cilium 的 eBPF 收发包路径</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/10.2.%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Cilium-%E7%9A%84-eBPF-%E6%94%B6%E5%8F%91%E5%8C%85%E8%B7%AF%E5%BE%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/10.2.%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Cilium-%E7%9A%84-eBPF-%E6%94%B6%E5%8F%91%E5%8C%85%E8%B7%AF%E5%BE%84/</guid><description>深入理解 Cilium 的 eBPF 收发包路径 本文翻译自 2019 年 DigitalOcean 的工程师 Nate Sweet 在 KubeCon 的一篇分享: Understanding (and Troubleshooting) the eBPF Datapath in Cilium[1] 。
由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。
以下是译文。
为什么要关注 eBPF？ 网络成为瓶颈
大家已经知道网络成为瓶颈，但我是从下面这个角度考虑的：近些年业界使用网络的方式 ，使其成为瓶颈（it is the bottleneck in a way that is actually pretty recent） 。
网络一直都是 I/O 密集型的，但直到最近，这件事情才变得尤其重要。
分布式任务（workloads）业界一直都在用，但直到近些年，这种模型才成为主流。虽然何时成为主流众说纷纭，但我认为最早不会早于 90 年代晚期。
公有云的崛起，我认为可能是网络成为瓶颈的最主要原因。
这种情况下，用于管理依赖和解决瓶颈的工具都已经过时了。
但像 eBPF 这样的技术使得网络调优和整流（tune and shape this traffic）变得简单很多。eBPF 提供的许多能力是其他工具无法提供的，或者即使提供了，其代价也要比 eBPF 大 的多。
eBPF 无处不在
eBPF 正在变得无处不在，我们可能会争论这到底是一件好事还是坏事（eBPF 也确实带了一 些安全问题），但当前无法忽视的事实是：Linux 内核的网络开发者们正在将 eBPF 应用 于各种地方（putting it everywhere）。其结果是，eBPF 与内核的默认收发包路径（ datapath）耦合得越来越紧（more and more tightly coupled with the default datapath）。</description></item><item><title>BPF 相关文章</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF-%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF-%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0/</guid><description>[译] 利用 eBPF 支撑大规模 K8S Service
为容器时代设计的高级 eBPF 内核特性（FOSDEM, 2021）</description></item><item><title>BPF 在网络领域的实现</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF-%E5%9C%A8%E7%BD%91%E7%BB%9C%E9%A2%86%E5%9F%9F%E7%9A%84%E5%AE%9E%E7%8E%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF-%E5%9C%A8%E7%BD%91%E7%BB%9C%E9%A2%86%E5%9F%9F%E7%9A%84%E5%AE%9E%E7%8E%B0/</guid><description>概述 参考：
arthurchiao.art 的文章 [译] 为容器时代设计的高级 eBPF 内核特性（FOSDEM, 2021) eBPF 架构 eBPF 能够让你在内核中创建新的 DataPath。eBPF 就是内核本身的代码，想象空间无限，并且热加载到内核；换句话说，一旦加载到内核，内核的行为就变了。在 eBPF 之前，改变内核行为这件事情，只能通过修改内核再重新编译，或者开发内 核模块才能实现。
由于上述原因，真正的 eBPF，应该是基于 eBPF 实现的数据路径，由于 eBPF 可以修改内核，所以可以在内核创建新的类似 Netfilter 的 Hook 点，以便跳过复杂的 Netfilter。甚至可以直接在网卡驱动中运行 eBPF 代码，而无需将数据包送到复杂的协议栈进行处理。</description></item><item><title>XDP eBPF 与 TC eBPF</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/XDP-eBPF-%E4%B8%8E-TC-eBPF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/BPF/BPF-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/XDP-eBPF-%E4%B8%8E-TC-eBPF/</guid><description>概述 参考：
Wiki, Express_Data_Path eXpress Data Path(特快数据路径，简称 XDP) 是从 4.8 版开始在 Linux 内核中合并的基于 eBPF 的高性能数据路径。
XDP 背后的想法是在内核的 RX 路径中添加一个早期钩子，然后让用户提供的 eBPF 程序来决定数据包的命运。该挂钩刚好在中断处理之后，在网络堆栈本身需要的任何内存分配之前放置在 NIC 驱动程序中，因为内存分配可能是一项昂贵的操作。由于这种设计，使用商用硬件，XDP 可以每秒每核心丢弃 2 千 6 百万个数据包。
eBPF 程序在加载之前必须通过预验证器测试，以避免在内核空间中执行恶意代码。预验证器检查程序是否不包含越界访问，循环或全局变量。
Linux 内核中的数据包流路径。XDP 绕过了网络堆栈和数据包元数据的内存分配。
允许程序编辑数据包数据，并且在 eBPF 程序返回后，操作代码确定如何处理数据包：
XDP_PASS：让数据包继续通过网络堆栈 XDP_DROP：静默丢弃数据包 XDP_ABORTED：丢弃具有跟踪点异常的数据包 XDP_TX：将数据包弹回到达的同一网卡 XDP_REDIRECT：通过 AF_XDP 地址族将数据包重定向到另一个 NIC 或用户空间套接字 XDP 需要 NIC 驱动程序的支持，但由于并非所有驱动程序都支持 XDP，因此它可以回退到通用实现，该通用实现在网络堆栈中执行 eBPF 处理，但性能较慢。
XDP 具有将 eBPF 程序卸载到支持它的 NIC 卡的基础结构，从而减少了 CPU 负载。当时只有 Netronome 卡支持它，[5]由 Intel 和 Mellanox 共同开发。
AF_XDP
与 XDP 一起，从 4.</description></item></channel></rss>