<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – Linux 网络栈管理</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/</link><description>Recent content in Linux 网络栈管理 on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Linux 网络栈管理</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/networking/index.html">Kernel 文档-Linux Networking Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/networking/kapi.html">Kernel 文档-Linux Networking and Network Devices APIs&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">[译] Linux 网络栈监控和调优：接收数据（2016）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">[译] Linux 网络栈监控和调优：发送数据（2017）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>和磁盘设备类似，Linux 用户想要使用网络功能，不能通过直接操作硬件完成，而需要直接或间接的操作一个 Linux 为我们抽象出来的设备，即通用的 &lt;strong>Linux 网络设备&lt;/strong>来完成。一个常见的情况是，系统里装有一个硬件网卡，Linux 会在系统里为其生成一个网络设备实例，如 eth0，用户需要对 eth0 发出命令以配置或使用它了。更多的硬件会带来更多的设备实例，虚拟的硬件也会带来更多的设备实例。&lt;/p>
&lt;p>网卡本身并不会连接连接任何网络，网卡需要相应的配置文件来告诉他们如何实现网络连接。而让网卡与配置文件关联的过程，就是 network.service 这类服务来实现的&lt;/p>
&lt;p>在 Linux 系统中，一般使用“网络设备”这种称呼，来描述硬件物理网卡设备在系统中的实例。在不同的语境中，有时也简称为 “设备”、“DEV” 等等。网络设备可以是一块真实机器上的网卡，也可以是创建的虚拟的网卡。&lt;/p>
&lt;p>而网络设备与网卡之间如何建立关系，就是网卡驱动程序的工作了，不同的网卡，驱动不一样，可以实现的功能也各有千秋。所以，想要系统出现 eth0 这种网络设备，网卡驱动程序是必须存在的，否则，没有驱动，也就无法识别硬件，无法识别硬件，在系统中也就不知道如何操作这个硬件。&lt;/p>
&lt;h2 id="常见术语">常见术语&lt;a class="td-heading-self-link" href="#%e5%b8%b8%e8%a7%81%e6%9c%af%e8%af%ad" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="datapath数据路径">DataPath(数据路径)&lt;a class="td-heading-self-link" href="#datapath%e6%95%b0%e6%8d%ae%e8%b7%af%e5%be%84" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>网络数据在内核中进行网络传输时，所经过的所有点组合起来，称为数据路径。&lt;/p>
&lt;h3 id="socket-buffer简称-sk_buff-或-skb">Socket Buffer(简称 sk_buff 或 skb)&lt;a class="td-heading-self-link" href="#socket-buffer%e7%ae%80%e7%a7%b0-sk_buff-%e6%88%96-skb" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>在内核代码中是一个名为 &lt;a href="https://www.kernel.org/doc/html/latest/networking/kapi.html#c.sk_buff">&lt;strong>sk_buff&lt;/strong>&lt;/a> 的结构体。内核显然需要一个数据结构来储存报文的信息。这就是 skb 的作用。&lt;/p>
&lt;p>sk_buff 结构自身并不存储报文内容，它通过多个指针指向真正的报文内存空间:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/efrsi8/1617849698535-471768e0-dcf8-4471-8dd2-605a1bc4e020.png" alt="image.png">&lt;/p>
&lt;p>sk_buff 是一个贯穿整个协议栈层次的结构，在各层间传递时，内核只需要调整 sk_buff 中的指针位置就行。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/efrsi8/1617849692989-54095177-b85c-449e-8c66-3b026e4925da.png" alt="image.png">&lt;/p>
&lt;h3 id="device设备">DEVICE(设备)&lt;a class="td-heading-self-link" href="#device%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>在内核代码中，是一个名为 &lt;a href="https://www.kernel.org/doc/html/latest/networking/kapi.html#c.net_device">&lt;strong>net_device&lt;/strong>&lt;/a> 的结构体。一个巨大的数据结构，描述一个网络设备的所有 属性、数据 等信息。&lt;/p>
&lt;h1 id="linux-网络功能的实现">Linux 网络功能的实现&lt;a class="td-heading-self-link" href="#linux-%e7%bd%91%e7%bb%9c%e5%8a%9f%e8%83%bd%e7%9a%84%e5%ae%9e%e7%8e%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="数据包的-transmit发送-与-receive接收-过程概览">数据包的 Transmit(发送) 与 Receive(接收) 过程概览&lt;a class="td-heading-self-link" href="#%e6%95%b0%e6%8d%ae%e5%8c%85%e7%9a%84-transmit%e5%8f%91%e9%80%81-%e4%b8%8e-receive%e6%8e%a5%e6%94%b6-%e8%bf%87%e7%a8%8b%e6%a6%82%e8%a7%88" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="receive接收-过程">Receive(接收) 过程&lt;a class="td-heading-self-link" href="#receive%e6%8e%a5%e6%94%b6-%e8%bf%87%e7%a8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文将拿 &lt;strong>Intel I350&lt;/strong> 网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下 载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从被网卡接收到进入 socket 接收队列的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>加载网卡驱动，初始化&lt;/li>
&lt;li>包从外部网络进入网卡&lt;/li>
&lt;li>网卡（通过 DMA）将包 copy 到内核内存中的 ring buffer&lt;/li>
&lt;li>产生硬件中断，通知系统收到了一个包&lt;/li>
&lt;li>驱动调用 NAPI，如果轮询（poll）还没开始，就开始轮询&lt;/li>
&lt;li>&lt;code>ksoftirqd&lt;/code> 进程调用 NAPI 的 &lt;code>poll&lt;/code> 函数从 ring buffer 收包（&lt;code>poll&lt;/code> 函数是网卡 驱动在初始化阶段注册的；每个 CPU 上都运行着一个 &lt;code>ksoftirqd&lt;/code> 进程，在系统启动期 间就注册了）&lt;/li>
&lt;li>ring buffer 里包对应的内存区域解除映射（unmapped）&lt;/li>
&lt;li>（通过 DMA 进入）内存的数据包以 &lt;code>skb&lt;/code> 的形式被送至更上层处理&lt;/li>
&lt;li>如果 packet steering 功能打开，或者网卡有多队列，网卡收到的包会被分发到多个 CPU&lt;/li>
&lt;li>包从队列进入协议层&lt;/li>
&lt;li>协议层处理包&lt;/li>
&lt;li>包从协议层进入相应 socket 的接收队列&lt;/li>
&lt;/ol>
&lt;p>接下来会详细介绍这个过程。&lt;/p>
&lt;h2 id="transmit发送-过程">Transmit(发送) 过程&lt;a class="td-heading-self-link" href="#transmit%e5%8f%91%e9%80%81-%e8%bf%87%e7%a8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文将拿&lt;strong>Intel I350&lt;/strong>网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从用户程序到达硬件网卡的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>使用 &lt;strong>系统调用&lt;/strong>（如 &lt;code>sendto&lt;/code>，&lt;code>sendmsg&lt;/code> 等）写数据&lt;/li>
&lt;li>数据穿过 &lt;strong>socket 子系统&lt;/strong>，进入&lt;strong>socket 协议族&lt;/strong>（protocol family）系统（在我们的例子中为 &lt;code>AF_INET&lt;/code>）&lt;/li>
&lt;li>协议族处理：数据穿过 &lt;strong>协议层&lt;/strong>，这一过程（在许多情况下）会将 &lt;strong>数据&lt;/strong>（data）转换成 &lt;strong>数据包&lt;/strong>（packet）&lt;/li>
&lt;li>数据穿过 &lt;strong>路由层&lt;/strong>，这会涉及路由缓存和 ARP 缓存的更新；如果目的 MAC 不在 ARP 缓存表中，将触发一次 ARP 广播来查找 MAC 地址&lt;/li>
&lt;li>穿过协议层，packet 到达 &lt;strong>设备无关层&lt;/strong>（device agnostic layer）&lt;/li>
&lt;li>使用 XPS（如果启用）或散列函数 &lt;strong>选择发送队列&lt;/strong>&lt;/li>
&lt;li>调用网卡驱动的 &lt;strong>发送函数&lt;/strong>&lt;/li>
&lt;li>数据传送到网卡的 &lt;code>qdisc&lt;/code>（queue discipline，排队规则）&lt;/li>
&lt;li>qdisc 会直接 &lt;strong>发送数据&lt;/strong>（如果可以），或者将其放到队列，下次触发 &lt;code>**NET_TX**&lt;/code> &lt;strong>类型软中断&lt;/strong>（softirq）的时候再发送&lt;/li>
&lt;li>数据从 qdisc 传送给驱动程序&lt;/li>
&lt;li>驱动程序创建所需的 &lt;strong>DMA 映射&lt;/strong>，以便网卡从 RAM 读取数据&lt;/li>
&lt;li>驱动向网卡发送信号，通知 &lt;strong>数据可以发送了&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网卡从 RAM 中获取数据并发送&lt;/strong>&lt;/li>
&lt;li>发送完成后，设备触发一个 &lt;strong>硬中断&lt;/strong>（IRQ），表示发送完成&lt;/li>
&lt;li>&lt;strong>硬中断处理函数&lt;/strong> 被唤醒执行。对许多设备来说，这会 &lt;strong>触发 &lt;code>NET_RX&lt;/code> 类型的软中断&lt;/strong>，然后 NAPI poll 循环开始收包&lt;/li>
&lt;li>poll 函数会调用驱动程序的相应函数，&lt;strong>解除 DMA 映射&lt;/strong>，释放数据&lt;/li>
&lt;/ol>
&lt;h1 id="网络栈关联文件">网络栈关联文件&lt;a class="td-heading-self-link" href="#%e7%bd%91%e7%bb%9c%e6%a0%88%e5%85%b3%e8%81%94%e6%96%87%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>不同的 Linux 发行版，所使用的上层网络配置程序各不相同，各种程序所读取的配置文件也各不相同。&lt;/p>
&lt;ul>
&lt;li>对于 RedHat 相关的发行版，网络配置在 /etc/sysconfig/network-scripts/ 目录中&lt;/li>
&lt;li>对于 Debian 相关的发行版，网络配置在 /etc/network/ 目录中&lt;/li>
&lt;/ul>
&lt;p>在这些目录中，其实都是通过脚本来实现的&lt;/p>
&lt;p>后来随着时代的发展，涌现出很多通用的网络管理程序，比如 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux%20%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Netplan/Netplan.md">Netplan&lt;/a>、&lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux%20%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/NetworkManager/NetworkManager.md">NetworkManager&lt;/a>、&lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux%20%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/systemd-networkd.md">systemd-networkd&lt;/a>、etc.，这样就可以让各个发行版使用相同的程序来管理网络了，减少切换发行版而需要学习对应配置的成本，并且也更利于发展。&lt;/p></description></item><item><title>Docs: Linux 网络设备详解</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/</guid><description/></item><item><title>Docs: Linux 网络包发送过程</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Linux-%E7%BD%91%E7%BB%9C%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B/</guid><description>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/wThfD9th9e_-YGHJJ3HXNQ">25 张图，一万字，拆解 Linux 网络包发送过程&lt;/a>&lt;/p>
&lt;p>大家好，我是飞哥!&lt;/p>
&lt;p>半年前我以源码的方式描述了网络包的接收过程。之后不断有粉丝提醒我还没聊发送过程呢。好，安排！&lt;/p>
&lt;p>在开始今天的文章之前，我先来请大家思考几个小问题。&lt;/p>
&lt;ul>
&lt;li>问 1：我们在查看内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？&lt;/li>
&lt;li>问 2：为什么你服务器上的 /proc/softirqs 里 NET_RX 要比 NET_TX 大的多的多？&lt;/li>
&lt;li>问 3：发送网络数据的时候都涉及到哪些内存拷贝操作？&lt;/li>
&lt;/ul>
&lt;p>这些问题虽然在线上经常看到，但我们似乎很少去深究。如果真的能透彻地把这些问题理解到位，我们对性能的掌控能力将会变得更强。&lt;/p>
&lt;p>带着这三个问题，我们开始今天对 Linux 内核网络发送过程的深度剖析。还是按照我们之前的传统，先从一段简单的代码作为切入。如下代码是一个典型服务器程序的典型的缩微代码：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">int&lt;/span> &lt;span style="color:#000">main&lt;/span>&lt;span style="color:#000;font-weight:bold">(){&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">fd&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">socket&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">AF_INET&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">SOCK_STREAM&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">bind&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">fd&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000;font-weight:bold">...);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">listen&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">fd&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000;font-weight:bold">...);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">cfd&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">accept&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">fd&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000;font-weight:bold">...);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">//  接收用户请求
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">&lt;/span> &lt;span style="color:#000">read&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">cfd&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000;font-weight:bold">...);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">//  用户请求处理
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">&lt;/span> &lt;span style="color:#000">dosometing&lt;/span>&lt;span style="color:#000;font-weight:bold">();&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">//  给用户返回结果
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">&lt;/span> &lt;span style="color:#000">send&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">cfd&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">buf&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#204a87;font-weight:bold">sizeof&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">buf&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>今天我们来讨论上述代码中，调用 send 之后内核是怎么样把数据包发送出去的。本文基于 Linux 3.10，网卡驱动采用 Intel 的 igb 网卡举例。&lt;/p>
&lt;p>&lt;strong>预警：本文共有一万多字，25 张图，长文慎入！&lt;/strong>&lt;/p>
&lt;p>&lt;strong>开发内功修炼&lt;/strong>&lt;/p>
&lt;p>飞哥有鹅厂、搜狗 10 年多的开发工作经验。通过本号，我把多年中对于性能的一些深度思考分享给大家。&lt;/p>
&lt;p>73 篇原创内容&lt;/p>
&lt;p>公众号&lt;/p>
&lt;h2 id="一linux-网络发送过程总览">一、Linux 网络发送过程总览&lt;a class="td-heading-self-link" href="#%e4%b8%80linux-%e7%bd%91%e7%bb%9c%e5%8f%91%e9%80%81%e8%bf%87%e7%a8%8b%e6%80%bb%e8%a7%88" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>我觉得看 Linux 源码最重要的是得有整体上的把握，而不是一开始就陷入各种细节。&lt;/p>
&lt;p>我这里先给大家准备了一个总的流程图，简单阐述下 send 发送了的数据是如何一步一步被发送到网卡的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/linux_networking/202405082017947.png" alt="">&lt;/p>
&lt;p>在这幅图中，我们看到用户数据被拷贝到内核态，然后经过协议栈处理后进入到了 RingBuffer 中。随后网卡驱动真正将数据发送了出去。当发送完成的时候，是通过硬中断来通知 CPU，然后清理 RingBuffer。&lt;/p>
&lt;p>因为文章后面要进入源码，所以我们再从源码的角度给出一个流程图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>虽然数据这时已经发送完毕，但是其实还有一件重要的事情没有做，那就是释放缓存队列等内存。&lt;/p>
&lt;p>那内核是如何知道什么时候才能释放内存的呢，当然是等网络发送完毕之后。网卡在发送完毕的时候，会给 CPU 发送一个硬中断来通知 CPU。更完整的流程看图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>注意，我们今天的主题虽然是发送数据，但是硬中断最终触发的软中断却是 NET_RX_SOFTIRQ，而并不是 NET_TX_SOFTIRQ ！！！（T 是 transmit 的缩写，R 表示 receive）&lt;/p>
&lt;p>&lt;strong>意不意外，惊不惊喜？？？&lt;/strong>&lt;/p>
&lt;p>所以这就是开篇问题 1 的一部分的原因（注意，这只是一部分原因）。&lt;/p>
&lt;blockquote>
&lt;p>问 1：在服务器上查看 /proc/softirqs，为什么 NET_RX 要比 NET_TX 大的多的多？&lt;/p>
&lt;/blockquote>
&lt;p>传输完成最终会触发 NET_RX，而不是 NET_TX。所以自然你观测 /proc/softirqs 也就能看到 NET_RX 更多了。&lt;/p>
&lt;p>好，现在你已经对内核是怎么发送网络包的有一个全局上的把握了。不要得意，我们需要了解的细节才是更有价值的地方，让我们继续！！&lt;/p>
&lt;h2 id="二网卡启动准备">二、网卡启动准备&lt;a class="td-heading-self-link" href="#%e4%ba%8c%e7%bd%91%e5%8d%a1%e5%90%af%e5%8a%a8%e5%87%86%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现在的服务器上的网卡一般都是支持多队列的。每一个队列上都是由一个 RingBuffer 表示的，开启了多队列以后的的网卡就会对应有多个 RingBuffer。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>网卡在启动时最重要的任务之一就是分配和初始化 RingBuffer，理解了 RingBuffer 将会非常有助于后面我们掌握发送。因为今天的主题是发送，所以就以传输队列为例，我们来看下网卡启动时分配 RingBuffer 的实际过程。&lt;/p>
&lt;p>在网卡启动的时候，会调用到 __igb_open 函数，RingBuffer 就是在这里分配的。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static int __igb_open(struct net_device _netdev, bool resuming)
{
 struct igb_adapter _adapter = netdev_priv(netdev);&lt;/p>
&lt;p>// 分配传输描述符数组
 err = igb_setup_all_tx_resources(adapter);&lt;/p>
&lt;p>// 分配接收描述符数组
 err = igb_setup_all_rx_resources(adapter);&lt;/p>
&lt;p>// 开启全部队列
 netif_tx_start_all_queues(netdev);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在上面 __igb_open 函数调用 igb_setup_all_tx_resources 分配所有的传输 RingBuffer, 调用 igb_setup_all_rx_resources 创建所有的接收 RingBuffer。&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static int igb_setup_all_tx_resources(struct igb_adapter *adapter) {  // 有几个队列就构造几个 RingBuffer  for (i = 0; i &amp;lt; adapter-&amp;gt;num_tx_queues; i++) {   igb_setup_tx_resources(adapter-&amp;gt;tx_ring[i]);  } }&lt;/code>&lt;/p>
&lt;p>真正的 RingBuffer 构造过程是在 igb_setup_tx_resources 中完成的。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb*main.c
int igb_setup_tx_resources(struct igb_ring _tx_ring)
{
 //1. 申请  igb_tx_buffer  数组内存
 size = sizeof(struct igb_tx_buffer) * tx_ring-&amp;gt;count;
 tx_ring-&amp;gt;tx_buffer_info = vzalloc(size);&lt;/p>
&lt;p>//2. 申请  e1000_adv_tx_desc DMA  数组内存
 tx_ring-&amp;gt;size = tx_ring-&amp;gt;count * sizeof(union e1000_adv_tx_desc);
 tx_ring-&amp;gt;size = ALIGN(tx_ring-&amp;gt;size, 4096);
 tx_ring-&amp;gt;desc = dma_alloc_coherent(dev, tx_ring-&amp;gt;size,
        &amp;amp;tx_ring-&amp;gt;dma, GFP_KERNEL);&lt;/p>
&lt;p>//3. 初始化队列成员
 tx_ring-&amp;gt;next_to_use = 0;
 tx_ring-&amp;gt;next_to_clean = 0;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>从上述源码可以看到，实际上一个 RingBuffer 的内部不仅仅是一个环形队列数组，而是有两个。&lt;/p>
&lt;p>1）igb_tx_buffer 数组：这个数组是内核使用的，通过 vzalloc 申请的。
2）e1000_adv_tx_desc 数组：这个数组是网卡硬件使用的，硬件是可以通过 DMA 直接访问这块内存，通过 dma_alloc_coherent 分配。&lt;/p>
&lt;p>这个时候它们之间还没有啥联系。将来在发送的时候，这两个环形数组中相同位置的指针将都将指向同一个 skb。这样，内核和硬件就能共同访问同样的数据了，内核往 skb 里写数据，网卡硬件负责发送。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>最后调用 netif_tx_start_all_queues 开启队列。另外，对于硬中断的处理函数 igb_msix_ring 其实也是在 __igb_open 中注册的。&lt;/p>
&lt;h2 id="三accept-创建新-socket">三、accept 创建新 socket&lt;a class="td-heading-self-link" href="#%e4%b8%89accept-%e5%88%9b%e5%bb%ba%e6%96%b0-socket" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>在发送数据之前，我们往往还需要一个已经建立好连接的 socket。&lt;/p>
&lt;p>我们就以开篇服务器缩微源代码中提到的 accept 为例，当 accept 之后，进程会创建一个新的 socket 出来，然后把它放到当前进程的打开文件列表中，专门用于和对应的客户端通信。&lt;/p>
&lt;p>假设服务器进程通过 accept 和客户端建立了两条连接，我们来简单看一下这两条连接和进程的关联关系。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>其中代表一条连接的 socket 内核对象更为具体一点的结构图如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>为了避免喧宾夺主，accept 详细的源码过程这里就不介绍了，感兴趣请参考 &lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484905&amp;amp;idx=1&amp;amp;sn=a74ed5d7551c4fb80a8abe057405ea5e&amp;amp;scene=21#wechat_redirect">《图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！》&lt;/a>。一文中的第一部分。&lt;/p>
&lt;p>今天我们还是把重点放到数据发送过程上。&lt;/p>
&lt;h2 id="四发送数据真正开始">四、发送数据真正开始&lt;a class="td-heading-self-link" href="#%e5%9b%9b%e5%8f%91%e9%80%81%e6%95%b0%e6%8d%ae%e7%9c%9f%e6%ad%a3%e5%bc%80%e5%a7%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="41-send-系统调用实现">4.1 send 系统调用实现&lt;a class="td-heading-self-link" href="#41-send-%e7%b3%bb%e7%bb%9f%e8%b0%83%e7%94%a8%e5%ae%9e%e7%8e%b0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>send 系统调用的源码位于文件 net/socket.c 中。在这个系统调用里，内部其实真正使用的是 sendto 系统调用。整个调用链条虽然不短，但其实主要只干了两件简单的事情，&lt;/p>
&lt;ul>
&lt;li>第一是在内核中把真正的 socket 找出来，在这个对象里记录着各种协议栈的函数地址。&lt;/li>
&lt;li>第二是构造一个 struct msghdr 对象，把用户传入的数据，比如 buffer 地址、数据长度啥的，统统都装进去.&lt;/li>
&lt;/ul>
&lt;p>剩下的事情就交给下一层，协议栈里的函数 inet_sendmsg 了，其中 inet_sendmsg 函数的地址是通过 socket 内核对象里的 ops 成员找到的。大致流程如图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>有了上面的了解，我们再看起源码就要容易许多了。源码如下：&lt;/p>
&lt;p>`//file: net/socket.c
SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len,
  unsigned int, flags)
{
 return sys_sendto(fd, buff, len, flags, NULL, 0);
}&lt;/p>
&lt;p>SYSCALL_DEFINE6(&amp;hellip;&amp;hellip;)
{
 //1. 根据  fd  查找到  socket
 sock = sockfd_lookup_light(fd, &amp;amp;err, &amp;amp;fput_needed);&lt;/p>
&lt;p>//2. 构造  msghdr
 struct msghdr msg;
 struct iovec iov;&lt;/p>
&lt;p>iov.iov_base = buff;
 iov.iov_len = len;
 msg.msg_iovlen = 1;&lt;/p>
&lt;p>msg.msg_iov = &amp;amp;iov;
 msg.msg_flags = flags;
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>//3. 发送数据
 sock_sendmsg(sock, &amp;amp;msg, len);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>从源码可以看到，我们在用户态使用的 send 函数和 sendto 函数其实都是 sendto 系统调用实现的。send 只是为了方便，封装出来的一个更易于调用的方式而已。&lt;/p>
&lt;p>在 sendto 系统调用里，首先根据用户传进来的 socket 句柄号来查找真正的 socket 内核对象。接着把用户请求的 buff、len、flag 等参数都统统打包到一个 struct msghdr 对象中。&lt;/p>
&lt;p>接着调用了 sock_sendmsg =&amp;gt; __sock_sendmsg ==&amp;gt;  __sock_sendmsg_nosec。在__sock_sendmsg_nosec 中，调用将会由系统调用进入到协议栈，我们来看它的源码。&lt;/p>
&lt;p>&lt;code>//file: net/socket.c static inline int __sock_sendmsg_nosec(...) {  ......  return sock-&amp;gt;ops-&amp;gt;sendmsg(iocb, sock, msg, size); }&lt;/code>&lt;/p>
&lt;p>通过第三节里的 socket 内核对象结构图，我们可以看到，这里调用的是 sock-&amp;gt;ops-&amp;gt;sendmsg 实际执行的是 inet_sendmsg。这个函数是 AF_INET 协议族提供的通用发送函数。&lt;/p>
&lt;h3 id="42-传输层处理">4.2 传输层处理&lt;a class="td-heading-self-link" href="#42-%e4%bc%a0%e8%be%93%e5%b1%82%e5%a4%84%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="1传输层拷贝">1）传输层拷贝&lt;a class="td-heading-self-link" href="#1%e4%bc%a0%e8%be%93%e5%b1%82%e6%8b%b7%e8%b4%9d" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>在进入到协议栈 inet_sendmsg 以后，内核接着会找到 socket 上的具体协议发送函数。对于 TCP 协议来说，那就是 tcp_sendmsg（同样也是通过 socket 内核对象找到的）。&lt;/p>
&lt;p>在这个函数中，内核会申请一个内核态的 skb 内存，将用户待发送的数据拷贝进去。注意这个时候不一定会真正开始发送，如果没有达到发送条件的话很可能这次调用直接就返回了。大概过程如图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看 inet_sendmsg 函数的源码。&lt;/p>
&lt;p>&lt;code>//file: net/ipv4/af_inet.c int inet_sendmsg(......) {  ......  return sk-&amp;gt;sk_prot-&amp;gt;sendmsg(iocb, sk, msg, size); }&lt;/code>&lt;/p>
&lt;p>在这个函数中会调用到具体协议的发送函数。同样参考第三节里的 socket 内核对象结构图，我们看到对于 TCP 协议下的 socket 来说，来说 sk-&amp;gt;sk_prot-&amp;gt;sendmsg 指向的是 tcp_sendmsg（对于 UPD 来说是 udp_sendmsg）。&lt;/p>
&lt;p>tcp_sendmsg 这个函数比较长，我们分多次来看它。先看这一段&lt;/p>
&lt;p>`//file: net/ipv4/tcp.c
int tcp_sendmsg(&amp;hellip;)
{
 while(&amp;hellip;){
  while(&amp;hellip;){
   // 获取发送队列
   skb = tcp_write_queue_tail(sk);&lt;/p>
&lt;p>// 申请 skb  并拷贝
   &amp;hellip;&amp;hellip;
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>&lt;code>//file: include/net/tcp.h static inline struct sk_buff *tcp_write_queue_tail(const struct sock *sk) {  return skb_peek_tail(&amp;amp;sk-&amp;gt;sk_write_queue); }&lt;/code>&lt;/p>
&lt;p>理解对 socket 调用 tcp_write_queue_tail 是理解发送的前提。如上所示，这个函数是在获取 socket 发送队列中的最后一个 skb。skb 是 struct sk_buff 对象的简称，用户的发送队列就是该对象组成的一个链表。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们再接着看 tcp_sendmsg 的其它部分。&lt;/p>
&lt;p>`//file: net/ipv4/tcp.c
int tcp_sendmsg(struct kiocb _iocb, struct sock _sk, struct msghdr *msg,
  size_t size)
{
 // 获取用户传递过来的数据和标志
 iov = msg-&amp;gt;msg_iov; // 用户数据地址
 iovlen = msg-&amp;gt;msg_iovlen; // 数据块数为 1
 flags = msg-&amp;gt;msg_flags; // 各种标志&lt;/p>
&lt;p>// 遍历用户层的数据块
 while (&amp;ndash;iovlen&amp;gt;= 0) {&lt;/p>
&lt;p>// 待发送数据块的地址
  unsigned char __user *from = iov-&amp;gt;iov_base;&lt;/p>
&lt;p>while (seglen&amp;gt; 0) {&lt;/p>
&lt;p>// 需要申请新的  skb
   if (copy &amp;lt;= 0) {&lt;/p>
&lt;p>// 申请  skb，并添加到发送队列的尾部
    skb = sk_stream_alloc_skb(sk,
         select_size(sk, sg),
         sk-&amp;gt;sk_allocation);&lt;/p>
&lt;p>// 把  skb  挂到 socket 的发送队列上
    skb_entail(sk, skb);
   }&lt;/p>
&lt;p>// skb  中有足够的空间
   if (skb_availroom(skb) &amp;gt; 0) {
    // 拷贝用户空间的数据到内核空间，同时计算校验和
    //from 是用户空间的数据地址  
    skb_add_data_nocache(sk, skb, from, copy);
   } 
   &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>`&lt;/p>
&lt;p>这个函数比较长，不过其实逻辑并不复杂。其中 msg-&amp;gt;msg_iov 存储的是用户态内存的要发送的数据的 buffer。接下来在内核态申请内核内存，比如 skb，并把用户内存里的数据拷贝到内核态内存中。&lt;strong>这就会涉及到一次或者几次内存拷贝的开销&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>至于内核什么时候真正把 skb 发送出去。在 tcp_sendmsg 中会进行一些判断。&lt;/p>
&lt;p>`//file: net/ipv4/tcp.c
int tcp_sendmsg(&amp;hellip;)
{
 while(&amp;hellip;){
  while(&amp;hellip;){
   // 申请内核内存并进行拷贝&lt;/p>
&lt;p>// 发送判断
   if (forced_push(tp)) {
    tcp_mark_push(tp, skb);
    __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);
   } else if (skb == tcp_send_head(sk))
    tcp_push_one(sk, mss_now);  
   }
   continue;
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>只有满足 forced_push(tp) 或者 skb == tcp_send_head(sk) 成立的时候，内核才会真正启动发送数据包。其中 forced_push(tp) 判断的是未发送的数据数据是否已经超过最大窗口的一半了。&lt;/p>
&lt;p>条件都不满足的话，&lt;strong>这次的用户要发送的数据只是拷贝到内核就算完事了！&lt;/strong>&lt;/p>
&lt;h4 id="2传输层发送">2）传输层发送&lt;a class="td-heading-self-link" href="#2%e4%bc%a0%e8%be%93%e5%b1%82%e5%8f%91%e9%80%81" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>假设现在内核发送条件已经满足了，我们再来跟踪一下实际的发送过程。对于上小节函数中，当满足真正发送条件的时候，无论调用的是 __tcp_push_pending_frames 还是 tcp_push_one 最终都实际会执行到 tcp_write_xmit。&lt;/p>
&lt;p>所以我们直接从 tcp_write_xmit 看起，这个函数处理了传输层的拥塞控制、滑动窗口相关的工作。满足窗口要求的时候，设置一下 TCP 头然后将 skb 传到更低的网络层进行处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看下 tcp_write_xmit 的源码。&lt;/p>
&lt;p>`//file: net/ipv4/tcp_output.c
static bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,
      int push_one, gfp_t gfp)
{
 // 循环获取待发送  skb
 while ((skb = tcp_send_head(sk))) 
 {
  // 滑动窗口相关
  cwnd_quota = tcp_cwnd_test(tp, skb);
  tcp_snd_wnd_test(tp, skb, mss_now);
  tcp_mss_split_point(&amp;hellip;);
  tso_fragment(sk, skb, &amp;hellip;);
  &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>// 真正开启发送
  tcp_transmit_skb(sk, skb, 1, gfp);
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>可以看到我们之前在网络协议里学的滑动窗口、拥塞控制就是在这个函数中完成的，这部分就不过多展开了，感兴趣同学自己找这段源码来读。我们今天只看发送主过程，那就走到了 tcp_transmit_skb。&lt;/p>
&lt;p>`//file: net/ipv4/tcp_output.c
static int tcp_transmit_skb(struct sock _sk, struct sk_buff _skb, int clone_it,
    gfp_t gfp_mask)
{
 //1. 克隆新  skb  出来
 if (likely(clone_it)) {
  skb = skb_clone(skb, gfp_mask);
  &amp;hellip;&amp;hellip;
 }&lt;/p>
&lt;p>//2. 封装  TCP  头
 th = tcp_hdr(skb);
 th-&amp;gt;source  = inet-&amp;gt;inet_sport;
 th-&amp;gt;dest  = inet-&amp;gt;inet_dport;
 th-&amp;gt;window  = &amp;hellip;;
 th-&amp;gt;urg   = &amp;hellip;;
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>//3. 调用网络层发送接口
 err = icsk-&amp;gt;icsk_af_ops-&amp;gt;queue_xmit(skb, &amp;amp;inet-&amp;gt;cork.fl);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>第一件事是先克隆一个新的 skb，这里重点说下为什么要复制一个 skb 出来呢？&lt;/p>
&lt;p>是因为 skb 后续在调用网络层，最后到达网卡发送完成的时候，这个 skb 会被释放掉。而我们知道 TCP 协议是支持丢失重传的，在收到对方的 ACK 之前，这个 skb 不能被删除。所以内核的做法就是每次调用网卡发送的时候，实际上传递出去的是 skb 的一个拷贝。等收到 ACK 再真正删除。&lt;/p>
&lt;p>第二件事是修改 skb 中的 TCP header，根据实际情况把 TCP 头设置好。这里要介绍一个小技巧，skb 内部其实包含了网络协议中所有的 header。在设置 TCP 头的时候，只是把指针指向 skb 的合适位置。后面再设置 IP 头的时候，在把指针挪一挪就行，避免频繁的内存申请和拷贝，效率很高。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>tcp_transmit_skb 是发送数据位于传输层的最后一步，接下来就可以进入到网络层进行下一层的操作了。调用了网络层提供的发送接口 icsk-&amp;gt;icsk_af_ops-&amp;gt;queue_xmit()。&lt;/p>
&lt;p>在下面的这个源码中，我们的知道了 queue_xmit 其实指向的是 ip_queue_xmit 函数。&lt;/p>
&lt;p>&lt;code>//file: net/ipv4/tcp_ipv4.c const struct inet_connection_sock_af_ops ipv4_specific = {  .queue_xmit    = ip_queue_xmit,  .send_check    = tcp_v4_send_check,  ... }&lt;/code>&lt;/p>
&lt;p>自此，传输层的工作也就都完成了。数据离开了传输层，接下来将会进入到内核在网络层的实现里。&lt;/p>
&lt;h3 id="43-网络层发送处理">4.3 网络层发送处理&lt;a class="td-heading-self-link" href="#43-%e7%bd%91%e7%bb%9c%e5%b1%82%e5%8f%91%e9%80%81%e5%a4%84%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Linux 内核网络层的发送的实现位于 net/ipv4/ip_output.c 这个文件。传输层调用到的 ip_queue_xmit 也在这里。（从文件名上也能看出来进入到 IP 层了，源文件名已经从 tcp_xxx 变成了 ip_xxx。）&lt;/p>
&lt;p>在网络层里主要处理路由项查找、IP 头设置、netfilter 过滤、skb 切分（大于 MTU 的话）等几项工作，处理完这些工作后会交给更下层的邻居子系统来处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看网络层入口函数 ip_queue_xmit 的源码：&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c
int ip_queue_xmit(struct sk_buff _skb, struct flowi _fl)
{
 // 检查  socket  中是否有缓存的路由表
 rt = (struct rtable *)__sk_dst_check(sk, 0);
 if (rt == NULL) {
  // 没有缓存则展开查找
  // 则查找路由项，  并缓存到  socket  中
  rt = ip_route_output_ports(&amp;hellip;);
  sk_setup_caps(sk, &amp;amp;rt-&amp;gt;dst);
 }&lt;/p>
&lt;p>// 为  skb  设置路由表
 skb_dst_set_noref(skb, &amp;amp;rt-&amp;gt;dst);&lt;/p>
&lt;p>// 设置  IP header
 iph = ip_hdr(skb);
 iph-&amp;gt;protocol = sk-&amp;gt;sk_protocol;
 iph-&amp;gt;ttl      = ip_select_ttl(inet, &amp;amp;rt-&amp;gt;dst);
 iph-&amp;gt;frag_off = &amp;hellip;;&lt;/p>
&lt;p>// 发送
 ip_local_out(skb);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>ip_queue_xmit 已经到了网络层，在这个函数里我们看到了网络层相关的功能路由项查找，如果找到了则设置到 skb 上（没有路由的话就直接报错返回了）。&lt;/p>
&lt;p>在 Linux 上通过 route 命令可以看到你本机的路由配置。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>在路由表中，可以查到某个目的网络应该通过哪个 Iface（网卡），哪个 Gateway（网卡）发送出去。查找出来以后缓存到 socket 上，下次再发送数据就不用查了。&lt;/p>
&lt;p>接着把路由表地址也放到 skb 里去。&lt;/p>
&lt;p>&lt;code>//file: include/linux/skbuff.h struct sk_buff {  // 保存了一些路由相关信息  unsigned long  _skb_refdst; }&lt;/code>&lt;/p>
&lt;p>接下来就是定位到 skb 里的 IP 头的位置上，然后开始按照协议规范设置 IP header。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>再通过 ip_local_out 进入到下一步的处理。&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c  
int ip_local_out(struct sk_buff *skb)
{
 // 执行  netfilter  过滤
 err = __ip_local_out(skb);&lt;/p>
&lt;p>// 开始发送数据
 if (likely(err == 1))
  err = dst_output(skb);
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 ip_local_out =&amp;gt; __ip_local_out =&amp;gt; nf_hook 会执行 netfilter 过滤。如果你使用 iptables 配置了一些规则，那么这里将检测是否命中规则。&lt;strong>如果你设置了非常复杂的 netfilter 规则，在这个函数这里将会导致你的进程 CPU 开销会极大增加&lt;/strong>。&lt;/p>
&lt;p>还是不多展开说，继续只聊和发送有关的过程 dst_output。&lt;/p>
&lt;p>&lt;code>//file: include/net/dst.h static inline int dst_output(struct sk_buff *skb) {  return skb_dst(skb)-&amp;gt;output(skb); }&lt;/code>&lt;/p>
&lt;p>此函数找到到这个 skb 的路由表（dst 条目） ，然后调用路由表的 output 方法。这又是一个函数指针，指向的是 ip_output 方法。&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c
int ip_output(struct sk_buff *skb)
{
 // 统计
 &amp;hellip;..&lt;/p>
&lt;p>// 再次交给  netfilter，完毕后回调  ip_finish_output
 return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,
    ip_finish_output,
    !(IPCB(skb)-&amp;gt;flags &amp;amp; IPSKB_REROUTED));
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 ip_output 中进行一些简单的，统计工作，再次执行 netfilter 过滤。过滤通过之后回调 ip_finish_output。&lt;/p>
&lt;p>&lt;code>//file: net/ipv4/ip_output.c static int ip_finish_output(struct sk_buff *skb) {  // 大于 mtu 的话就要进行分片了  if (skb-&amp;gt;len &amp;gt; ip_skb_dst_mtu(skb) &amp;amp;&amp;amp; !skb_is_gso(skb))   return ip_fragment(skb, ip_finish_output2);  else   return ip_finish_output2(skb); }&lt;/code>&lt;/p>
&lt;p>在 ip_finish_output 中我们看到，&lt;strong>如果数据大于 MTU 的话，是会执行分片的。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>实际 MTU 大小确定依赖 MTU 发现，以太网帧为 1500 字节。之前 QQ 团队在早期的时候，会尽量控制自己数据包尺寸小于 MTU，通过这种方式来优化网络性能。因为分片会带来两个问题：1、需要进行额外的切分处理，有额外性能开销。2、只要一个分片丢失，整个包都得重传。所以避免分片既杜绝了分片开销，也大大降低了重传率。&lt;/p>
&lt;/blockquote>
&lt;p>在 ip_finish_output2 中，终于发送过程会进入到下一层，邻居子系统中。&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c
static inline int ip_finish_output2(struct sk_buff *skb)
{
 // 根据下一跳 IP 地址查找邻居项，找不到就创建一个
 nexthop = (**force u32) rt_nexthop(rt, ip_hdr(skb)-&amp;gt;daddr);  
 neigh = **ipv4_neigh_lookup_noref(dev, nexthop);
 if (unlikely(!neigh))
  neigh = __neigh_create(&amp;amp;arp_tbl, &amp;amp;nexthop, dev, false);&lt;/p>
&lt;p>// 继续向下层传递
 int res = dst_neigh_output(dst, neigh, skb);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;h3 id="44-邻居子系统">4.4 邻居子系统&lt;a class="td-heading-self-link" href="#44-%e9%82%bb%e5%b1%85%e5%ad%90%e7%b3%bb%e7%bb%9f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>邻居子系统是位于网络层和数据链路层中间的一个系统，其作用是对网络层提供一个封装，让网络层不必关心下层的地址信息，让下层来决定发送到哪个 MAC 地址。&lt;/p>
&lt;p>而且这个邻居子系统并不位于协议栈 net/ipv4/ 目录内，而是位于 net/core/neighbour.c。因为无论是对于 IPv4 还是 IPv6 ，都需要使用该模块。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>在邻居子系统里主要是查找或者创建邻居项，在创造邻居项的时候，有可能会发出实际的 arp 请求。然后封装一下 MAC 头，将发送过程再传递到更下层的网络设备子系统。大致流程如图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>理解了大致流程，我们再回头看源码。在上面小节 ip_finish_output2 源码中调用了 __ipv4_neigh_lookup_noref。它是在 arp 缓存中进行查找，其第二个参数传入的是路由下一跳 IP 信息。&lt;/p>
&lt;p>`//file: include/net/arp.h
extern struct neigh_table arp_tbl;
static inline struct neighbour ___ipv4_neigh_lookup_noref(
 struct net_device _dev, u32 key)
{
 struct neigh_hash_table *nht = rcu_dereference_bh(arp_tbl.nht);&lt;/p>
&lt;p>// 计算  hash  值，加速查找
 hash_val = arp_hashfn(&amp;hellip;&amp;hellip;);
 for (n = rcu_dereference_bh(nht-&amp;gt;hash_buckets[hash_val]);
   n != NULL;
   n = rcu_dereference_bh(n-&amp;gt;next)) {
  if (n-&amp;gt;dev == dev &amp;amp;&amp;amp; _(u32 _)n-&amp;gt;primary_key == key)
   return n;
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>如果查找不到，则调用 __neigh_create 创建一个邻居。&lt;/p>
&lt;p>`//file: net/core/neighbour.c
struct neighbour ___neigh_create(&amp;hellip;&amp;hellip;)
{
 // 申请邻居表项
 struct neighbour _n1, _rc, _n = neigh_alloc(tbl, dev);&lt;/p>
&lt;p>// 构造赋值
 memcpy(n-&amp;gt;primary_key, pkey, key_len);
 n-&amp;gt;dev = dev;
 n-&amp;gt;parms-&amp;gt;neigh_setup(n);&lt;/p>
&lt;p>// 最后添加到邻居  hashtable  中
 rcu_assign_pointer(nht-&amp;gt;hash_buckets[hash_val], n);
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>`&lt;/p>
&lt;p>有了邻居项以后，此时仍然还不具备发送 IP 报文的能力，因为目的 MAC 地址还未获取。调用 dst_neigh_output 继续传递 skb。&lt;/p>
&lt;p>&lt;code>//file: include/net/dst.h static inline int dst_neigh_output(struct dst_entry *dst,       struct neighbour *n, struct sk_buff *skb) {  ......  return n-&amp;gt;output(n, skb); }&lt;/code>&lt;/p>
&lt;p>调用 output，实际指向的是 neigh_resolve_output。在这个函数内部有可能会发出 arp 网络请求。&lt;/p>
&lt;p>`//file: net/core/neighbour.c
int neigh_resolve_output(){&lt;/p>
&lt;p>// 注意：这里可能会触发 arp 请求
 if (!neigh_event_send(neigh, skb)) {&lt;/p>
&lt;p>//neigh-&amp;gt;ha  是  MAC  地址
  dev_hard_header(skb, dev, ntohs(skb-&amp;gt;protocol),
           neigh-&amp;gt;ha, NULL, skb-&amp;gt;len);
  // 发送
  dev_queue_xmit(skb);
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>当获取到硬件 MAC 地址以后，就可以封装 skb 的 MAC 头了。最后调用 dev_queue_xmit 将 skb 传递给 Linux 网络设备子系统。&lt;/p>
&lt;h3 id="45-网络设备子系统">4.5 网络设备子系统&lt;a class="td-heading-self-link" href="#45-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%ad%90%e7%b3%bb%e7%bb%9f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>邻居子系统通过 dev_queue_xmit 进入到网络设备子系统中来。&lt;/p>
&lt;p>`//file: net/core/dev.c 
int dev_queue_xmit(struct sk_buff *skb)
{
 // 选择发送队列
 txq = netdev_pick_tx(dev, skb);&lt;/p>
&lt;p>// 获取与此队列关联的排队规则
 q = rcu_dereference_bh(txq-&amp;gt;qdisc);&lt;/p>
&lt;p>// 如果有队列，则调用**dev_xmit_skb  继续处理数据
 if (q-&amp;gt;enqueue) {
  rc = **dev_xmit_skb(skb, q, dev, txq);
  goto out;
 }&lt;/p>
&lt;p>// 没有队列的是回环设备和隧道设备
 &amp;hellip;&amp;hellip;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>开篇第二节网卡启动准备里我们说过，网卡是有多个发送队列的（尤其是现在的网卡）。上面对 netdev_pick_tx 函数的调用就是选择一个队列进行发送。&lt;/p>
&lt;p>netdev_pick_tx 发送队列的选择受 XPS 等配置的影响，而且还有缓存，也是一套小复杂的逻辑。这里我们只关注两个逻辑，首先会获取用户的 XPS 配置，否则就自动计算了。代码见 netdev_pick_tx =&amp;gt; __netdev_pick_tx。&lt;/p>
&lt;p>`//file: net/core/flow_dissector.c
u16 __netdev_pick_tx(struct net_device _dev, struct sk_buff _skb)
{
 // 获取  XPS  配置
 int new_index = get_xps_queue(dev, skb);&lt;/p>
&lt;p>// 自动计算队列
 if (new_index &amp;lt; 0)
  new_index = skb_tx_hash(dev, skb);}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>然后获取与此队列关联的 qdisc。在 linux 上通过 tc 命令可以看到 qdisc 类型，例如对于我的某台多队列网卡机器上是 mq disc。&lt;/p>
&lt;p>&lt;code>#tc qdisc qdisc mq 0: dev eth0 root&lt;/code>&lt;/p>
&lt;p>大部分的设备都有队列（回环设备和隧道设备除外），所以现在我们进入到 __dev_xmit_skb。&lt;/p>
&lt;p>`//file: net/core/dev.c
static inline int __dev_xmit_skb(struct sk_buff _skb, struct Qdisc _q,
     struct net_device _dev,
     struct netdev_queue _txq)
{
 //1. 如果可以绕开排队系统
 if ((q-&amp;gt;flags &amp;amp; TCQ_F_CAN_BYPASS) &amp;amp;&amp;amp; !qdisc_qlen(q) &amp;amp;&amp;amp;
     qdisc_run_begin(q)) {
  &amp;hellip;&amp;hellip;
 }&lt;/p>
&lt;p>//2. 正常排队
 else {&lt;/p>
&lt;p>// 入队
  q-&amp;gt;enqueue(skb, q)&lt;/p>
&lt;p>// 开始发送
  __qdisc_run(q);
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>上述代码中分两种情况，1 是可以 bypass（绕过）排队系统的，另外一种是正常排队。我们只看第二种情况。&lt;/p>
&lt;p>先调用 q-&amp;gt;enqueue 把 skb 添加到队列里。然后调用 __qdisc_run 开始发送。&lt;/p>
&lt;p>`//file: net/sched/sch_generic.c
void __qdisc_run(struct Qdisc *q)
{
 int quota = weight_p;&lt;/p>
&lt;p>// 循环从队列取出一个  skb  并发送
 while (qdisc_restart(q)) {&lt;/p>
&lt;p>//  如果发生下面情况之一，则延后处理：
  // 1. quota  用尽
  // 2.  其他进程需要  CPU
  if (&amp;ndash;quota &amp;lt;= 0 || need_resched()) {
   // 将触发一次  NET_TX_SOFTIRQ  类型  softirq
   __netif_schedule(q);
   break;
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在上述代码中，我们看到 while 循环不断地从队列中取出 skb 并进行发送。注意，这个时候其实都占用的是用户进程的系统态时间 (sy)。只有当 quota 用尽或者其它进程需要 CPU 的时候才触发软中断进行发送。&lt;/p>
&lt;p>&lt;strong>所以这就是为什么一般服务器上查看 /proc/softirqs，一般 NET_RX 都要比 NET_TX 大的多的第二个原因&lt;/strong>。对于读来说，都是要经过 NET_RX 软中断，而对于发送来说，只有系统态配额用尽才让软中断上。&lt;/p>
&lt;p>我们来把精力在放到 qdisc_restart 上，继续看发送过程。&lt;/p>
&lt;p>`static inline int qdisc_restart(struct Qdisc *q)
{
 // 从  qdisc  中取出要发送的  skb
 skb = dequeue_skb(q);
 &amp;hellip;&lt;/p>
&lt;p>return sch_direct_xmit(skb, q, dev, txq, root_lock);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>qdisc_restart 从队列中取出一个 skb，并调用 sch_direct_xmit 继续发送。&lt;/p>
&lt;p>&lt;code>//file: net/sched/sch_generic.c int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,    struct net_device *dev, struct netdev_queue *txq,    spinlock_t *root_lock) {  // 调用驱动程序来发送数据  ret = dev_hard_start_xmit(skb, dev, txq); }&lt;/code>&lt;/p>
&lt;h3 id="46-软中断调度">4.6 软中断调度&lt;a class="td-heading-self-link" href="#46-%e8%bd%af%e4%b8%ad%e6%96%ad%e8%b0%83%e5%ba%a6" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>在 4.5 咱们看到了如果系统态 CPU 发送网络包不够用的时候，会调用 __netif_schedule 触发一个软中断。该函数会进入到 __netif_reschedule，由它来实际发出 NET_TX_SOFTIRQ 类型软中断。&lt;/p>
&lt;p>软中断是由内核线程来运行的，该线程会进入到 net_tx_action 函数，在该函数中能获取到发送队列，并也最终调用到驱动程序里的入口函数 dev_hard_start_xmit。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>`//file: net/core/dev.c
static inline void **netif_reschedule(struct Qdisc *q)
{
 sd = &amp;amp;**get_cpu_var(softnet_data);
 q-&amp;gt;next_sched = NULL;
 *sd-&amp;gt;output_queue_tailp = q;
 sd-&amp;gt;output_queue_tailp = &amp;amp;q-&amp;gt;next_sched;&lt;/p>
&lt;p>&amp;hellip;&amp;hellip;
 raise_softirq_irqoff(NET_TX_SOFTIRQ);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在该函数里在软中断能访问到的 softnet_data 里设置了要发送的数据队列，添加到了 output_queue 里了。紧接着触发了 NET_TX_SOFTIRQ 类型的软中断。（T 代表 transmit 传输）&lt;/p>
&lt;p>软中断的入口代码我这里也不详细扒了，感兴趣的同学参考&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">《图解 Linux 网络包接收过程》&lt;/a>一文中的 3.2 小节 - ksoftirqd 内核线程处理软中断。&lt;/p>
&lt;p>我们直接从 NET_TX_SOFTIRQ softirq 注册的回调函数 net_tx_action 讲起。用户态进程触发完软中断之后，会有一个软中断内核线程会执行到 net_tx_action。&lt;/p>
&lt;p>&lt;strong>牢记，这以后发送数据消耗的 CPU 就都显示在 si 这里了，不会消耗用户进程的系统时间了&lt;/strong>。&lt;/p>
&lt;p>`//file: net/core/dev.c
static void net_tx_action(struct softirq_action _h)
{
 // 通过  softnet_data  获取发送队列
 struct softnet_data _sd = &amp;amp;__get_cpu_var(softnet_data);&lt;/p>
&lt;p>//  如果  output queue  上有  qdisc
 if (sd-&amp;gt;output_queue) {&lt;/p>
&lt;p>//  将  head  指向第一个  qdisc
  head = sd-&amp;gt;output_queue;&lt;/p>
&lt;p>// 遍历  qdsics  列表
  while (head) {
   struct Qdisc *q = head;
   head = head-&amp;gt;next_sched;&lt;/p>
&lt;p>// 发送数据
   qdisc_run(q);
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>软中断这里会获取 softnet_data。前面我们看到进程内核态在调用 __netif_reschedule 的时候把发送队列写到 softnet_data 的 output_queue 里了。软中断循环遍历 sd-&amp;gt;output_queue 发送数据帧。&lt;/p>
&lt;p>来看 qdisc_run，它和进程用户态一样，也会调用到 __qdisc_run。&lt;/p>
&lt;p>&lt;code>//file: include/net/pkt_sched.h static inline void qdisc_run(struct Qdisc *q) {  if (qdisc_run_begin(q))   __qdisc_run(q); }&lt;/code>&lt;/p>
&lt;p>然后一样就是进入 qdisc_restart =&amp;gt; sch_direct_xmit，直到驱动程序函数 dev_hard_start_xmit。&lt;/p>
&lt;h3 id="47-igb-网卡驱动发送">4.7 igb 网卡驱动发送&lt;a class="td-heading-self-link" href="#47-igb-%e7%bd%91%e5%8d%a1%e9%a9%b1%e5%8a%a8%e5%8f%91%e9%80%81" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>我们前面看到，无论是对于用户进程的内核态，还是对于软中断上下文，都会调用到网络设备子系统中的 dev_hard_start_xmit 函数。在这个函数中，会调用到驱动里的发送函数 igb_xmit_frame。&lt;/p>
&lt;p>在驱动函数里，将 skb 会挂到 RingBuffer 上，驱动调用完毕后，数据包将真正从网卡发送出去。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看看实际的源码：&lt;/p>
&lt;p>`//file: net/core/dev.c
int dev_hard_start_xmit(struct sk_buff _skb, struct net_device _dev,
   struct netdev_queue _txq)
{
 // 获取设备的回调函数集合  ops
 const struct net_device_ops _ops = dev-&amp;gt;netdev_ops;&lt;/p>
&lt;p>// 获取设备支持的功能列表
 features = netif_skb_features(skb);&lt;/p>
&lt;p>// 调用驱动的  ops  里面的发送回调函数  ndo_start_xmit  将数据包传给网卡设备
 skb_len = skb-&amp;gt;len;
 rc = ops-&amp;gt;ndo_start_xmit(skb, dev);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>其中 ndo_start_xmit 是网卡驱动要实现的一个函数，是在 net_device_ops 中定义的。&lt;/p>
&lt;p>`//file: include/linux/netdevice.h
struct net_device_ops {
 netdev_tx_t  (_ndo_start_xmit) (struct sk_buff _skb,
         struct net_device *dev);&lt;/p>
&lt;p>}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 igb 网卡驱动源码中，我们找到了。&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static const struct net_device_ops igb_netdev_ops = {  .ndo_open  = igb_open,  .ndo_stop  = igb_close,  .ndo_start_xmit  = igb_xmit_frame,   ... };&lt;/code>&lt;/p>
&lt;p>也就是说，对于网络设备层定义的 ndo_start_xmit， igb 的实现函数是 igb_xmit_frame。这个函数是在网卡驱动初始化的时候被赋值的。具体初始化过程参见&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">《图解 Linux 网络包接收过程》&lt;/a>一文中的 2.4 节，网卡驱动初始化。&lt;/p>
&lt;p>所以在上面网络设备层调用 ops-&amp;gt;ndo_start_xmit 的时候，会实际上进入 igb_xmit_frame 这个函数中。我们进入这个函数来看看驱动程序是如何工作的。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static netdev_tx_t igb_xmit_frame(struct sk_buff _skb,
      struct net_device _netdev)
{
 &amp;hellip;&amp;hellip;
 return igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));
}&lt;/p>
&lt;p>netdev_tx_t igb_xmit_frame_ring(struct sk_buff _skb,
    struct igb_ring _tx_ring)
{
 // 获取 TX Queue  中下一个可用缓冲区信息
 first = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[tx_ring-&amp;gt;next_to_use];
 first-&amp;gt;skb = skb;
 first-&amp;gt;bytecount = skb-&amp;gt;len;
 first-&amp;gt;gso_segs = 1;&lt;/p>
&lt;p>//igb_tx_map 函数准备给设备发送的数据。
 igb_tx_map(tx_ring, first, hdr_len);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在这里从网卡的发送队列的 RingBuffer 中取下来一个元素，并将 skb 挂到元素上。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>igb_tx_map 函数处理将 skb 数据映射到网卡可访问的内存 DMA 区域。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static void igb_tx_map(struct igb_ring _tx_ring,
      struct igb_tx_buffer _first,
      const u8 hdr_len)
{
 // 获取下一个可用描述符指针
 tx_desc = IGB_TX_DESC(tx_ring, i);&lt;/p>
&lt;p>// 为  skb-&amp;gt;data  构造内存映射，以允许设备通过  DMA  从  RAM  中读取数据
 dma = dma_map_single(tx_ring-&amp;gt;dev, skb-&amp;gt;data, size, DMA_TO_DEVICE);&lt;/p>
&lt;p>// 遍历该数据包的所有分片, 为  skb  的每个分片生成有效映射
 for (frag = &amp;amp;skb_shinfo(skb)-&amp;gt;frags[0];; frag++) {&lt;/p>
&lt;p>tx_desc-&amp;gt;read.buffer_addr = cpu_to_le64(dma);
  tx_desc-&amp;gt;read.cmd_type_len = &amp;hellip;;
  tx_desc-&amp;gt;read.olinfo_status = 0;
 }&lt;/p>
&lt;p>// 设置最后一个 descriptor
 cmd_type |= size | IGB_TXD_DCMD;
 tx_desc-&amp;gt;read.cmd_type_len = cpu_to_le32(cmd_type);&lt;/p>
&lt;p>/_ Force memory writes to complete before letting h/w know there
  _ are new descriptors to fetch
  */
 wmb();
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>当所有需要的描述符都已建好，且 skb 的所有数据都映射到 DMA 地址后，驱动就会进入到它的最后一步，触发真实的发送。&lt;/p>
&lt;h3 id="48-发送完成硬中断">4.8 发送完成硬中断&lt;a class="td-heading-self-link" href="#48-%e5%8f%91%e9%80%81%e5%ae%8c%e6%88%90%e7%a1%ac%e4%b8%ad%e6%96%ad" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>当数据发送完成以后，其实工作并没有结束。因为内存还没有清理。当发送完成的时候，网卡设备会触发一个硬中断来释放内存。&lt;/p>
&lt;p>在&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">《图解 Linux 网络包接收过程》&lt;/a> 一文中的 3.1 和 3.2 小节，我们详细讲述过硬中断和软中断的处理过程。&lt;/p>
&lt;p>在发送完成硬中断里，会执行 RingBuffer 内存的清理工作，如图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>再回头看一下硬中断触发软中断的源码。&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static inline void ____napi_schedule(...){  list_add_tail(&amp;amp;napi-&amp;gt;poll_list, &amp;amp;sd-&amp;gt;poll_list);  __raise_softirq_irqoff(NET_RX_SOFTIRQ); }&lt;/code>&lt;/p>
&lt;p>这里有个很有意思的细节，无论硬中断是因为是有数据要接收，还是说发送完成通知，&lt;strong>从硬中断触发的软中断都是 NET_RX_SOFTIRQ&lt;/strong>。这个我们在第一节说过了，这是软中断统计中 RX 要高于 TX 的一个原因。&lt;/p>
&lt;p>好我们接着进入软中断的回调函数 igb_poll。在这个函数里，我们注意到有一行 igb_clean_tx_irq，参见源码：&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static int igb_poll(struct napi_struct *napi, int budget) {  //performs the transmit completion operations  if (q_vector-&amp;gt;tx.ring)   clean_complete = igb_clean_tx_irq(q_vector);  ... }&lt;/code>&lt;/p>
&lt;p>我们来看看当传输完成的时候，igb_clean_tx_irq 都干啥了。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static bool igb_clean_tx_irq(struct igb_q_vector *q_vector)
{
 //free the skb
 dev_kfree_skb_any(tx_buffer-&amp;gt;skb);&lt;/p>
&lt;p>//clear tx_buffer data
 tx_buffer-&amp;gt;skb = NULL;
 dma_unmap_len_set(tx_buffer, len, 0);&lt;/p>
&lt;p>// clear last DMA location and unmap remaining buffers */
 while (tx_desc != eop_desc) {
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>无非就是清理了 skb，解除了 DMA 映射等等。到了这一步，传输才算是基本完成了。&lt;/p>
&lt;p>为啥我说是基本完成，而不是全部完成了呢？因为传输层需要保证可靠性，所以 skb 其实还没有删除。它得等收到对方的 ACK 之后才会真正删除，那个时候才算是彻底的发送完毕。&lt;/p>
&lt;h2 id="最后">最后&lt;a class="td-heading-self-link" href="#%e6%9c%80%e5%90%8e" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>用一张图总结一下整个发送过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>了解了整个发送过程以后，我们回头再来回顾开篇提到的几个问题。&lt;/p>
&lt;p>&lt;strong>1. 我们在监控内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？&lt;/strong>&lt;/p>
&lt;p>在网络包的发送过程中，用户进程（在内核态）完成了绝大部分的工作，甚至连调用驱动的事情都干了。只有当内核态进程被切走前才会发起软中断。发送过程中，绝大部分（90%）以上的开销都是在用户进程内核态消耗掉的。&lt;/p>
&lt;p>只有一少部分情况下才会触发软中断（NET_TX 类型），由软中断 ksoftirqd 内核进程来发送。&lt;/p>
&lt;p>所以，在监控网络 IO 对服务器造成的 CPU 开销的时候，不能仅仅只看 si，而是应该把 si、sy 都考虑进来。&lt;/p>
&lt;p>&lt;strong>2. 在服务器上查看 /proc/softirqs，为什么 NET_RX 要比 NET_TX 大的多的多？&lt;/strong>&lt;/p>
&lt;p>之前我认为 NET_RX 是读取，NET_TX 是传输。对于一个既收取用户请求，又给用户返回的 Server 来说。这两块的数字应该差不多才对，至少不会有数量级的差异。但事实上，飞哥手头的一台服务器是这样的：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>经过今天的源码分析，发现这个问题的原因有两个。&lt;/p>
&lt;p>第一个原因是当数据发送完成以后，通过硬中断的方式来通知驱动发送完毕。但是硬中断无论是有数据接收，还是对于发送完毕，触发的软中断都是 NET_RX_SOFTIRQ，而并不是 NET_TX_SOFTIRQ。&lt;/p>
&lt;p>第二个原因是对于读来说，都是要经过 NET_RX 软中断的，都走 ksoftirqd 内核进程。而对于发送来说，绝大部分工作都是在用户进程内核态处理了，只有系统态配额用尽才会发出 NET_TX，让软中断上。&lt;/p>
&lt;p>综上两个原因，那么在机器上查看 NET_RX 比 NET_TX 大的多就不难理解了。&lt;/p>
&lt;p>&lt;strong>3. 发送网络数据的时候都涉及到哪些内存拷贝操作？&lt;/strong>&lt;/p>
&lt;p>这里的内存拷贝，我们只特指待发送数据的内存拷贝。&lt;/p>
&lt;p>第一次拷贝操作是内核申请完 skb 之后，这时候会将用户传递进来的 buffer 里的数据内容都拷贝到 skb 中。如果要发送的数据量比较大的话，这个拷贝操作开销还是不小的。&lt;/p>
&lt;p>第二次拷贝操作是从传输层进入网络层的时候，每一个 skb 都会被克隆一个新的副本出来。网络层以及下面的驱动、软中断等组件在发送完成的时候会将这个副本删除。传输层保存着原始的 skb，在当网络对方没有 ack 的时候，还可以重新发送，以实现 TCP 中要求的可靠传输。&lt;/p>
&lt;p>第三次拷贝不是必须的，只有当 IP 层发现 skb 大于 MTU 时才需要进行。会再申请额外的 skb，并将原来的 skb 拷贝为多个小的 skb。&lt;/p>
&lt;blockquote>
&lt;p>这里插入个题外话，大家在网络性能优化中经常听到的零拷贝，我觉得这有点点夸张的成分。TCP 为了保证可靠性，第二次的拷贝根本就没法省。如果包再大于 MTU 的话，分片时的拷贝同样也避免不了。&lt;/p>
&lt;/blockquote>
&lt;p>看到这里，相信内核发送数据包对于你来说，已经不再是一个完全不懂的黑盒了。本文哪怕你只看懂十分之一，你也已经掌握了这个黑盒的打开方式。这在你将来优化网络性能时你就会知道从哪儿下手了。&lt;/p>
&lt;p>还愣着干啥，赶紧帮飞哥&lt;strong>赞、再看、转发&lt;/strong>三连走起！&lt;/p>
&lt;p>&lt;strong>Github:&lt;/strong>&lt;a href="https://github.com/yanfeizhang/coder-kung-fu">https://github.com/yanfeizhang/coder-kung-fu&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;hr>
&lt;p>由于本文比较长，在公众号上看起来确实是有点费劲。所以飞哥搞了个 pdf，带目录结构，可快速跳转，看起来更方便。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>不过获取方式仍然有点小门槛：&lt;strong>带任意推荐语转发本文到朋友圈，加飞哥微信知会&lt;/strong>即可。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p></description></item><item><title>Docs: Netlink</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Netlink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Netlink/</guid><description/></item><item><title>Docs: Netplan</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Netplan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/Netplan/</guid><description/></item><item><title>Docs: network-scripts</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/network-scripts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/network-scripts/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s1-networkscripts-interfaces">RedHat 官方文档，生产文档-RedHatEnterpriseLinux-6-部署指南-11.2.接口配置文件&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/OpenMandrivaSoftware/initscripts/blob/master/sysconfig.txt">/usr/share/doc/initscripts-XX/sysconfig.txt&lt;/a>中的 ifcfg-&amp;lt;interface-name&amp;gt; 部分&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-ifcfg-rh.html">Manual(手册),nm-settings-ifcfg-rh(5)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>RedHad 相关发行版的网络配置通过一系列脚本实现，随着时代的发展，已经逐渐启用，并由 NetworkManager 取代，NetworkManager 还单独出了一个适用于 RedHad 发行版的插件，名为 nm-setting-ifcfg-rh。这样，NetworkManager 可以将原本的配置目录中文件的格式，转变为适应 RedHad 的格式，并将配置文件保存到 /etc/sysconfig/network-scripts/ 目录下。&lt;/p>
&lt;h1 id="关联文件">关联文件&lt;a class="td-heading-self-link" href="#%e5%85%b3%e8%81%94%e6%96%87%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;strong>/etc/sysconfig/&lt;/strong> # 全局&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./network&lt;/strong> # 全局网络配置&lt;/li>
&lt;li>&lt;strong>./network-scripts/&lt;/strong> # 曾经是网络配置脚本文件所在目录。CentOS 8 以后，移除了所有脚本，只用来为网络配置程序提供网络设备的配置文件
&lt;ul>
&lt;li>&lt;strong>./ifcfg-INTERFACE&lt;/strong> # 名为 INTERFACE 网络设备配置文件。通常情况下，INTERFACE 的值通常与配置文件中 DEVICE 指令的值相同。&lt;/li>
&lt;li>&lt;strong>./route-INTERFACE&lt;/strong> # IPv4 静态路由配置文件。INTERFACE 为网络设备名称，该路由条目仅对名为 INTERFACE 的网络设备起作用&lt;/li>
&lt;li>&lt;strong>./route6-INTERFACE&lt;/strong> # IPv6 静态路由配置文件。INTERFACE 为网络设备名称，该路由条目仅对名为 INTERFACE 的网络设备起作用&lt;/li>
&lt;li>&lt;strong>./rule-INTERFACE&lt;/strong> # 定义内核将流量路由到特定路由表的 IPv4 源网络规则。&lt;/li>
&lt;li>&lt;strong>./rule6-INTERFACE&lt;/strong> # 定义内核将流量路由到特定路由表的 IPv6 源网络规则。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>./networking/*&lt;/strong> #
&lt;ul>
&lt;li>注意：在 &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/ch-network_interfaces#s1-networkscripts-files">RedHat 6 文档&lt;/a>中表示，/etc/sysconfig/networking/ 目录由现在已经弃用的网络管理工具(system-config-network) 管理，这个内容不应该手动编辑。推荐使用 NetworkManager。并且在后续的版本中， NetworkManager 也接管了这些文件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/iproute2/rt_tables&lt;/strong> # 如果您想要使用名称而不是数字来引用特定的路由表，这个文件会定义映射映射。&lt;/p>
&lt;h2 id="rule-interface-文件">rule-INTERFACE 文件&lt;a class="td-heading-self-link" href="#rule-interface-%e6%96%87%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>from 192.0.2.0/24 lookup &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>from 203.0.113.0/24 lookup &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>来自 192.0.2.0/24 的流量根据 1 号路由表规则进行路由&lt;/p>
&lt;p>来自 203.0.113.0/24 的流量根据 2 号路由表规则进行路由&lt;/p>
&lt;h2 id="route-interface-文件">route-INTERFACE 文件&lt;a class="td-heading-self-link" href="#route-interface-%e6%96%87%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>第一种格式：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 默认路由下一条是 192.168.1.1，从 eth0 网络设备发出&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default via 192.168.1.1 dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># 目的网段是 10.0.0.1 且掩码是 255.255.255.0，从 eth1 网络设备发出数据包，下一跳为 192.168.0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.0.0.1 192.168.0.1 255.255.255.0 eth1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>第二种格式：每一个路由用 0,1,2&amp;hellip;.等表示&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">ADDRESS0&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>10.10.10.0 &lt;span style="color:#8f5902;font-style:italic">#目的网段IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NETMASK0&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>255.255.255.0 &lt;span style="color:#8f5902;font-style:italic">#目的网段掩码&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">GATEWAY0&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>192.168.0.10 &lt;span style="color:#8f5902;font-style:italic">#下一跳IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">ADDRESS1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>172.16.1.10 &lt;span style="color:#8f5902;font-style:italic">#目的网段IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NETMASK1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>255.255.255.0 &lt;span style="color:#8f5902;font-style:italic">#目的网段掩码&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">GATEWAY1&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>192.168.0.10 &lt;span style="color:#8f5902;font-style:italic">#下一跳IP&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ifcfg-interface-文件">ifcfg-INTERFACE 文件&lt;a class="td-heading-self-link" href="#ifcfg-interface-%e6%96%87%e4%bb%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggdfnf/1616165813293-bc7fde9f-4810-4fc8-b6b8-67282aaef73d.jpeg" alt="">&lt;/p>
&lt;p>ifcfg-INTERFACE 文件由多个 &lt;strong>Items(条目)&lt;/strong> 组成，每个 Item 类似于 键值对，以 &lt;code>=&lt;/code> 分割，Item 分为很多类型：&lt;/p>
&lt;ul>
&lt;li>Base items:&lt;/li>
&lt;li>Mandriva specific items for DHCP clients:&lt;/li>
&lt;li>Mandriva items:&lt;/li>
&lt;li>Base items being deprecated:&lt;/li>
&lt;li>Alias specific items:&lt;/li>
&lt;li>IPv6-only items for real interfaces:&lt;/li>
&lt;li>IPv6-only items for static tunnel interface:&lt;/li>
&lt;li>Ethernet-only items:&lt;/li>
&lt;li>Ethernet 802.1q VLAN items:&lt;/li>
&lt;li>PPP/SLIP items:&lt;/li>
&lt;li>PPP-specific items&lt;/li>
&lt;li>IPPP-specific items (ISDN)&lt;/li>
&lt;li>ippp0 items being deprecated:&lt;/li>
&lt;li>Wireless-specific items:&lt;/li>
&lt;li>IPSEC specific items&lt;/li>
&lt;li>Bonding-specific items&lt;/li>
&lt;li>Tunnel-specific items:&lt;/li>
&lt;li>Bridge-specific items:&lt;/li>
&lt;li>TUN/TAP-specific items:&lt;/li>
&lt;/ul>
&lt;h3 id="base-items基本条目">Base items(基本条目)&lt;a class="td-heading-self-link" href="#base-items%e5%9f%ba%e6%9c%ac%e6%9d%a1%e7%9b%ae" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h3 id="bonding-specific-items特定于-bonding-的条目">Bonding-specific items(特定于 Bonding 的条目)&lt;a class="td-heading-self-link" href="#bonding-specific-items%e7%89%b9%e5%ae%9a%e4%ba%8e-bonding-%e7%9a%84%e6%9d%a1%e7%9b%ae" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>SLAVE={yes|no}&lt;/strong> # 指定设备是否为 slave 设备。&lt;code>默认值：no&lt;/code>
&lt;strong>MASTER=bondXX&lt;/strong> # 指定要绑定的主设备
&lt;strong>BONDING_OPTS=&amp;lt;OPTS&amp;gt;&lt;/strong> # Bonding 驱动运行时选项，多个选项以空格分割&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;mode=active-backup arp_interval=60 arp_ip_target=192.168.1.1,192.168.1.2&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h4 id="配置示例">配置示例&lt;a class="td-heading-self-link" href="#%e9%85%8d%e7%bd%ae%e7%a4%ba%e4%be%8b" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">DEVICE&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>bond0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">IPADDR&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>192.168.1.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NETMASK&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>255.255.255.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">ONBOOT&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">BOOTPROTO&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">USERCTL&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">NM_CONTROLLED&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">BONDING_OPTS&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;bonding parameters separated by spaces&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: NetworkManager</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/NetworkManager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/NetworkManager/</guid><description/></item><item><title>Docs: systemd-networkd</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/systemd-networkd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/systemd-networkd/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;/blockquote>
&lt;h1 id="配置">配置&lt;a class="td-heading-self-link" href="#%e9%85%8d%e7%bd%ae" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;strong>/run/systemd/network/&lt;/strong> # 读取网络设备配置的目录&lt;/p></description></item><item><title>Docs: 数据包发送过程详解</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="译-linux-网络栈监控和调优发送数据2017">[译] Linux 网络栈监控和调优：发送数据（2017）&lt;a class="td-heading-self-link" href="#%e8%af%91-linux-%e7%bd%91%e7%bb%9c%e6%a0%88%e7%9b%91%e6%8e%a7%e5%92%8c%e8%b0%83%e4%bc%98%e5%8f%91%e9%80%81%e6%95%b0%e6%8d%ae2017" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Published at 2018-12-17 | Last Update 2020-09-29&lt;/p>
&lt;h2 id="译者序">译者序&lt;a class="td-heading-self-link" href="#%e8%af%91%e8%80%85%e5%ba%8f" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文翻译自 2017 年的一篇英文博客 &lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data">Monitoring and Tuning the Linux Networking Stack: Sending Data&lt;/a>。&lt;strong>如果能看懂英文，建议阅读原文，或者和本文对照看。&lt;/strong>
这篇文章写的是 &lt;strong>“Linux networking stack”&lt;/strong>，这里的 ”stack“ 不仅仅是内核协议栈， 而是包括内核协议栈在内的，从应用程序通过系统调用&lt;strong>写数据到 socket&lt;/strong>，到数据被组织 成一个或多个数据包最终被物理网卡发出去的整个路径。所以文章有三方面，交织在一起， 看起来非常累（但是很过瘾）：&lt;/p>
&lt;ol>
&lt;li>原理及代码实现：网络各层，包括驱动、硬中断、软中断、内核协议栈、socket 等等。&lt;/li>
&lt;li>监控：对代码中的重要计数进行监控，一般在&lt;code>/proc&lt;/code> 或&lt;code>/sys&lt;/code> 下面有对应输出。&lt;/li>
&lt;li>调优：修改网络配置参数。&lt;/li>
&lt;/ol>
&lt;p>本文的另一个特色是，几乎所有讨论的内核代码，都在相应的地方给出了 github 上的链接， 具体到行。
网络栈非常复杂，原文太长又没有任何章节号，看起来非常累。因此本文翻译时添加了适当 的章节号，以期按图索骥。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>2020 更新&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>基于 Prometheus+Grafana 监控网络栈：&lt;a href="http://arthurchiao.art/blog/monitoring-network-stack/">Monitoring Network Stack&lt;/a>。&lt;/li>
&lt;/ul>
&lt;p>以下是翻译。&lt;/p>
&lt;hr>
&lt;h2 id="太长不读tl-dr">太长不读（TL; DR）&lt;a class="td-heading-self-link" href="#%e5%a4%aa%e9%95%bf%e4%b8%8d%e8%af%bbtl-dr" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文介绍了运行 Linux 内核的机器是如何&lt;strong>发包&lt;/strong>（send packets）的，包是怎样从用户程 序一步步到达硬件网卡并被发出去的，以及如何&lt;strong>监控&lt;/strong>（monitoring）和&lt;strong>调优&lt;/strong>（ tuning）这一路径上的各个网络栈组件。
本文的姊妹篇是 &lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">Linux 网络栈监控和调优：接收数据&lt;/a>， 对应的原文是 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data&lt;/a> 。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E8%AF%91%E8%80%85%E5%BA%8F">译者序&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E5%A4%AA%E9%95%BF%E4%B8%8D%E8%AF%BBtl-dr">太长不读（TL; DR）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#1-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%9B%91%E6%8E%A7%E5%92%8C%E8%B0%83%E4%BC%98%E5%B8%B8%E8%A7%84%E5%BB%BA%E8%AE%AE">1 网络栈监控和调优：常规建议&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#2-%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E4%BF%AF%E7%9E%B0">2 发包过程俯瞰&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#3-%E5%8D%8F%E8%AE%AE%E5%B1%82%E6%B3%A8%E5%86%8C">3 协议层注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#4-%E9%80%9A%E8%BF%87-socket-%E5%8F%91%E9%80%81%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE">4 通过 socket 发送网络数据&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#41-sock_sendmsg-__sock_sendmsg-__sock_sendmsg_nosec">4.1 &lt;code>sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg_nosec&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#42-inet_sendmsg">4.2 &lt;code>inet_sendmsg&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#5-udp-%E5%8D%8F%E8%AE%AE%E5%B1%82">5 UDP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#51-udp_sendmsg">5.1 &lt;code>udp_sendmsg&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#511-udp-corking%E8%BD%AF%E6%9C%A8%E5%A1%9E">5.1.1 UDP corking（软木塞）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#512-%E8%8E%B7%E5%8F%96%E7%9B%AE%E7%9A%84-ip-%E5%9C%B0%E5%9D%80%E5%92%8C%E7%AB%AF%E5%8F%A3">5.1.2 获取目的 IP 地址和端口&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#513-socket-%E5%8F%91%E9%80%81bookkeeping-%E5%92%8C%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3">5.1.3 Socket 发送：bookkeeping 和打时间戳&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#514-%E8%BE%85%E5%8A%A9%E6%B6%88%E6%81%AFancillary-messages">5.1.4 辅助消息（Ancillary messages）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#515-%E8%AE%BE%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89-ip-%E9%80%89%E9%A1%B9">5.1.5 设置自定义 IP 选项&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#516-%E5%A4%9A%E6%92%AD%E6%88%96%E5%8D%95%E6%92%ADmulticast-or-unicast">5.1.6 多播或单播（Multicast or unicast）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#517-%E8%B7%AF%E7%94%B1">5.1.7 路由&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#518-msg_confirm-%E9%98%BB%E6%AD%A2-arp-%E7%BC%93%E5%AD%98%E8%BF%87%E6%9C%9F">5.1.8 &lt;code>MSG_CONFIRM&lt;/code>: 阻止 ARP 缓存过期&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#519-uncorked-udp-sockets-%E5%BF%AB%E9%80%9F%E8%B7%AF%E5%BE%84%E5%87%86%E5%A4%87%E5%BE%85%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE">5.1.9 uncorked UDP sockets 快速路径：准备待发送数据&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#ip_make_skb">&lt;code>ip_make_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE">发送数据&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#5110-%E6%B2%A1%E6%9C%89%E8%A2%AB-cork-%E7%9A%84%E6%95%B0%E6%8D%AE%E6%97%B6%E7%9A%84%E6%85%A2%E8%B7%AF%E5%BE%84">5.1.10 没有被 cork 的数据时的慢路径&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#ip_append_data">&lt;code>ip_append_data&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#__ip_append_data">&lt;code>__ip_append_data&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#flushing-corked-sockets">Flushing corked sockets&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#5111-error-accounting">5.1.11 Error accounting&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#52-udp_send_skb">5.2 &lt;code>udp_send_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#53-%E7%9B%91%E6%8E%A7udp-%E5%B1%82%E7%BB%9F%E8%AE%A1">5.3 监控：UDP 层统计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetsnmp">/proc/net/snmp&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetudp">/proc/net/udp&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#54-%E8%B0%83%E4%BC%98socket-%E5%8F%91%E9%80%81%E9%98%9F%E5%88%97%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F">5.4 调优：socket 发送队列内存大小&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#6-ip-%E5%8D%8F%E8%AE%AE%E5%B1%82">6 IP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#61-ip_send_skb">6.1 &lt;code>ip_send_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#62-ip_local_out-and-__ip_local_out">6.2 &lt;code>ip_local_out&lt;/code> and &lt;code>__ip_local_out&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#63-netfilter-and-nf_hook">6.3 netfilter and nf_hook&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#64-%E7%9B%AE%E7%9A%84%E8%B7%AF%E7%94%B1%E7%BC%93%E5%AD%98">6.4 目的（路由）缓存&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#65-ip_output">6.5 &lt;code>ip_output&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#66-ip_finish_output">6.6 &lt;code>ip_finish_output&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#path-mtu-discovery">Path MTU Discovery&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#67-ip_finish_output2">6.7 &lt;code>ip_finish_output2&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#68-dst_neigh_output">6.8 &lt;code>dst_neigh_output&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#69-neigh_hh_output">6.9 &lt;code>neigh_hh_output&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#610-n-output">6.10 &lt;code>n-&amp;gt;output&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#neigh_resolve_output">neigh_resolve_output&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#611-%E7%9B%91%E6%8E%A7-ip-%E5%B1%82">6.11 监控: IP 层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetsnmp-1">/proc/net/snmp&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetnetstat">/proc/net/netstat&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#7-linux-netdevice-%E5%AD%90%E7%B3%BB%E7%BB%9F">7 Linux netdevice 子系统&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#71-linux-traffic-control%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6">7.1 Linux traffic control（流量控制）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#72-dev_queue_xmit-and-__dev_queue_xmit">7.2 &lt;code>dev_queue_xmit&lt;/code> and &lt;code>__dev_queue_xmit&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#721-netdev_pick_tx">7.2.1 &lt;code>netdev_pick_tx&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#722-__netdev_pick_tx">7.2.2 &lt;code>__netdev_pick_tx&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#transmit-packet-steering-xps">Transmit Packet Steering (XPS)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#skb_tx_hash">&lt;code>skb_tx_hash&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#73-%E7%BB%A7%E7%BB%AD__dev_queue_xmit">7.3 继续&lt;code>__dev_queue_xmit&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#74-__dev_xmit_skb">7.4 &lt;code>__dev_xmit_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#75-%E8%B0%83%E4%BC%98-transmit-packet-steering-xps">7.5 调优: Transmit Packet Steering (XPS)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#8-queuing-disciplines%E6%8E%92%E9%98%9F%E8%A7%84%E5%88%99">8 Queuing Disciplines（排队规则）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#81-qdisc_run_begin-and-qdisc_run_end%E4%BB%85%E8%AE%BE%E7%BD%AE-qdisc-%E7%8A%B6%E6%80%81%E4%BD%8D">8.1 &lt;code>qdisc_run_begin()&lt;/code> and &lt;code>qdisc_run_end()&lt;/code>：仅设置 qdisc 状态位&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#82-__qdisc_run%E7%9C%9F%E6%AD%A3%E7%9A%84-qdisc-%E6%89%A7%E8%A1%8C%E5%85%A5%E5%8F%A3">8.2 &lt;code>__qdisc_run()&lt;/code>：真正的 qdisc 执行入口&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#83-qdisc_restart%E4%BB%8E-qdisc-%E9%98%9F%E5%88%97%E4%B8%AD%E5%8F%96%E5%8C%85%E5%8F%91%E9%80%81%E7%BB%99%E7%BD%91%E7%BB%9C%E9%A9%B1%E5%8A%A8">8.3 &lt;code>qdisc_restart&lt;/code>：从 qdisc 队列中取包，发送给网络驱动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#831-dequeue_skb%E4%BB%8E-qdisc-%E9%98%9F%E5%88%97%E5%8F%96%E5%BE%85%E5%8F%91%E9%80%81-skb">8.3.1 &lt;code>dequeue_skb()&lt;/code>：从 qdisc 队列取待发送 skb&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#832-sch_direct_xmit%E5%8F%91%E9%80%81%E7%BB%99%E7%BD%91%E5%8D%A1%E9%A9%B1%E5%8A%A8">8.3.2 &lt;code>sch_direct_xmit()&lt;/code>：发送给网卡驱动&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#833-handle_dev_cpu_collision">8.3.3 &lt;code>handle_dev_cpu_collision()&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#834-dev_requeue_skb%E9%87%8D%E6%96%B0%E5%8E%8B%E5%85%A5-qdisc-%E9%98%9F%E5%88%97%E7%AD%89%E5%BE%85%E4%B8%8B%E6%AC%A1%E5%8F%91%E9%80%81">8.3.4 &lt;code>dev_requeue_skb()&lt;/code>：重新压入 qdisc 队列，等待下次发送&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#84-%E5%A4%8D%E4%B9%A0__qdisc_run-%E4%B8%BB%E9%80%BB%E8%BE%91">8.4 复习：&lt;code>__qdisc_run()&lt;/code> 主逻辑&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#841-__netif_schedule">8.4.1 &lt;code>__netif_schedule&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#842-net_tx_action">8.4.2 &lt;code>net_tx_action()&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#843-net_tx_action-completion-queue%E5%BE%85%E9%87%8A%E6%94%BE-skb-%E9%98%9F%E5%88%97">8.4.3 &lt;code>net_tx_action()&lt;/code> completion queue：待释放 skb 队列&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#844-net_tx_action-output-queue%E5%BE%85%E5%8F%91%E9%80%81-skb-%E9%98%9F%E5%88%97">8.4.4 &lt;code>net_tx_action&lt;/code> output queue：待发送 skb 队列&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#85-%E6%9C%80%E7%BB%88%E6%9D%A5%E5%88%B0-dev_hard_start_xmit">8.5 最终来到 &lt;code>dev_hard_start_xmit&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#86-monitoring-qdiscs">8.6 Monitoring qdiscs&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#using-the-tc-command-line-tool">Using the tc command line tool&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#87-tuning-qdiscs">8.7 Tuning qdiscs&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E8%B0%83%E6%95%B4__qdisc_run-%E5%A4%84%E7%90%86%E6%9D%83%E9%87%8D">调整&lt;code>__qdisc_run&lt;/code> 处理权重&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E5%A2%9E%E5%8A%A0%E5%8F%91%E9%80%81%E9%98%9F%E5%88%97%E9%95%BF%E5%BA%A6">增加发送队列长度&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#9-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8">9 网络设备驱动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#91-%E9%A9%B1%E5%8A%A8%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E6%B3%A8%E5%86%8C">9.1 驱动回调函数注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#92-%E9%80%9A%E8%BF%87-ndo_start_xmit-%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE">9.2 通过 &lt;code>ndo_start_xmit&lt;/code> 发送数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#93-igb_tx_map">9.3 &lt;code>igb_tx_map&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#dynamic-queue-limits-dql">Dynamic Queue Limits (DQL)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#94-%E5%8F%91%E9%80%81%E5%AE%8C%E6%88%90transmit-completions">9.4 发送完成（Transmit completions）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#941-transmit-completion-irq">9.4.1 Transmit completion IRQ&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#942-igb_poll">9.4.2 &lt;code>igb_poll&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#943-igb_clean_tx_irq">9.4.3 &lt;code>igb_clean_tx_irq&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#944-igb_poll-%E8%BF%94%E5%9B%9E%E5%80%BC">9.4.4 &lt;code>igb_poll&lt;/code> 返回值&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#95-%E7%9B%91%E6%8E%A7%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87">9.5 监控网络设备&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#951-%E4%BD%BF%E7%94%A8-ethtool--s-%E5%91%BD%E4%BB%A4">9.5.1 使用 &lt;code>ethtool -S&lt;/code> 命令&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#952-%E4%BD%BF%E7%94%A8-sysfs">9.5.2 使用 &lt;code>sysfs&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#953-%E4%BD%BF%E7%94%A8procnetdev">9.5.3 使用&lt;code>/proc/net/dev&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#96-%E7%9B%91%E6%8E%A7-dql">9.6 监控 DQL&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#97-%E8%B0%83%E4%BC%98%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87">9.7 调优网络设备&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#971-%E6%9F%A5%E8%AF%A2-tx-queue-%E6%95%B0%E9%87%8F">9.7.1 查询 TX Queue 数量&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#972-%E8%B0%83%E6%95%B4-tx-queue-%E6%95%B0%E9%87%8F">9.7.2 调整 TX queue 数量&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#973-%E8%B0%83%E6%95%B4-tx-queue-%E5%A4%A7%E5%B0%8F">9.7.3 调整 TX queue 大小&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#10-%E7%BD%91%E7%BB%9C%E6%A0%88%E4%B9%8B%E6%97%85%E7%BB%93%E6%9D%9F">10 网络栈之旅：结束&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#11-extras">11 Extras&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#111-%E5%87%8F%E5%B0%91-arp-%E6%B5%81%E9%87%8F-msg_confirm">11.1 减少 ARP 流量 (MSG_CONFIRM)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#112-udp-corking%E8%BD%AF%E6%9C%A8%E5%A1%9E">11.2 UDP Corking（软木塞）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#113-%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3">11.3 打时间戳&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#12-%E7%BB%93%E8%AE%BA">12 结论&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#13-%E9%A2%9D%E5%A4%96%E5%B8%AE%E5%8A%A9">13 额外帮助&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>想对 Linux 网络栈进行监控或调优，必须对其正在发生什么有一个深入的理解， 而这离不开读内核源码。希望本文可以给那些正准备投身于此的人提供一份参考。&lt;/p>
&lt;h1 id="1-网络栈监控和调优常规建议">1 网络栈监控和调优：常规建议&lt;a class="td-heading-self-link" href="#1-%e7%bd%91%e7%bb%9c%e6%a0%88%e7%9b%91%e6%8e%a7%e5%92%8c%e8%b0%83%e4%bc%98%e5%b8%b8%e8%a7%84%e5%bb%ba%e8%ae%ae" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>正如我们前一篇文章提到的，网络栈很复杂，没有一种方式适用于所有场景。如果性能和网络 健康状态对你或你的业务非常重要，那你没有别的选择，只能花大量的时间、精力和金钱去 深入理解系统的各个部分之间是如何交互的。
本文中的一些示例配置仅为了方便理解（效果），并不作为任何特定配置或默认配置的建议 。在做任何配置改动之前，你应该有一个能够对系统进行监控的框架，以查看变更是否带来 预期的效果。
对远程连接上的机器进行网络变更是相当危险的，机器很可能失联。另外，不要在生产环境 直接调整这些配置；如果可能的话，在新机器上改配置，然后将机器灰度上线到生产。&lt;/p>
&lt;h1 id="2-发包过程俯瞰">2 发包过程俯瞰&lt;a class="td-heading-self-link" href="#2-%e5%8f%91%e5%8c%85%e8%bf%87%e7%a8%8b%e4%bf%af%e7%9e%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>本文将拿&lt;strong>Intel I350&lt;/strong>网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从用户程序到达硬件网卡的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>使用&lt;strong>系统调用&lt;/strong>（如 &lt;code>sendto&lt;/code>，&lt;code>sendmsg&lt;/code> 等）写数据&lt;/li>
&lt;li>数据穿过&lt;strong>socket 子系统&lt;/strong>，进入&lt;strong>socket 协议族&lt;/strong>（protocol family）系统（在我们的例子中为 &lt;code>AF_INET&lt;/code>）&lt;/li>
&lt;li>协议族处理：数据穿过&lt;strong>协议层&lt;/strong>，这一过程（在许多情况下）会将&lt;strong>数据&lt;/strong>（data）转换成&lt;strong>数据包&lt;/strong>（packet）&lt;/li>
&lt;li>数据穿过&lt;strong>路由层&lt;/strong>，这会涉及路由缓存和 ARP 缓存的更新；如果目的 MAC 不在 ARP 缓存表中，将触发一次 ARP 广播来查找 MAC 地址&lt;/li>
&lt;li>穿过协议层，packet 到达&lt;strong>设备无关层&lt;/strong>（device agnostic layer）&lt;/li>
&lt;li>使用 XPS（如果启用）或散列函数&lt;strong>选择发送队列&lt;/strong>&lt;/li>
&lt;li>调用网卡驱动的&lt;strong>发送函数&lt;/strong>&lt;/li>
&lt;li>数据传送到网卡的 &lt;code>qdisc&lt;/code>（queue discipline，排队规则）&lt;/li>
&lt;li>qdisc 会直接&lt;strong>发送数据&lt;/strong>（如果可以），或者将其放到队列，下次触发**&lt;code>NET_TX&lt;/code> 类型软中断**（softirq）的时候再发送&lt;/li>
&lt;li>数据从 qdisc 传送给驱动程序&lt;/li>
&lt;li>驱动程序创建所需的&lt;strong>DMA 映射&lt;/strong>，以便网卡从 RAM 读取数据&lt;/li>
&lt;li>驱动向网卡发送信号，通知&lt;strong>数据可以发送了&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网卡从 RAM 中获取数据并发送&lt;/strong>&lt;/li>
&lt;li>发送完成后，设备触发一个&lt;strong>硬中断&lt;/strong>（IRQ），表示发送完成&lt;/li>
&lt;li>&lt;strong>硬中断处理函数&lt;/strong>被唤醒执行。对许多设备来说，这会&lt;strong>触发 &lt;code>NET_RX&lt;/code> 类型的软中断&lt;/strong>，然后 NAPI poll 循环开始收包&lt;/li>
&lt;li>poll 函数会调用驱动程序的相应函数，&lt;strong>解除 DMA 映射&lt;/strong>，释放数据&lt;/li>
&lt;/ol>
&lt;p>接下来会详细介绍整个过程。&lt;/p>
&lt;h1 id="3-协议层注册">3 协议层注册&lt;a class="td-heading-self-link" href="#3-%e5%8d%8f%e8%ae%ae%e5%b1%82%e6%b3%a8%e5%86%8c" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>协议层分析我们将会关注 IP 和 UDP 层，其他协议层可参考这个过程。
我们首先来看协议族是如何注册到内核，并被 socket 子系统使用的。
当用户程序像下面这样创建 UDP socket 时会发生什么？&lt;/p>
&lt;pre>&lt;code>sock = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP)
&lt;/code>&lt;/pre>
&lt;p>简单来说，内核会去查找由 UDP 协议栈导出的一组函数（其中包括用于发送和接收网络数据 的函数），并赋给 socket 的相应字段。准确理解这个过程需要查看 &lt;code>AF_INET&lt;/code> 地址族的 代码。
内核初始化的很早阶段就执行了 &lt;code>inet_init&lt;/code> 函数，这个函数会注册 &lt;code>AF_INET&lt;/code> 协议族 ，以及该协议族内的各协议栈（TCP，UDP，ICMP 和 RAW），并调用初始化函数使协议栈准备 好处理网络数据。&lt;code>inet_init&lt;/code> 定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1678-L1804">net/ipv4/af_inet.c&lt;/a> 。
&lt;code>AF_INET&lt;/code> 协议族导出一个包含 &lt;code>create&lt;/code> 方法的 &lt;code>struct net_proto_family&lt;/code> 类型实例。当从 用户程序创建 socket 时，内核会调用此方法：&lt;/p>
&lt;pre>&lt;code>static const struct net_proto_family inet_family_ops = {
.family = PF_INET,
.create = inet_create,
.owner = THIS_MODULE,
};
&lt;/code>&lt;/pre>
&lt;p>&lt;code>inet_create&lt;/code> 根据传递的 socket 参数，在已注册的协议中查找对应的协议。我们来看一下：&lt;/p>
&lt;pre>&lt;code>/* Look for the requested type/protocol pair. */
lookup_protocol:
err = -ESOCKTNOSUPPORT;
rcu_read_lock();
list_for_each_entry_rcu(answer, &amp;amp;inetsw[sock-&amp;gt;type], list) {
err = 0;
/* Check the non-wild match. */
if (protocol == answer-&amp;gt;protocol) {
if (protocol != IPPROTO_IP)
break;
} else {
/* Check for the two wild cases. */
if (IPPROTO_IP == protocol) {
protocol = answer-&amp;gt;protocol;
break;
}
if (IPPROTO_IP == answer-&amp;gt;protocol)
break;
}
err = -EPROTONOSUPPORT;
}
&lt;/code>&lt;/pre>
&lt;p>然后，将该协议的回调方法（集合）赋给这个新创建的 socket：&lt;/p>
&lt;pre>&lt;code>sock-&amp;gt;ops = answer-&amp;gt;ops;
&lt;/code>&lt;/pre>
&lt;p>可以在 &lt;code>af_inet.c&lt;/code> 中看到所有协议的初始化参数。 下面是&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L998-L1020">TCP 和 UDP&lt;/a>的初始化参数：&lt;/p>
&lt;pre>&lt;code>/* Upon startup we insert all the elements in inetsw_array[] into
* the linked list inetsw.
*/
static struct inet_protosw inetsw_array[] =
{
{
.type = SOCK_STREAM,
.protocol = IPPROTO_TCP,
.prot = &amp;amp;tcp_prot,
.ops = &amp;amp;inet_stream_ops,
.no_check = 0,
.flags = INET_PROTOSW_PERMANENT |
INET_PROTOSW_ICSK,
},
{
.type = SOCK_DGRAM,
.protocol = IPPROTO_UDP,
.prot = &amp;amp;udp_prot,
.ops = &amp;amp;inet_dgram_ops,
.no_check = UDP_CSUM_DEFAULT,
.flags = INET_PROTOSW_PERMANENT,
},
/* .... more protocols ... */
&lt;/code>&lt;/pre>
&lt;p>&lt;code>IPPROTO_UDP&lt;/code> 协议类型有一个 &lt;code>ops&lt;/code> 变量，包含&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L935-L960">很多信息  &lt;/a>，包 括用于发送和接收数据的回调函数：&lt;/p>
&lt;pre>&lt;code>const struct proto_ops inet_dgram_ops = {
.family = PF_INET,
.owner = THIS_MODULE,
/* ... */
.sendmsg = inet_sendmsg,
.recvmsg = inet_recvmsg,
/* ... */
};
EXPORT_SYMBOL(inet_dgram_ops);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>prot&lt;/code> 字段指向一个协议相关的变量（的地址），对于 UDP 协议，其中包含了 UDP 相关的 回调函数。 UDP 协议对应的 &lt;code>prot&lt;/code> 变量为 &lt;code>udp_prot&lt;/code>，定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L2171-L2203">net/ipv4/udp.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>struct proto udp_prot = {
.name = &amp;quot;UDP&amp;quot;,
.owner = THIS_MODULE,
/* ... */
.sendmsg = udp_sendmsg,
.recvmsg = udp_recvmsg,
/* ... */
};
EXPORT_SYMBOL(udp_prot);
&lt;/code>&lt;/pre>
&lt;p>现在，让我们转向发送 UDP 数据的用户程序，看看 &lt;code>udp_sendmsg&lt;/code> 是如何在内核中被调用的。&lt;/p>
&lt;h1 id="4-通过-socket-发送网络数据">4 通过 socket 发送网络数据&lt;a class="td-heading-self-link" href="#4-%e9%80%9a%e8%bf%87-socket-%e5%8f%91%e9%80%81%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>用户程序想发送 UDP 网络数据，因此它使用 &lt;code>sendto&lt;/code> 系统调用，看起来可能是这样的：&lt;/p>
&lt;pre>&lt;code>ret = sendto(socket, buffer, buflen, 0, &amp;amp;dest, sizeof(dest));
&lt;/code>&lt;/pre>
&lt;p>该系统调用穿过&lt;a href="https://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/">Linux 系统调用（system call）层&lt;/a>，最后到达&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/socket.c#L1756-L1803">net/socket.c&lt;/a>中的这个函数：&lt;/p>
&lt;pre>&lt;code>/*
* Send a datagram to a given address. We move the address into kernel
* space and check the user space data area is readable before invoking
* the protocol.
*/
SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
unsigned int, flags, struct sockaddr __user *, addr,
int, addr_len)
{
/* ... code ... */
err = sock_sendmsg(sock, &amp;amp;msg, len);
/* ... code ... */
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>SYSCALL_DEFINE6&lt;/code> 宏会展开成一堆宏，后者经过一波复杂操作创建出一个带 6 个参数的系统 调用（因此叫 &lt;code>DEFINE6&lt;/code>）。作为结果之一，你会看到内核中的所有系统调用都带 &lt;code>sys_&lt;/code>前 缀。
&lt;code>sendto&lt;/code> 代码会先将数据整理成底层可以处理的格式，然后调用 &lt;code>sock_sendmsg&lt;/code>。特别地， 它将传递给 &lt;code>sendto&lt;/code> 的地址放到另一个变量（&lt;code>msg&lt;/code>）中：&lt;/p>
&lt;pre>&lt;code>iov.iov_base = buff;
iov.iov_len = len;
msg.msg_name = NULL;
msg.msg_iov = &amp;amp;iov;
msg.msg_iovlen = 1;
msg.msg_control = NULL;
msg.msg_controllen = 0;
msg.msg_namelen = 0;
if (addr) {
err = move_addr_to_kernel(addr, addr_len, &amp;amp;address);
if (err &amp;lt; 0)
goto out_put;
msg.msg_name = (struct sockaddr *)&amp;amp;address;
msg.msg_namelen = addr_len;
}
&lt;/code>&lt;/pre>
&lt;p>这段代码将用户程序传入到内核的（存放待发送数据的）地址，作为 &lt;code>msg_name&lt;/code> 字段嵌入到 &lt;code>struct msghdr&lt;/code> 类型变量中。这和用户程序直接调用 &lt;code>sendmsg&lt;/code> 而不是 &lt;code>sendto&lt;/code> 发送 数据差不多，这之所以可行，是因为 &lt;code>sendto&lt;/code> 和 &lt;code>sendmsg&lt;/code> 底层都会调用 &lt;code>sock_sendmsg&lt;/code>。&lt;/p>
&lt;h2 id="41-sock_sendmsg-__sock_sendmsg-__sock_sendmsg_nosec">4.1 &lt;code>sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg_nosec&lt;/code>&lt;a class="td-heading-self-link" href="#41-sock_sendmsg-__sock_sendmsg-__sock_sendmsg_nosec" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>sock_sendmsg&lt;/code> 做一些错误检查，然后调用&lt;code>__sock_sendmsg&lt;/code>；后者做一些自己的错误检查 ，然后调用&lt;code>__sock_sendmsg_nosec&lt;/code>。&lt;code>__sock_sendmsg_nosec&lt;/code> 将数据传递到 socket 子系统 的更深处：&lt;/p>
&lt;pre>&lt;code>static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock,
struct msghdr *msg, size_t size)
{
struct sock_iocb *si = ....
/* other code ... */
return sock-&amp;gt;ops-&amp;gt;sendmsg(iocb, sock, msg, size);
}
&lt;/code>&lt;/pre>
&lt;p>通过我们前面介绍的 socket 创建过程，你应该能看懂，注册到这里的 &lt;code>sendmsg&lt;/code> 方法就是 &lt;code>inet_sendmsg&lt;/code>。&lt;/p>
&lt;h2 id="42-inet_sendmsg">4.2 &lt;code>inet_sendmsg&lt;/code>&lt;a class="td-heading-self-link" href="#42-inet_sendmsg" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>从名字可以猜到，这是 &lt;code>AF_INET&lt;/code> 协议族提供的通用函数。 此函数首先调用 &lt;code>sock_rps_record_flow&lt;/code> 来记录最后一个处理该（数据所属的）flow 的 CPU; Receive Packet Steering 会用到这个信息。接下来，调用 socket 的协议类型（本例是 UDP）对应的 &lt;code>sendmsg&lt;/code> 方法：&lt;/p>
&lt;pre>&lt;code>int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
size_t size)
{
struct sock *sk = sock-&amp;gt;sk;
sock_rps_record_flow(sk);
/* We may need to bind the socket. */
if (!inet_sk(sk)-&amp;gt;inet_num &amp;amp;&amp;amp; !sk-&amp;gt;sk_prot-&amp;gt;no_autobind &amp;amp;&amp;amp; inet_autobind(sk))
return -EAGAIN;
return sk-&amp;gt;sk_prot-&amp;gt;sendmsg(iocb, sk, msg, size);
}
EXPORT_SYMBOL(inet_sendmsg);
&lt;/code>&lt;/pre>
&lt;p>本例是 UDP 协议，因此上面的 &lt;code>sk-&amp;gt;sk_prot-&amp;gt;sendmsg&lt;/code> 指向的是我们之前看到的（通过 &lt;code>udp_prot&lt;/code> 导出的）&lt;code>udp_sendmsg&lt;/code> 函数。
&lt;strong>sendmsg()函数作为分界点，处理逻辑从 &lt;code>AF_INET&lt;/code> 协议族通用处理转移到具体的 UDP 协议的处理。&lt;/strong>&lt;/p>
&lt;h1 id="5-udp-协议层">5 UDP 协议层&lt;a class="td-heading-self-link" href="#5-udp-%e5%8d%8f%e8%ae%ae%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="51-udp_sendmsg">5.1 &lt;code>udp_sendmsg&lt;/code>&lt;a class="td-heading-self-link" href="#51-udp_sendmsg" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>这个函数定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L845-L1088">net/ipv4/udp.c&lt;/a> ，函数非常长，我们分段来看。&lt;/p>
&lt;h3 id="511-udp-corking软木塞">5.1.1 UDP corking（软木塞）&lt;a class="td-heading-self-link" href="#511-udp-corking%e8%bd%af%e6%9c%a8%e5%a1%9e" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>在变量声明和基本错误检查之后，&lt;code>udp_sendmsg&lt;/code> 所做的第一件事就是检查 socket 是否“ 塞住”了（corked）。 UDP corking 是一项优化技术，允许内核将多次数据累积成单个数据报发 送。在用户程序中有两种方法可以启用此选项：&lt;/p>
&lt;ol>
&lt;li>使用 &lt;code>setsockopt&lt;/code> 系统调用设置 socket 的 &lt;code>UDP_CORK&lt;/code> 选项&lt;/li>
&lt;li>程序调用 &lt;code>send&lt;/code>，&lt;code>sendto&lt;/code> 或 &lt;code>sendmsg&lt;/code> 时，带 &lt;code>MSG_MORE&lt;/code> 参数&lt;/li>
&lt;/ol>
&lt;p>详细信息参考 &lt;a href="http://man7.org/linux/man-pages/man7/udp.7.html">UDP man page&lt;/a>和 &lt;a href="http://man7.org/linux/man-pages/man2/send.2.html">&lt;code>send/sendto/sendmsg&lt;/code> man page&lt;/a>。
&lt;code>udp_sendmsg&lt;/code> 代码检查 &lt;code>up-&amp;gt;pending&lt;/code> 以确定 socket 当前是否已被塞住(corked)，如果是， 则直接跳到 &lt;code>do_append_data&lt;/code> 进行数据追加(append)。 我们将在稍后看到如何追加数据。&lt;/p>
&lt;pre>&lt;code>int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
size_t len)
{
/* variables and error checking ... */
fl4 = &amp;amp;inet-&amp;gt;cork.fl.u.ip4;
if (up-&amp;gt;pending) {
/*
* There are pending frames.
* The socket lock must be held while it's corked.
*/
lock_sock(sk);
if (likely(up-&amp;gt;pending)) {
if (unlikely(up-&amp;gt;pending != AF_INET)) {
release_sock(sk);
return -EINVAL;
}
goto do_append_data;
}
release_sock(sk);
}
&lt;/code>&lt;/pre>
&lt;h3 id="512-获取目的-ip-地址和端口">5.1.2 获取目的 IP 地址和端口&lt;a class="td-heading-self-link" href="#512-%e8%8e%b7%e5%8f%96%e7%9b%ae%e7%9a%84-ip-%e5%9c%b0%e5%9d%80%e5%92%8c%e7%ab%af%e5%8f%a3" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>接下来获取目标地址和端口，有两个可能的来源：&lt;/p>
&lt;ol>
&lt;li>如果之前 socket 已经建立连接，那 socket 本身就存储了目标地址&lt;/li>
&lt;li>地址通过辅助结构（&lt;code>struct msghdr&lt;/code>）传入，正如我们在 &lt;code>sendto&lt;/code> 的内核代码中看到的那样&lt;/li>
&lt;/ol>
&lt;p>具体逻辑：&lt;/p>
&lt;pre>&lt;code>/*
* Get and verify the address.
*/
if (msg-&amp;gt;msg_name) {
struct sockaddr_in *usin = (struct sockaddr_in *)msg-&amp;gt;msg_name;
if (msg-&amp;gt;msg_namelen &amp;lt; sizeof(*usin))
return -EINVAL;
if (usin-&amp;gt;sin_family != AF_INET) {
if (usin-&amp;gt;sin_family != AF_UNSPEC)
return -EAFNOSUPPORT;
}
daddr = usin-&amp;gt;sin_addr.s_addr;
dport = usin-&amp;gt;sin_port;
if (dport == 0)
return -EINVAL;
} else {
if (sk-&amp;gt;sk_state != TCP_ESTABLISHED)
return -EDESTADDRREQ;
daddr = inet-&amp;gt;inet_daddr;
dport = inet-&amp;gt;inet_dport;
/* Open fast path for connected socket.
Route will not be used, if at least one option is set.
*/
connected = 1;
}
&lt;/code>&lt;/pre>
&lt;p>是的，你没看错，UDP 代码中出现了 &lt;code>TCP_ESTABLISHED&lt;/code>！UDP socket 的状态使用了 TCP 状态 来描述，不知道是好是坏。
回想前面我们看到用户程序调用 &lt;code>sendto&lt;/code> 时，内核如何替用户初始化一个 &lt;code>struct msghdr&lt;/code> 变量。上面的代码显示了内核如何解析该变量以便设置 &lt;code>daddr&lt;/code> 和 &lt;code>dport&lt;/code>。
如果没有 &lt;code>struct msghdr&lt;/code> 变量，内核函数到达 &lt;code>udp_sendmsg&lt;/code> 函数时，会从 socket 本身检索 目标地址和端口，并将 socket 标记为“已连接”。&lt;/p>
&lt;h3 id="513-socket-发送bookkeeping-和打时间戳">5.1.3 Socket 发送：bookkeeping 和打时间戳&lt;a class="td-heading-self-link" href="#513-socket-%e5%8f%91%e9%80%81bookkeeping-%e5%92%8c%e6%89%93%e6%97%b6%e9%97%b4%e6%88%b3" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>接下来，获取存储在 socket 上的源地址、设备索引（device index）和时间戳选项（例 如 &lt;code>SOCK_TIMESTAMPING_TX_HARDWARE&lt;/code>, &lt;code>SOCK_TIMESTAMPING_TX_SOFTWARE&lt;/code>, &lt;code>SOCK_WIFI_STATUS&lt;/code>）：&lt;/p>
&lt;pre>&lt;code>ipc.addr = inet-&amp;gt;inet_saddr;
ipc.oif = sk-&amp;gt;sk_bound_dev_if;
sock_tx_timestamp(sk, &amp;amp;ipc.tx_flags);
&lt;/code>&lt;/pre>
&lt;h3 id="514-辅助消息ancillary-messages">5.1.4 辅助消息（Ancillary messages）&lt;a class="td-heading-self-link" href="#514-%e8%be%85%e5%8a%a9%e6%b6%88%e6%81%afancillary-messages" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>除了发送或接收数据包之外，&lt;code>sendmsg&lt;/code> 和 &lt;code>recvmsg&lt;/code> 系统调用还允许用户设置或请求辅助数 据。用户程序可以通过将请求信息组织成 &lt;code>struct msghdr&lt;/code> 类型变量来利用此辅助数据。一些辅 助数据类型记录在&lt;a href="http://man7.org/linux/man-pages/man7/ip.7.html">IP man page&lt;/a>中 。
辅助数据的一个常见例子是 &lt;code>IP_PKTINFO&lt;/code>。对于 &lt;code>sendmsg&lt;/code>，&lt;code>IP_PKTINFO&lt;/code> 允许程序在发送 数据时设置一个 &lt;code>in_pktinfo&lt;/code> 变量。程序可以通过填写 &lt;code>struct in_pktinfo&lt;/code> 变量中的字段 来指定要在 packet 上使用的源地址。如果程序是监听多个 IP 地址的服务端程序，那这是一个 很有用的选项。在这种情况下，服务端可能想使用客户端连接服务端的那个 IP 地址来回复客 户端，&lt;code>IP_PKTINFO&lt;/code> 非常适合这种场景。
&lt;code>setsockopt&lt;/code> 可以在&lt;strong>socket 级别&lt;/strong>设置发送包的 &lt;a href="https://en.wikipedia.org/wiki/Time_to_live#IP_packets">IP_TTL&lt;/a>和 &lt;a href="https://en.wikipedia.org/wiki/Type_of_service">IP_TOS&lt;/a>。而辅助消息允 许在每个&lt;strong>数据包级别&lt;/strong>设置 TTL 和 TOS 值。Linux 内核会使用一个&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L179-L197">数组  &lt;/a>将 TOS 转换为优先级，后者会影响数据包如何以及合适从 qdisc 中发送出去。我们稍后会了解到这 意味着什么。
我们可以看到内核如何在 UDP socket 上处理 &lt;code>sendmsg&lt;/code> 的辅助消息：&lt;/p>
&lt;pre>&lt;code>if (msg-&amp;gt;msg_controllen) {
err = ip_cmsg_send(sock_net(sk), msg, &amp;amp;ipc,
sk-&amp;gt;sk_family == AF_INET6);
if (err)
return err;
if (ipc.opt)
free = 1;
connected = 0;
}
&lt;/code>&lt;/pre>
&lt;p>解析辅助消息的工作是由 &lt;code>ip_cmsg_send&lt;/code> 完成的，定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_sockglue.c#L190-L241">net/ipv4/ip_sockglue.c&lt;/a> 。注意，传递一个未初始化的辅助数据，将会把这个 socket 标记为“未建立连接的”（译者注 ：因为从 5.1.2 的代码可以看出，有辅助消息时优先处理辅助消息，没有辅助消息才从 socket 里面拿信息）。&lt;/p>
&lt;h3 id="515-设置自定义-ip-选项">5.1.5 设置自定义 IP 选项&lt;a class="td-heading-self-link" href="#515-%e8%ae%be%e7%bd%ae%e8%87%aa%e5%ae%9a%e4%b9%89-ip-%e9%80%89%e9%a1%b9" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>接下来，&lt;code>sendmsg&lt;/code> 将检查用户是否通过辅助消息设置了的任何自定义 IP 选项。如果设置了 ，将使用这些自定义值；如果没有，那就使用 socket 中（已经在用）的参数：&lt;/p>
&lt;pre>&lt;code>if (!ipc.opt) {
struct ip_options_rcu *inet_opt;
rcu_read_lock();
inet_opt = rcu_dereference(inet-&amp;gt;inet_opt);
if (inet_opt) {
memcpy(&amp;amp;opt_copy, inet_opt,
sizeof(*inet_opt) + inet_opt-&amp;gt;opt.optlen);
ipc.opt = &amp;amp;opt_copy.opt;
}
rcu_read_unlock();
}
&lt;/code>&lt;/pre>
&lt;p>接下来，该函数检查是否设置了源记录路由（source record route, SRR）IP 选项。 SRR 有两种类型：&lt;a href="https://en.wikipedia.org/wiki/Loose_Source_Routing">宽松源记录路由和严格源记录路由&lt;/a>。 如果设置了此选项，则会记录第一跳地址并将其保存到 &lt;code>faddr&lt;/code>，并将 socket 标记为“未连接”。 这将在后面用到：&lt;/p>
&lt;pre>&lt;code>ipc.addr = faddr = daddr;
if (ipc.opt &amp;amp;&amp;amp; ipc.opt-&amp;gt;opt.srr) {
if (!daddr)
return -EINVAL;
faddr = ipc.opt-&amp;gt;opt.faddr;
connected = 0;
}
&lt;/code>&lt;/pre>
&lt;p>处理完 SRR 选项后，将处理 TOS 选项，这可以从辅助消息中获取，或者从 socket 当前值中获取。 然后检查：&lt;/p>
&lt;ol>
&lt;li>是否（使用 &lt;code>setsockopt&lt;/code>）在 socket 上设置了 &lt;code>SO_DONTROUTE&lt;/code>，或&lt;/li>
&lt;li>是否（调用 &lt;code>sendto&lt;/code> 或 &lt;code>sendmsg&lt;/code> 时）指定了 &lt;code>MSG_DONTROUTE&lt;/code> 标志，或&lt;/li>
&lt;li>是否已设置了 &lt;code>is_strictroute&lt;/code>，表示需要严格的 &lt;a href="http://www.networksorcery.com/enp/protocol/ip/option009.htm">SRR&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>任何一个为真，&lt;code>tos&lt;/code> 字段的 &lt;code>RTO_ONLINK&lt;/code> 位将置 1，并且 socket 被视为“未连接”：&lt;/p>
&lt;pre>&lt;code>tos = get_rttos(&amp;amp;ipc, inet);
if (sock_flag(sk, SOCK_LOCALROUTE) ||
(msg-&amp;gt;msg_flags &amp;amp; MSG_DONTROUTE) ||
(ipc.opt &amp;amp;&amp;amp; ipc.opt-&amp;gt;opt.is_strictroute)) {
tos |= RTO_ONLINK;
connected = 0;
}
&lt;/code>&lt;/pre>
&lt;h3 id="516-多播或单播multicast-or-unicast">5.1.6 多播或单播（Multicast or unicast）&lt;a class="td-heading-self-link" href="#516-%e5%a4%9a%e6%92%ad%e6%88%96%e5%8d%95%e6%92%admulticast-or-unicast" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>接下来代码开始处理 multicast。这有点复杂，因为用户可以通过 &lt;code>IP_PKTINFO&lt;/code> 辅助消息 来指定发送包的源地址或设备号，如前所述。
如果目标地址是多播地址：&lt;/p>
&lt;ol>
&lt;li>将多播设备（device）的索引（index）设置为发送（写）这个 packet 的设备索引，并且&lt;/li>
&lt;li>packet 的源地址将设置为 multicast 源地址&lt;/li>
&lt;/ol>
&lt;p>如果目标地址不是一个组播地址，则发送 packet 的设备制定为 &lt;code>inet-&amp;gt;uc_index&lt;/code>（单播）， 除非用户使用 &lt;code>IP_PKTINFO&lt;/code> 辅助消息覆盖了它。&lt;/p>
&lt;pre>&lt;code>if (ipv4_is_multicast(daddr)) {
if (!ipc.oif)
ipc.oif = inet-&amp;gt;mc_index;
if (!saddr)
saddr = inet-&amp;gt;mc_addr;
connected = 0;
} else if (!ipc.oif)
ipc.oif = inet-&amp;gt;uc_index;
&lt;/code>&lt;/pre>
&lt;h3 id="517-路由">5.1.7 路由&lt;a class="td-heading-self-link" href="#517-%e8%b7%af%e7%94%b1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>现在开始路由！
UDP 层中处理路由的代码以&lt;strong>快速路径&lt;/strong>（fast path）开始。 如果 socket 已连接，则直接尝试获取路由：&lt;/p>
&lt;pre>&lt;code>if (connected)
rt = (struct rtable *)sk_dst_check(sk, 0);
&lt;/code>&lt;/pre>
&lt;p>如果 socket 未连接，或者虽然已连接，但路由辅助函数 &lt;code>sk_dst_check&lt;/code> 认定路由已过期，则代码将进入&lt;strong>慢速路径&lt;/strong>（slow path）以生成一条路由记录。首先调用 &lt;code>flowi4_init_output&lt;/code> 构造一个描述此 UDP 流的变量：&lt;/p>
&lt;pre>&lt;code>if (rt == NULL) {
struct net *net = sock_net(sk);
fl4 = &amp;amp;fl4_stack;
flowi4_init_output(fl4, ipc.oif, sk-&amp;gt;sk_mark, tos,
RT_SCOPE_UNIVERSE, sk-&amp;gt;sk_protocol,
inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP,
faddr, saddr, dport, inet-&amp;gt;inet_sport);
&lt;/code>&lt;/pre>
&lt;p>然后，socket 及其 flow 实例会传递给安全子系统，这样&lt;a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">SELinux&lt;/a>或&lt;a href="https://en.wikipedia.org/wiki/Smack_(software)">SMACK&lt;/a>这样的系统就可以在 flow 实例上设置安全 ID。 接下来，&lt;code>ip_route_output_flow&lt;/code> 将调用 IP 路由代码，创建一个路由实例：&lt;/p>
&lt;pre>&lt;code>security_sk_classify_flow(sk, flowi4_to_flowi(fl4));
rt = ip_route_output_flow(net, fl4, sk);
&lt;/code>&lt;/pre>
&lt;p>如果创建路由实例失败，并且返回码是 &lt;code>ENETUNREACH&lt;/code>, 则 &lt;code>OUTNOROUTES&lt;/code> 计数器将会加 1。&lt;/p>
&lt;pre>&lt;code>if (IS_ERR(rt)) {
err = PTR_ERR(rt);
rt = NULL;
if (err == -ENETUNREACH)
IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);
goto out;
}
&lt;/code>&lt;/pre>
&lt;p>这些统计计数器所在的源文件、其他可用的计数器及其含义，将将在下面的 UDP 监控部分讨 论。
接下来，如果是广播路由，但 socket 的 &lt;code>SOCK_BROADCAST&lt;/code> 选项未设置，则处理过程终止。 如果 socket 被视为“已连接”，则路由实例将缓存到 socket 上：&lt;/p>
&lt;pre>&lt;code>err = -EACCES;
if ((rt-&amp;gt;rt_flags &amp;amp; RTCF_BROADCAST) &amp;amp;&amp;amp;
!sock_flag(sk, SOCK_BROADCAST))
goto out;
if (connected)
sk_dst_set(sk, dst_clone(&amp;amp;rt-&amp;gt;dst));
&lt;/code>&lt;/pre>
&lt;h3 id="518-msg_confirm-阻止-arp-缓存过期">5.1.8 &lt;code>MSG_CONFIRM&lt;/code>: 阻止 ARP 缓存过期&lt;a class="td-heading-self-link" href="#518-msg_confirm-%e9%98%bb%e6%ad%a2-arp-%e7%bc%93%e5%ad%98%e8%bf%87%e6%9c%9f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果调用 &lt;code>send&lt;/code>, &lt;code>sendto&lt;/code> 或 &lt;code>sendmsg&lt;/code> 的时候指定了 &lt;code>MSG_CONFIRM&lt;/code> 参数，UDP 协议层将会如下处理：&lt;/p>
&lt;pre>&lt;code>if (msg-&amp;gt;msg_flags&amp;amp;MSG_CONFIRM)
goto do_confirm;
back_from_confirm:
&lt;/code>&lt;/pre>
&lt;p>该标志提示系统去确认一下 ARP 缓存条目是否仍然有效，防止其被垃圾回收。 &lt;code>do_confirm&lt;/code> 标签位于此函数末尾处，很简单：&lt;/p>
&lt;pre>&lt;code>do_confirm:
dst_confirm(&amp;amp;rt-&amp;gt;dst);
if (!(msg-&amp;gt;msg_flags&amp;amp;MSG_PROBE) || len)
goto back_from_confirm;
err = 0;
goto out;
&lt;/code>&lt;/pre>
&lt;p>&lt;code>dst_confirm&lt;/code> 函数只是在相应的缓存条目上设置一个标记位，稍后当查询邻居缓存并找到 条目时将检查该标志，我们后面一些会看到。此功能通常用于 UDP 网络应用程序，以减少 不必要的 ARP 流量。
此代码确认缓存条目然后跳回 &lt;code>back_from_confirm&lt;/code> 标签。
一旦 &lt;code>do_confirm&lt;/code> 代码跳回到 &lt;code>back_from_confirm&lt;/code>（或者之前就没有执行到 &lt;code>do_confirm&lt;/code> ），代码接下来将处理 UDP cork 和 uncorked 情况。&lt;/p>
&lt;h3 id="519-uncorked-udp-sockets-快速路径准备待发送数据">5.1.9 uncorked UDP sockets 快速路径：准备待发送数据&lt;a class="td-heading-self-link" href="#519-uncorked-udp-sockets-%e5%bf%ab%e9%80%9f%e8%b7%af%e5%be%84%e5%87%86%e5%a4%87%e5%be%85%e5%8f%91%e9%80%81%e6%95%b0%e6%8d%ae" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果不需要 corking，数据就可以封装到一个 &lt;code>struct sk_buff&lt;/code> 实例中并传递给 &lt;code>udp_send_skb&lt;/code>，离 IP 协议层更进了一步。这是通过调用 &lt;code>ip_make_skb&lt;/code> 来完成的。
注意，先前通过调用 &lt;code>ip_route_output_flow&lt;/code> 生成的路由条目也会一起传进来， 它将保存到 skb 里，稍后在 IP 协议层中被使用。&lt;/p>
&lt;pre>&lt;code>/* Lockless fast path for the non-corking case. */
if (!corkreq) {
skb = ip_make_skb(sk, fl4, getfrag, msg-&amp;gt;msg_iov, ulen,
sizeof(struct udphdr), &amp;amp;ipc, &amp;amp;rt,
msg-&amp;gt;msg_flags);
err = PTR_ERR(skb);
if (!IS_ERR_OR_NULL(skb))
err = udp_send_skb(skb, fl4);
goto out;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ip_make_skb&lt;/code> 函数将创建一个 skb，其中需要考虑到很多的事情，例如：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/Maximum_transmission_unit">MTU&lt;/a>&lt;/li>
&lt;li>UDP corking（如果启用）&lt;/li>
&lt;li>UDP Fragmentation Offloading（&lt;a href="https://wiki.linuxfoundation.org/networking/ufo">UFO&lt;/a>）&lt;/li>
&lt;li>Fragmentation（分片）：如果硬件不支持 UFO，但是要传输的数据大于 MTU，需要软件做分片&lt;/li>
&lt;/ol>
&lt;p>大多数网络设备驱动程序不支持 UFO，因为网络硬件本身不支持此功能。我们来看下这段代码，先看 corking 禁用的情况，启用的情况我们更后面再看。&lt;/p>
&lt;h4 id="ip_make_skb">&lt;code>ip_make_skb&lt;/code>&lt;a class="td-heading-self-link" href="#ip_make_skb" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>定义在&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">net/ipv4/ip_output.c&lt;/a>，这个函数有点复杂。
构建 skb 的时候，&lt;code>ip_make_skb&lt;/code> 依赖的底层代码需要使用一个 corking 变量和一个 queue 变量 ，skb 将通过 queue 变量传入。如果 socket 未被 cork，则会传入一个假的 corking 变量和一个 空队列。
我们来看看假 corking 变量和空队列是如何初始化的：&lt;/p>
&lt;pre>&lt;code>struct sk_buff *ip_make_skb(struct sock *sk, /* more args */)
{
struct inet_cork cork;
struct sk_buff_head queue;
int err;
if (flags &amp;amp; MSG_PROBE)
return NULL;
__skb_queue_head_init(&amp;amp;queue);
cork.flags = 0;
cork.addr = 0;
cork.opt = NULL;
err = ip_setup_cork(sk, &amp;amp;cork, /* more args */);
if (err)
return ERR_PTR(err);
&lt;/code>&lt;/pre>
&lt;p>如上所示，cork 和 queue 都是在栈上分配的，&lt;code>ip_make_skb&lt;/code> 根本不需要它。 &lt;code>ip_setup_cork&lt;/code> 初始化 cork 变量。接下来，调用&lt;code>__ip_append_data&lt;/code> 并传入 cork 和 queue 变 量：&lt;/p>
&lt;pre>&lt;code>err = __ip_append_data(sk, fl4, &amp;amp;queue, &amp;amp;cork,
&amp;amp;current-&amp;gt;task_frag, getfrag,
from, length, transhdrlen, flags);
&lt;/code>&lt;/pre>
&lt;p>我们将在后面看到这个函数是如何工作的，因为不管 socket 是否被 cork，最后都会执行它。
现在，我们只需要知道&lt;code>__ip_append_data&lt;/code> 将创建一个 skb，向其追加数据，并将该 skb 添加 到传入的 queue 变量中。如果追加数据失败，则调用&lt;code>__ip_flush_pending_frame&lt;/code> 丢弃数据 并向上返回错误（指针类型）：&lt;/p>
&lt;pre>&lt;code>if (err) {
__ip_flush_pending_frames(sk, &amp;amp;queue, &amp;amp;cork);
return ERR_PTR(err);
}
&lt;/code>&lt;/pre>
&lt;p>最后，如果没有发生错误，&lt;code>__ip_make_skb&lt;/code> 将 skb 出队，添加 IP 选项，并返回一个准备好传 递给更底层发送的 skb：&lt;/p>
&lt;pre>&lt;code>return __ip_make_skb(sk, fl4, &amp;amp;queue, &amp;amp;cork);
&lt;/code>&lt;/pre>
&lt;h4 id="发送数据">发送数据&lt;a class="td-heading-self-link" href="#%e5%8f%91%e9%80%81%e6%95%b0%e6%8d%ae" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>如果没有错误，skb 就会交给 &lt;code>udp_send_skb&lt;/code>，后者会继续将其传给下一层协议，IP 协议：&lt;/p>
&lt;pre>&lt;code>err = PTR_ERR(skb);
if (!IS_ERR_OR_NULL(skb))
err = udp_send_skb(skb, fl4);
goto out;
&lt;/code>&lt;/pre>
&lt;p>如果有错误，错误计数就会有相应增加。后面的“错误计数”部分会详细介绍。&lt;/p>
&lt;h3 id="5110-没有被-cork-的数据时的慢路径">5.1.10 没有被 cork 的数据时的慢路径&lt;a class="td-heading-self-link" href="#5110-%e6%b2%a1%e6%9c%89%e8%a2%ab-cork-%e7%9a%84%e6%95%b0%e6%8d%ae%e6%97%b6%e7%9a%84%e6%85%a2%e8%b7%af%e5%be%84" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果使用了 UDP corking，但之前没有数据被 cork，则慢路径开始：&lt;/p>
&lt;ol>
&lt;li>对 socket 加锁&lt;/li>
&lt;li>检查应用程序是否有 bug：已经被 cork 的 socket 是否再次被 cork&lt;/li>
&lt;li>设置该 UDP flow 的一些参数，为 corking 做准备&lt;/li>
&lt;li>将要发送的数据追加到现有数据&lt;/li>
&lt;/ol>
&lt;p>&lt;code>udp_sendmsg&lt;/code> 代码继续向下看，就是这一逻辑：&lt;/p>
&lt;pre>&lt;code>lock_sock(sk);
if (unlikely(up-&amp;gt;pending)) {
/* The socket is already corked while preparing it. */
/* ... which is an evident application bug. --ANK */
release_sock(sk);
LIMIT_NETDEBUG(KERN_DEBUG pr_fmt(&amp;quot;cork app bug 2\n&amp;quot;));
err = -EINVAL;
goto out;
}
/*
* Now cork the socket to pend data.
*/
fl4 = &amp;amp;inet-&amp;gt;cork.fl.u.ip4;
fl4-&amp;gt;daddr = daddr;
fl4-&amp;gt;saddr = saddr;
fl4-&amp;gt;fl4_dport = dport;
fl4-&amp;gt;fl4_sport = inet-&amp;gt;inet_sport;
up-&amp;gt;pending = AF_INET;
do_append_data:
up-&amp;gt;len += ulen;
err = ip_append_data(sk, fl4, getfrag, msg-&amp;gt;msg_iov, ulen,
sizeof(struct udphdr), &amp;amp;ipc, &amp;amp;rt,
corkreq ? msg-&amp;gt;msg_flags|MSG_MORE : msg-&amp;gt;msg_flags);
&lt;/code>&lt;/pre>
&lt;h4 id="ip_append_data">&lt;code>ip_append_data&lt;/code>&lt;a class="td-heading-self-link" href="#ip_append_data" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>这个函数简单封装了&lt;code>__ip_append_data&lt;/code>，在调用后者之前，做了两件重要的事情：&lt;/p>
&lt;ol>
&lt;li>检查是否从用户传入了 &lt;code>MSG_PROBE&lt;/code> 标志。该标志表示用户不想真正发送数据，只是做路 径探测（例如，确定&lt;a href="https://en.wikipedia.org/wiki/Path_MTU_Discovery">PMTU&lt;/a>）&lt;/li>
&lt;li>检查 socket 的发送队列是否为空。如果为空，意味着没有 cork 数据等待处理，因此调用 &lt;code>ip_setup_cork&lt;/code> 来设置 corking&lt;/li>
&lt;/ol>
&lt;p>一旦处理了上述条件，就调用&lt;code>__ip_append_data&lt;/code> 函数，该函数包含用于将数据处理成数据 包的大量逻辑。&lt;/p>
&lt;h4 id="__ip_append_data">&lt;code>__ip_append_data&lt;/code>&lt;a class="td-heading-self-link" href="#__ip_append_data" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>如果 socket 是 corked，则从 &lt;code>ip_append_data&lt;/code> 调用此函数；如果 socket 未被 cork，则从 &lt;code>ip_make_skb&lt;/code> 调用此函数。在任何一种情况下，函数都将分配一个新缓冲区来存储传入 的数据，或者将数据附加到现有数据中。
这种工作的方式围绕 socket 的发送队列。等待发送的现有数据（例如，如果 socket 被 cork） 将在队列中有一个对应条目，可以被追加数据。
这个函数很复杂;它执行很多计算以确定如何构造传递给下面的网络层的 skb。
该函数的重点包括：&lt;/p>
&lt;ol>
&lt;li>如果硬件支持，则处理 UDP Fragmentation Offload（UFO）。绝大多数网络硬件不支持 UFO。如果你的网卡驱动程序支持它，它将设置 &lt;code>NETIF_F_UFO&lt;/code> 标记位&lt;/li>
&lt;li>处理支持分散/收集（ &lt;a href="https://en.wikipedia.org/wiki/Vectored_I/O">scatter/gather&lt;/a>）IO 的网卡。许多 卡都支持此功能，并使用 &lt;code>NETIF_F_SG&lt;/code> 标志进行通告。支持该特性的网卡可以处理数据 被分散到多个 buffer 的数据包;内核不需要花时间将多个缓冲区合并成一个缓冲区中。避 免这种额外的复制会提升性能，大多数网卡都支持此功能&lt;/li>
&lt;li>通过调用 &lt;code>sock_wmalloc&lt;/code> 跟踪发送队列的大小。当分配新的 skb 时，skb 的大小由创建它 的 socket 计费（charge），并计入 socket 发送队列的已分配字节数。如果发送队列已经 没有足够的空间（超过计费限制），则 skb 并分配失败并返回错误。我们将在下面的调优 部分中看到如何设置 socket 发送队列大小（txqueuelen）&lt;/li>
&lt;li>更新错误统计信息。此函数中的任何错误都会增加“discard”计数。我们将在下面的监控部分中 看到如何读取此值&lt;/li>
&lt;/ol>
&lt;p>函数执行成功后返回 0，以及一个适用于网络设备传输的 skb。
在 unorked 情况下，持有 skb 的 queue 被作为参数传递给上面描述的&lt;code>__ip_make_skb&lt;/code>，在那里 它被出队并通过 &lt;code>udp_send_skb&lt;/code> 发送到更底层。
在 cork 的情况下，&lt;code>__ip_append_data&lt;/code> 的返回值向上传递。数据位于发送队列中，直到 &lt;code>udp_sendmsg&lt;/code> 确定是时候调用 &lt;code>udp_push_pending_frames&lt;/code> 来完成 skb，后者会进一步调用 &lt;code>udp_send_skb&lt;/code>。&lt;/p>
&lt;h4 id="flushing-corked-sockets">Flushing corked sockets&lt;a class="td-heading-self-link" href="#flushing-corked-sockets" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>现在，&lt;code>udp_sendmsg&lt;/code> 会继续，检查&lt;code>__ip_append_skb&lt;/code> 的返回值（错误码）：&lt;/p>
&lt;pre>&lt;code>if (err)
udp_flush_pending_frames(sk);
else if (!corkreq)
err = udp_push_pending_frames(sk);
else if (unlikely(skb_queue_empty(&amp;amp;sk-&amp;gt;sk_write_queue)))
up-&amp;gt;pending = 0;
release_sock(sk);
&lt;/code>&lt;/pre>
&lt;p>我们来看看每个情况：&lt;/p>
&lt;ol>
&lt;li>如果出现错误（错误为非零），则调用 &lt;code>udp_flush_pending_frames&lt;/code>，这将取消 cork 并从 socket 的发送队列中删除所有数据&lt;/li>
&lt;li>如果在未指定 &lt;code>MSG_MORE&lt;/code> 的情况下发送此数据，则调用 &lt;code>udp_push_pending_frames&lt;/code>，它将数据传递到更下面的网络层&lt;/li>
&lt;li>如果发送队列为空，请将 socket 标记为不再 cork&lt;/li>
&lt;/ol>
&lt;p>如果追加操作完成并且有更多数据要进入 cork，则代码将做一些清理工作，并返回追加数据的长度：&lt;/p>
&lt;pre>&lt;code>ip_rt_put(rt);
if (free)
kfree(ipc.opt);
if (!err)
return len;
&lt;/code>&lt;/pre>
&lt;p>这就是内核如何处理 corked UDP sockets 的。&lt;/p>
&lt;h3 id="5111-error-accounting">5.1.11 Error accounting&lt;a class="td-heading-self-link" href="#5111-error-accounting" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果：&lt;/p>
&lt;ol>
&lt;li>non-corking 快速路径创建 skb 失败，或 &lt;code>udp_send_skb&lt;/code> 返回错误，或&lt;/li>
&lt;li>&lt;code>ip_append_data&lt;/code> 无法将数据附加到 corked UDP socket，或&lt;/li>
&lt;li>当 &lt;code>udp_push_pending_frames&lt;/code> 调用 &lt;code>udp_send_skb&lt;/code> 发送 corked skb 时后者返回错误&lt;/li>
&lt;/ol>
&lt;p>仅当返回的错误是 &lt;code>ENOBUFS&lt;/code>（内核无可用内存）或 socket 已设置 &lt;code>SOCK_NOSPACE&lt;/code>（发送队 列已满）时，&lt;code>SNDBUFERRORS&lt;/code> 统计信息才会增加：&lt;/p>
&lt;pre>&lt;code>/*
* ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space. Reporting
* ENOBUFS might not be good (it's not tunable per se), but otherwise
* we don't have a good statistic (IpOutDiscards but it can be too many
* things). We could add another new stat but at least for now that
* seems like overkill.
*/
if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &amp;amp;sk-&amp;gt;sk_socket-&amp;gt;flags)) {
UDP_INC_STATS_USER(sock_net(sk),
UDP_MIB_SNDBUFERRORS, is_udplite);
}
return err;
&lt;/code>&lt;/pre>
&lt;p>我们接下来会在监控小节里看到如何读取这些计数。&lt;/p>
&lt;h2 id="52-udp_send_skb">5.2 &lt;code>udp_send_skb&lt;/code>&lt;a class="td-heading-self-link" href="#52-udp_send_skb" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>udp_sendmsg&lt;/code> 通过调用 &lt;code>udp_send_skb&lt;/code> 函数将 skb 送到下一网络层，在本例中是 IP 协议层。 这个函数做了一些重要的事情：&lt;/p>
&lt;ol>
&lt;li>向 skb 添加 UDP 头&lt;/li>
&lt;li>处理校验和：软件校验和，硬件校验和或无校验和（如果禁用）&lt;/li>
&lt;li>调用 &lt;code>ip_send_skb&lt;/code> 将 skb 发送到 IP 协议层&lt;/li>
&lt;li>更新发送成功或失败的统计计数器&lt;/li>
&lt;/ol>
&lt;p>让我们来看看。首先，创建 UDP 头：&lt;/p>
&lt;pre>&lt;code>static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)
{
/* useful variables ... */
/*
* Create a UDP header
*/
uh = udp_hdr(skb);
uh-&amp;gt;source = inet-&amp;gt;inet_sport;
uh-&amp;gt;dest = fl4-&amp;gt;fl4_dport;
uh-&amp;gt;len = htons(len);
uh-&amp;gt;check = 0;
&lt;/code>&lt;/pre>
&lt;p>接下来，处理校验和。有几种情况：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>首先处理&lt;a href="https://en.wikipedia.org/wiki/UDP-Lite">UDP-Lite&lt;/a>校验和&lt;/p>
&lt;/li>
&lt;li>
&lt;p>接下来，如果 socket 校验和选项被关闭（&lt;code>setsockopt&lt;/code> 带 &lt;code>SO_NO_CHECK&lt;/code> 参数），它将被标记为校 验和关闭&lt;/p>
&lt;/li>
&lt;li>
&lt;p>接下来，如果硬件支持 UDP 校验和，则将调用 &lt;code>udp4_hwcsum&lt;/code> 来设置它。请注意，如果数 据包是分段的，内核将在软件中生成校验和，你可以在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L720-L763">udp4_hwcsum&lt;/a> 的源代码中看到这一点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后，通过调用 &lt;code>udp_csum&lt;/code> 生成软件校验和&lt;/p>
&lt;p>if (is_udplite) /* UDP-Lite &lt;em>/
csum = udplite_csum(skb);
else if (sk-&amp;gt;sk_no_check == UDP_CSUM_NOXMIT) { /&lt;/em> UDP csum disabled &lt;em>/
skb-&amp;gt;ip_summed = CHECKSUM_NONE;
goto send;
} else if (skb-&amp;gt;ip_summed == CHECKSUM_PARTIAL) { /&lt;/em> UDP hardware csum */
udp4_hwcsum(skb, fl4-&amp;gt;saddr, fl4-&amp;gt;daddr);
goto send;
} else
csum = udp_csum(skb);&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>接下来，添加了&lt;a href="https://en.wikipedia.org/wiki/User_Datagram_Protocol#IPv4_Pseudo_Header">伪头  &lt;/a>：&lt;/p>
&lt;pre>&lt;code>uh-&amp;gt;check = csum_tcpudp_magic(fl4-&amp;gt;saddr, fl4-&amp;gt;daddr, len,
sk-&amp;gt;sk_protocol, csum);
if (uh-&amp;gt;check == 0)
uh-&amp;gt;check = CSUM_MANGLED_0;
&lt;/code>&lt;/pre>
&lt;p>如果校验和为 0，则根据&lt;a href="https://tools.ietf.org/html/rfc768">RFC 768&lt;/a>，校验为全 1（ transmitted as all ones (the equivalent in one’s complement arithmetic)）。最 后，将 skb 传递给 IP 协议层并增加统计计数：&lt;/p>
&lt;pre>&lt;code>send:
err = ip_send_skb(sock_net(sk), skb);
if (err) {
if (err == -ENOBUFS &amp;amp;&amp;amp; !inet-&amp;gt;recverr) {
UDP_INC_STATS_USER(sock_net(sk),
UDP_MIB_SNDBUFERRORS, is_udplite);
err = 0;
}
} else
UDP_INC_STATS_USER(sock_net(sk),
UDP_MIB_OUTDATAGRAMS, is_udplite);
return err;
&lt;/code>&lt;/pre>
&lt;p>如果 &lt;code>ip_send_skb&lt;/code> 成功，将更新 &lt;code>OUTDATAGRAMS&lt;/code> 统计。如果 IP 协议层报告错误，并且错误 是 &lt;code>ENOBUFS&lt;/code>（内核缺少内存）而且错误 queue（&lt;code>inet-&amp;gt;recverr&lt;/code>）没有启用，则更新 &lt;code>SNDBUFERRORS&lt;/code>。
在继续讨论 IP 协议层之前，让我们先看看如何在 Linux 内核中监视和调优 UDP 协议层。&lt;/p>
&lt;h2 id="53-监控udp-层统计">5.3 监控：UDP 层统计&lt;a class="td-heading-self-link" href="#53-%e7%9b%91%e6%8e%a7udp-%e5%b1%82%e7%bb%9f%e8%ae%a1" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>两个非常有用的获取 UDP 协议统计文件：&lt;/p>
&lt;ul>
&lt;li>&lt;code>/proc/net/snmp&lt;/code>&lt;/li>
&lt;li>&lt;code>/proc/net/udp&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="procnetsnmp">/proc/net/snmp&lt;a class="td-heading-self-link" href="#procnetsnmp" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>监控 UDP 协议层统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/snmp | grep Udp\:
Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors
Udp: 16314 0 0 17161 0 0
&lt;/code>&lt;/pre>
&lt;p>要准确地理解这些计数，你需要仔细地阅读内核代码。一些类型的错误计数并不是只出现在 一种计数中，而可能是出现在多个计数中。&lt;/p>
&lt;ul>
&lt;li>&lt;code>InDatagrams&lt;/code>: Incremented when recvmsg was used by a userland program to read datagram. Also incremented when a UDP packet is encapsulated and sent back for processing.&lt;/li>
&lt;li>&lt;code>NoPorts&lt;/code>: Incremented when UDP packets arrive destined for a port where no program is listening.&lt;/li>
&lt;li>&lt;code>InErrors&lt;/code>: Incremented in several cases: no memory in the receive queue, when a bad checksum is seen, and if sk_add_backlog fails to add the datagram.&lt;/li>
&lt;li>&lt;code>OutDatagrams&lt;/code>: Incremented when a UDP packet is handed down without error to the IP protocol layer to be sent.&lt;/li>
&lt;li>&lt;code>RcvbufErrors&lt;/code>: Incremented when sock_queue_rcv_skb reports that no memory is available; this happens if sk-&amp;gt;sk_rmem_alloc is greater than or equal to sk-&amp;gt;sk_rcvbuf.&lt;/li>
&lt;li>&lt;code>SndbufErrors&lt;/code>: Incremented if the IP protocol layer reported an error when trying to send the packet and no error queue has been setup. Also incremented if no send queue space or kernel memory are available.&lt;/li>
&lt;li>&lt;code>InCsumErrors&lt;/code>: Incremented when a UDP checksum failure is detected. Note that in all cases I could find, InCsumErrors is incremented at the same time as InErrors. Thus, InErrors - InCsumErros should yield the count of memory related errors on the receive side.&lt;/li>
&lt;/ul>
&lt;p>注意，UDP 协议层发现的某些错误会出现在其他协议层的统计信息中。一个例子：路由错误 。 &lt;code>udp_sendmsg&lt;/code> 发现的路由错误将导致 IP 协议层的 &lt;code>OutNoRoutes&lt;/code> 统计增加。&lt;/p>
&lt;h3 id="procnetudp">/proc/net/udp&lt;a class="td-heading-self-link" href="#procnetudp" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>监控 UDP socket 统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/udp
sl local_address rem_address st tx_queue rx_queue tr tm-&amp;gt;when retrnsmt uid timeout inode ref pointer drops
515: 00000000:B346 00000000:0000 07 00000000:00000000 00:00000000 00000000 104 0 7518 2 0000000000000000 0
558: 00000000:0371 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7408 2 0000000000000000 0
588: 0100007F:038F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7511 2 0000000000000000 0
769: 00000000:0044 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7673 2 0000000000000000 0
812: 00000000:006F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7407 2 0000000000000000 0
&lt;/code>&lt;/pre>
&lt;p>每一列的意思：&lt;/p>
&lt;ul>
&lt;li>&lt;code>sl&lt;/code>: Kernel hash slot for the socket&lt;/li>
&lt;li>&lt;code>local_address&lt;/code>: Hexadecimal local address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>rem_address&lt;/code>: Hexadecimal remote address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>st&lt;/code>: The state of the socket. Oddly enough, the UDP protocol layer seems to use some TCP socket states. In the example above, 7 is TCP_CLOSE.&lt;/li>
&lt;li>&lt;code>tx_queue&lt;/code>: The amount of memory allocated in the kernel for outgoing UDP datagrams.&lt;/li>
&lt;li>&lt;code>rx_queue&lt;/code>: The amount of memory allocated in the kernel for incoming UDP datagrams.&lt;/li>
&lt;li>&lt;code>tr&lt;/code>, tm-&amp;gt;when, retrnsmt: These fields are unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>uid&lt;/code>: The effective user id of the user who created this socket.&lt;/li>
&lt;li>&lt;code>timeout&lt;/code>: Unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>inode&lt;/code>: The inode number corresponding to this socket. You can use this to help you determine which user process has this socket open. Check /proc/[pid]/fd, which will contain symlinks to socket[:inode].&lt;/li>
&lt;li>&lt;code>ref&lt;/code>: The current reference count for the socket.&lt;/li>
&lt;li>&lt;code>pointer&lt;/code>: The memory address in the kernel of the struct sock.&lt;/li>
&lt;li>&lt;code>drops&lt;/code>: The number of datagram drops associated with this socket. Note that this does not include any drops related to sending datagrams (on corked UDP sockets or otherwise); this is only incremented in receive paths as of the kernel version examined by this blog post.&lt;/li>
&lt;/ul>
&lt;p>打印这些计数的代码在&lt;a href="https://github.com/torvalds/linux/blob/master/net/ipv4/udp.c#L2396-L2431">net/ipv4/udp.c&lt;/a>。&lt;/p>
&lt;h2 id="54-调优socket-发送队列内存大小">5.4 调优：socket 发送队列内存大小&lt;a class="td-heading-self-link" href="#54-%e8%b0%83%e4%bc%98socket-%e5%8f%91%e9%80%81%e9%98%9f%e5%88%97%e5%86%85%e5%ad%98%e5%a4%a7%e5%b0%8f" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>发送队列（也叫“写队列”）的最大值可以通过设置 &lt;code>net.core.wmem_max sysctl&lt;/code> 进行修改。&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.wmem_max=8388608
&lt;/code>&lt;/pre>
&lt;p>&lt;code>sk-&amp;gt;sk_write_queue&lt;/code> 用 &lt;code>net.core.wmem_default&lt;/code> 初始化， 这个值也可以调整。
调整初始发送 buffer 大小：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.wmem_default=8388608
&lt;/code>&lt;/pre>
&lt;p>也可以通过从应用程序调用 &lt;code>setsockopt&lt;/code> 并传递 &lt;code>SO_SNDBUF&lt;/code> 来设置 &lt;code>sk-&amp;gt;sk_write_queue&lt;/code> 。通过 &lt;code>setsockopt&lt;/code> 设置的最大值是 &lt;code>net.core.wmem_max&lt;/code>。
不过，可以通过 &lt;code>setsockopt&lt;/code> 并传递 &lt;code>SO_SNDBUFFORCE&lt;/code> 来覆盖 &lt;code>net.core.wmem_max&lt;/code> 限制， 这需要 &lt;code>CAP_NET_ADMIN&lt;/code> 权限。
每次调用&lt;code>__ip_append_data&lt;/code> 分配 skb 时，&lt;code>sk-&amp;gt;sk_wmem_alloc&lt;/code> 都会递增。正如我们所看到 的，UDP 数据报传输速度很快，通常不会在发送队列中花费太多时间。&lt;/p>
&lt;h1 id="6-ip-协议层">6 IP 协议层&lt;a class="td-heading-self-link" href="#6-ip-%e5%8d%8f%e8%ae%ae%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>UDP 协议层通过调用 &lt;code>ip_send_skb&lt;/code> 将 skb 交给 IP 协议层，所以我们从这里开始，探索一下 IP 协议层。&lt;/p>
&lt;h2 id="61-ip_send_skb">6.1 &lt;code>ip_send_skb&lt;/code>&lt;a class="td-heading-self-link" href="#61-ip_send_skb" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>ip_send_skb&lt;/code> 函数定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_output.c#L1367-L1380">net/ipv4/ip_output.c&lt;/a> 中，非常简短。它只是调用 &lt;code>ip_local_out&lt;/code>，如果调用失败，就更新相应的错误计数。让 我们来看看：&lt;/p>
&lt;pre>&lt;code>int ip_send_skb(struct net *net, struct sk_buff *skb)
{
int err;
err = ip_local_out(skb);
if (err) {
if (err &amp;gt; 0)
err = net_xmit_errno(err);
if (err)
IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);
}
return err;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>net_xmit_errno&lt;/code> 函数将低层错误转换为 IP 和 UDP 协议层所能理解的错误。如果发生错误， IP 协议计数器 &lt;code>OutDiscards&lt;/code> 会递增。稍后我们将看到读取哪些文件可以获取此统计信 息。现在，让我们继续，看看 &lt;code>ip_local_out&lt;/code> 带我们去哪。&lt;/p>
&lt;h2 id="62-ip_local_out-and-__ip_local_out">6.2 &lt;code>ip_local_out&lt;/code> and &lt;code>__ip_local_out&lt;/code>&lt;a class="td-heading-self-link" href="#62-ip_local_out-and-__ip_local_out" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>幸运的是，&lt;code>ip_local_out&lt;/code> 和&lt;code>__ip_local_out&lt;/code> 都很简单。&lt;code>ip_local_out&lt;/code> 只需调用 &lt;code>__ip_local_out&lt;/code>，如果返回值为 1，则调用路由层 &lt;code>dst_output&lt;/code> 发送数据包：&lt;/p>
&lt;pre>&lt;code>int ip_local_out(struct sk_buff *skb)
{
int err;
err = __ip_local_out(skb);
if (likely(err == 1))
err = dst_output(skb);
return err;
}
&lt;/code>&lt;/pre>
&lt;p>我们来看看&lt;code>__ip_local_out&lt;/code> 的代码：&lt;/p>
&lt;pre>&lt;code>int __ip_local_out(struct sk_buff *skb)
{
struct iphdr *iph = ip_hdr(skb);
iph-&amp;gt;tot_len = htons(skb-&amp;gt;len);
ip_send_check(iph);
return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,
skb_dst(skb)-&amp;gt;dev, dst_output);
}
&lt;/code>&lt;/pre>
&lt;p>可以看到，该函数首先做了两件重要的事情：&lt;/p>
&lt;ol>
&lt;li>设置 IP 数据包的长度&lt;/li>
&lt;li>调用 &lt;code>ip_send_check&lt;/code> 来计算要写入 IP 头的校验和。&lt;code>ip_send_check&lt;/code> 函数将进一步调用 名为 &lt;code>ip_fast_csum&lt;/code> 的函数来计算校验和。在 x86 和 x86_64 体系结构上，此函数用汇编实 现，代码：&lt;a href="https://github.com/torvalds/linux/blob/v3.13/arch/x86/include/asm/checksum_64.h#L40-L73">64 位实现&lt;/a> 和&lt;a href="https://github.com/torvalds/linux/blob/v3.13/arch/x86/include/asm/checksum_32.h#L63-L98">32 位实现&lt;/a> 。&lt;/li>
&lt;/ol>
&lt;p>接下来，IP 协议层将通过调用 &lt;code>nf_hook&lt;/code> 进入 netfilter，其返回值将传递回 &lt;code>ip_local_out&lt;/code> 。 如果 &lt;code>nf_hook&lt;/code> 返回 1，则表示允许数据包通过，并且调用者应该自己发送数据包。这正 是我们在上面看到的情况：&lt;code>ip_local_out&lt;/code> 检查返回值 1 时，自己通过调用 &lt;code>dst_output&lt;/code> 发 送数据包。&lt;/p>
&lt;h2 id="63-netfilter-and-nf_hook">6.3 netfilter and nf_hook&lt;a class="td-heading-self-link" href="#63-netfilter-and-nf_hook" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>简洁起见，我决定跳过对 netfilter，iptables 和 conntrack 的深入研究。如果你想深入了解 netfilter 的代码实现，可以从 &lt;code>include/linux/netfilter.h&lt;/code>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netfilter.h#L142-L147">这里  &lt;/a>和 &lt;code>net/netfilter/core.c&lt;/code>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/netfilter/core.c#L168-L209">这里  &lt;/a>开 始。
简短版本是：&lt;code>nf_hook&lt;/code> 只是一个 wrapper，它调用 &lt;code>nf_hook_thresh&lt;/code>，首先检查是否有为这 个&lt;strong>协议族&lt;/strong>和&lt;strong>hook 类型&lt;/strong>（这里分别为 &lt;code>NFPROTO_IPV4&lt;/code> 和 &lt;code>NF_INET_LOCAL_OUT&lt;/code>）安装 的过滤器，然后将返回到 IP 协议层，避免深入到 netfilter 或更下面，比如 iptables 和 conntrack。
请记住：如果你有非常多或者非常复杂的 netfilter 或 iptables 规则，那些规则将在触发 &lt;code>sendmsg&lt;/code> 系统调的用户进程的上下文中执行。如果对这个用户进程设置了 CPU 亲和性，相应 的 CPU 将花费系统时间（system time）处理出站（outbound）iptables 规则。如果你在做性 能回归测试，那可能要考虑根据系统的负载，将相应的用户进程绑到到特定的 CPU，或者是 减少 netfilter/iptables 规则的复杂度，以减少对性能测试的影响。
出于讨论目的，我们假设 &lt;code>nf_hook&lt;/code> 返回 1，表示调用者（在这种情况下是 IP 协议层）应该 自己发送数据包。&lt;/p>
&lt;h2 id="64-目的路由缓存">6.4 目的（路由）缓存&lt;a class="td-heading-self-link" href="#64-%e7%9b%ae%e7%9a%84%e8%b7%af%e7%94%b1%e7%bc%93%e5%ad%98" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>dst 代码在 Linux 内核中实现&lt;strong>协议无关&lt;/strong>的目标缓存。为了继续学习发送 UDP 数据报的流程 ，我们需要了解 dst 条目是如何被设置的，首先来看 dst 条目和路由是如何生成的。 目标缓 存，路由和邻居子系统，任何一个都可以拿来单独详细的介绍。我们不深入细节，只是快速 地看一下它们是如何组合到一起的。
我们上面看到的代码调用了 &lt;code>dst_output(skb)&lt;/code>。 此函数只是查找关联到这个 skb 的 dst 条目 ，然后调用 &lt;code>output&lt;/code> 方法。代码如下：&lt;/p>
&lt;pre>&lt;code>/* Output packet to network from transport. */
static inline int dst_output(struct sk_buff *skb)
{
return skb_dst(skb)-&amp;gt;output(skb);
}
&lt;/code>&lt;/pre>
&lt;p>看起来很简单，但是 &lt;code>output&lt;/code> 方法之前是如何关联到 dst 条目的？
首先很重要的一点，目标缓存条目是以多种不同方式添加的。到目前为止，我们已经在代码 中看到的一种方法是从 &lt;code>udp_sendmsg&lt;/code> 调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L2252-L2267">ip_route_output_flow&lt;/a> 。&lt;code>ip_route_output_flow&lt;/code> 函数调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L1990-L2173">__ip_route_output_key&lt;/a> ，后者进而调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L1868-L1988">__mkroute_output&lt;/a> 。 &lt;code>__mkroute_output&lt;/code> 函数创建路由和目标缓存条目。当它执行创建操作时，它会判断哪 个 &lt;code>output&lt;/code> 方法适合此 dst。大多数时候，这个函数是 &lt;code>ip_output&lt;/code>。&lt;/p>
&lt;h2 id="65-ip_output">6.5 &lt;code>ip_output&lt;/code>&lt;a class="td-heading-self-link" href="#65-ip_output" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>在 UDP IPv4 情况下，上面的 &lt;code>output&lt;/code> 方法指向的是 &lt;code>ip_output&lt;/code>。&lt;code>ip_output&lt;/code> 函数很简单：&lt;/p>
&lt;pre>&lt;code>int ip_output(struct sk_buff *skb)
{
struct net_device *dev = skb_dst(skb)-&amp;gt;dev;
IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb-&amp;gt;len);
skb-&amp;gt;dev = dev;
skb-&amp;gt;protocol = htons(ETH_P_IP);
return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,
ip_finish_output,
!(IPCB(skb)-&amp;gt;flags &amp;amp; IPSKB_REROUTED));
}
&lt;/code>&lt;/pre>
&lt;p>首先，更新 &lt;code>IPSTATS_MIB_OUT&lt;/code> 统计计数。&lt;code>IP_UPD_PO_STATS&lt;/code> 宏将更新字节数和包数统计。 我们将在后面的部分中看到如何获取 IP 协议层统计信息以及它们各自的含义。接下来，设置 要发送此 skb 的设备，以及协议。
最后，通过调用 &lt;code>NF_HOOK_COND&lt;/code> 将控制权交给 netfilter。查看 &lt;code>NF_HOOK_COND&lt;/code> 的函数原型 有助于更清晰地解释它如何工作。来自 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netfilter.h#L177-L188">include/linux/netfilter.h&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>static inline int
NF_HOOK_COND(uint8_t pf, unsigned int hook, struct sk_buff *skb,
struct net_device *in, struct net_device *out,
int (*okfn)(struct sk_buff *), bool cond)
&lt;/code>&lt;/pre>
&lt;p>&lt;code>NF_HOOK_COND&lt;/code> 通过检查传入的条件来工作。在这里条件是&lt;code>!(IPCB(skb)-&amp;gt;flags &amp;amp; IPSKB_REROUTED&lt;/code>。如果此条件为真，则 skb 将发送给 netfilter。如果 netfilter 允许包通过 ，&lt;code>okfn&lt;/code> 回调函数将被调用。在这里，&lt;code>okfn&lt;/code> 是 &lt;code>ip_finish_output&lt;/code>。&lt;/p>
&lt;h2 id="66-ip_finish_output">6.6 &lt;code>ip_finish_output&lt;/code>&lt;a class="td-heading-self-link" href="#66-ip_finish_output" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;pre>&lt;code>static int ip_finish_output(struct sk_buff *skb)
{
#if defined(CONFIG_NETFILTER) &amp;amp;&amp;amp; defined(CONFIG_XFRM)
/* Policy lookup after SNAT yielded a new policy */
if (skb_dst(skb)-&amp;gt;xfrm != NULL) {
IPCB(skb)-&amp;gt;flags |= IPSKB_REROUTED;
return dst_output(skb);
}
#endif
if (skb-&amp;gt;len &amp;gt; ip_skb_dst_mtu(skb) &amp;amp;&amp;amp; !skb_is_gso(skb))
return ip_fragment(skb, ip_finish_output2);
else
return ip_finish_output2(skb);
}
&lt;/code>&lt;/pre>
&lt;p>如果内核启用了 netfilter 和数据包转换（XFRM），则更新 skb 的标志并通过 &lt;code>dst_output&lt;/code> 将 其发回。
更常见的两种情况是：&lt;/p>
&lt;ol>
&lt;li>如果数据包的长度大于 MTU 并且分片不会 offload 到设备，则会调用 &lt;code>ip_fragment&lt;/code> 在发送之前对数据包进行分片&lt;/li>
&lt;li>否则，数据包将直接发送到 ip_finish_output2&lt;/li>
&lt;/ol>
&lt;p>在继续我们的内核之前，让我们简单地谈谈 Path MTU Discovery。&lt;/p>
&lt;h3 id="path-mtu-discovery">Path MTU Discovery&lt;a class="td-heading-self-link" href="#path-mtu-discovery" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Linux 提供了一个功能，我迄今为止一直避免提及：&lt;a href="https://en.wikipedia.org/wiki/Path_MTU_Discovery">路径 MTU 发现  &lt;/a>。此功能允许内核自动确定 路由的最大传输单元（ &lt;a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit">MTU&lt;/a> ）。发送小于或等于该路由的 MTU 的包意味着可以避免 IP 分片，这是推荐设置，因为数 据包分片会消耗系统资源，而避免分片看起来很容易：只需发送足够小的不需要分片的数据 包。
你可以在应用程序中通过调用 &lt;code>setsockopt&lt;/code> 带 &lt;code>SOL_IP&lt;/code> 和 &lt;code>IP_MTU_DISCOVER&lt;/code> 选项，在 packet 级别来调整路径 MTU 发现设置，相应的合法值参考 IP 协议的&lt;a href="http://man7.org/linux/man-pages/man7/ip.7.html">man page&lt;/a>。例如，你可能想设置的值是 ：&lt;code>IP_PMTUDISC_DO&lt;/code>，表示“始终执行路径 MTU 发现”。更高级的网络应用程序或诊断工具可 能选择自己实现&lt;a href="https://www.ietf.org/rfc/rfc4821.txt">RFC 4821&lt;/a>，以在应用程序启动 时针对特定的路由做 PMTU。在这种情况下，你可以使用 &lt;code>IP_PMTUDISC_PROBE&lt;/code> 选项告诉内核 设置“Do not Fragment”位，这就会允许你发送大于 PMTU 的数据。
应用程序可以通过调用 &lt;code>getsockopt&lt;/code> 带 &lt;code>SOL_IP&lt;/code> 和 &lt;code>IP_MTU&lt;/code> 选项来查看当前 PMTU。可以使 用它指导应用程序在发送之前，构造 UDP 数据报的大小。
如果已启用 PMTU 发现，则发送大于 PMTU 的 UDP 数据将导致应用程序收到 &lt;code>EMSGSIZE&lt;/code> 错误。 这种情况下，应用程序只能减小 packet 大小重试。
强烈建议启用 PTMU 发现，因此我将不再详细描述 IP 分片的代码。当我们查看 IP 协议层统计信 息时，我将解释所有统计信息，包括与分片相关的统计信息。其中许多计数都在 &lt;code>ip_fragment&lt;/code> 中更新的。不管分片与否，代码最后都会调到 &lt;code>ip_finish_output2&lt;/code>，所以让 我们继续。&lt;/p>
&lt;h2 id="67-ip_finish_output2">6.7 &lt;code>ip_finish_output2&lt;/code>&lt;a class="td-heading-self-link" href="#67-ip_finish_output2" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>IP 分片后调用 &lt;code>ip_finish_output2&lt;/code>，另外 &lt;code>ip_finish_output&lt;/code> 也会直接调用它。这个函数 在将包发送到邻居缓存之前处理各种统计计数器。让我们看看它是如何工作的：&lt;/p>
&lt;pre>&lt;code>static inline int ip_finish_output2(struct sk_buff *skb)
{
/* variable declarations */
if (rt-&amp;gt;rt_type == RTN_MULTICAST) {
IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTMCAST, skb-&amp;gt;len);
} else if (rt-&amp;gt;rt_type == RTN_BROADCAST)
IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTBCAST, skb-&amp;gt;len);
/* Be paranoid, rather than too clever. */
if (unlikely(skb_headroom(skb) &amp;lt; hh_len &amp;amp;&amp;amp; dev-&amp;gt;header_ops)) {
struct sk_buff *skb2;
skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(dev));
if (skb2 == NULL) {
kfree_skb(skb);
return -ENOMEM;
}
if (skb-&amp;gt;sk)
skb_set_owner_w(skb2, skb-&amp;gt;sk);
consume_skb(skb);
skb = skb2;
}
&lt;/code>&lt;/pre>
&lt;p>如果与此数据包关联的路由是多播类型，则使用 &lt;code>IP_UPD_PO_STATS&lt;/code> 宏来增加 &lt;code>OutMcastPkts&lt;/code> 和 &lt;code>OutMcastOctets&lt;/code> 计数。如果广播路由，则会增加 &lt;code>OutBcastPkts&lt;/code> 和 &lt;code>OutBcastOctets&lt;/code> 计数。
接下来，确保 skb 结构有足够的空间容纳需要添加的任何链路层头。如果空间不够，则调用 &lt;code>skb_realloc_headroom&lt;/code> 分配额外的空间，并且新的 skb 的费用（charge）记在相关的 socket 上。&lt;/p>
&lt;pre>&lt;code>rcu_read_lock_bh();
nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)-&amp;gt;daddr);
neigh = __ipv4_neigh_lookup_noref(dev, nexthop);
if (unlikely(!neigh))
neigh = __neigh_create(&amp;amp;arp_tbl, &amp;amp;nexthop, dev, false);
&lt;/code>&lt;/pre>
&lt;p>继续，查询路由层找到下一跳，再根据下一跳信息查找邻居缓存。如果未找到，则 调用&lt;code>__neigh_create&lt;/code> 创建一个邻居。例如，第一次将数据发送到另一 台主机的时候，就是这种情况。请注意，创建邻居缓存的时候带了 &lt;code>arp_tbl&lt;/code>（ &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L160-L187">net/ipv4/arp.c&lt;/a> 中定义）参数。其他系统（如 IPv6 或 &lt;a href="https://en.wikipedia.org/wiki/DECnet">DECnet&lt;/a>）维护自己的 ARP 表，并将不同的变量 传给&lt;code>__neigh_create&lt;/code>。 这篇文章的目的并不是要详细介绍邻居缓存，但注意如果创建， 会导致缓存表增大。本文后面会介绍有关邻居缓存的更多详细信息。 邻居缓存会导出一组 统计信息，以便可以衡量这种增长。有关详细信息，请参阅下面的监控部分。&lt;/p>
&lt;pre>&lt;code>if (!IS_ERR(neigh)) {
int res = dst_neigh_output(dst, neigh, skb);
rcu_read_unlock_bh();
return res;
}
rcu_read_unlock_bh();
net_dbg_ratelimited(&amp;quot;%s: No header cache and no neighbour!\n&amp;quot;,
__func__);
kfree_skb(skb);
return -EINVAL;
}
&lt;/code>&lt;/pre>
&lt;p>最后，如果创建邻居缓存成功，则调用 &lt;code>dst_neigh_output&lt;/code> 继续传递 skb；否则，释放 skb 并返 回 &lt;code>EINVAL&lt;/code>，这会向上传递，导致 &lt;code>OutDiscards&lt;/code> 在 &lt;code>ip_send_skb&lt;/code> 中递增。让我们继续在 &lt;code>dst_neigh_output&lt;/code> 中接近 Linux 内核的 netdevice 子系统。&lt;/p>
&lt;h2 id="68-dst_neigh_output">6.8 &lt;code>dst_neigh_output&lt;/code>&lt;a class="td-heading-self-link" href="#68-dst_neigh_output" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>dst_neigh_output&lt;/code> 函数做了两件重要的事情。首先，回想一下之前在本文中我 们看到，如果用户调用 &lt;code>sendmsg&lt;/code> 并通过辅助消息指定 &lt;code>MSG_CONFIRM&lt;/code> 参数，则会设置一个标 志位以指示目标高速缓存条目仍然有效且不应进行垃圾回收。这个检查就是在这个函数里面 做的，并且邻居上的 &lt;code>confirm&lt;/code> 字段设置为当前的 jiffies 计数。&lt;/p>
&lt;pre>&lt;code>static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n,
struct sk_buff *skb)
{
const struct hh_cache *hh;
if (dst-&amp;gt;pending_confirm) {
unsigned long now = jiffies;
dst-&amp;gt;pending_confirm = 0;
/* avoid dirtying neighbour */
if (n-&amp;gt;confirmed != now)
n-&amp;gt;confirmed = now;
}
&lt;/code>&lt;/pre>
&lt;p>其次，检查邻居的状态并调用适当的 &lt;code>output&lt;/code> 函数。让我们看一下这些条件，并尝试了解发 生了什么：&lt;/p>
&lt;pre>&lt;code>hh = &amp;amp;n-&amp;gt;hh;
if ((n-&amp;gt;nud_state &amp;amp; NUD_CONNECTED) &amp;amp;&amp;amp; hh-&amp;gt;hh_len)
return neigh_hh_output(hh, skb);
else
return n-&amp;gt;output(n, skb);
}
&lt;/code>&lt;/pre>
&lt;p>邻居被认为是 &lt;code>NUD_CONNECTED&lt;/code>，如果它满足以下一个或多个条件：&lt;/p>
&lt;ol>
&lt;li>&lt;code>NUD_PERMANENT&lt;/code>：静态路由&lt;/li>
&lt;li>&lt;code>NUD_NOARP&lt;/code>：不需要 ARP 请求（例如，目标是多播或广播地址，或环回设备）&lt;/li>
&lt;li>&lt;code>NUD_REACHABLE&lt;/code>：邻居是“可达的。”只要&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L905-L923">成功处理了&lt;/a>ARP 请求，目标就会被标记为可达&lt;/li>
&lt;/ol>
&lt;p>进一步，如果“硬件头”（hh）被缓存（之前已经发送过数据，并生成了缓存），将调 用 &lt;code>neigh_hh_output&lt;/code>。
否则，调用 &lt;code>output&lt;/code> 函数。
以上两种情况，最后都会到 &lt;code>dev_queue_xmit&lt;/code>，它将 skb 发送给 Linux 网络设备子系统，在它 进入设备驱动程序层之前将对其进行更多处理。让我们沿着 &lt;code>neigh_hh_output&lt;/code> 和 &lt;code>n-&amp;gt;output&lt;/code> 代码继续向下，直到达到 &lt;code>dev_queue_xmit&lt;/code>。&lt;/p>
&lt;h2 id="69-neigh_hh_output">6.9 &lt;code>neigh_hh_output&lt;/code>&lt;a class="td-heading-self-link" href="#69-neigh_hh_output" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果目标是 &lt;code>NUD_CONNECTED&lt;/code> 并且硬件头已被缓存，则将调用 &lt;code>neigh_hh_output&lt;/code>，在将 skb 移交 给 &lt;code>dev_queue_xmit&lt;/code> 之前执行一小部分处理。 我们来看看 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/net/neighbour.h#L336-L356">include/net/neighbour.h&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>static inline int neigh_hh_output(const struct hh_cache *hh, struct sk_buff *skb)
{
unsigned int seq;
int hh_len;
do {
seq = read_seqbegin(&amp;amp;hh-&amp;gt;hh_lock);
hh_len = hh-&amp;gt;hh_len;
if (likely(hh_len &amp;lt;= HH_DATA_MOD)) {
/* this is inlined by gcc */
memcpy(skb-&amp;gt;data - HH_DATA_MOD, hh-&amp;gt;hh_data, HH_DATA_MOD);
} else {
int hh_alen = HH_DATA_ALIGN(hh_len);
memcpy(skb-&amp;gt;data - hh_alen, hh-&amp;gt;hh_data, hh_alen);
}
} while (read_seqretry(&amp;amp;hh-&amp;gt;hh_lock, seq));
skb_push(skb, hh_len);
return dev_queue_xmit(skb);
}
&lt;/code>&lt;/pre>
&lt;p>这个函数理解有点难，部分原因是&lt;a href="https://en.wikipedia.org/wiki/Seqlock">seqlock&lt;/a>这 个东西，它用于在缓存的硬件头上做读/写锁。可以将上面的 &lt;code>do {} while ()&lt;/code>循环想象成 一个简单的重试机制，它将尝试在循环中执行，直到成功。
循环里处理硬件头的长度对齐。这是必需的，因为某些硬件头（如&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/ieee80211.h#L210-L218">IEEE 802.11&lt;/a> 头）大于 &lt;code>HH_DATA_MOD&lt;/code>（16 字节）。
将头数据复制到 skb 后，&lt;code>skb_push&lt;/code> 将更新 skb 内指向数据缓冲区的指针。最后调用 &lt;code>dev_queue_xmit&lt;/code> 将 skb 传递给 Linux 网络设备子系统。&lt;/p>
&lt;h2 id="610-n-output">6.10 &lt;code>n-&amp;gt;output&lt;/code>&lt;a class="td-heading-self-link" href="#610-n-output" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果目标不是 &lt;code>NUD_CONNECTED&lt;/code> 或硬件头尚未缓存，则代码沿 &lt;code>n-&amp;gt;output&lt;/code> 路径向下。 neigbour 结构上的 &lt;code>output&lt;/code> 指针指向哪个函数？这得看情况。要了解这是如何设置的，我们 需要更多地了解邻居缓存的工作原理。
&lt;code>struct neighbour&lt;/code> 包含几个重要字段：我们在上面看到的 &lt;code>nud_state&lt;/code> 字段，&lt;code>output&lt;/code> 函数和 &lt;code>ops&lt;/code> 结构。回想一下，我们之前看到如果在缓存中找不到现有条目，会从 &lt;code>ip_finish_output2&lt;/code> 调用&lt;code>__neigh_create&lt;/code> 创建一个。当调用&lt;code>__neigh_creaet&lt;/code> 时，将分配邻居，其 &lt;code>output&lt;/code> 函 数&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/neighbour.c#L294">最初&lt;/a> 设置为 &lt;code>neigh_blackhole&lt;/code>。随着&lt;code>__neigh_create&lt;/code> 代码的进行，它将根据邻居的状态修改 &lt;code>output&lt;/code> 值以指向适当的发送方法。
例如，当代码确定是“已连接的”邻居时，&lt;code>neigh_connect&lt;/code> 会将 &lt;code>output&lt;/code> 设置为 &lt;code>neigh-&amp;gt;ops-&amp;gt;connected_output&lt;/code>。或者，当代码怀疑邻居可能已关闭时，&lt;code>neigh_suspect&lt;/code> 会将 &lt;code>output&lt;/code> 设置为 &lt;code>neigh-&amp;gt;ops-&amp;gt;output&lt;/code>（例如，如果已超过 &lt;code>/proc/sys/net/ipv4/neigh/default/delay_first_probe_time&lt;/code> 自发送探测以来的 &lt;code>delay_first_probe_time&lt;/code> 秒）。
换句话说：&lt;code>neigh-&amp;gt;output&lt;/code> 会被设置为 &lt;code>neigh-&amp;gt;ops_connected_output&lt;/code> 或 &lt;code>neigh-&amp;gt;ops-&amp;gt;output&lt;/code>，具体取决于邻居的状态。&lt;code>neigh-&amp;gt;ops&lt;/code> 来自哪里？
分配邻居后，调用 &lt;code>arp_constructor&lt;/code>（ &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L220-L313">net/ipv4/arp.c&lt;/a> ）来设置 &lt;code>struct neighbor&lt;/code> 的某些字段。特别是，此函数会检查与此邻居关联的设备是否 导出来一个 &lt;code>struct header_ops&lt;/code> 实例（&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ethernet/eth.c#L342-L348">以太网设备是这样做的  &lt;/a>）， 该结构体有一个 &lt;code>cache&lt;/code> 方法。
&lt;code>neigh-&amp;gt;ops&lt;/code> 设置为 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L138-L144">net/ipv4/arp&lt;/a> 中定义的以下实例：&lt;/p>
&lt;pre>&lt;code>static const struct neigh_ops arp_hh_ops = {
.family = AF_INET,
.solicit = arp_solicit,
.error_report = arp_error_report,
.output = neigh_resolve_output,
.connected_output = neigh_resolve_output,
};
&lt;/code>&lt;/pre>
&lt;p>所以，不管 neighbor 是不是“已连接的”，或者邻居缓存代码是否怀疑连接“已关闭”， &lt;code>neigh_resolve_output&lt;/code> 最终都会被赋给 &lt;code>neigh-&amp;gt;output&lt;/code>。当执行到 &lt;code>n-&amp;gt;output&lt;/code> 时就会调 用它。&lt;/p>
&lt;h3 id="neigh_resolve_output">neigh_resolve_output&lt;a class="td-heading-self-link" href="#neigh_resolve_output" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>此函数的目的是解析未连接的邻居，或已连接但没有缓存硬件头的邻居。我们来看看这个 函数是如何工作的：&lt;/p>
&lt;pre>&lt;code>/* Slow and careful. */
int neigh_resolve_output(struct neighbour *neigh, struct sk_buff *skb)
{
struct dst_entry *dst = skb_dst(skb);
int rc = 0;
if (!dst)
goto discard;
if (!neigh_event_send(neigh, skb)) {
int err;
struct net_device *dev = neigh-&amp;gt;dev;
unsigned int seq;
&lt;/code>&lt;/pre>
&lt;p>代码首先进行一些基本检查，然后调用 &lt;code>neigh_event_send&lt;/code>。 &lt;code>neigh_event_send&lt;/code> 函数是 &lt;code>__neigh_event_send&lt;/code> 的简单封装，后者干大部分脏话累活。可以在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/neighbour.c#L964-L1028">net/core/neighbour.c&lt;/a> 中读&lt;code>__neigh_event_send&lt;/code> 的源代码，从大的层面看，三种情况：&lt;/p>
&lt;ol>
&lt;li>&lt;code>NUD_NONE&lt;/code> 状态（默认状态）的邻居：假设 &lt;code>/proc/sys/net/ipv4/neigh/default/app_solicit&lt;/code> 和 &lt;code>/proc/sys/net/ipv4/neigh/default/mcast_solicit&lt;/code> 配置允许发送探测（如果不是， 则将状态标记为 &lt;code>NUD_FAILED&lt;/code>），将导致立即发送 ARP 请求。邻居状态将更新为 &lt;code>NUD_INCOMPLETE&lt;/code>&lt;/li>
&lt;li>&lt;code>NUD_STALE&lt;/code> 状态的邻居：将更新为 &lt;code>NUD_DELAYED&lt;/code> 并且将设置计时器以稍后探测它们（ 稍后是现在的时间+&lt;code>/proc/sys/net/ipv4/neigh/default/delay_first_probe_time&lt;/code> 秒 ）&lt;/li>
&lt;li>检查 &lt;code>NUD_INCOMPLETE&lt;/code> 状态的邻居（包括上面第一种情形），以确保未解析邻居的排 队 packet 的数量小于等于&lt;code>/proc/sys/net/ipv4/neigh/default/unres_qlen&lt;/code>。如果超过 ，则数据包会出列并丢弃，直到小于等于 proc 中的值。 统计信息中有个计数器会因此 更新&lt;/li>
&lt;/ol>
&lt;p>如果需要 ARP 探测，ARP 将立即被发送。&lt;code>__neigh_event_send&lt;/code> 将返回 0，表示邻居被视为“已 连接”或“已延迟”，否则返回 1。返回值 0 允许 &lt;code>neigh_resolve_output&lt;/code> 继续：&lt;/p>
&lt;pre>&lt;code>if (dev-&amp;gt;header_ops-&amp;gt;cache &amp;amp;&amp;amp; !neigh-&amp;gt;hh.hh_len)
neigh_hh_init(neigh, dst);
&lt;/code>&lt;/pre>
&lt;p>如果邻居关联的设备的协议实现（在我们的例子中是以太网）支持缓存硬件头，并且当前没 有缓存，&lt;code>neigh_hh_init&lt;/code> 将缓存它。&lt;/p>
&lt;pre>&lt;code>do {
__skb_pull(skb, skb_network_offset(skb));
seq = read_seqbegin(&amp;amp;neigh-&amp;gt;ha_lock);
err = dev_hard_header(skb, dev, ntohs(skb-&amp;gt;protocol),
neigh-&amp;gt;ha, NULL, skb-&amp;gt;len);
} while (read_seqretry(&amp;amp;neigh-&amp;gt;ha_lock, seq));
&lt;/code>&lt;/pre>
&lt;p>接下来，seqlock 锁控制对邻居的硬件地址字段（&lt;code>neigh-&amp;gt;ha&lt;/code>）的访问。 &lt;code>dev_hard_header&lt;/code> 为 skb 创建以太网头时将读取该字段。
之后是错误检查：&lt;/p>
&lt;pre>&lt;code>if (err &amp;gt;= 0)
rc = dev_queue_xmit(skb);
else
goto out_kfree_skb;
}
&lt;/code>&lt;/pre>
&lt;p>如果以太网头写入成功，将调用 &lt;code>dev_queue_xmit&lt;/code> 将 skb 传递给 Linux 网络设备子系统进行发 送。如果出现错误，goto 将删除 skb，设置并返回错误码：&lt;/p>
&lt;pre>&lt;code>out:
return rc;
discard:
neigh_dbg(1, &amp;quot;%s: dst=%p neigh=%p\n&amp;quot;, __func__, dst, neigh);
out_kfree_skb:
rc = -EINVAL;
kfree_skb(skb);
goto out;
}
EXPORT_SYMBOL(neigh_resolve_output);
&lt;/code>&lt;/pre>
&lt;p>在我们进入 Linux 网络设备子系统之前，让我们看看一些用于监控和转换 IP 协议层的文件。&lt;/p>
&lt;h2 id="611-监控-ip-层">6.11 监控: IP 层&lt;a class="td-heading-self-link" href="#611-%e7%9b%91%e6%8e%a7-ip-%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="procnetsnmp-1">/proc/net/snmp&lt;a class="td-heading-self-link" href="#procnetsnmp-1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;pre>&lt;code>$ cat /proc/net/snmp
Ip: Forwarding DefaultTTL InReceives InHdrErrors InAddrErrors ForwDatagrams InUnknownProtos InDiscards InDelivers OutRequests OutDiscards OutNoRoutes ReasmTimeout ReasmReqds ReasmOKs ReasmFails FragOKs FragFails FragCreates
Ip: 1 64 25922988125 0 0 15771700 0 0 25898327616 22789396404 12987882 51 1 10129840 2196520 1 0 0 0
...
&lt;/code>&lt;/pre>
&lt;p>这个文件包扩多种协议的统计，IP 层的在最前面，每一列代表什么有说明。
前面我们已经看到 IP 协议层有一些地方会更新计数器。这些计数器的类型是 C 枚举类型，定 义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/snmp.h#L10-L59">include/uapi/linux/snmp.h&lt;/a>:&lt;/p>
&lt;pre>&lt;code>enum
{
IPSTATS_MIB_NUM = 0,
/* frequently written fields in fast path, kept in same cache line */
IPSTATS_MIB_INPKTS, /* InReceives */
IPSTATS_MIB_INOCTETS, /* InOctets */
IPSTATS_MIB_INDELIVERS, /* InDelivers */
IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */
IPSTATS_MIB_OUTPKTS, /* OutRequests */
IPSTATS_MIB_OUTOCTETS, /* OutOctets */
/* ... */
&lt;/code>&lt;/pre>
&lt;p>一些有趣的统计：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OutRequests&lt;/code>: Incremented each time an IP packet is attempted to be sent. It appears that this is incremented for every send, successful or not.&lt;/li>
&lt;li>&lt;code>OutDiscards&lt;/code>: Incremented each time an IP packet is discarded. This can happen if appending data to the skb (for corked sockets) fails, or if the layers below IP return an error.&lt;/li>
&lt;li>&lt;code>OutNoRoute&lt;/code>: Incremented in several places, for example in the UDP protocol layer (udp_sendmsg) if no route can be generated for a given destination. Also incremented when an application calls “connect” on a UDP socket but no route can be found.&lt;/li>
&lt;li>&lt;code>FragOKs&lt;/code>: Incremented once per packet that is fragmented. For example, a packet split into 3 fragments will cause this counter to be incremented once.&lt;/li>
&lt;li>&lt;code>FragCreates&lt;/code>: Incremented once per fragment that is created. For example, a packet split into 3 fragments will cause this counter to be incremented thrice.&lt;/li>
&lt;li>&lt;code>FragFails&lt;/code>: Incremented if fragmentation was attempted, but is not permitted (because the “Don’t Fragment” bit is set). Also incremented if outputting the fragment fails.&lt;/li>
&lt;/ul>
&lt;p>其他（接收数据部分）的统计可以见本文的姊妹篇：&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#monitoring-ip-protocol-layer-statistics">原文  &lt;/a>，&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">中文翻译版&lt;/a>。&lt;/p>
&lt;h3 id="procnetnetstat">/proc/net/netstat&lt;a class="td-heading-self-link" href="#procnetnetstat" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;pre>&lt;code>$ cat /proc/net/netstat | grep IpExt
IpExt: InNoRoutes InTruncatedPkts InMcastPkts OutMcastPkts InBcastPkts OutBcastPkts InOctets OutOctets InMcastOctets OutMcastOctets InBcastOctets OutBcastOctets InCsumErrors InNoECTPkts InECT0Pktsu InCEPkts
IpExt: 0 0 0 0 277959 0 14568040307695 32991309088496 0 0 58649349 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>格式与前面的类似，除了每列的名称都有 &lt;code>IpExt&lt;/code> 前缀之外。
一些有趣的统计：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OutMcastPkts&lt;/code>: Incremented each time a packet destined for a multicast address is sent.&lt;/li>
&lt;li>&lt;code>OutBcastPkts&lt;/code>: Incremented each time a packet destined for a broadcast address is sent.&lt;/li>
&lt;li>&lt;code>OutOctects&lt;/code>: The number of packet bytes output.&lt;/li>
&lt;li>&lt;code>OutMcastOctets&lt;/code>: The number of multicast packet bytes output.&lt;/li>
&lt;li>&lt;code>OutBcastOctets&lt;/code>: The number of broadcast packet bytes output.&lt;/li>
&lt;/ul>
&lt;p>其他（接收数据部分）的统计可以见本文的姊妹篇：&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#monitoring-ip-protocol-layer-statistics">原文  &lt;/a>，&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">中文翻译版&lt;/a>。
注意这些计数分别在 IP 层的不同地方被更新。由于代码一直在更新，重复计数或者计数错误 的 bug 可能会引入。如果这些计数对你非常重要，强烈建议你阅读内核的相应源码，确定它 们是在哪里被更新的，以及更新的对不对，是不是有 bug。&lt;/p>
&lt;h1 id="7-linux-netdevice-子系统">7 Linux netdevice 子系统&lt;a class="td-heading-self-link" href="#7-linux-netdevice-%e5%ad%90%e7%b3%bb%e7%bb%9f" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>在继续跟进 &lt;code>dev_queue_xmit&lt;/code> 发送数据包之前，让我们花点时间介绍几个将在下一部分中出 现的重要概念。&lt;/p>
&lt;h2 id="71-linux-traffic-control流量控制">7.1 Linux traffic control（流量控制）&lt;a class="td-heading-self-link" href="#71-linux-traffic-control%e6%b5%81%e9%87%8f%e6%8e%a7%e5%88%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Linux 支持称为流量控制（&lt;a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html">traffic control&lt;/a>）的功能。此功能 允许系统管理员控制数据包如何从机器发送出去。本文不会深入探讨 Linux 流量控制 的各个方面的细节。&lt;a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/">这篇文档&lt;/a>对流量 控制系统、它如何控制流量，及其其特性进行了深入的介绍。
这里介绍一些值得一提的概念，使后面的代码更容易理解。
流量控制系统包含几组不同的 queue system，每种有不同的排队特征。各个排队系统通常称 为 qdisc，也称为排队规则。你可以将 qdisc 视为&lt;strong>调度程序&lt;/strong>; qdisc 决定数据包的发送时 间和方式。
在 Linux 上，每个 device 都有一个与之关联的默认 qdisc。对于仅支持单发送队列的网卡，使 用默认的 qdisc &lt;code>pfifo_fast&lt;/code>。支持多个发送队列的网卡使用 mq 的默认 qdisc。可以运行 &lt;code>tc qdisc&lt;/code> 来查看系统 qdisc 信息。
某些设备支持硬件流量控制，这允许管理员将流量控制 offload 到网络硬件，节省系统的 CPU 资源。
现在已经介绍了这些概念，让我们从 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2890-L2894">net/core/dev.c&lt;/a> 继续 &lt;code>dev_queue_xmit&lt;/code>。&lt;/p>
&lt;h2 id="72-dev_queue_xmit-and-__dev_queue_xmit">7.2 &lt;code>dev_queue_xmit&lt;/code> and &lt;code>__dev_queue_xmit&lt;/code>&lt;a class="td-heading-self-link" href="#72-dev_queue_xmit-and-__dev_queue_xmit" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>dev_queue_xmit&lt;/code> 简单封装了&lt;code>__dev_queue_xmit&lt;/code>:&lt;/p>
&lt;pre>&lt;code>int dev_queue_xmit(struct sk_buff *skb)
{
return __dev_queue_xmit(skb, NULL);
}
EXPORT_SYMBOL(dev_queue_xmit);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>__dev_queue_xmit&lt;/code> 才是干脏活累活的地方。我们&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2808-L2825">一段段  &lt;/a>来看：&lt;/p>
&lt;pre>&lt;code>static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)
{
struct net_device *dev = skb-&amp;gt;dev;
struct netdev_queue *txq;
struct Qdisc *q;
int rc = -ENOMEM;
skb_reset_mac_header(skb);
/* Disable soft irqs for various locks below. Also
* stops preemption for RCU.
*/
rcu_read_lock_bh();
skb_update_prio(skb);
&lt;/code>&lt;/pre>
&lt;p>开始的逻辑：&lt;/p>
&lt;ol>
&lt;li>声明变量&lt;/li>
&lt;li>调用 &lt;code>skb_reset_mac_header&lt;/code>，准备发送 skb。这会重置 skb 内部的指针，使得 ether 头可 以被访问&lt;/li>
&lt;li>调用 &lt;code>rcu_read_lock_bh&lt;/code>，为接下来的读操作加锁。更多关于使用 RCU 安全访问数据的信 息，可以参考&lt;a href="https://www.kernel.org/doc/Documentation/RCU/checklist.txt">这里&lt;/a>&lt;/li>
&lt;li>调用 &lt;code>skb_update_prio&lt;/code>，如果启用了&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/cgroups/net_prio.txt">网络优先级 cgroup&lt;/a>，这会设置 skb 的优先级&lt;/li>
&lt;/ol>
&lt;p>现在，我们来看更复杂的部分：&lt;/p>
&lt;pre>&lt;code>txq = netdev_pick_tx(dev, skb, accel_priv);
&lt;/code>&lt;/pre>
&lt;p>这会选择发送队列。本文后面会看到，一些网卡支持多发送队列。我们来看这是如何工作的。&lt;/p>
&lt;h3 id="721-netdev_pick_tx">7.2.1 &lt;code>netdev_pick_tx&lt;/code>&lt;a class="td-heading-self-link" href="#721-netdev_pick_tx" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>netdev_pick_tx&lt;/code> 定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/flow_dissector.c#L397-L417">net/core/flow_dissector.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>struct netdev_queue *netdev_pick_tx(struct net_device *dev,
struct sk_buff *skb,
void *accel_priv)
{
int queue_index = 0;
if (dev-&amp;gt;real_num_tx_queues != 1) {
const struct net_device_ops *ops = dev-&amp;gt;netdev_ops;
if (ops-&amp;gt;ndo_select_queue)
queue_index = ops-&amp;gt;ndo_select_queue(dev, skb,
accel_priv);
else
queue_index = __netdev_pick_tx(dev, skb);
if (!accel_priv)
queue_index = dev_cap_txqueue(dev, queue_index);
}
skb_set_queue_mapping(skb, queue_index);
return netdev_get_tx_queue(dev, queue_index);
}
&lt;/code>&lt;/pre>
&lt;p>如上所示，如果网络设备仅支持单个 TX 队列，则会跳过复杂的代码，直接返回单个 TX 队列。 大多高端服务器上使用的设备都有多个 TX 队列。具有多个 TX 队列的设备有两种情况：&lt;/p>
&lt;ol>
&lt;li>驱动程序实现 &lt;code>ndo_select_queue&lt;/code>，以硬件或 feature-specific 的方式更智能地选择 TX 队列&lt;/li>
&lt;li>驱动程序没有实现 &lt;code>ndo_select_queue&lt;/code>，这种情况需要内核自己选择设备&lt;/li>
&lt;/ol>
&lt;p>从 3.13 内核开始，没有多少驱动程序实现 &lt;code>ndo_select_queue&lt;/code>。bnx2x 和 ixgbe 驱动程序实 现了此功能，但仅用于以太网光纤通道（&lt;a href="https://en.wikipedia.org/wiki/Fibre_Channel_over_Ethernet">FCoE&lt;/a>）。鉴于此，我们假设网络设备没有实现 &lt;code>ndo_select_queue&lt;/code> 和/或没有使用 FCoE。在这种情况下，内核将使用&lt;code>__netdev_pick_tx&lt;/code> 选择 tx 队列。
一旦&lt;code>__netdev_pick_tx&lt;/code> 确定了队列号，&lt;code>skb_set_queue_mapping&lt;/code> 将缓存该值（稍后将在 流量控制代码中使用），&lt;code>netdev_get_tx_queue&lt;/code> 将查找并返回指向该队列的指针。让我们 看一下&lt;code>__netdev_pick_tx&lt;/code> 在返回&lt;code>__dev_queue_xmit&lt;/code> 之前的工作原理。&lt;/p>
&lt;h3 id="722-__netdev_pick_tx">7.2.2 &lt;code>__netdev_pick_tx&lt;/code>&lt;a class="td-heading-self-link" href="#722-__netdev_pick_tx" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>我们来看内核如何选择 TX 队列。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/flow_dissector.c#L375-L395">net/core/flow_dissector.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)
{
struct sock *sk = skb-&amp;gt;sk;
int queue_index = sk_tx_queue_get(sk);
if (queue_index &amp;lt; 0 || skb-&amp;gt;ooo_okay ||
queue_index &amp;gt;= dev-&amp;gt;real_num_tx_queues) {
int new_index = get_xps_queue(dev, skb);
if (new_index &amp;lt; 0)
new_index = skb_tx_hash(dev, skb);
if (queue_index != new_index &amp;amp;&amp;amp; sk &amp;amp;&amp;amp;
rcu_access_pointer(sk-&amp;gt;sk_dst_cache))
sk_tx_queue_set(sk, new_index);
queue_index = new_index;
}
return queue_index;
}
&lt;/code>&lt;/pre>
&lt;p>代码首先调用 &lt;code>sk_tx_queue_get&lt;/code> 检查发送队列是否已经缓存在 socket 上，如果尚未缓存， 则返回-1。
下一个 if 语句检查是否满足以下任一条件：&lt;/p>
&lt;ol>
&lt;li>&lt;code>queue_index &amp;lt; 0&lt;/code>：表示尚未设置 TX queue 的情况&lt;/li>
&lt;li>&lt;code>ooo_okay&lt;/code> 标志是否非零：如果不为 0，则表示现在允许无序（out of order）数据包。 协议层必须正确地地设置此标志。当 flow 的所有 outstanding（需要确认的？）数据包都 已确认时，TCP 协议层将设置此标志。当发生这种情况时，内核可以为此数据包选择不同 的 TX 队列。UDP 协议层不设置此标志 - 因此 UDP 数据包永远不会将 &lt;code>ooo_okay&lt;/code> 设置为非零 值。&lt;/li>
&lt;li>TX queue index 大于 TX queue 数量：如果用户最近通过 ethtool 更改了设备上的队列数， 则会发生这种情况。稍后会详细介绍。&lt;/li>
&lt;/ol>
&lt;p>以上任何一种情况，都表示没有找到合适的 TX queue，因此接下来代码会进入慢路径以继续 寻找合适的发送队列。首先调用 &lt;code>get_xps_queue&lt;/code>，它会使用一个由用户配置的 TX queue 到 CPU 的映射，这称为 XPS（Transmit Packet Steering ，发送数据包控制），我们将更详细 地了解 XPS 是什么以及它如何工作。
如果内核不支持 XPS，或者系统管理员未配置 XPS，或者配置的映射引用了无效队列， &lt;code>get_xps_queue&lt;/code> 返回-1，则代码将继续调用 &lt;code>skb_tx_hash&lt;/code>。
一旦 XPS 或内核使用 &lt;code>skb_tx_hash&lt;/code> 自动选择了发送队列，&lt;code>sk_tx_queue_set&lt;/code> 会将队列缓存 在 socket 对象上，然后返回。让我们看看 XPS，以及 &lt;code>skb_tx_hash&lt;/code> 在继续调用 &lt;code>dev_queue_xmit&lt;/code> 之前是如何工作的。&lt;/p>
&lt;h4 id="transmit-packet-steering-xps">Transmit Packet Steering (XPS)&lt;a class="td-heading-self-link" href="#transmit-packet-steering-xps" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>发送数据包控制（XPS）是一项功能，允许系统管理员配置哪些 CPU 可以处理网卡的哪些发送 队列。XPS 的主要目的是&lt;strong>避免处理发送请求时的锁竞争&lt;/strong>。使用 XPS 还可以减少缓存驱逐， 避免&lt;a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA&lt;/a>机器上的远程 内存访问等。
可以查看内核有关 XPS 的&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L364-L422">文档  &lt;/a>了解其如何工作的更多信息。我们后面会介绍如何调整系统的 XPS，现在，你只需要知道 配置 XPS，系统管理员需要定义 TX queue 到 CPU 的映射（bitmap 形式）。
上面代码中，&lt;code>get_xps_queue&lt;/code> 将查询这个用户指定的映射，以确定应使用哪个发送 队列。如果 &lt;code>get_xps_queue&lt;/code> 返回-1，则将改为使用 &lt;code>skb_tx_hash&lt;/code>。&lt;/p>
&lt;h4 id="skb_tx_hash">&lt;code>skb_tx_hash&lt;/code>&lt;a class="td-heading-self-link" href="#skb_tx_hash" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>如果 XPS 未包含在内核中，或 XPS 未配置，或配置的队列不可用（可能因为用户调整了队列数 ），&lt;code>skb_tx_hash&lt;/code> 将接管以确定应在哪个队列上发送数据。准确理解 &lt;code>skb_tx_hash&lt;/code> 的工作 原理非常重要，具体取决于你的发送负载。请注意，这段代码已经随时间做过一些更新，因 此如果你使用的内核版本与本文不同，则应直接查阅相应版本的 j 内核源代码。
让我们看看它是如何工作的，来自 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netdevice.h#L2331-L2340">include/linux/netdevice.h&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>/*
* Returns a Tx hash for the given packet when dev-&amp;gt;real_num_tx_queues is used
* as a distribution range limit for the returned value.
*/
static inline u16 skb_tx_hash(const struct net_device *dev,
const struct sk_buff *skb)
{
return __skb_tx_hash(dev, skb, dev-&amp;gt;real_num_tx_queues);
}
&lt;/code>&lt;/pre>
&lt;p>直接调用了&lt;code> __skb_tx_hash&lt;/code>, &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/flow_dissector.c#L239-L271">net/core/flow_dissector.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>/*
* Returns a Tx hash based on the given packet descriptor a Tx queues' number
* to be used as a distribution range.
*/
u16 __skb_tx_hash(const struct net_device *dev, const struct sk_buff *skb,
unsigned int num_tx_queues)
{
u32 hash;
u16 qoffset = 0;
u16 qcount = num_tx_queues;
if (skb_rx_queue_recorded(skb)) {
hash = skb_get_rx_queue(skb);
while (unlikely(hash &amp;gt;= num_tx_queues))
hash -= num_tx_queues;
return hash;
}
&lt;/code>&lt;/pre>
&lt;p>这个函数中的第一个 if 是一个有趣的短路。函数名 &lt;code>skb_rx_queue_recorded&lt;/code> 有点误导。skb 有一个 &lt;code>queue_mapping&lt;/code> 字段，rx 和 tx 都会用到这个字段。无论如何，如果系统正在接收数 据包并将其转发到其他地方，则此 if 语句都为 &lt;code>true&lt;/code>。否则，代码将继续向下：&lt;/p>
&lt;pre>&lt;code>if (dev-&amp;gt;num_tc) {
u8 tc = netdev_get_prio_tc_map(dev, skb-&amp;gt;priority);
qoffset = dev-&amp;gt;tc_to_txq[tc].offset;
qcount = dev-&amp;gt;tc_to_txq[tc].count;
}
&lt;/code>&lt;/pre>
&lt;p>要理解这段代码，首先要知道，程序可以设置 socket 上发送的数据的优先级。这可以通过 &lt;code>setsockopt&lt;/code> 带 &lt;code>SOL_SOCKET&lt;/code> 和 &lt;code>SO_PRIORITY&lt;/code> 选项来完成。有关 &lt;code>SO_PRIORITY&lt;/code> 的更多信息 ，请参见&lt;a href="http://man7.org/linux/man-pages/man7/socket.7.html">socket (7) man page&lt;/a>。
请注意，如果使用 &lt;code>setsockopt&lt;/code> 带 &lt;code>IP_TOS&lt;/code> 选项来设置在 socket 上发送的 IP 包的 TOS 标志（ 或者作为辅助消息传递给 &lt;code>sendmsg&lt;/code>，在数据包级别设置），内核会将其转换为 &lt;code>skb-&amp;gt;priority&lt;/code>。
如前所述，一些网络设备支持基于硬件的流量控制系统。&lt;strong>如果 num_tc 不为零，则表示此设 备支持基于硬件的流量控制&lt;/strong>。这种情况下，将查询一个&lt;strong>packet priority 到该硬件支持 的流量控制&lt;/strong>的映射，根据此映射选择适当的流量类型（traffic class）。
接下来，将计算出该 traffic class 的 TX queue 的范围，它将用于确定发送队列。
如果 &lt;code>num_tc&lt;/code> 为零（网络设备不支持硬件流量控制），则 &lt;code>qcount&lt;/code> 和 &lt;code>qoffset&lt;/code> 变量分 别设置为发送队列数和 0。
使用 &lt;code>qcount&lt;/code> 和 &lt;code>qoffset&lt;/code>，将计算发送队列的 index：&lt;/p>
&lt;pre>&lt;code>if (skb-&amp;gt;sk &amp;amp;&amp;amp; skb-&amp;gt;sk-&amp;gt;sk_hash)
hash = skb-&amp;gt;sk-&amp;gt;sk_hash;
else
hash = (__force u16) skb-&amp;gt;protocol;
hash = __flow_hash_1word(hash);
return (u16) (((u64) hash * qcount) &amp;gt;&amp;gt; 32) + qoffset;
}
EXPORT_SYMBOL(__skb_tx_hash);
&lt;/code>&lt;/pre>
&lt;p>最后，通过&lt;code>__netdev_pick_tx&lt;/code> 返回选出的 TX queue index。&lt;/p>
&lt;h2 id="73-继续__dev_queue_xmit">7.3 继续&lt;code>__dev_queue_xmit&lt;/code>&lt;a class="td-heading-self-link" href="#73-%e7%bb%a7%e7%bb%ad__dev_queue_xmit" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>至此已经选到了合适的发送队列。
继续&lt;code>__dev_queue_xmit can continue&lt;/code>:&lt;/p>
&lt;pre>&lt;code>q = rcu_dereference_bh(txq-&amp;gt;qdisc);
#ifdef CONFIG_NET_CLS_ACT
skb-&amp;gt;tc_verd = SET_TC_AT(skb-&amp;gt;tc_verd, AT_EGRESS);
#endif
trace_net_dev_queue(skb);
if (q-&amp;gt;enqueue) {
rc = __dev_xmit_skb(skb, q, dev, txq);
goto out;
}
&lt;/code>&lt;/pre>
&lt;p>首先获取与此队列关联的 qdisc。回想一下，之前我们看到单发送队列设备的默认类型是 &lt;code>pfifo_fast&lt;/code> qdisc，而对于多队列设备，默认类型是 &lt;code>mq&lt;/code> qdisc。
接下来，如果内核中已启用数据包分类 API，则代码会为 packet 分配 traffic class。 接下 来，检查 disc 是否有合适的队列来存放 packet。像 &lt;code>noqueue&lt;/code> 这样的 qdisc 没有队列。 如果 有队列，则代码调用&lt;code>__dev_xmit_skb&lt;/code> 继续处理数据，然后跳转到此函数的末尾。我们很快 就会看到&lt;code>__dev_xmit_skb&lt;/code>。现在，让我们看看如果没有队列会发生什么，从一个非常有用 的注释开始：&lt;/p>
&lt;pre>&lt;code>/* The device has no queue. Common case for software devices:
loopback, all the sorts of tunnels...
Really, it is unlikely that netif_tx_lock protection is necessary
here. (f.e. loopback and IP tunnels are clean ignoring statistics
counters.)
However, it is possible, that they rely on protection
made by us here.
Check this and shot the lock. It is not prone from deadlocks.
Either shot noqueue qdisc, it is even simpler 8)
*/
if (dev-&amp;gt;flags &amp;amp; IFF_UP) {
int cpu = smp_processor_id(); /* ok because BHs are off */
&lt;/code>&lt;/pre>
&lt;p>正如注释所示，&lt;strong>唯一可以拥有”没有队列的 qdisc”的设备是环回设备和隧道设备&lt;/strong>。如果 设备当前处于运行状态，则获取当前 CPU，然后判断此设备队列上的发送锁是否由此 CPU 拥有 ：&lt;/p>
&lt;pre>&lt;code>if (txq-&amp;gt;xmit_lock_owner != cpu) {
if (__this_cpu_read(xmit_recursion) &amp;gt; RECURSION_LIMIT)
goto recursion_alert;
&lt;/code>&lt;/pre>
&lt;p>如果发送锁不由此 CPU 拥有，则在此处检查 per-CPU 计数器变量 &lt;code>xmit_recursion&lt;/code>，判断其是 否超过 &lt;code>RECURSION_LIMIT&lt;/code>。 一个程序可能会在这段代码这里持续发送数据，然后被抢占， 调度程序选择另一个程序来运行。第二个程序也可能驻留在此持续发送数据。因此， &lt;code>xmit_recursion&lt;/code> 计数器用于确保在此处竞争发送数据的程序不超过 &lt;code>RECURSION_LIMIT&lt;/code> 个 。
我们继续：&lt;/p>
&lt;pre>&lt;code>HARD_TX_LOCK(dev, txq, cpu);
if (!netif_xmit_stopped(txq)) {
__this_cpu_inc(xmit_recursion);
rc = dev_hard_start_xmit(skb, dev, txq);
__this_cpu_dec(xmit_recursion);
if (dev_xmit_complete(rc)) {
HARD_TX_UNLOCK(dev, txq);
goto out;
}
}
HARD_TX_UNLOCK(dev, txq);
net_crit_ratelimited(&amp;quot;Virtual device %s asks to queue packet!\n&amp;quot;,
dev-&amp;gt;name);
} else {
/* Recursion is detected! It is possible,
* unfortunately
*/
recursion_alert:
net_crit_ratelimited(&amp;quot;Dead loop on virtual device %s, fix it urgently!\n&amp;quot;,
dev-&amp;gt;name);
}
}
&lt;/code>&lt;/pre>
&lt;p>接下来的代码首先尝试获取发送锁，然后检查要使用的设备的发送队列是否被停用。如果没 有停用，则更新 &lt;code>xmit_recursion&lt;/code> 计数，然后将数据向下传递到更靠近发送的设备。我们稍 后会更详细地看到 &lt;code>dev_hard_start_xmit&lt;/code>。
或者，如果当前 CPU 是发送锁定的拥有者，或者如果 &lt;code>RECURSION_LIMIT&lt;/code> 被命中，则不进行发 送，而会打印告警日志。
函数剩余部分的代码设置错误码并返回。
由于我们对真正的以太网设备感兴趣，让我们来看一下之前就需要跟进去的 &lt;code>__dev_xmit_skb&lt;/code> 函数，这是发送主线上的函数。&lt;/p>
&lt;h2 id="74-__dev_xmit_skb">7.4 &lt;code>__dev_xmit_skb&lt;/code>&lt;a class="td-heading-self-link" href="#74-__dev_xmit_skb" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现在我们带着排队规则 &lt;code>qdisc&lt;/code>、网络设备 &lt;code>dev&lt;/code> 和发送队列 &lt;code>txq&lt;/code> 三个变量来到 &lt;code>__dev_xmit_skb&lt;/code>， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2684-L2745">net/core/dev.c&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
struct net_device *dev,
struct netdev_queue *txq)
{
spinlock_t *root_lock = qdisc_lock(q);
bool contended;
int rc;
qdisc_pkt_len_init(skb);
qdisc_calculate_pkt_len(skb, q);
/*
* Heuristic to force contended enqueues to serialize on a
* separate lock before trying to get qdisc main lock.
* This permits __QDISC_STATE_RUNNING owner to get the lock more often
* and dequeue packets faster.
*/
contended = qdisc_is_running(q);
if (unlikely(contended))
spin_lock(&amp;amp;q-&amp;gt;busylock);
&lt;/code>&lt;/pre>
&lt;p>代码首先使用 &lt;code>qdisc_pkt_len_init&lt;/code> 和 &lt;code>qdisc_calculate_pkt_len&lt;/code> 来计算数据的准确长度 ，稍后 qdisc 会用到该值。 对于硬件 offload（例如 UFO）这是必需的，因为添加的额外的头 信息，硬件 offload 的时候回用到。
接下来，使用另一个锁来帮助减少 qdisc 主锁上的竞争（我们稍后会看到这第二个锁）。 如 果 qdisc 当前正在运行，那么试图发送的其他程序将在 qdisc 的 &lt;code>busylock&lt;/code> 上竞争。 这允许 运行 qdisc 的程序在处理数据包的同时，与较少量的程序竞争第二个主锁。随着竞争者数量 的减少，这种技巧增加了吞吐量。&lt;a href="https://github.com/torvalds/linux/commit/79640a4ca6955e3ebdb7038508fa7a0cd7fa5527">原始 commit 描述  &lt;/a>。 接下来是主锁：&lt;/p>
&lt;pre>&lt;code>spin_lock(root_lock);
&lt;/code>&lt;/pre>
&lt;p>接下来处理 3 种可能情况：&lt;/p>
&lt;ol>
&lt;li>如果 qdisc 已停用&lt;/li>
&lt;li>如果 qdisc 允许数据包 bypass 排队系统，并且没有其他包要发送，并且 qdisc 当前没有运 行。允许包 bypass 所谓的**“work-conserving qdisc” - 那些用于流量整形（traffic reshaping）目的并且不会引起发送延迟的 qdisc**&lt;/li>
&lt;li>所有其他情况&lt;/li>
&lt;/ol>
&lt;p>让我们来看看每种情况下发生什么，从 qdisc 停用开始：&lt;/p>
&lt;pre>&lt;code>if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &amp;amp;q-&amp;gt;state))) {
kfree_skb(skb);
rc = NET_XMIT_DROP;
&lt;/code>&lt;/pre>
&lt;p>这很简单。 如果 qdisc 停用，则释放数据并将返回代码设置为 &lt;code>NET_XMIT_DROP&lt;/code>。
接下来，如果 qdisc 允许数据包 bypass，并且没有其他包要发送，并且 qdisc 当前没有运行：&lt;/p>
&lt;pre>&lt;code>} else if ((q-&amp;gt;flags &amp;amp; TCQ_F_CAN_BYPASS) &amp;amp;&amp;amp; !qdisc_qlen(q) &amp;amp;&amp;amp;
qdisc_run_begin(q)) {
/*
* This is a work-conserving queue; there are no old skbs
* waiting to be sent out; and the qdisc is not running -
* xmit the skb directly.
*/
if (!(dev-&amp;gt;priv_flags &amp;amp; IFF_XMIT_DST_RELEASE))
skb_dst_force(skb);
qdisc_bstats_update(q, skb);
if (sch_direct_xmit(skb, q, dev, txq, root_lock)) {
if (unlikely(contended)) {
spin_unlock(&amp;amp;q-&amp;gt;busylock);
contended = false;
}
__qdisc_run(q);
} else
qdisc_run_end(q);
rc = NET_XMIT_SUCCESS;
&lt;/code>&lt;/pre>
&lt;p>这个 if 语句有点复杂，如果满足以下所有条件，则整个语句的计算结果为 true：&lt;/p>
&lt;ol>
&lt;li>&lt;code>q-&amp;gt; flags＆TCQ_F_CAN_BYPASS&lt;/code>：qdisc 允许数据包绕过排队系统。对于所谓的“ work-conserving” qdiscs 这会是 &lt;code>true&lt;/code>；即，允许 packet bypass 流量整形 qdisc。 &lt;code>pfifo_fast&lt;/code> qdisc 允许数据包 bypass&lt;/li>
&lt;li>&lt;code>!qdisc_qlen(q)&lt;/code>：qdisc 的队列中没有待发送的数据&lt;/li>
&lt;li>&lt;code>qdisc_run_begin(p)&lt;/code>：如果 qdisc 未运行，此函数将设置 qdisc 的状态为“running”并返 回 &lt;code>true&lt;/code>，如果 qdisc 已在运行，则返回 &lt;code>false&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>如果以上三个条件都为 &lt;code>true&lt;/code>，那么：&lt;/p>
&lt;ul>
&lt;li>检查 &lt;code>IFF_XMIT_DST_RELEASE&lt;/code> 标志，此标志允许内核释放 skb 的目标缓存。如果标志已禁用，将强制对 skb 进行引用计数&lt;/li>
&lt;li>调用 &lt;code>qdisc_bstats_update&lt;/code> 更新 qdisc 发送的字节数和包数统计&lt;/li>
&lt;li>调用 &lt;code>sch_direct_xmit&lt;/code> 用于发送数据包。我们将很快深入研究 &lt;code>sch_direct_xmit&lt;/code>，因为慢路径也会调用到它&lt;/li>
&lt;/ul>
&lt;p>&lt;code>sch_direct_xmit&lt;/code> 的返回值有两种情况：&lt;/p>
&lt;ol>
&lt;li>队列不为空（返回&amp;gt; 0）。在这种情况下，&lt;code>busylock&lt;/code> 将被释放，然后调用&lt;code>__qdisc_run&lt;/code> 重新启动 qdisc 处理&lt;/li>
&lt;li>队列为空（返回 0）。在这种情况下，&lt;code>qdisc_run_end&lt;/code> 用于关闭 qdisc 处理&lt;/li>
&lt;/ol>
&lt;p>在任何一种情况下，都会返回 &lt;code>NET_XMIT_SUCCESS&lt;/code>，这不是太糟糕。
让我们检查最后一种情况：&lt;/p>
&lt;pre>&lt;code>} else {
skb_dst_force(skb);
rc = q-&amp;gt;enqueue(skb, q) &amp;amp; NET_XMIT_MASK;
if (qdisc_run_begin(q)) {
if (unlikely(contended)) {
spin_unlock(&amp;amp;q-&amp;gt;busylock);
contended = false;
}
__qdisc_run(q);
}
}
&lt;/code>&lt;/pre>
&lt;p>在所有其他情况下：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>skb_dst_force&lt;/code> 强制对 skb 的目标缓存进行引用计数&lt;/li>
&lt;li>调用 qdisc 的 &lt;code>enqueue&lt;/code> 方法将数据入队，保存函数返回值&lt;/li>
&lt;li>调用 &lt;code>qdisc_run_begin(p)&lt;/code>将 qdisc 标记为正在运行。如果它尚未运行（&lt;code>contended == false&lt;/code>），则释放 &lt;code>busylock&lt;/code>，然后调用&lt;code>__qdisc_run(p)&lt;/code>启动 qdisc 处理&lt;/li>
&lt;/ol>
&lt;p>函数最后释放相应的锁，并返回状态码：&lt;/p>
&lt;pre>&lt;code>spin_unlock(root_lock);
if (unlikely(contended))
spin_unlock(&amp;amp;q-&amp;gt;busylock);
return rc;
&lt;/code>&lt;/pre>
&lt;h2 id="75-调优-transmit-packet-steering-xps">7.5 调优: Transmit Packet Steering (XPS)&lt;a class="td-heading-self-link" href="#75-%e8%b0%83%e4%bc%98-transmit-packet-steering-xps" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>使用 XPS 需要在内核配置中启用它（Ubuntu 上内核 3.13.0 有 XPS），并提供一个位掩码，用于 描述&lt;strong>CPU 和 TX queue 的对应关系&lt;/strong>。
这些位掩码类似于 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#receive-packet-steering-rps">RPS&lt;/a> 位掩码，你可以在内核&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L147-L150">文档  &lt;/a>中找到有关这些位掩码的一些资料。
简而言之，要修改的位掩码位于以下位置：&lt;/p>
&lt;pre>&lt;code>/sys/class/net/DEVICE_NAME/queues/QUEUE/xps_cpus
&lt;/code>&lt;/pre>
&lt;p>因此，对于 eth0 和 TX queue 0，你需要使用十六进制数修改文件： &lt;code>/sys/class/net/eth0/queues/tx-0/xps_cpus&lt;/code>，制定哪些 CPU 应处理来自 eth0 的发送队列 0 的发送过程。另外，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L412-L422">文档  &lt;/a>指出，在某些配置中可能不需要 XPS。&lt;/p>
&lt;h1 id="8-queuing-disciplines排队规则">8 Queuing Disciplines（排队规则）&lt;a class="td-heading-self-link" href="#8-queuing-disciplines%e6%8e%92%e9%98%9f%e8%a7%84%e5%88%99" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>至此，我们需要先看一些 qdisc 代码。本文不打算涵盖 TX 所有选项的具体细节。 如果对此感兴趣，可以查看&lt;a href="http://lartc.org/howto/index.html">这篇&lt;/a>很棒的指南。
接下来将查看&lt;strong>通用的数据包调度程序&lt;/strong>（generic packet scheduler）是如何工作的 。特别地，我们将分析 &lt;code>qdisc_run_begin()&lt;/code>、&lt;code>qdisc_run_end()&lt;/code>、&lt;code>__ qdisc_run()&lt;/code> 和 &lt;code>sch_direct_xmit()&lt;/code> 函数是如何一层层将数据传递给驱动程序的。
从 &lt;code>qdisc_run_begin()&lt;/code> 的工作原理开始。&lt;/p>
&lt;h2 id="81-qdisc_run_begin-and-qdisc_run_end仅设置-qdisc-状态位">8.1 &lt;code>qdisc_run_begin()&lt;/code> and &lt;code>qdisc_run_end()&lt;/code>：仅设置 qdisc 状态位&lt;a class="td-heading-self-link" href="#81-qdisc_run_begin-and-qdisc_run_end%e4%bb%85%e8%ae%be%e7%bd%ae-qdisc-%e7%8a%b6%e6%80%81%e4%bd%8d" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/net/sch_generic.h#L101-L107">include/net/sch_generic.h&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static inline bool qdisc_run_begin(struct Qdisc *qdisc)
{
if (qdisc_is_running(qdisc))
return false;
qdisc-&amp;gt;__state |= __QDISC___STATE_RUNNING;
return true;
}
static inline void qdisc_run_end(struct Qdisc *qdisc)
{
qdisc-&amp;gt;__state &amp;amp;= ~__QDISC___STATE_RUNNING;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>qdisc_run_begin()&lt;/code> 检查 qdisc 是否设置了&lt;code>__QDISC___STATE_RUNNING&lt;/code> 状态 位。如果设置了，直接返回 &lt;code>false&lt;/code>；否则，设置此状态位，然后返回 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>qdisc_run_end()&lt;/code> 执行相反的操作，清除此状态位。&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，这两个函数都&lt;strong>只是设置状态位，并没有真正干活&lt;/strong>。真正的处理过程是从 &lt;code>__qdisc_run()&lt;/code> 开始的。&lt;/p>
&lt;h2 id="82-__qdisc_run真正的-qdisc-执行入口">8.2 &lt;code>__qdisc_run()&lt;/code>：真正的 qdisc 执行入口&lt;a class="td-heading-self-link" href="#82-__qdisc_run%e7%9c%9f%e6%ad%a3%e7%9a%84-qdisc-%e6%89%a7%e8%a1%8c%e5%85%a5%e5%8f%a3" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>这个函数乍看非常简单，甚至让人产生错觉：&lt;/p>
&lt;pre>&lt;code>void __qdisc_run(struct Qdisc *q)
{
int quota = weight_p;
while (qdisc_restart(q)) { // 从队列取出一个 skb 并发送，剩余队列不为空时返回非零，见 8.3
// 如果发生下面情况之一，则延后处理：
// 1. quota 用尽
// 2. 其他进程需要 CPU
if (--quota &amp;lt;= 0 || need_resched()) {
__netif_schedule(q);
break;
}
}
qdisc_run_end(q); // 清除 RUNNING 状态位
}
&lt;/code>&lt;/pre>
&lt;p>函数首先获取 &lt;code>weight_p&lt;/code>，这个变量通常是通过 sysctl 设置的，收包路径也会用到。我们稍 后会看到如何调整此值。这个循环做两件事：&lt;/p>
&lt;ol>
&lt;li>在 &lt;code>while&lt;/code> 循环中调用 &lt;code>qdisc_restart()&lt;/code>，直到它返回 &lt;code>false&lt;/code>（或触发下面的中断）。&lt;/li>
&lt;li>判断是否还有 quota，或 &lt;code>need_resched()&lt;/code> 是否返回 &lt;code>true&lt;/code>。其中任何一个为真， 将调用 &lt;code>__netif_schedule()&lt;/code> 然后跳出循环。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注意：用户程序调用 &lt;code>sendmsg&lt;/code> &lt;strong>系统调用之后，内核便接管了执行过程，一路执行到 这里;用户程序一直在累积系统时间（system time）&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>如果用户程序在内核中用完其 time quota，&lt;code>need_resched()&lt;/code> 将返回 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>如果仍有 quota，且用户程序的时间片尚未使用，则将再次调用 &lt;code>qdisc_restart()&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>先来看看 &lt;code>qdisc_restart(q)&lt;/code>是如何工作的，然后将深入研究&lt;code>__netif_schedule(q)&lt;/code>。&lt;/p>
&lt;h2 id="83-qdisc_restart从-qdisc-队列中取包发送给网络驱动">8.3 &lt;code>qdisc_restart&lt;/code>：从 qdisc 队列中取包，发送给网络驱动&lt;a class="td-heading-self-link" href="#83-qdisc_restart%e4%bb%8e-qdisc-%e9%98%9f%e5%88%97%e4%b8%ad%e5%8f%96%e5%8c%85%e5%8f%91%e9%80%81%e7%bb%99%e7%bd%91%e7%bb%9c%e9%a9%b1%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L156-L192">qdisc_restart()&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/*
* NOTE: Called under qdisc_lock(q) with locally disabled BH.
*
* __QDISC_STATE_RUNNING guarantees only one CPU can process
* this qdisc at a time. qdisc_lock(q) serializes queue accesses for this queue.
*
* netif_tx_lock serializes accesses to device driver.
*
* qdisc_lock(q) and netif_tx_lock are mutually exclusive,
* if one is grabbed, another must be free.
*
* Returns to the caller:
* 0 - queue is empty or throttled.
* &amp;gt;0 - queue is not empty.
*/
static inline int qdisc_restart(struct Qdisc *q)
{
struct sk_buff *skb = dequeue_skb(q);
if (!skb)
return 0;
spinlock_t *root_lock = qdisc_lock(q);
struct net_device *dev = qdisc_dev(q);
struct netdev_queue *txq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));
return sch_direct_xmit(skb, q, dev, txq, root_lock);
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>qdisc_restart()&lt;/code> 函数开头的注释非常有用，描述了用到的三个锁：&lt;/p>
&lt;ol>
&lt;li>&lt;code>__QDISC_STATE_RUNNING&lt;/code> 保证了同一时间只有一个 CPU 可以处理这个 qdisc。&lt;/li>
&lt;li>&lt;code>qdisc_lock(q)&lt;/code> 将&lt;strong>访问此 qdisc&lt;/strong> 的操作顺序化。&lt;/li>
&lt;li>&lt;code>netif_tx_lock&lt;/code> 将&lt;strong>访问设备驱动&lt;/strong>的操作顺序化。&lt;/li>
&lt;/ol>
&lt;p>函数逻辑：&lt;/p>
&lt;ol>
&lt;li>首先调用 &lt;code>dequeue_skb()&lt;/code> 从 qdisc 中取出要发送的 skb。如果队列为空，返回 0， 这将导致上层的 &lt;code>qdisc_restart()&lt;/code> 返回 &lt;code>false&lt;/code>，继而退出 &lt;code>while&lt;/code> 循环。&lt;/li>
&lt;li>如果 skb 不为空，接下来获取 qdisc 队列锁，然后找到相关的发送设备 &lt;code>dev&lt;/code> 和发送 队列 &lt;code>txq&lt;/code>，最后带着这些参数调用 &lt;code>sch_direct_xmit()&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>先来看 &lt;code>dequeue_skb()&lt;/code>，然后再回到 &lt;code>sch_direct_xmit()&lt;/code>。&lt;/p>
&lt;h3 id="831-dequeue_skb从-qdisc-队列取待发送-skb">8.3.1 &lt;code>dequeue_skb()&lt;/code>：从 qdisc 队列取待发送 skb&lt;a class="td-heading-self-link" href="#831-dequeue_skb%e4%bb%8e-qdisc-%e9%98%9f%e5%88%97%e5%8f%96%e5%be%85%e5%8f%91%e9%80%81-skb" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L59-L78">net/sched/sch_generic.c&lt;/a>。&lt;/p>
&lt;pre>&lt;code>static inline struct sk_buff *dequeue_skb(struct Qdisc *q)
{
struct sk_buff *skb = q-&amp;gt;gso_skb; // 待发送包
struct netdev_queue *txq = q-&amp;gt;dev_queue; // 之前发送失败的包所在的队列
if (unlikely(skb)) {
/* check the reason of requeuing without tx lock first */
txq = netdev_get_tx_queue(txq-&amp;gt;dev, skb_get_queue_mapping(skb));
if (!netif_xmit_frozen_or_stopped(txq)) {
q-&amp;gt;gso_skb = NULL;
q-&amp;gt;q.qlen--;
} else
skb = NULL;
} else {
if (!(q-&amp;gt;flags &amp;amp; TCQ_F_ONETXQUEUE) || !netif_xmit_frozen_or_stopped(txq))
skb = q-&amp;gt;dequeue(q);
}
return skb;
&lt;/code>&lt;/pre>
&lt;p>函数首先声明一个 &lt;code>struct sk_buff *skb&lt;/code> 变量，这是接下来要处理的数据。这个变量后 面会依不同情况而被赋不同的值，最后作为返回值返回给调用方。
变量 &lt;code>skb&lt;/code> 初始化为 qdisc 的 &lt;code>gso_skb&lt;/code> 字段，这是&lt;strong>之前由于发送失败而重新入队的数据&lt;/strong>。
接下来分为两种情况，根据 &lt;code>skb = q-&amp;gt;gso_skb&lt;/code> 是否为空：&lt;/p>
&lt;ol>
&lt;li>如果不为空，会将之前重新入队的 skb 出队，作为待处理数据返回。
&lt;ol>
&lt;li>检查发送队列是否已停止。&lt;/li>
&lt;li>如果队列未停止，则 &lt;code>gso_skb&lt;/code> 字段置空，队列长度减 1，返回 skb。&lt;/li>
&lt;li>如果队列已停止，则 &lt;code>gso_skb&lt;/code> 不动，返回空。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>如果为空（即之前没有数据重新入队），则从要处理的 qdisc 中取出一个新 skb，作为待处理数据返回。进入另一个 tricky 的 if 语句，如果：
&lt;ol>
&lt;li>qdisc 不是单发送队列，或&lt;/li>
&lt;li>发送队列未停止工作&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>则调用 qdisc 的 &lt;code>dequeue()&lt;/code> 方法获取新数据并返回。dequeue 的内部实现依 qdisc 的实现和功能而有所不同。&lt;/li>
&lt;/ol>
&lt;p>该函数最后返回变量 &lt;code>skb&lt;/code>，这是接下来要处理的数据包。&lt;/p>
&lt;h3 id="832-sch_direct_xmit发送给网卡驱动">8.3.2 &lt;code>sch_direct_xmit()&lt;/code>：发送给网卡驱动&lt;a class="td-heading-self-link" href="#832-sch_direct_xmit%e5%8f%91%e9%80%81%e7%bb%99%e7%bd%91%e5%8d%a1%e9%a9%b1%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>现在来到 &lt;code>sch_direct_xmit()&lt;/code>（定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L109-L154">net/sched/sch_generic.c&lt;/a> ），这是将数据向下发送到网络设备的重要一步。&lt;/p>
&lt;pre>&lt;code>/*
* Transmit one skb, and handle the return status as required. Holding the
* __QDISC_STATE_RUNNING bit guarantees that only one CPU can execute this
* function.
*
* Returns to the caller:
* 0 - queue is empty or throttled.
* &amp;gt;0 - queue is not empty.
*/
int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,
struct net_device *dev, struct netdev_queue *txq,
spinlock_t *root_lock)
{
int ret = NETDEV_TX_BUSY;
spin_unlock(root_lock);
if (!netif_xmit_frozen_or_stopped(txq))
ret = dev_hard_start_xmit(skb, dev, txq);
spin_lock(root_lock);
if (dev_xmit_complete(ret)) { // 1. 驱动发送成功
ret = qdisc_qlen(q); // 将 qdisc 队列的剩余长度作为返回值
} else if (ret == NETDEV_TX_LOCKED) { // 2. 驱动获取发送锁失败
ret = handle_dev_cpu_collision(skb, txq, q);
} else { // 3. 驱动发送“正忙”，当前无法发送
ret = dev_requeue_skb(skb, q); // 将数据重新入队，等下次发送。
}
if (ret &amp;amp;&amp;amp; netif_xmit_frozen_or_stopped(txq))
ret = 0;
return ret;
&lt;/code>&lt;/pre>
&lt;p>这段代码首先释放 qdisc（发送队列）锁，然后获取（设备驱动的）发送锁。
接下来，如果发送队列没有停止，就会调用 &lt;code>dev_hard_start_xmit()&lt;/code>。稍后将看到， 后者会把数据从 Linux 内核的网络设备子系统发送到设备驱动程序。
&lt;code>dev_hard_start_xmit()&lt;/code> 执行之后，（或因发送队列停止而跳过执行），队列的发送锁就会被释放。
接下来，再次获取此 qdisc 的锁，然后通过调用 &lt;code>dev_xmit_complete()&lt;/code> 检查 &lt;code>dev_hard_start_xmit()&lt;/code> 的返回值。&lt;/p>
&lt;ol>
&lt;li>如果 &lt;code>dev_xmit_complete()&lt;/code> 返回 &lt;code>true&lt;/code>，数据已成功发送，则将 qdisc 队列长度设置为返回值，否则&lt;/li>
&lt;li>如果 &lt;code>dev_hard_start_xmit()&lt;/code> 返回的是 &lt;code>NETDEV_TX_LOCKED&lt;/code>，调用 &lt;code>handle_dev_cpu_collision()&lt;/code> 来处理锁竞争。
当驱动程序锁定发送队列失败时，支持 &lt;code>NETIF_F_LLTX&lt;/code> 功能的设备会返回 &lt;code>NETDEV_TX_LOCKED&lt;/code>。 稍后会仔细研究 &lt;code>handle_dev_cpu_collision&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>现在，让我们继续关注 &lt;code>sch_direct_xmit()&lt;/code> 并查看，以上两种情况都不满足时的情况。 如果发送失败，而且不是以上两种情况，那还有第三种可能：由于 &lt;code>NETDEV_TX_BUSY&lt;/code>。驱动 程序返回 &lt;code>NETDEV_TX_BUSY&lt;/code> 表示设备或驱动程序“正忙”，数据现在无法发送。这种情 况下，调用 &lt;code>dev_requeue_skb()&lt;/code> 将数据重新入队，等下次发送。
来深入地看一下 &lt;code>handle_dev_cpu_collision()&lt;/code> 和 &lt;code>dev_requeue_skb()&lt;/code>。&lt;/p>
&lt;h3 id="833-handle_dev_cpu_collision">8.3.3 &lt;code>handle_dev_cpu_collision()&lt;/code>&lt;a class="td-heading-self-link" href="#833-handle_dev_cpu_collision" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L80-L107">net/sched/sch_generic.c&lt;/a>，处理两种情况：&lt;/p>
&lt;ol>
&lt;li>发送锁由当前 CPU 保持&lt;/li>
&lt;li>发送锁由其他 CPU 保存&lt;/li>
&lt;/ol>
&lt;p>第一种情况认为是配置问题，打印一条警告。
第二种情况，更新统计计数器 &lt;code>cpu_collision&lt;/code>，通过 &lt;code>dev_requeue_skb&lt;/code> 将数据重新入队 以便稍后发送。回想一下，我们在 &lt;code>dequeue_skb&lt;/code> 中看到了专门处理重新入队的 skb 的代码。
代码很简短，可以快速阅读：&lt;/p>
&lt;pre>&lt;code>static inline int handle_dev_cpu_collision(struct sk_buff *skb,
struct netdev_queue *dev_queue,
struct Qdisc *q)
{
int ret;
if (unlikely(dev_queue-&amp;gt;xmit_lock_owner == smp_processor_id())) {
/*
* Same CPU holding the lock. It may be a transient
* configuration error, when hard_start_xmit() recurses. We
* detect it by checking xmit owner and drop the packet when
* deadloop is detected. Return OK to try the next skb.
*/
kfree_skb(skb);
net_warn_ratelimited(&amp;quot;Dead loop on netdevice %s, fix it urgently!\n&amp;quot;,
dev_queue-&amp;gt;dev-&amp;gt;name);
ret = qdisc_qlen(q);
} else {
/*
* Another cpu is holding lock, requeue &amp;amp; delay xmits for
* some time.
*/
__this_cpu_inc(softnet_data.cpu_collision);
ret = dev_requeue_skb(skb, q);
}
return ret;
}
&lt;/code>&lt;/pre>
&lt;p>接下来看看 &lt;code>dev_requeue_skb&lt;/code> 做了什么，后面会看到，&lt;code>sch_direct_xmit&lt;/code> 会调用它.&lt;/p>
&lt;h3 id="834-dev_requeue_skb重新压入-qdisc-队列等待下次发送">8.3.4 &lt;code>dev_requeue_skb()&lt;/code>：重新压入 qdisc 队列，等待下次发送&lt;a class="td-heading-self-link" href="#834-dev_requeue_skb%e9%87%8d%e6%96%b0%e5%8e%8b%e5%85%a5-qdisc-%e9%98%9f%e5%88%97%e7%ad%89%e5%be%85%e4%b8%8b%e6%ac%a1%e5%8f%91%e9%80%81" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这个函数很简短，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L39-L57">net/sched/sch_generic.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/* Modifications to data participating in scheduling must be protected with
* qdisc_lock(qdisc) spinlock.
*
* The idea is the following:
* - enqueue, dequeue are serialized via qdisc root lock
* - ingress filtering is also serialized via qdisc root lock
* - updates to tree and tree walking are only done under the rtnl mutex.
*/
static inline int dev_requeue_skb(struct sk_buff *skb, struct Qdisc *q)
{
skb_dst_force(skb); // skb 上强制增加一次引用计数
q-&amp;gt;gso_skb = skb; // 回想一下，dequeue_skb() 中取出一个 skb 时会检查该字段
q-&amp;gt;qstats.requeues++; // 更新 `requeue` 计数
q-&amp;gt;q.qlen++; // 更新 qdisc 队列长度
__netif_schedule(q); // 触发 softirq
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>接下来再回忆一遍我们一步步到达这里的过程，然后查看 &lt;code>__netif_schedule()&lt;/code>。&lt;/p>
&lt;h2 id="84-复习__qdisc_run-主逻辑">8.4 复习：&lt;code>__qdisc_run()&lt;/code> 主逻辑&lt;a class="td-heading-self-link" href="#84-%e5%a4%8d%e4%b9%a0__qdisc_run-%e4%b8%bb%e9%80%bb%e8%be%91" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>回想一下，我们是从 &lt;code>__qdisc_run()&lt;/code> 开始到达这里的：&lt;/p>
&lt;pre>&lt;code>void __qdisc_run(struct Qdisc *q)
{
int quota = weight_p;
while (qdisc_restart(q)) { // dequeue skb, send it
if (--quota &amp;lt;= 0 || need_resched()) {// Ordered by possible occurrence: Postpone processing if
__netif_schedule(q); // 1. we've exceeded packet quota
break; // 2. another process needs the CPU
}
}
qdisc_run_end(q);
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>while&lt;/code> 循环调用 &lt;code>qdisc_restart()&lt;/code>，后者取出一个 skb，然后尝试通过 &lt;code>sch_direct_xmit()&lt;/code> 来发送；&lt;code>sch_direct_xmit&lt;/code> 调用 &lt;code>dev_hard_start_xmit&lt;/code> 来向驱动 程序进行实际发送。任何无法发送的 skb 都重新入队，将在 &lt;code>NET_TX&lt;/code> softirq 中进行 发送。
发送过程的下一步是查看 &lt;code>dev_hard_start_xmit()&lt;/code>，了解如何调用驱动程序来发送数据。但 在此之前，应该先查看 &lt;code>__netif_schedule()&lt;/code> 以完全理解 &lt;code>__qdisc_run()&lt;/code> 和 &lt;code>dev_requeue_skb()&lt;/code> 的工作方式。&lt;/p>
&lt;h3 id="841-__netif_schedule">8.4.1 &lt;code>__netif_schedule&lt;/code>&lt;a class="td-heading-self-link" href="#841-__netif_schedule" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>现在来看 &lt;code>__netif_schedule()&lt;/code>， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2127-L2146">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>void __netif_schedule(struct Qdisc *q)
{
if (!test_and_set_bit(__QDISC_STATE_SCHED, &amp;amp;q-&amp;gt;state))
__netif_reschedule(q);
}
EXPORT_SYMBOL(__netif_schedule);
static inline void __netif_reschedule(struct Qdisc *q)
{
struct softnet_data *sd;
unsigned long flags;
local_irq_save(flags); // 保存硬中断状态，并禁用硬中断（IRQ）
sd = &amp;amp;__get_cpu_var(softnet_data); // 获取当前 CPU 的 struct softnet_data 实例
q-&amp;gt;next_sched = NULL;
*sd-&amp;gt;output_queue_tailp = q; // 将 qdisc 添加到 softnet_data 的 output 队列中
sd-&amp;gt;output_queue_tailp = &amp;amp;q-&amp;gt;next_sched;
raise_softirq_irqoff(NET_TX_SOFTIRQ); // 重要步骤：触发 NET_TX_SOFTIRQ 类型软中断（softirq）
local_irq_restore(flags); // 恢复 IRQ 状态并重新启用硬中断
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>test_and_set_bit()&lt;/code> 检查 &lt;code>q-&amp;gt;state&lt;/code> 中的 &lt;code>__QDISC_STATE_SCHED&lt;/code> 位，如果为该位为 0，会将其置 1。 如果置位成功（意味着之前处于非 &lt;code>__QDISC_STATE_SCHED&lt;/code> 状态），代码将调用 &lt;code>__netif_reschedule()&lt;/code>，这个函数不长，但做的事情非常重要。&lt;/p>
&lt;blockquote>
&lt;p>更多有关 &lt;code>struct softnet_data&lt;/code> 初始化的内容，可参考我们之前关于网络栈接收数据的 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#linux-network-device-subsystem">文章&lt;/a>。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>__netif_reschedule()&lt;/code> 中的重要步骤是 &lt;code>raise_softirq_irqoff()&lt;/code>，它触发一次 &lt;code>NET_TX_SOFTIRQ&lt;/code> 类型 softirq。 softirqs 及其注册过程也包含在我们之前的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#softirqs">文章  &lt;/a>中。简单来说，可以认为 &lt;strong>softirqs 是以很高优先级在执行的内核线程，并代表内核处理数据&lt;/strong>， 用于网络数据的收发处理（incoming 和 outgoing）。
正如在&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">上一篇&lt;/a>文章中看到的，&lt;code>NET_TX_SOFTIRQ&lt;/code> softirq 有一个注册的回调函数 &lt;code>net_tx_action()&lt;/code>，这意味着有一个内核线程将会执行 &lt;code>net_tx_action()&lt;/code>。该线程偶尔会被暂 停（pause），&lt;code>raise_softirq_irqoff()&lt;/code> 会恢复（resume）其执行。让我们看一下 &lt;code>net_tx_action()&lt;/code> 的作用，以便了解内核如何处理发送数据请求。&lt;/p>
&lt;h3 id="842-net_tx_action">8.4.2 &lt;code>net_tx_action()&lt;/code>&lt;a class="td-heading-self-link" href="#842-net_tx_action" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3297-L3353">net/core/dev.c&lt;/a> ，由两个 if 组成，分别处理 executing CPU 的 &lt;strong>&lt;code>softnet_data&lt;/code> 实例的两个 queue&lt;/strong>：&lt;/p>
&lt;ol>
&lt;li>completion queue&lt;/li>
&lt;li>output queue&lt;/li>
&lt;/ol>
&lt;p>让我们分别来看这两种情况，注意，&lt;strong>这段代码在 softirq 上下文中作为一个独立的内核线 程执行&lt;/strong>。网络栈发送侧的&lt;strong>热路径中不适合执行的代码，将被延后（defer），然 后由执行 &lt;code>net_tx_action()&lt;/code> 的线程处理&lt;/strong>。&lt;/p>
&lt;h3 id="843-net_tx_action-completion-queue待释放-skb-队列">8.4.3 &lt;code>net_tx_action()&lt;/code> completion queue：待释放 skb 队列&lt;a class="td-heading-self-link" href="#843-net_tx_action-completion-queue%e5%be%85%e9%87%8a%e6%94%be-skb-%e9%98%9f%e5%88%97" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>softnet_data&lt;/code> 的 completion queue 存放&lt;strong>等待释放的 skb&lt;/strong>。函数 &lt;code>dev_kfree_skb_irq&lt;/code> 可以将 skbs 添加到队列中以便稍后释放。设备驱动程序通常使用它来推迟释放已经发送成功的 skbs。驱动 程序推迟释放 skb 的原因是，释放内存可能需要时间，而且有些代码（如 hardirq 处理程序） 需要尽可能快的执行并返回。
看一下 &lt;code>net_tx_action&lt;/code> 第一段代码，该代码处理 completion queue 中等待释放的 skb：&lt;/p>
&lt;pre>&lt;code>if (sd-&amp;gt;completion_queue) {
struct sk_buff *clist;
local_irq_disable();
clist = sd-&amp;gt;completion_queue;
sd-&amp;gt;completion_queue = NULL;
local_irq_enable();
while (clist) {
struct sk_buff *skb = clist;
clist = clist-&amp;gt;next;
__kfree_skb(skb);
}
}
&lt;/code>&lt;/pre>
&lt;p>如果 completion queue 非空，&lt;code>while&lt;/code> 循环将遍历这个列表并&lt;code>__kfree_skb&lt;/code> 释放每个 skb 占 用的内存。&lt;strong>牢记，此代码在一个名为 softirq 的独立“线程”中运行 - 它并没有占用用 户程序的系统时间（system time）&lt;/strong>。&lt;/p>
&lt;h3 id="844-net_tx_action-output-queue待发送-skb-队列">8.4.4 &lt;code>net_tx_action&lt;/code> output queue：待发送 skb 队列&lt;a class="td-heading-self-link" href="#844-net_tx_action-output-queue%e5%be%85%e5%8f%91%e9%80%81-skb-%e9%98%9f%e5%88%97" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>output queue 存储 &lt;strong>待发送的 skb&lt;/strong>。如前所述，&lt;code>__netif_reschedule()&lt;/code> 将数据添加到 output queue 中，通常从&lt;code>__netif_schedule&lt;/code> 调用过来。
目前，我们看到 &lt;code>__netif_schedule()&lt;/code> 函数在两个地方被调用：&lt;/p>
&lt;ol>
&lt;li>&lt;code>dev_requeue_skb()&lt;/code>：如果驱动程序返回 &lt;code>NETDEV_TX_BUSY&lt;/code> 或者存在 CPU 冲突，可以调用此函数。&lt;/li>
&lt;li>&lt;code>__qdisc_run()&lt;/code>：一旦超出 quota 或者需要 reschedule，会调用&lt;code>__netif_schedule&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>这个函数会将 qdisc 添加到 softnet_data 的 output queue 进行处理。 这里将输出队列处理代码拆分为三个块。
我们来看看第一块：&lt;/p>
&lt;pre>&lt;code>if (sd-&amp;gt;output_queue) { // 如果 output queue 上有 qdisc
struct Qdisc *head;
local_irq_disable();
head = sd-&amp;gt;output_queue; // 将 head 指向第一个 qdisc
sd-&amp;gt;output_queue = NULL;
sd-&amp;gt;output_queue_tailp = &amp;amp;sd-&amp;gt;output_queue; // 更新队尾指针
local_irq_enable();
&lt;/code>&lt;/pre>
&lt;p>如果 output queue 上有 qdisc，则将 &lt;code>head&lt;/code> 变量指向第一个 qdisc，并 更新队尾指针。
接下来，一个 &lt;strong>&lt;code>while&lt;/code> 循环开始遍历 qdsics 列表&lt;/strong>：&lt;/p>
&lt;pre>&lt;code>while (head) {
struct Qdisc *q = head;
head = head-&amp;gt;next_sched;
spinlock_t *root_lock = qdisc_lock(q);
if (spin_trylock(root_lock)) { // 非阻塞：尝试获取 qdisc root lock
smp_mb__before_clear_bit();
clear_bit(__QDISC_STATE_SCHED, &amp;amp;q-&amp;gt;state); // 清除 q-&amp;gt;state SCHED 状态位
qdisc_run(q); // 执行 qdisc 规则，这会设置 q-&amp;gt;state 的 RUNNING 状态位
spin_unlock(root_lock); // 释放 qdisc 锁
} else {
if (!test_bit(__QDISC_STATE_DEACTIVATED, &amp;amp;q-&amp;gt;state)) { // qdisc 还在运行
__netif_reschedule(q); // 重新放入 queue，稍后继续尝试获取 root lock
} else { // qdisc 已停止运行，清除 SCHED 状态位
smp_mb__before_clear_bit();
clear_bit(__QDISC_STATE_SCHED, &amp;amp;q-&amp;gt;state);
}
}
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>spin_trylock()&lt;/code> 获得 root lock 后，&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>clear_bit()&lt;/code> 清除 qdisc 的 &lt;code>__QDISC_STATE_SCHED&lt;/code> 状态位。&lt;/li>
&lt;li>然后执行 &lt;code>qdisc_run()&lt;/code>，这会将 &lt;code>__QDISC___STATE_RUNNING&lt;/code> 状态位置 1，并执行&lt;code>__qdisc_run()&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>这里很重要。从系统调用开始的发送过程代表 applition 执行，花费的是系统时间；但接 下来它将转入 softirq 上下文中执行（这个 qdisc 的 skb 之前没有被发送出去发），花 费的是 softirq 时间。这种区分非常重要，因为这&lt;strong>直接影响着应用程序的 CPU 使用量监 控&lt;/strong>，尤其是发送大量数据的应用。换一种陈述方式：&lt;/p>
&lt;ol>
&lt;li>无论发送完成还是驱动程序返回错误，程序的系统时间都包括调用驱动程序发送数据所花的时间。&lt;/li>
&lt;li>如果驱动层发送失败（例如，设备忙于发送其他内容），则会将 qdisc 添加到 output queue，稍后由 softirq 线程处理。在这种情况下，将会额外花费一些 softirq（ &lt;code>si&lt;/code>）时间在发送数据上。&lt;/li>
&lt;/ol>
&lt;p>因此，发送数据花费的总时间是下面二者之和：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>系统调用的系统时间&lt;/strong>（sys time）&lt;/li>
&lt;li>&lt;strong>&lt;code>NET_TX&lt;/code> 类型的 softirq 时间&lt;/strong>（softirq time）&lt;/li>
&lt;/ol>
&lt;p>如果 &lt;code>spin_trylock()&lt;/code> 失败，则检查 qdisc 是否已经停止运行（&lt;code>__QDISC_STATE_DEACTIVATED&lt;/code> 状态位），两种情况：&lt;/p>
&lt;ol>
&lt;li>qdisc 未停用：调用 &lt;code>__netif_reschedule()&lt;/code>，这会将 qdisc 放回到原 queue 中，稍后再次尝试获取 qdisc 锁。&lt;/li>
&lt;li>qdisc 已停用：清除 &lt;code>__QDISC_STATE_SCHED&lt;/code> 状态位。&lt;/li>
&lt;/ol>
&lt;h2 id="85-最终来到-dev_hard_start_xmit">8.5 最终来到 &lt;code>dev_hard_start_xmit&lt;/code>&lt;a class="td-heading-self-link" href="#85-%e6%9c%80%e7%bb%88%e6%9d%a5%e5%88%b0-dev_hard_start_xmit" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>至此，我们已经穿过了整个网络栈，最终来到 &lt;code>dev_hard_start_xmit&lt;/code>。也许你是从 &lt;code>sendmsg&lt;/code> 系统调用直接到达这里的，或者你是通过 qdisc 上的 softirq 线程处理网络数据来 到这里的。&lt;code>dev_hard_start_xmit&lt;/code> 将调用设备驱动程序来实际执行发送操作。
这个函数处理两种主要情况：&lt;/p>
&lt;ol>
&lt;li>已经准备好要发送的数据，或&lt;/li>
&lt;li>需要 segmentation offloading 的数据&lt;/li>
&lt;/ol>
&lt;p>先看第一种情况，要发送的数据已经准备好的情况。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2541-L2652">net/code/dev.c&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
struct netdev_queue *txq)
{
const struct net_device_ops *ops = dev-&amp;gt;netdev_ops;
int rc = NETDEV_TX_OK;
unsigned int skb_len;
if (likely(!skb-&amp;gt;next)) {
netdev_features_t features;
/*
* If device doesn't need skb-&amp;gt;dst, release it right now while
* its hot in this cpu cache
*/
if (dev-&amp;gt;priv_flags &amp;amp; IFF_XMIT_DST_RELEASE)
skb_dst_drop(skb);
features = netif_skb_features(skb);
&lt;/code>&lt;/pre>
&lt;p>代码首先获取设备的回调函数集合 &lt;code>ops&lt;/code>，后面让驱动程序做一些发送数据的工作时会用到 。检查 &lt;code>skb-&amp;gt;next&lt;/code> 以确定此数据不是已分片数据的一部分，然后继续执行以下两项操作：
首先，检查设备是否设置了 &lt;code>IFF_XMIT_DST_RELEASE&lt;/code> 标志。这个版本的内核中的任何“真实” 以太网设备都不使用此标志，但环回设备和其他一些软件设备使用。如果启用此特性，则可 以减少目标高速缓存条目上的引用计数，因为驱动程序不需要它。
接下来，&lt;code>netif_skb_features&lt;/code> 获取设备支持的功能列表，并根据数据的协议类型（ &lt;code>dev-&amp;gt;protocol&lt;/code>）对特性列表进行一些修改。例如，如果设备支持此协议的校验和计算， 则将对 skb 进行相应的标记。 VLAN tag（如果已设置）也会导致功能标记被修改。
接下来，将检查 vlan 标记，如果设备无法 offload VLAN tag，将通过&lt;code>__vlan_put_tag&lt;/code> 在软 件中执行此操作：&lt;/p>
&lt;pre>&lt;code>if (vlan_tx_tag_present(skb) &amp;amp;&amp;amp;
!vlan_hw_offload_capable(features, skb-&amp;gt;vlan_proto)) {
skb = __vlan_put_tag(skb, skb-&amp;gt;vlan_proto,
vlan_tx_tag_get(skb));
if (unlikely(!skb))
goto out;
skb-&amp;gt;vlan_tci = 0;
}
&lt;/code>&lt;/pre>
&lt;p>然后，检查数据以确定这是不是 encapsulation （隧道封装）offload 请求，例如， &lt;a href="https://en.wikipedia.org/wiki/Generic_Routing_Encapsulation">GRE&lt;/a>。 在这种情况 下，feature flags 将被更新，以添加任何特定于设备的硬件封装功能：&lt;/p>
&lt;pre>&lt;code>/* If encapsulation offload request, verify we are testing
* hardware encapsulation features instead of standard
* features for the netdev
*/
if (skb-&amp;gt;encapsulation)
features &amp;amp;= dev-&amp;gt;hw_enc_features;
&lt;/code>&lt;/pre>
&lt;p>接下来，&lt;code>netif_needs_gso&lt;/code> 用于确定 skb 是否需要分片。 如果需要，但设备不支持，则 &lt;code>netif_needs_gso&lt;/code> 将返回 &lt;code>true&lt;/code>，表示分片应在软件中进行。 在这种情况下，调用 &lt;code>dev_gso_segment&lt;/code> 进行分片，代码将跳转到 gso 以发送数据包。我们稍后会看到 GSO 路径。&lt;/p>
&lt;pre>&lt;code>if (netif_needs_gso(skb, features)) {
if (unlikely(dev_gso_segment(skb, features)))
goto out_kfree_skb;
if (skb-&amp;gt;next)
goto gso;
}
&lt;/code>&lt;/pre>
&lt;p>如果数据不需要分片，则处理一些其他情况。 首先，数据是否需要顺序化？ 也就是说，如 果数据分布在多个缓冲区中，设备是否支持发送网络数据，还是首先需要将它们组合成单个 有序缓冲区？ 绝大多数网卡不需要在发送之前将数据顺序化，因此在几乎所有情况下， &lt;code>skb_needs_linearize&lt;/code> 将为 &lt;code>false&lt;/code> 然后被跳过。&lt;/p>
&lt;pre>&lt;code>else {
if (skb_needs_linearize(skb, features) &amp;amp;&amp;amp;
__skb_linearize(skb))
goto out_kfree_skb;
&lt;/code>&lt;/pre>
&lt;p>从接下来的一段注释我们可以了解到，下面的代码判断数据包是否仍然需要计算校验和。 如果设备不支持计算校验和，则在这里通过软件计算：&lt;/p>
&lt;pre>&lt;code>/* If packet is not checksummed and device does not
* support checksumming for this protocol, complete
* checksumming here.
*/
if (skb-&amp;gt;ip_summed == CHECKSUM_PARTIAL) {
if (skb-&amp;gt;encapsulation)
skb_set_inner_transport_header(skb,
skb_checksum_start_offset(skb));
else
skb_set_transport_header(skb,
skb_checksum_start_offset(skb));
if (!(features &amp;amp; NETIF_F_ALL_CSUM) &amp;amp;&amp;amp;
skb_checksum_help(skb))
goto out_kfree_skb;
}
}
&lt;/code>&lt;/pre>
&lt;p>再往前，我们来到了 packet taps（tap 是包过滤器的安插点，例如抓包执行的地方）。回想 一下在&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#netifreceiveskbcore-special-box-delivers-data-to-packet-taps-and-protocol-layers">接收数据的文章  &lt;/a>中，我们看到了数据包是如何传递给 tap（如 &lt;a href="http://www.tcpdump.org/manpages/pcap.3pcap.html">PCAP&lt;/a>）的。 该函数中的下一个代 码块将要发送的数据包传递给 tap（如果有的话）：&lt;/p>
&lt;pre>&lt;code>if (!list_empty(&amp;amp;ptype_all))
dev_queue_xmit_nit(skb, dev);
&lt;/code>&lt;/pre>
&lt;p>最终，调用驱动的 &lt;code>ops&lt;/code> 里面的发送回调函数 &lt;code>ndo_start_xmit&lt;/code> 将数据包传给网卡设备：&lt;/p>
&lt;pre>&lt;code>skb_len = skb-&amp;gt;len;
rc = ops-&amp;gt;ndo_start_xmit(skb, dev);
trace_net_dev_xmit(skb, rc, dev, skb_len);
if (rc == NETDEV_TX_OK)
txq_trans_update(txq);
return rc;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ndo_start_xmit&lt;/code> 的返回值表示发送成功与否，并作为这个函数的返回值被返回给更上层。 我们看到了这个返回值将如何影响上层：数据可能会被此时的 qdisc 重新入队，因此 稍后尝试再次发送。
我们来看看 GSO 的 case。如果此函数的前面部分完成了分片，或者之前已经完成了分片但是 上次发送失败，则会进入下面的代码：&lt;/p>
&lt;pre>&lt;code>gso:
do {
struct sk_buff *nskb = skb-&amp;gt;next;
skb-&amp;gt;next = nskb-&amp;gt;next;
nskb-&amp;gt;next = NULL;
if (!list_empty(&amp;amp;ptype_all))
dev_queue_xmit_nit(nskb, dev);
skb_len = nskb-&amp;gt;len;
rc = ops-&amp;gt;ndo_start_xmit(nskb, dev);
trace_net_dev_xmit(nskb, rc, dev, skb_len);
if (unlikely(rc != NETDEV_TX_OK)) {
if (rc &amp;amp; ~NETDEV_TX_MASK)
goto out_kfree_gso_skb;
nskb-&amp;gt;next = skb-&amp;gt;next;
skb-&amp;gt;next = nskb;
return rc;
}
txq_trans_update(txq);
if (unlikely(netif_xmit_stopped(txq) &amp;amp;&amp;amp; skb-&amp;gt;next))
return NETDEV_TX_BUSY;
} while (skb-&amp;gt;next);
&lt;/code>&lt;/pre>
&lt;p>你可能已经猜到，此 &lt;code>while&lt;/code> 循环会遍历分片生成的 skb 列表。
每个数据包将被：&lt;/p>
&lt;ul>
&lt;li>传给包过滤器（tap，如果有的话）&lt;/li>
&lt;li>通过 &lt;code>ndo_start_xmit&lt;/code> 传递给驱动程序进行发送&lt;/li>
&lt;/ul>
&lt;p>设备驱动 &lt;code>ndo_start_xmit()&lt;/code>返回错误时，会进行一些错误处理，并将错误返回给更上层。 未发送的 skbs 可能会被重新入队以便稍后再次发送。
该函数的最后一部分做一些清理工作，在上面发生错误时释放一些资源：&lt;/p>
&lt;pre>&lt;code>out_kfree_gso_skb:
if (likely(skb-&amp;gt;next == NULL)) {
skb-&amp;gt;destructor = DEV_GSO_CB(skb)-&amp;gt;destructor;
consume_skb(skb);
return rc;
}
out_kfree_skb:
kfree_skb(skb);
out:
return rc;
}
EXPORT_SYMBOL_GPL(dev_hard_start_xmit);
&lt;/code>&lt;/pre>
&lt;p>在继续进入到设备驱动程序之前，先来看一些和前面分析过的代码有关的监控和调优的内容。&lt;/p>
&lt;h2 id="86-monitoring-qdiscs">8.6 Monitoring qdiscs&lt;a class="td-heading-self-link" href="#86-monitoring-qdiscs" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="using-the-tc-command-line-tool">Using the tc command line tool&lt;a class="td-heading-self-link" href="#using-the-tc-command-line-tool" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>使用 &lt;code>tc&lt;/code> 工具监控 qdisc 统计：&lt;/p>
&lt;pre>&lt;code>$ tc -s qdisc show dev eth1
qdisc mq 0: root
Sent 31973946891907 bytes 2298757402 pkt (dropped 0, overlimits 0 requeues 1776429)
backlog 0b 0p requeues 1776429
&lt;/code>&lt;/pre>
&lt;p>网络设备的 qdisc 统计对于监控系统发送数据包的运行状况至关重要。你可以通过运行命令 行工具 tc 来查看状态。 上面的示例显示了如何检查 eth1 的统计信息。&lt;/p>
&lt;ul>
&lt;li>&lt;code>bytes&lt;/code>: The number of bytes that were pushed down to the driver for transmit.&lt;/li>
&lt;li>&lt;code>pkt&lt;/code>: The number of packets that were pushed down to the driver for transmit.&lt;/li>
&lt;li>&lt;code>dropped&lt;/code>: The number of packets that were dropped by the qdisc. This can happen if transmit queue length is not large enough to fit the data being queued to it.&lt;/li>
&lt;li>&lt;code>overlimits&lt;/code>: Depends on the queuing discipline, but can be either the number of packets that could not be enqueued due to a limit being hit, and/or the number of packets which triggered a throttling event when dequeued.&lt;/li>
&lt;li>&lt;code>requeues&lt;/code>: Number of times dev_requeue_skb has been called to requeue an skb. Note that an skb which is requeued multiple times will bump this counter each time it is requeued.&lt;/li>
&lt;li>&lt;code>backlog&lt;/code>: Number of bytes currently on the qdisc’s queue. This number is usually bumped each time a packet is enqueued.&lt;/li>
&lt;/ul>
&lt;p>一些 qdisc 还会导出额外的统计信息。每个 qdisc 都不同，对同一个 counter 可能会累积不同 的次数。你需要查看相应 qdisc 的源代码，弄清楚每个 counter 是在哪里、什么条件下被更新 的，如果这些数据对你非常重要，那你必须这么谨慎。&lt;/p>
&lt;h2 id="87-tuning-qdiscs">8.7 Tuning qdiscs&lt;a class="td-heading-self-link" href="#87-tuning-qdiscs" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="调整__qdisc_run-处理权重">调整&lt;code>__qdisc_run&lt;/code> 处理权重&lt;a class="td-heading-self-link" href="#%e8%b0%83%e6%95%b4__qdisc_run-%e5%a4%84%e7%90%86%e6%9d%83%e9%87%8d" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>你可以调整前面看到的&lt;code>__qdisc_run&lt;/code> 循环的权重（上面看到的 &lt;code>quota&lt;/code> 变量），这将导致 &lt;code>__netif_schedule&lt;/code> 更多的被调用执行。 结果将是当前 qdisc 将被更多的添加到当前 CPU 的 output_queue，最终会使发包所占的时间变多。
例如：调整所有 qdisc 的&lt;code>__qdisc_run&lt;/code> 权重：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.dev_weight=600
&lt;/code>&lt;/pre>
&lt;h3 id="增加发送队列长度">增加发送队列长度&lt;a class="td-heading-self-link" href="#%e5%a2%9e%e5%8a%a0%e5%8f%91%e9%80%81%e9%98%9f%e5%88%97%e9%95%bf%e5%ba%a6" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>每个网络设备都有一个可以修改的 txqueuelen。 大多数 qdisc 在将数据插入到其发送队列之 前，会检查 txqueuelen 是否足够（表示的是字节数？）。 你可以调整这个参数以增加 qdisc 队列的字节数。
Example: increase the &lt;code>txqueuelen&lt;/code> of &lt;code>eth0&lt;/code> to &lt;code>10000&lt;/code>.&lt;/p>
&lt;pre>&lt;code>$ sudo ifconfig eth0 txqueuelen 10000
&lt;/code>&lt;/pre>
&lt;p>默认值是 1000，你可以通过 ifconfig 命令的输出，查看每个网络设备的 txqueuelen。&lt;/p>
&lt;h1 id="9-网络设备驱动">9 网络设备驱动&lt;a class="td-heading-self-link" href="#9-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e9%a9%b1%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>我们即将结束我们的网络栈之旅。
要理解数据包的发送过程，有一个重要的概念。大多数设备和驱动程序通过两个阶段处理数 据包发送：&lt;/p>
&lt;ol>
&lt;li>合理地组织数据，然后触发设备通过 DMA 从 RAM 中读取数据并将其发送到网络中&lt;/li>
&lt;li>发送完成后，设备发出中断，驱动程序解除映射缓冲区、释放内存或清除其状态&lt;/li>
&lt;/ol>
&lt;p>第二阶段通常称为“发送完成”（transmit completion）阶段。我们将对以上两阶段进行研 究，先从第一个开始：发送阶段。
之前已经看到，&lt;code>dev_hard_start_xmit&lt;/code> 通过调用 &lt;code>ndo_start_xmit&lt;/code>（保持一个锁）来发送 数据，所以接下来先看驱动程序是如何注册 &lt;code>ndo_start_xmit&lt;/code> 的，然后再深入理解该函数的 工作原理。
与上篇&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">Linux 网络栈监控和调优：接收数据&lt;/a> 一样，我们将拿 &lt;code>igb&lt;/code> 驱动作为例子。&lt;/p>
&lt;h2 id="91-驱动回调函数注册">9.1 驱动回调函数注册&lt;a class="td-heading-self-link" href="#91-%e9%a9%b1%e5%8a%a8%e5%9b%9e%e8%b0%83%e5%87%bd%e6%95%b0%e6%b3%a8%e5%86%8c" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>驱动程序实现了一系列方法来支持设备操作，例如：&lt;/p>
&lt;ol>
&lt;li>发送数据（&lt;code>ndo_start_xmit&lt;/code>）&lt;/li>
&lt;li>获取统计信息（&lt;code>ndo_get_stats64&lt;/code>）&lt;/li>
&lt;li>处理设备 &lt;code>ioctl&lt;/code>s（&lt;code>ndo_do_ioctl&lt;/code>）&lt;/li>
&lt;/ol>
&lt;p>这些方法通过一个 &lt;code>struct net_device_ops&lt;/code> 实例导出。让我们来看看&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1905-L1928">igb 驱动程序  &lt;/a>中这些操作：&lt;/p>
&lt;pre>&lt;code>static const struct net_device_ops igb_netdev_ops = {
.ndo_open = igb_open,
.ndo_stop = igb_close,
.ndo_start_xmit = igb_xmit_frame,
.ndo_get_stats64 = igb_get_stats64,
/* ... more fields ... */
};
&lt;/code>&lt;/pre>
&lt;p>这个 &lt;code>igb_netdev_ops&lt;/code> 变量在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2090">&lt;code>igb_probe&lt;/code>&lt;/a> 函数中注册给设备：&lt;/p>
&lt;pre>&lt;code>static int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
/* ... lots of other stuff ... */
netdev-&amp;gt;netdev_ops = &amp;amp;igb_netdev_ops;
/* ... more code ... */
}
&lt;/code>&lt;/pre>
&lt;p>正如我们在上一节中看到的，更上层的代码将通过设备的 &lt;code>netdev_ops&lt;/code> 字段 调用适当的回调函数。想了解更多关于 PCI 设备是如何启动的，以及何时/何处调用 &lt;code>igb_probe&lt;/code>，请查看我们之前文章中的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#initialization">驱动程序初始化  &lt;/a>部分。&lt;/p>
&lt;h2 id="92-通过-ndo_start_xmit-发送数据">9.2 通过 &lt;code>ndo_start_xmit&lt;/code> 发送数据&lt;a class="td-heading-self-link" href="#92-%e9%80%9a%e8%bf%87-ndo_start_xmit-%e5%8f%91%e9%80%81%e6%95%b0%e6%8d%ae" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>上层的网络栈通过 &lt;code>struct net_device_ops&lt;/code> 实例里的回调函数，调用驱动程序来执行各种 操作。正如我们之前看到的，qdisc 代码调用 &lt;code>ndo_start_xmit&lt;/code> 将数据传递给驱动程序进行 发送。对于大多数硬件设备，都是在保持一个锁时调用 &lt;code>ndo_start_xmit&lt;/code> 函数。
在 igb 设备驱动程序中，&lt;code>ndo_start_xmit&lt;/code> 字段初始化为 &lt;code>igb_xmit_frame&lt;/code> 函数，所以 我们接下来从 &lt;code>igb_xmit_frame&lt;/code> 开始，查看该驱动程序是如何发送数据的。跟随 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L4664-L4741">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a> ，并记得以下代码在整个执行过程中都 hold 着一个锁：&lt;/p>
&lt;pre>&lt;code>netdev_tx_t igb_xmit_frame_ring(struct sk_buff *skb,
struct igb_ring *tx_ring)
{
struct igb_tx_buffer *first;
int tso;
u32 tx_flags = 0;
u16 count = TXD_USE_COUNT(skb_headlen(skb));
__be16 protocol = vlan_get_protocol(skb);
u8 hdr_len = 0;
/* need: 1 descriptor per page * PAGE_SIZE/IGB_MAX_DATA_PER_TXD,
* + 1 desc for skb_headlen/IGB_MAX_DATA_PER_TXD,
* + 2 desc gap to keep tail from touching head,
* + 1 desc for context descriptor,
* otherwise try next time
*/
if (NETDEV_FRAG_PAGE_MAX_SIZE &amp;gt; IGB_MAX_DATA_PER_TXD) {
unsigned short f;
for (f = 0; f &amp;lt; skb_shinfo(skb)-&amp;gt;nr_frags; f++)
count += TXD_USE_COUNT(skb_shinfo(skb)-&amp;gt;frags[f].size);
} else {
count += skb_shinfo(skb)-&amp;gt;nr_frags;
}
&lt;/code>&lt;/pre>
&lt;p>函数首先使用 &lt;code>TXD_USER_COUNT&lt;/code> 宏来计算发送 skb 所需的描述符数量，用 &lt;code>count&lt;/code> 变量表示。然后根据分片情况，对 &lt;code>count&lt;/code> 进行相应调整。&lt;/p>
&lt;pre>&lt;code>if (igb_maybe_stop_tx(tx_ring, count + 3)) {
/* this is a hard error */
return NETDEV_TX_BUSY;
}
&lt;/code>&lt;/pre>
&lt;p>然后驱动程序调用内部函数 &lt;code>igb_maybe_stop_tx&lt;/code>，检查 TX Queue 以确保有足够可用的描 述符。如果没有，则返回 &lt;code>NETDEV_TX_BUSY&lt;/code>。正如我们之前在 qdisc 代码中看到的那样，这 将导致 qdisc 将 skb 重新入队以便稍后重试。&lt;/p>
&lt;pre>&lt;code>/* record the location of the first descriptor for this packet */
first = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[tx_ring-&amp;gt;next_to_use];
first-&amp;gt;skb = skb;
first-&amp;gt;bytecount = skb-&amp;gt;len;
first-&amp;gt;gso_segs = 1;
&lt;/code>&lt;/pre>
&lt;p>然后，获取 TX Queue 中下一个可用缓冲区信息，用 &lt;code>struct igb_tx_buffer *first&lt;/code> 表 示，这个信息稍后将用于设置缓冲区描述符。数据包 &lt;code>skb&lt;/code> 指针及其大小 &lt;code>skb-&amp;gt;len&lt;/code> 也存储到 &lt;code>first&lt;/code>。&lt;/p>
&lt;pre>&lt;code>skb_tx_timestamp(skb);
&lt;/code>&lt;/pre>
&lt;p>接下来代码调用 &lt;code>skb_tx_timestamp&lt;/code>，获取基于软件的发送时间戳。应用程序可以 使用发送时间戳来确定数据包通过网络栈的发送路径所花费的时间。
某些设备还支持硬件时间戳，这允许系统将打时间戳任务 offload 到设备。程序员因此可以 获得更准确的时间戳，因为它更接近于硬件实际发送的时间。
某些网络设备可以使用&lt;a href="https://events.linuxfoundation.org/sites/events/files/slides/lcjp14_ichikawa_0.pdf">Precision Time Protocol&lt;/a> （PTP，精确时间协议）在硬件中为数据包加时间戳。驱动程序处理用户的硬件时间戳请求。
我们现在看到这个代码：&lt;/p>
&lt;pre>&lt;code>if (unlikely(skb_shinfo(skb)-&amp;gt;tx_flags &amp;amp; SKBTX_HW_TSTAMP)) {
struct igb_adapter *adapter = netdev_priv(tx_ring-&amp;gt;netdev);
if (!(adapter-&amp;gt;ptp_tx_skb)) {
skb_shinfo(skb)-&amp;gt;tx_flags |= SKBTX_IN_PROGRESS;
tx_flags |= IGB_TX_FLAGS_TSTAMP;
adapter-&amp;gt;ptp_tx_skb = skb_get(skb);
adapter-&amp;gt;ptp_tx_start = jiffies;
if (adapter-&amp;gt;hw.mac.type == e1000_82576)
schedule_work(&amp;amp;adapter-&amp;gt;ptp_tx_work);
}
}
&lt;/code>&lt;/pre>
&lt;p>上面的 if 语句检查 &lt;code>SKBTX_HW_TSTAMP&lt;/code> 标志，该标志表示用户请求了硬件时间戳。接下来检 查是否设置了 &lt;code>ptp_tx_skb&lt;/code>。一次只能给一个数据包加时间戳，因此给正在打时间戳的 skb 上设置了 &lt;code>SKBTX_IN_PROGRESS&lt;/code> 标志。然后更新 &lt;code>tx_flags&lt;/code>，将 &lt;code>IGB_TX_FLAGS_TSTAMP&lt;/code> 标志 置位。&lt;code>tx_flags&lt;/code> 变量稍后将被复制到缓冲区信息结构中。
当前的 &lt;code>jiffies&lt;/code> 值赋给 &lt;code>ptp_tx_start&lt;/code>。驱动程序中的其他代码将使用这个值， 以确保 TX 硬件打时间戳不会 hang 住。最后，如果这是一个 82576 以太网硬件网卡，将用 &lt;code>schedule_work&lt;/code> 函数启动&lt;a href="http://www.makelinux.net/ldd3/chp-7-sect-6">工作队列&lt;/a>。&lt;/p>
&lt;pre>&lt;code>if (vlan_tx_tag_present(skb)) {
tx_flags |= IGB_TX_FLAGS_VLAN;
tx_flags |= (vlan_tx_tag_get(skb) &amp;lt;&amp;lt; IGB_TX_FLAGS_VLAN_SHIFT);
}
&lt;/code>&lt;/pre>
&lt;p>上面的代码将检查 skb 的 &lt;code>vlan_tci&lt;/code> 字段是否设置了，如果是，将设置 &lt;code>IGB_TX_FLAGS_VLAN&lt;/code> 标记，并保存 VLAN ID。&lt;/p>
&lt;pre>&lt;code>/* record initial flags and protocol */
first-&amp;gt;tx_flags = tx_flags;
first-&amp;gt;protocol = protocol;
&lt;/code>&lt;/pre>
&lt;p>最后将 &lt;code>tx_flags&lt;/code> 和 &lt;code>protocol&lt;/code> 值都保存到 &lt;code>first&lt;/code> 变量里面。&lt;/p>
&lt;pre>&lt;code>tso = igb_tso(tx_ring, first, &amp;amp;hdr_len);
if (tso &amp;lt; 0)
goto out_drop;
else if (!tso)
igb_tx_csum(tx_ring, first);
&lt;/code>&lt;/pre>
&lt;p>接下来，驱动程序调用其内部函数 &lt;code>igb_tso&lt;/code>，判断 skb 是否需要分片。如果需要 ，缓冲区信息变量（&lt;code>first&lt;/code>）将更新标志位，以提示硬件需要做 TSO。
如果不需要 TSO，则 &lt;code>igb_tso&lt;/code> 返回 0；否则返回 1。 如果返回 0，则将调用 &lt;code>igb_tx_csum&lt;/code> 来 处理校验和 offload 信息（是否需要 offload，是否支持此协议的 offload）。 &lt;code>igb_tx_csum&lt;/code> 函数将检查 skb 的属性，修改 &lt;code>first&lt;/code> 变量中的一些标志位，以表示需要校验 和 offload。&lt;/p>
&lt;pre>&lt;code>igb_tx_map(tx_ring, first, hdr_len);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>igb_tx_map&lt;/code> 函数准备给设备发送的数据。我们后面会仔细查看这个函数。&lt;/p>
&lt;pre>&lt;code>/* Make sure there is space in the ring for the next send. */
igb_maybe_stop_tx(tx_ring, DESC_NEEDED);
return NETDEV_TX_OK;
&lt;/code>&lt;/pre>
&lt;p>发送结束之后，驱动要检查确保有足够的描述符用于下一次发送。如果不够，TX Queue 将被 关闭。最后返回 &lt;code>NETDEV_TX_OK&lt;/code> 给上层（qdisc 代码）。&lt;/p>
&lt;pre>&lt;code>out_drop:
igb_unmap_and_free_tx_resource(tx_ring, first);
return NETDEV_TX_OK;
}
&lt;/code>&lt;/pre>
&lt;p>最后是一些错误处理代码，只有当 &lt;code>igb_tso&lt;/code> 遇到某种错误时才会触发此代码。 &lt;code>igb_unmap_and_free_tx_resource&lt;/code> 用于清理数据。在这种情况下也返回 &lt;code>NETDEV_TX_OK&lt;/code> 。发送没有成功，但驱动程序释放了相关资源，没有什么需要做的了。请注意，在这种情 况下，此驱动程序不会增加 drop 计数，但或许它应该增加。&lt;/p>
&lt;h2 id="93-igb_tx_map">9.3 &lt;code>igb_tx_map&lt;/code>&lt;a class="td-heading-self-link" href="#93-igb_tx_map" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>igb_tx_map&lt;/code> 函数处理将 skb 数据映射到 RAM 的 DMA 区域的细节。它还会更新设备 TX Queue 的 尾部指针，从而触发设备“被唤醒”，从 RAM 获取数据并开始发送。
让我们简单地看一下这个&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L4501-L4627">函数  &lt;/a>的工作原理：&lt;/p>
&lt;pre>&lt;code>static void igb_tx_map(struct igb_ring *tx_ring,
struct igb_tx_buffer *first,
const u8 hdr_len)
{
struct sk_buff *skb = first-&amp;gt;skb;
/* ... other variables ... */
u32 tx_flags = first-&amp;gt;tx_flags;
u32 cmd_type = igb_tx_cmd_type(skb, tx_flags);
u16 i = tx_ring-&amp;gt;next_to_use;
tx_desc = IGB_TX_DESC(tx_ring, i);
igb_tx_olinfo_status(tx_ring, tx_desc, tx_flags, skb-&amp;gt;len - hdr_len);
size = skb_headlen(skb);
data_len = skb-&amp;gt;data_len;
dma = dma_map_single(tx_ring-&amp;gt;dev, skb-&amp;gt;data, size, DMA_TO_DEVICE);
&lt;/code>&lt;/pre>
&lt;p>上面的代码所做的一些事情：&lt;/p>
&lt;ol>
&lt;li>声明变量并初始化&lt;/li>
&lt;li>使用 &lt;code>IGB_TX_DESC&lt;/code> 获取下一个可用描述符的指针&lt;/li>
&lt;li>&lt;code>igb_tx_olinfo_status&lt;/code> 函数更新 &lt;code>tx_flags&lt;/code>，并将它们复制到描述符（&lt;code>tx_desc&lt;/code>）中&lt;/li>
&lt;li>计算 skb 头长度和数据长度&lt;/li>
&lt;li>调用 &lt;code>dma_map_single&lt;/code> 为 &lt;code>skb-&amp;gt;data&lt;/code> 构造内存映射，以允许设备通过 DMA 从 RAM 中读取数据&lt;/li>
&lt;/ol>
&lt;p>接下来是驱动程序中的一个&lt;strong>非常长的循环，用于为 skb 的每个分片生成有效映射&lt;/strong>。具体如何 做的细节并不是特别重要，但如下步骤值得一提：&lt;/p>
&lt;ul>
&lt;li>驱动程序遍历该数据包的所有分片&lt;/li>
&lt;li>当前描述符有其数据的 DMA 地址信息&lt;/li>
&lt;li>如果分片的大小大于单个 IGB 描述符可以发送的大小，则构造多个描述符指向可 DMA 区域的块，直到描述符指向整个分片&lt;/li>
&lt;li>更新描述符迭代器&lt;/li>
&lt;li>更新剩余长度&lt;/li>
&lt;li>当没有剩余分片或者已经消耗了整个数据长度时，循环终止&lt;/li>
&lt;/ul>
&lt;p>下面提供循环的代码以供以上描述参考。这里的代码进一步向读者说明，&lt;strong>如果可能的话，避 免分片是一个好主意&lt;/strong>。分片需要大量额外的代码来处理网络栈的每一层，包括驱动层。&lt;/p>
&lt;pre>&lt;code>tx_buffer = first;
for (frag = &amp;amp;skb_shinfo(skb)-&amp;gt;frags[0];; frag++) {
if (dma_mapping_error(tx_ring-&amp;gt;dev, dma))
goto dma_error;
/* record length, and DMA address */
dma_unmap_len_set(tx_buffer, len, size);
dma_unmap_addr_set(tx_buffer, dma, dma);
tx_desc-&amp;gt;read.buffer_addr = cpu_to_le64(dma);
while (unlikely(size &amp;gt; IGB_MAX_DATA_PER_TXD)) {
tx_desc-&amp;gt;read.cmd_type_len =
cpu_to_le32(cmd_type ^ IGB_MAX_DATA_PER_TXD);
i++;
tx_desc++;
if (i == tx_ring-&amp;gt;count) {
tx_desc = IGB_TX_DESC(tx_ring, 0);
i = 0;
}
tx_desc-&amp;gt;read.olinfo_status = 0;
dma += IGB_MAX_DATA_PER_TXD;
size -= IGB_MAX_DATA_PER_TXD;
tx_desc-&amp;gt;read.buffer_addr = cpu_to_le64(dma);
}
if (likely(!data_len))
break;
tx_desc-&amp;gt;read.cmd_type_len = cpu_to_le32(cmd_type ^ size);
i++;
tx_desc++;
if (i == tx_ring-&amp;gt;count) {
tx_desc = IGB_TX_DESC(tx_ring, 0);
i = 0;
}
tx_desc-&amp;gt;read.olinfo_status = 0;
size = skb_frag_size(frag);
data_len -= size;
dma = skb_frag_dma_map(tx_ring-&amp;gt;dev, frag, 0,
size, DMA_TO_DEVICE);
tx_buffer = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[i];
}
&lt;/code>&lt;/pre>
&lt;p>所有需要的描述符都已建好，且 &lt;code>skb&lt;/code> 的所有数据都映射到 DMA 地址后，驱动就会 进入到它的最后一步，触发一次发送：&lt;/p>
&lt;pre>&lt;code>/* write last descriptor with RS and EOP bits */
cmd_type |= size | IGB_TXD_DCMD;
tx_desc-&amp;gt;read.cmd_type_len = cpu_to_le32(cmd_type);
&lt;/code>&lt;/pre>
&lt;p>对最后一个描述符设置 &lt;code>RS&lt;/code> 和 &lt;code>EOP&lt;/code> 位，以提示设备这是最后一个描述符了。&lt;/p>
&lt;pre>&lt;code>netdev_tx_sent_queue(txring_txq(tx_ring), first-&amp;gt;bytecount);
/* set the timestamp */
first-&amp;gt;time_stamp = jiffies;
&lt;/code>&lt;/pre>
&lt;p>调用 &lt;code>netdev_tx_sent_queue&lt;/code> 函数，同时带着将发送的字节数作为参数。这个函数是 byte query limit（字节查询限制）功能的一部分，我们将在稍后详细介绍。当前的 jiffies 存 储到 &lt;code>first&lt;/code> 的时间戳字段。
接下来，有点 tricky：&lt;/p>
&lt;pre>&lt;code>/* Force memory writes to complete before letting h/w know there
* are new descriptors to fetch. (Only applicable for weak-ordered
* memory model archs, such as IA-64).
*
* We also need this memory barrier to make certain all of the
* status bits have been updated before next_to_watch is written.
*/
wmb();
/* set next_to_watch value indicating a packet is present */
first-&amp;gt;next_to_watch = tx_desc;
i++;
if (i == tx_ring-&amp;gt;count)
i = 0;
tx_ring-&amp;gt;next_to_use = i;
writel(i, tx_ring-&amp;gt;tail);
/* we need this if more than one processor can write to our tail
* at a time, it synchronizes IO on IA64/Altix systems
*/
mmiowb();
return;
&lt;/code>&lt;/pre>
&lt;p>上面的代码做了一些重要的事情：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>wmb&lt;/code> 函数强制完成内存写入。这通常称作**“写屏障”**（write barrier） ，是通过 CPU 平台相关的特殊指令完成的。这对某些 CPU 架构非常重要，因为如果触发 设备启动 DMA 时不能确保所有内存写入已经完成，那设备可能从 RAM 中读取不一致 状态的数据。&lt;a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">这篇文章&lt;/a>和&lt;a href="http://www.cs.utexas.edu/~pingali/CS378/2012fa/lectures/consistency.pdf">这个课程&lt;/a>深 入探讨了内存顺序的细节&lt;/li>
&lt;li>设置 &lt;code>next_to_watch&lt;/code> 字段，它将在 completion 阶段后期使用&lt;/li>
&lt;li>更新计数，并且 TX Queue 的 &lt;code>next_to_use&lt;/code> 字段设置为下一个可用的描述符。使用 &lt;code>writel&lt;/code> 函数更新 TX Queue 的尾部。&lt;code>writel&lt;/code> 向&lt;a href="https://en.wikipedia.org/wiki/Memory-mapped_I/O">内存映射 I/O&lt;/a>地址写入一个 &lt;code>long&lt;/code> 型数据 ，这里地址是 &lt;code>tx_ring-&amp;gt;tail&lt;/code>（一个硬件地址），要写入的值是 &lt;code>i&lt;/code>。这次写操作会让 设备知道其他数据已经准备好，可以通过 DMA 从 RAM 中读取并写入网络&lt;/li>
&lt;li>最后，调用 &lt;code>mmiowb&lt;/code> 函数。它执行特定于 CPU 体系结构的指令，对内存映射的 写操作进行排序。它也是一个写屏障，用于内存映射的 I/O 写&lt;/li>
&lt;/ol>
&lt;p>想了解更多关于 &lt;code>wmb&lt;/code>，&lt;code>mmiowb&lt;/code> 以及何时使用它们的信息，可以阅读 Linux 内核中一些包含 内存屏障的优秀&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/memory-barriers.txt">文档&lt;/a> 。
最后，代码包含了一些错误处理。只有 DMA API（将 skb 数据地址映射到 DMA 地址）返回错误 时，才会执行此代码。&lt;/p>
&lt;pre>&lt;code>dma_error:
dev_err(tx_ring-&amp;gt;dev, &amp;quot;TX DMA map failed\n&amp;quot;);
/* clear dma mappings for failed tx_buffer_info map */
for (;;) {
tx_buffer = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[i];
igb_unmap_and_free_tx_resource(tx_ring, tx_buffer);
if (tx_buffer == first)
break;
if (i == 0)
i = tx_ring-&amp;gt;count;
i--;
}
tx_ring-&amp;gt;next_to_use = i;
&lt;/code>&lt;/pre>
&lt;p>在继续跟进“发送完成”（transmit completion）过程之前，让我们来看下之前跳过了的一 个东西：dynamic queue limits（动态队列限制）。&lt;/p>
&lt;h3 id="dynamic-queue-limits-dql">Dynamic Queue Limits (DQL)&lt;a class="td-heading-self-link" href="#dynamic-queue-limits-dql" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>正如在本文中看到的，&lt;strong>数据在逐步接近网络设备的过程中，花费了大量时间在 不同阶段的 Queue 里面&lt;/strong>。队列越大，在队列中所花费的时间就越多。
解决这个问题的一种方式是&lt;strong>背压&lt;/strong>（back pressure）。动态队列限制（DQL）系统是一种 机制，驱动程序可以使用该机制向网络系统（network system）施加反压，以避免设备 无法发送时有过多的数据积压在队列。
要使用 DQL，驱动需要在其发送和完成例程（transmit and completion routines）中调用 几次简单的 API。DQL 内部算法判断何时数据已足够多，达到此阈值后，DQL 将暂时禁用 TX Queue，从而对网络系统产生背压。当足够的数据已发送完后，DQL 再自动重新启用 该队列。
&lt;a href="https://www.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf">这里&lt;/a> 给出了 DQL 的一些性能数据及 DQL 内部算法的说明。
我们刚刚看到的 &lt;code>netdev_tx_sent_queue&lt;/code> 函数就是 DQL API 一部分。当数据排 队到设备进行发送时，将调用此函数。发送完成后，驱动程序调用 &lt;code>netdev_tx_completed_queue&lt;/code>。在内部，这两个函数都将调用 DQL 库（在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/lib/dynamic_queue_limits.c">lib/dynamic_queue_limits.c&lt;/a> 和 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/dynamic_queue_limits.h">include/linux/dynamic_queue_limits.h&lt;/a> ），以判断是否禁用、重新启用 DQL，或保持配置不动。
DQL 在 sysfs 中导出了一些统计信息和调优参数。调整 DQL 不是必需的；算法自己会随着时间 变化调整其参数。尽管如此，为了完整性，我们稍后会看到如何监控和调整 DQL。&lt;/p>
&lt;h2 id="94-发送完成transmit-completions">9.4 发送完成（Transmit completions）&lt;a class="td-heading-self-link" href="#94-%e5%8f%91%e9%80%81%e5%ae%8c%e6%88%90transmit-completions" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>设备发送数据之后会产生一个中断，表示发送已完成。然后，设备驱动程序可以调度一些长 时间运行的工作，例如解除 DMA 映射、释放数据。这是如何工作的取决于不同设备。对于 &lt;code>igb&lt;/code> 驱动程序（及其关联设备），发送完成和数据包接收所触发的 IRQ 是相同的。这意味着 对于 &lt;code>igb&lt;/code> 驱动程序，&lt;code>NET_RX&lt;/code> 既用于处理发送完成，又用于处理数据包接收。
让我重申一遍，以强调这一点的重要性：&lt;strong>你的设备可能会发出与“接收到数据包时触发的中 断”相同的中断来表示“数据包发送已完成”&lt;/strong>。如果是这种情况，则 &lt;code>NET_RX&lt;/code> softirq 会被用于 处理&lt;strong>数据包接收&lt;/strong>和&lt;strong>发送完成&lt;/strong>两种情况。
由于两个操作共享相同的 IRQ，因此只能注册一个 IRQ 处理函数来处理这两种情况。 回忆以下收到网络数据时的流程：&lt;/p>
&lt;ol>
&lt;li>收到网络数据&lt;/li>
&lt;li>网络设备触发 IRQ&lt;/li>
&lt;li>驱动的 IRQ 处理程序执行，清除 IRQ 并运行 softIRQ（如果尚未运行）。这里触发的 softIRQ 是 &lt;code>NET_RX&lt;/code> 类型&lt;/li>
&lt;li>softIRQ 本质上作为单独的内核线程，执行 NAPI 轮询循环&lt;/li>
&lt;li>只要有足够的预算，NAPI 轮询循环就一直接收数据包&lt;/li>
&lt;li>每次处理数据包后，预算都会减少，直到没有更多数据包要处理、预算达到 0 或时间片已过期为止&lt;/li>
&lt;/ol>
&lt;p>在 igb（和 ixgbe）驱动中，上面的步骤 5 在处理接收数据之前会先处理发送完成（TX completion）。请记住，&lt;strong>根据驱动程序的实现，处理发送完成和接收数据的函数可能共享一 份处理预算&lt;/strong>。igb 和 ixgbe 驱动程序分别跟踪发送完成和接收数据包的预算，因此处理发送完 成不一定会消耗完 RX 预算。
也就是说，整个 NAPI 轮询循环在 hard code 时间片内运行。这意味着如果要处理大量的 TX 完成 ，TX 完成可能会比处理接收数据时占用更多的时间片。对于在高负载环境中运行网络硬 件的人来说，这可能是一个重要的考虑因素。
让我们看看 igb 驱动程序在实际是如何实现的。&lt;/p>
&lt;h3 id="941-transmit-completion-irq">9.4.1 Transmit completion IRQ&lt;a class="td-heading-self-link" href="#941-transmit-completion-irq" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>收包过程我们已经在&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">数据接收部分的博客&lt;/a> 中介绍过，这里不再赘述，只给出相应链接。
那么，让我们从头开始：&lt;/p>
&lt;ol>
&lt;li>网络设备&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#bringing-a-network-device-up">启用&lt;/a>（bring up）&lt;/li>
&lt;li>IRQ 处理函数完成&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#register-an-interrupt-handler">注册&lt;/a>&lt;/li>
&lt;li>用户程序将数据发送到 socket。数据穿过网络栈，最后被网络设备从内存中取出并发送&lt;/li>
&lt;li>设备完成数据发送并触发 IRQ 表示发送完成&lt;/li>
&lt;li>驱动程序的 IRQ 处理函数开始&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#interrupt-handler">处理中断&lt;/a>&lt;/li>
&lt;li>IRQ 处理程序调用 &lt;code>napi_schedule&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#napi-and-napischedule">NAPI 代码&lt;/a>触发 &lt;code>NET_RX&lt;/code> 类型 softirq&lt;/li>
&lt;li>&lt;code>NET_RX&lt;/code> 类型 sofitrq 的中断处理函数 &lt;code>net_rx_action&lt;/code>&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#network-data-processing-begins">开始执行&lt;/a>&lt;/li>
&lt;li>&lt;code>net_rx_action&lt;/code> 函数调用驱动程序注册的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#napi-poll-function-and-weight">NAPI 轮询函数&lt;/a>&lt;/li>
&lt;li>NAPI 轮询函数 &lt;code>igb_poll&lt;/code>&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#igbpoll">开始运行&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>poll 函数 &lt;code>igb_poll&lt;/code> 同时处理接收数据包和发送完成（transmit completion）逻辑。让我 们深入研究这个函数的代码，看看发生了什么。&lt;/p>
&lt;h3 id="942-igb_poll">9.4.2 &lt;code>igb_poll&lt;/code>&lt;a class="td-heading-self-link" href="#942-igb_poll" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/**
* igb_poll - NAPI Rx polling callback
* @napi: napi polling structure
* @budget: count of how many packets we should handle
**/
static int igb_poll(struct napi_struct *napi, int budget)
{
struct igb_q_vector *q_vector = container_of(napi,
struct igb_q_vector,
napi);
bool clean_complete = true;
#ifdef CONFIG_IGB_DCA
if (q_vector-&amp;gt;adapter-&amp;gt;flags &amp;amp; IGB_FLAG_DCA_ENABLED)
igb_update_dca(q_vector);
#endif
if (q_vector-&amp;gt;tx.ring)
clean_complete = igb_clean_tx_irq(q_vector);
if (q_vector-&amp;gt;rx.ring)
clean_complete &amp;amp;= igb_clean_rx_irq(q_vector, budget);
/* If all work not completed, return budget and keep polling */
if (!clean_complete)
return budget;
/* If not enough Rx work done, exit the polling mode */
napi_complete(napi);
igb_ring_irq_enable(q_vector);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>函数按顺序执行以下操作：&lt;/p>
&lt;ol>
&lt;li>如果在内核中启用了直接缓存访问（&lt;a href="https://lwn.net/Articles/247493/">DCA&lt;/a>）功能 ，则更新 CPU 缓存（预热，warm up），后续对 RX Ring Buffer 的访问将命中 CPU 缓存。可以在接 收数据博客的 Extras 部分中阅读&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#direct-cache-access-dca">有关 DCA 的更多信息&lt;/a>&lt;/li>
&lt;li>调用 &lt;code>igb_clean_tx_irq&lt;/code> 执行发送完成操作&lt;/li>
&lt;li>调用 &lt;code>igb_clean_rx_irq&lt;/code> 处理收到的数据包&lt;/li>
&lt;li>最后，检查 &lt;code>clean_complete&lt;/code> 变量，判断是否还有更多工作可以完成。如果是，则返 回预算。如果是这种情况，&lt;code>net_rx_action&lt;/code> 会将此 NAPI 实例移动到轮询列表的末尾， 以便稍后再次处理&lt;/li>
&lt;/ol>
&lt;p>要了解 &lt;code>igb_clean_rx_irq&lt;/code> 如何工作的，请阅读上一篇博客文章的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#igbcleanrxirq">这一部分  &lt;/a>。
本文主要关注发送方面，因此我们将继续研究上面的 &lt;code>igb_clean_tx_irq&lt;/code> 如何工作。&lt;/p>
&lt;h3 id="943-igb_clean_tx_irq">9.4.3 &lt;code>igb_clean_tx_irq&lt;/code>&lt;a class="td-heading-self-link" href="#943-igb_clean_tx_irq" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>来看一下这个函数的实现， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L6020-L6189">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>。
这个函数有点长，分成几部分来看：&lt;/p>
&lt;pre>&lt;code>static bool igb_clean_tx_irq(struct igb_q_vector *q_vector)
{
struct igb_adapter *adapter = q_vector-&amp;gt;adapter;
struct igb_ring *tx_ring = q_vector-&amp;gt;tx.ring;
struct igb_tx_buffer *tx_buffer;
union e1000_adv_tx_desc *tx_desc;
unsigned int total_bytes = 0, total_packets = 0;
unsigned int budget = q_vector-&amp;gt;tx.work_limit;
unsigned int i = tx_ring-&amp;gt;next_to_clean;
if (test_bit(__IGB_DOWN, &amp;amp;adapter-&amp;gt;state))
return true;
&lt;/code>&lt;/pre>
&lt;p>该函数首先初始化一些变量，其中比较重要的是预算（变量 &lt;code>budget&lt;/code>） ，初始化为此队列的 &lt;code>tx.work_limit&lt;/code>。在 igb 驱动程序中，&lt;code>tx.work_limit&lt;/code> 初始化为 hard code 值 &lt;code>IGB_DEFAULT_TX_WORK&lt;/code>（128）。
值得注意的是，虽然我们现在看到的 TX 完成代码与 RX 处理在同一个 &lt;code>NET_RX&lt;/code> softirq 中运行 ，但 igb 驱动的 TX 和 RX 函数&lt;strong>不共享处理预算&lt;/strong>。由于整个轮询函数在同一时间片内运行，因此 每次 &lt;code>igb_poll&lt;/code> 运行不会出现 RX 或 TX 饥饿，只要调用 &lt;code>igb_poll&lt;/code>，两者都将被处理。
继续前进，代码检查网络设备是否已关闭。如果是，则返回 &lt;code>true&lt;/code> 并退出 &lt;code>igb_clean_tx_irq&lt;/code>。&lt;/p>
&lt;pre>&lt;code>tx_buffer = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[i];
tx_desc = IGB_TX_DESC(tx_ring, i);
i -= tx_ring-&amp;gt;count;
&lt;/code>&lt;/pre>
&lt;p>接下来：&lt;/p>
&lt;ol>
&lt;li>&lt;code>tx_buffer&lt;/code> 变量初始化为 &lt;code>tx_ring-&amp;gt;next_to_clean&lt;/code>（其本身被初始化为 0）&lt;/li>
&lt;li>&lt;code>tx_desc&lt;/code> 变量初始化为相关描述符的指针&lt;/li>
&lt;li>计数器 &lt;code>i&lt;/code> 减去 TX Queue 的大小。可以调整此值（我们将在调优部分中看到），但初始化为 &lt;code>IGB_DEFAULT_TXD&lt;/code>（256）&lt;/li>
&lt;/ol>
&lt;p>接下来，循环开始。它包含一些有用的注释，用于解释每个步骤中发生的情况：&lt;/p>
&lt;pre>&lt;code>do {
union e1000_adv_tx_desc *eop_desc = tx_buffer-&amp;gt;next_to_watch;
/* if next_to_watch is not set then there is no work pending */
if (!eop_desc)
break;
/* prevent any other reads prior to eop_desc */
read_barrier_depends();
/* if DD is not set pending work has not been completed */
if (!(eop_desc-&amp;gt;wb.status &amp;amp; cpu_to_le32(E1000_TXD_STAT_DD)))
break;
/* clear next_to_watch to prevent false hangs */
tx_buffer-&amp;gt;next_to_watch = NULL;
/* update the statistics for this packet */
total_bytes += tx_buffer-&amp;gt;bytecount;
total_packets += tx_buffer-&amp;gt;gso_segs;
/* free the skb */
dev_kfree_skb_any(tx_buffer-&amp;gt;skb);
/* unmap skb header data */
dma_unmap_single(tx_ring-&amp;gt;dev,
dma_unmap_addr(tx_buffer, dma),
dma_unmap_len(tx_buffer, len),
DMA_TO_DEVICE);
/* clear tx_buffer data */
tx_buffer-&amp;gt;skb = NULL;
dma_unmap_len_set(tx_buffer, len, 0);
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>首先将 &lt;code>eop_desc&lt;/code>（eop = end of packet）设置为发送缓冲区 &lt;code>tx_buffer&lt;/code> 的 &lt;code>next_to_watch&lt;/code>，后者是在我们之前看到的发送代码中设置的&lt;/li>
&lt;li>如果 &lt;code>eop_desc&lt;/code> 为 &lt;code>NULL&lt;/code>，则表示没有待处理的工作&lt;/li>
&lt;li>接下来调用 &lt;code>read_barrier_depends&lt;/code> 函数，该函数执行此 CPU 体系结构相关的指令，通过屏障防止其他任何读操作&lt;/li>
&lt;li>接下来，检查描述符 &lt;code>eop_desc&lt;/code> 的状态位。如果 &lt;code>E1000_TXD_STAT_DD&lt;/code> 未设置，则表示发送尚未完成，因此跳出循环&lt;/li>
&lt;li>清除 &lt;code>tx_buffer-&amp;gt;next_to_watch&lt;/code>。驱动中的 watchdog 定时器将监视此字段以判断发送是否 hang 住。清除此字段将不会触发 watchdog&lt;/li>
&lt;li>统计发送的总字节数和包数，这些计数将被复制到驱动的相应计数中&lt;/li>
&lt;li>释放 skb&lt;/li>
&lt;li>调用 &lt;code>dma_unmap_single&lt;/code> 取消 skb 数据区映射&lt;/li>
&lt;li>&lt;code>tx_buffer-&amp;gt;skb&lt;/code> 设置为 &lt;code>NULL&lt;/code>，解除 &lt;code>tx_buffer&lt;/code> 映射&lt;/li>
&lt;/ol>
&lt;p>接下来，在上面的循环内部开始了另一个循环：&lt;/p>
&lt;pre>&lt;code>/* clear last DMA location and unmap remaining buffers */
while (tx_desc != eop_desc) {
tx_buffer++;
tx_desc++;
i++;
if (unlikely(!i)) {
i -= tx_ring-&amp;gt;count;
tx_buffer = tx_ring-&amp;gt;tx_buffer_info;
tx_desc = IGB_TX_DESC(tx_ring, 0);
}
/* unmap any remaining paged data */
if (dma_unmap_len(tx_buffer, len)) {
dma_unmap_page(tx_ring-&amp;gt;dev,
dma_unmap_addr(tx_buffer, dma),
dma_unmap_len(tx_buffer, len),
DMA_TO_DEVICE);
dma_unmap_len_set(tx_buffer, len, 0);
}
}
&lt;/code>&lt;/pre>
&lt;p>这个内层循环会遍历每个发送描述符，直到 &lt;code>tx_desc&lt;/code> 等于 &lt;code>eop_desc&lt;/code>，并会解除被其他描 述符引用的被 DMA 映射的数据。
外层循环继续：&lt;/p>
&lt;pre>&lt;code>/* move us one more past the eop_desc for start of next pkt */
tx_buffer++;
tx_desc++;
i++;
if (unlikely(!i)) {
i -= tx_ring-&amp;gt;count;
tx_buffer = tx_ring-&amp;gt;tx_buffer_info;
tx_desc = IGB_TX_DESC(tx_ring, 0);
}
/* issue prefetch for next Tx descriptor */
prefetch(tx_desc);
/* update budget accounting */
budget--;
} while (likely(budget));
&lt;/code>&lt;/pre>
&lt;p>外层循环递增迭代器，更新 budget，然后检查是否要进入下一次循环。&lt;/p>
&lt;pre>&lt;code>netdev_tx_completed_queue(txring_txq(tx_ring),
total_packets, total_bytes);
i += tx_ring-&amp;gt;count;
tx_ring-&amp;gt;next_to_clean = i;
u64_stats_update_begin(&amp;amp;tx_ring-&amp;gt;tx_syncp);
tx_ring-&amp;gt;tx_stats.bytes += total_bytes;
tx_ring-&amp;gt;tx_stats.packets += total_packets;
u64_stats_update_end(&amp;amp;tx_ring-&amp;gt;tx_syncp);
q_vector-&amp;gt;tx.total_bytes += total_bytes;
q_vector-&amp;gt;tx.total_packets += total_packets;
&lt;/code>&lt;/pre>
&lt;p>这段代码：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>netdev_tx_completed_queue&lt;/code>，它是上面解释的 DQL API 的一部分。如果处理了足够的发送完成，这可能会重新启用 TX Queue&lt;/li>
&lt;li>更新各处的统计信息，以便用户可以访问它们，我们稍后会看到&lt;/li>
&lt;/ol>
&lt;p>代码继续，首先检查是否设置了 &lt;code>IGB_RING_FLAG_TX_DETECT_HANG&lt;/code> 标志。每次运行定时器 回调函数时，watchdog 定时器都会设置此标志，以强制定期检查 TX Queue。如果该标志被设 置了，则代码将检查 TX Queue 是否 hang 住：&lt;/p>
&lt;pre>&lt;code>if (test_bit(IGB_RING_FLAG_TX_DETECT_HANG, &amp;amp;tx_ring-&amp;gt;flags)) {
struct e1000_hw *hw = &amp;amp;adapter-&amp;gt;hw;
/* Detect a transmit hang in hardware, this serializes the
* check with the clearing of time_stamp and movement of i
*/
clear_bit(IGB_RING_FLAG_TX_DETECT_HANG, &amp;amp;tx_ring-&amp;gt;flags);
if (tx_buffer-&amp;gt;next_to_watch &amp;amp;&amp;amp;
time_after(jiffies, tx_buffer-&amp;gt;time_stamp +
(adapter-&amp;gt;tx_timeout_factor * HZ)) &amp;amp;&amp;amp;
!(rd32(E1000_STATUS) &amp;amp; E1000_STATUS_TXOFF)) {
/* detected Tx unit hang */
dev_err(tx_ring-&amp;gt;dev,
&amp;quot;Detected Tx Unit Hang\n&amp;quot;
&amp;quot; Tx Queue &amp;lt;%d&amp;gt;\n&amp;quot;
&amp;quot; TDH &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot; TDT &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot; next_to_use &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot; next_to_clean &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot;buffer_info[next_to_clean]\n&amp;quot;
&amp;quot; time_stamp &amp;lt;%lx&amp;gt;\n&amp;quot;
&amp;quot; next_to_watch &amp;lt;%p&amp;gt;\n&amp;quot;
&amp;quot; jiffies &amp;lt;%lx&amp;gt;\n&amp;quot;
&amp;quot; desc.status &amp;lt;%x&amp;gt;\n&amp;quot;,
tx_ring-&amp;gt;queue_index,
rd32(E1000_TDH(tx_ring-&amp;gt;reg_idx)),
readl(tx_ring-&amp;gt;tail),
tx_ring-&amp;gt;next_to_use,
tx_ring-&amp;gt;next_to_clean,
tx_buffer-&amp;gt;time_stamp,
tx_buffer-&amp;gt;next_to_watch,
jiffies,
tx_buffer-&amp;gt;next_to_watch-&amp;gt;wb.status);
netif_stop_subqueue(tx_ring-&amp;gt;netdev,
tx_ring-&amp;gt;queue_index);
/* we are about to reset, no point in enabling stuff */
return true;
}
&lt;/code>&lt;/pre>
&lt;p>上面的 if 语句检查：&lt;/p>
&lt;ol>
&lt;li>&lt;code>tx_buffer-&amp;gt;next_to_watch&lt;/code> 已设置，并且&lt;/li>
&lt;li>当前 jiffies 大于 &lt;code>tx_buffer&lt;/code> 发送路径上记录的 &lt;code>time_stamp&lt;/code> 加上超时因子，并且&lt;/li>
&lt;li>设备的发送状态寄存器未设置 &lt;code>E1000_STATUS_TXOFF&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>如果这三个条件都为真，则会打印一个错误，表明已检测到挂起。&lt;code>netif_stop_subqueue&lt;/code> 用于关闭队列，最后函数返回 true。
让我们继续阅读代码，看看如果没有发送挂起检查会发生什么，或者如果有，但没有检测到 挂起：&lt;/p>
&lt;pre>&lt;code>#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)
if (unlikely(total_packets &amp;amp;&amp;amp;
netif_carrier_ok(tx_ring-&amp;gt;netdev) &amp;amp;&amp;amp;
igb_desc_unused(tx_ring) &amp;gt;= TX_WAKE_THRESHOLD)) {
/* Make sure that anybody stopping the queue after this
* sees the new next_to_clean.
*/
smp_mb();
if (__netif_subqueue_stopped(tx_ring-&amp;gt;netdev,
tx_ring-&amp;gt;queue_index) &amp;amp;&amp;amp;
!(test_bit(__IGB_DOWN, &amp;amp;adapter-&amp;gt;state))) {
netif_wake_subqueue(tx_ring-&amp;gt;netdev,
tx_ring-&amp;gt;queue_index);
u64_stats_update_begin(&amp;amp;tx_ring-&amp;gt;tx_syncp);
tx_ring-&amp;gt;tx_stats.restart_queue++;
u64_stats_update_end(&amp;amp;tx_ring-&amp;gt;tx_syncp);
}
}
return !!budget;
&lt;/code>&lt;/pre>
&lt;p>在上面的代码中，如果先前已禁用，则驱动程序将重新启动 TX Queue。 它首先检查：&lt;/p>
&lt;ol>
&lt;li>是否有数据包处理完成（&lt;code>total_packets&lt;/code> 非零）&lt;/li>
&lt;li>调用 &lt;code>netif_carrier_ok&lt;/code>，确保设备没有被关闭&lt;/li>
&lt;li>TX Queue 中未使用的描述符数量大于等于 &lt;code>TX_WAKE_THRESHOLD&lt;/code>（我的 x86_64 系统上此阈值为 42）&lt;/li>
&lt;/ol>
&lt;p>如果满足以上所有条件，则执行&lt;strong>写屏障&lt;/strong>（&lt;code>smp_mb&lt;/code>）。
接下来检查另一组条件。如果：&lt;/p>
&lt;ol>
&lt;li>队列停止了&lt;/li>
&lt;li>设备未关闭&lt;/li>
&lt;/ol>
&lt;p>则调用 &lt;code>netif_wake_subqueue&lt;/code> 唤醒 TX Queue，并向更高层发信号通知它们可能需要将数据 再次入队。&lt;code>restart_queue&lt;/code> 统计计数器递增。我们接下来会看到如何阅读这个值。
最后，返回一个布尔值。如果有任何剩余的未使用预算，则返回 true，否则为 false。在 &lt;code>igb_poll&lt;/code> 中检查此值以确定返回 &lt;code>net_rx_action&lt;/code> 的内容。&lt;/p>
&lt;h3 id="944-igb_poll-返回值">9.4.4 &lt;code>igb_poll&lt;/code> 返回值&lt;a class="td-heading-self-link" href="#944-igb_poll-%e8%bf%94%e5%9b%9e%e5%80%bc" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>igb_poll&lt;/code> 函数通过以下逻辑决定返回什么值给 &lt;code>net_rx_action&lt;/code>：&lt;/p>
&lt;pre>&lt;code>if (q_vector-&amp;gt;tx.ring)
clean_complete = igb_clean_tx_irq(q_vector);
if (q_vector-&amp;gt;rx.ring)
clean_complete &amp;amp;= igb_clean_rx_irq(q_vector, budget);
/* If all work not completed, return budget and keep polling */
if (!clean_complete)
return budget;
&lt;/code>&lt;/pre>
&lt;p>换句话说，如果：&lt;/p>
&lt;ol>
&lt;li>&lt;code>igb_clean_tx_irq&lt;/code> 清除了所有&lt;strong>待发送&lt;/strong>数据包，且未用完其 TX 预算（transmit completion budget），并且&lt;/li>
&lt;li>&lt;code>igb_clean_rx_irq&lt;/code> 清除了所有&lt;strong>接收到的&lt;/strong>数据包，且未用完其 RX 预算（packet processing budget）&lt;/li>
&lt;/ol>
&lt;p>那么，最后将返回整个预算值（包括 igb 在内的大多数驱动程序 hard code 为 64）；否则，如果 RX 或 TX 处理中的任何用完了其 budget（因为还有更多工作要做），则调用 &lt;code>napi_complete&lt;/code> 禁用 NAPI 并返回 0：&lt;/p>
&lt;pre>&lt;code>/* If not enough Rx work done, exit the polling mode */
napi_complete(napi);
igb_ring_irq_enable(q_vector);
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="95-监控网络设备">9.5 监控网络设备&lt;a class="td-heading-self-link" href="#95-%e7%9b%91%e6%8e%a7%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>监控网络设备有多种方式，每种方式提供的监控粒度和复杂度各不相同。我们先从最粗 大粒度开始，然后逐步到最细的粒度。&lt;/p>
&lt;h3 id="951-使用-ethtool--s-命令">9.5.1 使用 &lt;code>ethtool -S&lt;/code> 命令&lt;a class="td-heading-self-link" href="#951-%e4%bd%bf%e7%94%a8-ethtool--s-%e5%91%bd%e4%bb%a4" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Ubuntu 安装 ethtool：&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get install ethtool.
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ethtool -S &amp;lt;NIC&amp;gt;&lt;/code>可以打印设备的收发统计信息（例如，发送错误）：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -S eth0
NIC statistics:
rx_packets: 597028087
tx_packets: 5924278060
rx_bytes: 112643393747
tx_bytes: 990080156714
rx_broadcast: 96
tx_broadcast: 116
rx_multicast: 20294528
....
&lt;/code>&lt;/pre>
&lt;p>监控这个数据不是太容易，因为并无统一的标准规定&lt;code>-S&lt;/code> 应该打印出哪些字段。不同的设备 ，甚至是相同设备的不同版本，都可能打印出名字不同但意思相同的字段。
你首先需要检查里面的“drop”、“buffer”、“miss”、“errors”等字段，然后查看驱动程序的 代码，以确定哪些计数是在软件里更新的（例如，内存不足时更新），哪些是直接来自硬件 寄存器更新的。如果是硬件寄存器值，那你需要查看网卡的 data sheet，确定这个计数真正 表示什么，因为 ethtool 给出的很多字段都是有误导性的（misleading）。&lt;/p>
&lt;h3 id="952-使用-sysfs">9.5.2 使用 &lt;code>sysfs&lt;/code>&lt;a class="td-heading-self-link" href="#952-%e4%bd%bf%e7%94%a8-sysfs" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>sysfs 也提供了很多统计值，但比网卡层的统计更上层一些。
例如，你可以通过 &lt;code>cat &amp;lt;file&amp;gt;&lt;/code>的方式查看 eth0 接收的丢包数。
示例：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/net/eth0/statistics/tx_aborted_errors
2
&lt;/code>&lt;/pre>
&lt;p>每个 counter 对应一个文件，包括 &lt;code>tx_aborted_errors&lt;/code>, &lt;code>tx_carrier_errors&lt;/code>, &lt;code>tx_compressed&lt;/code>, &lt;code>tx_dropped&lt;/code>,等等。
**不幸的是，每个值代表什么是由驱动决定的，因此，什么时候更新它们，在什么条件下更新 ，都是驱动决定的。**例如，你可能已经注意到，对于同一种错误，有的驱动将其视为 drop ，而有的驱动将其视为 miss。
如果这些值对你非常重要，那你必须阅读驱动代码和网卡 data sheet，以确定每个值真正代 表什么。&lt;/p>
&lt;h3 id="953-使用procnetdev">9.5.3 使用&lt;code>/proc/net/dev&lt;/code>&lt;a class="td-heading-self-link" href="#953-%e4%bd%bf%e7%94%a8procnetdev" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>/proc/net/dev&lt;/code> 提供了更高一层的统计，它给系统中的每个网络设备一个统计摘要。&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/dev
Inter-| Receive | Transmit
face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed
eth0: 110346752214 597737500 0 2 0 0 0 20963860 990024805984 6066582604 0 0 0 0 0 0
lo: 428349463836 1579868535 0 0 0 0 0 0 428349463836 1579868535 0 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>这里打印出来的字段是上面 sysfs 里字段的一个子集，可以作为通用 general reference。
上面的建议在这里同样适用，即： 如果这些值对你非常重要，那你必须阅读驱动代码和网卡 data sheet，以确定每个值真正代 表什么。&lt;/p>
&lt;h2 id="96-监控-dql">9.6 监控 DQL&lt;a class="td-heading-self-link" href="#96-%e7%9b%91%e6%8e%a7-dql" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>可以通过&lt;code>/sys/class/net/&amp;lt;NIC&amp;gt;/queues/tx-&amp;lt;QUEUE_ID&amp;gt;/byte_queue_limits/&lt;/code> 监控网络设备的动态队列限制（DQL）信息。&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/net/eth0/queues/tx-0/byte_queue_limits/inflight
350
&lt;/code>&lt;/pre>
&lt;p>文件包括：&lt;/p>
&lt;ol>
&lt;li>&lt;code>hold_time&lt;/code>: Initialized to HZ (a single hertz). If the queue has been full for hold_time, then the maximum size is decreased.&lt;/li>
&lt;li>&lt;code>inflight&lt;/code>: This value is equal to (number of packets queued - number of packets completed). It is the current number of packets being transmit for which a completion has not been processed.&lt;/li>
&lt;li>&lt;code>limit_max&lt;/code>: A hardcoded value, set to DQL_MAX_LIMIT (1879048192 on my x86_64 system).&lt;/li>
&lt;li>&lt;code>limit_min&lt;/code>: A hardcoded value, set to 0.&lt;/li>
&lt;li>&lt;code>limit&lt;/code>: A value between limit_min and limit_max which represents the current maximum number of objects which can be queued.&lt;/li>
&lt;/ol>
&lt;p>在修改这些值之前，强烈建议先阅读&lt;a href="https://www.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf">这些资料  &lt;/a>，以更深入地了解其算法。&lt;/p>
&lt;h2 id="97-调优网络设备">9.7 调优网络设备&lt;a class="td-heading-self-link" href="#97-%e8%b0%83%e4%bc%98%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="971-查询-tx-queue-数量">9.7.1 查询 TX Queue 数量&lt;a class="td-heading-self-link" href="#971-%e6%9f%a5%e8%af%a2-tx-queue-%e6%95%b0%e9%87%8f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果网络及其驱动支持多 TX Queue，那可以用 ethtool 调整 TX queue（也叫 TX channel）的数量。
查看网卡 TX Queue 数量：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Pre-set maximums:
RX: 0
TX: 0
Other: 0
Combined: 8
Current hardware settings:
RX: 0
TX: 0
Other: 0
Combined: 4
&lt;/code>&lt;/pre>
&lt;p>这里显示了（由驱动和硬件）预设的最大值，以及当前值。
注意：不是所有设备驱动都支持这个选项。
如果你的网卡不支持，会遇到以下错误：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Cannot get device channel parameters
: Operation not supported
&lt;/code>&lt;/pre>
&lt;p>这表示设备驱动没有实现 ethtool 的 &lt;code>get_channels&lt;/code> 方法，这可能是由于网卡不支持调整 queue 数量，不支持多 TX Queue，或者驱动版本太旧导致不支持此操作。&lt;/p>
&lt;h3 id="972-调整-tx-queue-数量">9.7.2 调整 TX queue 数量&lt;a class="td-heading-self-link" href="#972-%e8%b0%83%e6%95%b4-tx-queue-%e6%95%b0%e9%87%8f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>ethtool -L&lt;/code> 可以修改 TX Queue 数量。
注意：一些设备及其驱动只支持 combined queue，这种情况下一个 TX queue 和和一个 RX queue 绑定到一起的。前面的例子中我们已经看到了。
例子：设置收发队列数量为 8：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 combined 8
&lt;/code>&lt;/pre>
&lt;p>如果你的设备和驱动支持分别设置 TX queue 和 RX queue 的数量，那你可以分别设置。&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 tx 8
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，调整以上设置会导致网卡先 down 再 up，经过这个网卡的连接会断掉 。如果只是一次性改动，那这可能不是太大问题。&lt;/p>
&lt;h3 id="973-调整-tx-queue-大小">9.7.3 调整 TX queue 大小&lt;a class="td-heading-self-link" href="#973-%e8%b0%83%e6%95%b4-tx-queue-%e5%a4%a7%e5%b0%8f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>一些设备及其驱动支持修改 TX queue 大小，这是如何实现的取决于具体的硬件，但是， ethtool 提供了一个通用的接口可以调整这个大小。由于 DQL 在更高层面处理数据排队的问题 ，因此调整队列大小可能不会产生明显的影响。然而，你可能还是想要将 TX queue 调到最大 ，然后再把剩下的事情交给 DQL：
&lt;code>ethtool -g&lt;/code> 查看队列当前的大小：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX: 4096
RX Mini: 0
RX Jumbo: 0
TX: 4096
Current hardware settings:
RX: 512
RX Mini: 0
RX Jumbo: 0
TX: 512
&lt;/code>&lt;/pre>
&lt;p>以上显示硬件支持最大 4096 个接收和发送描述符，但当前只使用了 512 个。
&lt;code>-G&lt;/code> 修改 queue 大小：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -G eth0 tx 4096
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，调整以上设置会导致网卡先 down 再 up，经过这个网卡的连接会断掉 。如果只是一次性改动，那这可能不是太大问题。&lt;/p>
&lt;h1 id="10-网络栈之旅结束">10 网络栈之旅：结束&lt;a class="td-heading-self-link" href="#10-%e7%bd%91%e7%bb%9c%e6%a0%88%e4%b9%8b%e6%97%85%e7%bb%93%e6%9d%9f" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>至此，你已经知道关于 Linux 如何发送数据包的全部内容了：从用户程序直到驱动，以及反 方向。&lt;/p>
&lt;h1 id="11-extras">11 Extras&lt;a class="td-heading-self-link" href="#11-extras" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="111-减少-arp-流量-msg_confirm">11.1 减少 ARP 流量 (MSG_CONFIRM)&lt;a class="td-heading-self-link" href="#111-%e5%87%8f%e5%b0%91-arp-%e6%b5%81%e9%87%8f-msg_confirm" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>send&lt;/code>, &lt;code>sendto&lt;/code> 和 &lt;code>sendmsg&lt;/code> 系统调用都支持一个 &lt;code>flags&lt;/code> 参数。如果你调用的时候传递了 &lt;code>MSG_CONFIRM&lt;/code> flag，它会使内核里的 &lt;code>dst_neigh_output&lt;/code> 函数更新邻居（ARP）缓存的时 间戳。所导致的结果是，相应的邻居缓存不会被垃圾回收。这会减少发出的 ARP 请求的数量 。&lt;/p>
&lt;h2 id="112-udp-corking软木塞">11.2 UDP Corking（软木塞）&lt;a class="td-heading-self-link" href="#112-udp-corking%e8%bd%af%e6%9c%a8%e5%a1%9e" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>在查看 UDP 协议栈的时候我们深入地研究过了 UDP corking 这个选项。如果你想在应用中使用 这个选项，可以在调用 &lt;code>setsockopt&lt;/code> 设置 IPPROTO_UDP 类型 socket 的时候，将 UDP_CORK 标记 位置 1。&lt;/p>
&lt;h2 id="113-打时间戳">11.3 打时间戳&lt;a class="td-heading-self-link" href="#113-%e6%89%93%e6%97%b6%e9%97%b4%e6%88%b3" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文已经看到，网络栈可以收集发送包的时间戳信息。我们在文章中已经看到了软 件部分哪里可以设置时间戳；而一些网卡甚至还支持硬件时间戳。
如果你想看内核网络栈给收包增加了多少延迟，那这个特性非常有用。
内核&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/timestamping.txt">关于时间戳的文档&lt;/a> 非常优秀，甚至还包括一个&lt;a href="https://github.com/torvalds/linux/tree/v3.13/Documentation/networking/timestamping">示例程序和相应的 Makefile&lt;/a>，有兴趣的话可以上手试试。
使用 &lt;code>ethtool -T&lt;/code> 可以查看网卡和驱动支持哪种打时间戳方式：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -T eth0
Time stamping parameters for eth0:
Capabilities:
software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
PTP Hardware Clock: none
Hardware Transmit Timestamp Modes: none
Hardware Receive Filter Modes: none
&lt;/code>&lt;/pre>
&lt;p>从上面这个信息看，该网卡不支持硬件打时间戳。但这个系统上的软件打时间戳，仍然可以 帮助我判断内核在接收路径上到底带来多少延迟。&lt;/p>
&lt;h1 id="12-结论">12 结论&lt;a class="td-heading-self-link" href="#12-%e7%bb%93%e8%ae%ba" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Linux 网络栈很复杂。
我们已经看到，即使是 &lt;code>NET_RX&lt;/code> 这样看起来极其简单的（名字），也不是按照我们（字面上 ）理解的方式在运行，虽然名字带 RX，但其实发送数据也在 &lt;code>NET_RX&lt;/code> 软中断处理函数中被处 理。
这揭示了我认为的问题的核心：&lt;strong>不深入阅读和理解网络栈，就不可能优化和监控它&lt;/strong>。 &lt;strong>你监控不了你没有深入理解的代码&lt;/strong>。&lt;/p>
&lt;h1 id="13-额外帮助">13 额外帮助&lt;a class="td-heading-self-link" href="#13-%e9%a2%9d%e5%a4%96%e5%b8%ae%e5%8a%a9" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>需要一些额外的关于网络栈的指导(navigating the network stack)？对本文有疑问，或有 相关内容本文没有提到？以上问题，都可以发邮件给&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/support@packagecloud.io">我们&lt;/a>， 以便我们知道如何提供帮助。&lt;/p></description></item><item><title>Docs: 数据包接收过程详解</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E6%8E%A5%E6%94%B6%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Kernel/Network/Linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E6%8E%A5%E6%94%B6%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="译-linux-网络栈监控和调优接收数据2016">[译] Linux 网络栈监控和调优：接收数据（2016）&lt;a class="td-heading-self-link" href="#%e8%af%91-linux-%e7%bd%91%e7%bb%9c%e6%a0%88%e7%9b%91%e6%8e%a7%e5%92%8c%e8%b0%83%e4%bc%98%e6%8e%a5%e6%94%b6%e6%95%b0%e6%8d%ae2016" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Published at 2018-12-05 | Last Update 2020-03-29&lt;/p>
&lt;h2 id="译者序">译者序&lt;a class="td-heading-self-link" href="#%e8%af%91%e8%80%85%e5%ba%8f" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文翻译自 2016 年的一篇英文博客 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data&lt;/a>。&lt;strong>如果能看懂英文，建议阅读原文，或者和本文对照看。&lt;/strong>
这篇文章写的是 &lt;strong>“Linux networking stack”&lt;/strong>，这里的 ”stack“ 指的不仅仅是内核协议 栈，而是包括内核协议栈在内的、从数据包到达物理网卡到最终被用户态程序收起的整个路 径。所以文章有三方面，交织在一起，看起来非常累（但是很过瘾）：&lt;/p>
&lt;ol>
&lt;li>原理及代码实现：网络各层，包括驱动、硬中断、软中断、内核协议栈、socket 等等&lt;/li>
&lt;li>监控：对代码中的重要计数进行监控，一般在 &lt;code>/proc&lt;/code> 或 &lt;code>/sys&lt;/code> 下面有对应输出&lt;/li>
&lt;li>调优：修改网络配置参数&lt;/li>
&lt;/ol>
&lt;p>本文的另一个特色是，几乎所有讨论的内核代码，都在相应的地方给出了 github 上的链接 ，具体到行。
网络栈非常复杂，原文太长又没有任何章节号，看起来非常累。因此本文翻译时添加了适当 的章节号，以期按图索骥。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>2020 更新：&lt;/strong>
基于 Prometheus+Grafana 监控网络栈：&lt;a href="http://arthurchiao.art/blog/monitoring-network-stack/">Monitoring Network Stack&lt;/a>。
以下是翻译。&lt;/p>
&lt;hr>
&lt;h2 id="太长不读tl-dr">太长不读（TL; DR）&lt;a class="td-heading-self-link" href="#%e5%a4%aa%e9%95%bf%e4%b8%8d%e8%af%bbtl-dr" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文介绍了 Linux 内核是如何&lt;strong>收包&lt;/strong>（receive packets）的，包是怎样从网络栈到达用 户空间程序的，以及如何&lt;strong>监控&lt;/strong>（monitoring）和&lt;strong>调优&lt;/strong>（tuning）这一路径上的各个 网络栈组件。
这篇文章的姊妹篇 &lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">Monitoring and Tuning the Linux Networking Stack: Sending Data&lt;/a>。
这篇文章的图文注释版 &lt;a href="https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/">the Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data&lt;/a>。
想对 Linux 网络栈进行监控或调优，必须对它的行为（what exactly is happening）和原 理有深入的理解，而这是离不开读内核源码的。希望本文可以给那些正准备投身于此的人提 供一份参考。
&lt;strong>特别鸣谢&lt;/strong>
特别感谢 &lt;a href="https://privateinternetaccess.com/">Private Internet Access&lt;/a> 的各位同 僚。公司雇佣我们做一些包括本文主题在内的网络研究，并非常慷慨地允许我们将研究成果 以文章的形式发表。
本文基于在 &lt;a href="https://privateinternetaccess.com/">Private Internet Access&lt;/a> 时的研 究成果，最开始以 &lt;a href="https://www.privateinternetaccess.com/blog/2016/01/linux-networking-stack-from-the-ground-up-part-1/">5 篇连载  &lt;/a>的形式出现。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%AF%91%E8%80%85%E5%BA%8F">译者序&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E5%A4%AA%E9%95%BF%E4%B8%8D%E8%AF%BBtl-dr">太长不读（TL; DR）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%9B%91%E6%8E%A7%E5%92%8C%E8%B0%83%E4%BC%98%E5%B8%B8%E8%A7%84%E5%BB%BA%E8%AE%AE">1 网络栈监控和调优：常规建议&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#2-%E6%94%B6%E5%8C%85%E8%BF%87%E7%A8%8B%E4%BF%AF%E7%9E%B0">2 收包过程俯瞰&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#3-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8">3 网络设备驱动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#31-%E5%88%9D%E5%A7%8B%E5%8C%96">3.1 初始化&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#pci-%E5%88%9D%E5%A7%8B%E5%8C%96">PCI 初始化&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#32-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%88%9D%E5%A7%8B%E5%8C%96">3.2 网络设备初始化&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E6%9B%B4%E5%A4%9A-pci-%E9%A9%B1%E5%8A%A8%E4%BF%A1%E6%81%AF">更多 PCI 驱动信息&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#33-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%90%AF%E5%8A%A8">3.3 网络设备启动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#331-struct-net_device_ops">3.3.1 &lt;code>struct net_device_ops&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#332-ethtool-%E5%87%BD%E6%95%B0%E6%B3%A8%E5%86%8C">3.3.2 &lt;code>ethtool&lt;/code> 函数注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#333-%E8%BD%AF%E4%B8%AD%E6%96%ADirq">3.3.3 软中断（IRQ）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#334-napi">3.3.4 NAPI&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#napi">NAPI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#335-igb-%E9%A9%B1%E5%8A%A8%E7%9A%84-napi-%E5%88%9D%E5%A7%8B%E5%8C%96">3.3.5 &lt;code>igb&lt;/code> 驱动的 NAPI 初始化&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#34-%E5%90%AF%E7%94%A8%E7%BD%91%E5%8D%A1-bring-a-network-device-up">3.4 启用网卡 (Bring A Network Device Up)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#341-%E5%87%86%E5%A4%87%E4%BB%8E%E7%BD%91%E7%BB%9C%E6%8E%A5%E6%94%B6%E6%95%B0%E6%8D%AE">3.4.1 准备从网络接收数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#342-enable-napi">3.4.2 Enable NAPI&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#343-%E6%B3%A8%E5%86%8C%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0">3.4.3 注册中断处理函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#344-enable-interrupts">3.4.4 Enable Interrupts&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#35-%E7%BD%91%E5%8D%A1%E7%9B%91%E6%8E%A7">3.5 网卡监控&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#351-using-ethtool--s">3.5.1 Using &lt;code>ethtool -S&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#352-using-sysfs">3.5.2 Using &lt;code>sysfs&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#353-using-procnetdev">3.5.3 Using &lt;code>/proc/net/dev&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#36-%E7%BD%91%E5%8D%A1%E8%B0%83%E4%BC%98">3.6 网卡调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#361-%E6%9F%A5%E7%9C%8B-rx-%E9%98%9F%E5%88%97%E6%95%B0%E9%87%8F">3.6.1 查看 RX 队列数量&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#362-%E8%B0%83%E6%95%B4-rx-queues">3.6.2 调整 RX queues&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#363-%E8%B0%83%E6%95%B4-rx-queue-%E7%9A%84%E5%A4%A7%E5%B0%8F">3.6.3 调整 RX queue 的大小&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#364-%E8%B0%83%E6%95%B4-rx-queue-%E7%9A%84%E6%9D%83%E9%87%8Dweight">3.6.4 调整 RX queue 的权重（weight）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#365-%E8%B0%83%E6%95%B4-rx-%E5%93%88%E5%B8%8C%E5%AD%97%E6%AE%B5-for-network-flows">3.6.5 调整 RX 哈希字段 for network flows&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#366-ntuple-filtering-for-steering-network-flows">3.6.6 ntuple filtering for steering network flows&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#4-%E8%BD%AF%E4%B8%AD%E6%96%ADsoftirq">4 软中断（SoftIRQ）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#41-%E8%BD%AF%E4%B8%AD%E6%96%AD%E6%98%AF%E4%BB%80%E4%B9%88">4.1 软中断是什么&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#42-ksoftirqd">4.2 &lt;code>ksoftirqd&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#43-__do_softirq">4.3 &lt;code>__do_softirq&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#44-%E7%9B%91%E6%8E%A7">4.4 监控&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#5-linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%AD%90%E7%B3%BB%E7%BB%9F">5 Linux 网络设备子系统&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#51-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%AD%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96">5.1 网络设备子系统的初始化&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#struct-softnet_data-%E5%8F%98%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96">&lt;code>struct softnet_data&lt;/code> 变量初始化&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#softirq-handler-%E5%88%9D%E5%A7%8B%E5%8C%96">SoftIRQ Handler 初始化&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#52-%E6%95%B0%E6%8D%AE%E6%9D%A5%E4%BA%86">5.2 数据来了&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#521-%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0">5.2.1 中断处理函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#522-napi-%E5%92%8C-napi_schedule">5.2.2 NAPI 和 &lt;code>napi_schedule&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#523-%E5%85%B3%E4%BA%8E-cpu-%E5%92%8C%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E4%B8%80%E7%82%B9%E7%AC%94%E8%AE%B0">5.2.3 关于 CPU 和网络数据处理的一点笔记&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#524-%E7%9B%91%E6%8E%A7%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%88%B0%E8%BE%BE">5.2.4 监控网络数据到达&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%A1%AC%E4%B8%AD%E6%96%AD%E8%AF%B7%E6%B1%82">硬中断请求&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#525-%E6%95%B0%E6%8D%AE%E6%8E%A5%E6%94%B6%E8%B0%83%E4%BC%98">5.2.5 数据接收调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E4%B8%AD%E6%96%AD%E5%90%88%E5%B9%B6interrupt-coalescing">中断合并（Interrupt coalescing）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E6%95%B4%E7%A1%AC%E4%B8%AD%E6%96%AD%E4%BA%B2%E5%92%8C%E6%80%A7irq-affinities">调整硬中断亲和性（IRQ affinities）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#53-%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%BC%80%E5%A7%8B">5.3 网络数据处理：开始&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#531-net_rx_action-%E5%A4%84%E7%90%86%E5%BE%AA%E7%8E%AF">5.3.1 &lt;code>net_rx_action&lt;/code> 处理循环&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#532-napi-poll-%E5%87%BD%E6%95%B0%E5%8F%8A%E6%9D%83%E9%87%8D">5.3.2 NAPI poll 函数及权重&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#533-napi-%E5%92%8C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%90%88%E7%BA%A6contract">5.3.3 NAPI 和设备驱动的合约（contract）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#534-finishing-the-net_rx_action-loop">5.3.4 Finishing the &lt;code>net_rx_action&lt;/code> loop&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#535-%E5%88%B0%E8%BE%BE-limit-%E6%97%B6%E9%80%80%E5%87%BA%E5%BE%AA%E7%8E%AF">5.3.5 到达 limit 时退出循环&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#536-napi-poll">5.3.6 NAPI &lt;code>poll&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#igb_poll">&lt;code>igb_poll&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#igb_clean_rx_irq">&lt;code>igb_clean_rx_irq&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#537-%E7%9B%91%E6%8E%A7%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">5.3.7 监控网络数据处理&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#procnetsoftnet_stat">&lt;code>/proc/net/softnet_stat&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#538-%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E8%B0%83%E4%BC%98">5.3.8 网络数据处理调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E6%95%B4-net_rx_action-budget">调整 &lt;code>net_rx_action&lt;/code> budget&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#54-grogeneric-receive-offloading">5.4 GRO（Generic Receive Offloading）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E4%BD%BF%E7%94%A8-ethtool-%E4%BF%AE%E6%94%B9-gro-%E9%85%8D%E7%BD%AE">使用 ethtool 修改 GRO 配置&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#55-napi_gro_receive">5.5 &lt;code>napi_gro_receive&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#dev_gro_receive">&lt;code>dev_gro_receive&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#56-napi_skb_finish">5.6 &lt;code>napi_skb_finish&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#6-rps-receive-packet-steering">6 RPS (Receive Packet Steering)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#rps-%E8%B0%83%E4%BC%98">RPS 调优&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#7-rfs-receive-flow-steering">7 RFS (Receive Flow Steering)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98%E6%89%93%E5%BC%80-rfs">调优：打开 RFS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#8-arfs-hardware-accelerated-rfs">8 aRFS (Hardware accelerated RFS)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98-%E5%90%AF%E7%94%A8-arfs">调优: 启用 aRFS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#9-%E4%BB%8E-netif_receive_skb-%E8%BF%9B%E5%85%A5%E5%8D%8F%E8%AE%AE%E6%A0%88">9 从 &lt;code>netif_receive_skb&lt;/code> 进入协议栈&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#91-%E8%B0%83%E4%BC%98-%E6%94%B6%E5%8C%85%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3rx-packet-timestamping">9.1 调优: 收包打时间戳（RX packet timestamping）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#10-netif_receive_skb">10 &lt;code>netif_receive_skb&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#101-%E4%B8%8D%E4%BD%BF%E7%94%A8-rps%E9%BB%98%E8%AE%A4">10.1 不使用 RPS（默认）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#102-%E4%BD%BF%E7%94%A8-rps">10.2 使用 RPS&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#103-enqueue_to_backlog">10.3 &lt;code>enqueue_to_backlog&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#flow-limits">Flow limits&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%91%E6%8E%A7%E7%94%B1%E4%BA%8E-input_pkt_queue-%E6%89%93%E6%BB%A1%E6%88%96-flow-limit-%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%A2%E5%8C%85">监控：由于 &lt;code>input_pkt_queue&lt;/code> 打满或 flow limit 导致的丢包&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98">调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-adjusting-netdev_max_backlog-to-prevent-drops">Tuning: Adjusting netdev_max_backlog to prevent drops&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-adjust-the-napi-weight-of-the-backlog-poll-loop">Tuning: Adjust the NAPI weight of the backlog poll loop&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-enabling-flow-limits-and-tuning-flow-limit-hash-table-size">Tuning: Enabling flow limits and tuning flow limit hash table size&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#104-%E5%A4%84%E7%90%86-backlog-%E9%98%9F%E5%88%97napi-poller">10.4 处理 backlog 队列：NAPI poller&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#105-process_backlog">10.5 &lt;code>process_backlog&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#106-__netif_receive_skb_core%E5%B0%86%E6%95%B0%E6%8D%AE%E9%80%81%E5%88%B0%E6%8A%93%E5%8C%85%E7%82%B9tap%E6%88%96%E5%8D%8F%E8%AE%AE%E5%B1%82">10.6 &lt;code>__netif_receive_skb_core&lt;/code>：将数据送到抓包点（tap）或协议层&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#107-%E9%80%81%E5%88%B0%E6%8A%93%E5%8C%85%E7%82%B9tap">10.7 送到抓包点（tap）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#108-%E9%80%81%E5%88%B0%E5%8D%8F%E8%AE%AE%E5%B1%82">10.8 送到协议层&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#11-%E5%8D%8F%E8%AE%AE%E5%B1%82%E6%B3%A8%E5%86%8C">11 协议层注册&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#111-ip-%E5%8D%8F%E8%AE%AE%E5%B1%82">11.1 IP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1111-ip_rcv">11.1.1 &lt;code>ip_rcv&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#netfilter-and-iptables">netfilter and iptables&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1112-ip_rcv_finish">11.1.2 &lt;code>ip_rcv_finish&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98-%E6%89%93%E5%BC%80%E6%88%96%E5%85%B3%E9%97%AD-ip-%E5%8D%8F%E8%AE%AE%E7%9A%84-early-demux-%E9%80%89%E9%A1%B9">调优: 打开或关闭 IP 协议的 early demux 选项&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1113-ip_local_deliver">11.1.3 &lt;code>ip_local_deliver&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1114-ip_local_deliver_finish">11.1.4 &lt;code>ip_local_deliver_finish&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#monitoring-ip-protocol-layer-statistics">Monitoring: IP protocol layer statistics&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#112-%E9%AB%98%E5%B1%82%E5%8D%8F%E8%AE%AE%E6%B3%A8%E5%86%8C">11.2 高层协议注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#113-udp-%E5%8D%8F%E8%AE%AE%E5%B1%82">11.3 UDP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1131-udp_rcv">11.3.1 &lt;code>udp_rcv&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1132-__udp4_lib_rcv">11.3.2 &lt;code>__udp4_lib_rcv&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1133-udp_queue_rcv_skb">11.3.3 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1334-sk_rcvqueues_full">13.3.4 &lt;code>sk_rcvqueues_full&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98-socket-receive-queue-memory">调优: Socket receive queue memory&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1135-udp_queue_rcv_skb">11.3.5 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1137-__udp_queue_rcv_skb">11.3.7 &lt;code>__udp_queue_rcv_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1138-monitoring-udp-protocol-layer-statistics">11.3.8 Monitoring: UDP protocol layer statistics&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%91%E6%8E%A7-udp-%E5%8D%8F%E8%AE%AE%E7%BB%9F%E8%AE%A1procnetsnmp">监控 UDP 协议统计：&lt;code>/proc/net/snmp&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%91%E6%8E%A7-udp-socket-%E7%BB%9F%E8%AE%A1procnetudp">监控 UDP socket 统计：&lt;code>/proc/net/udp&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#114-%E5%B0%86%E6%95%B0%E6%8D%AE%E6%94%BE%E5%88%B0-socket-%E9%98%9F%E5%88%97">11.4 将数据放到 socket 队列&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#12-%E5%85%B6%E4%BB%96">12 其他&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#121-%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3-timestamping">12.1 打时间戳 (timestamping)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#122-socket-%E4%BD%8E%E5%BB%B6%E8%BF%9F%E9%80%89%E9%A1%B9busy-polling">12.2 socket 低延迟选项：busy polling&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#123-netpoll%E7%89%B9%E6%AE%8A%E7%BD%91%E7%BB%9C%E5%9C%BA%E6%99%AF%E6%94%AF%E6%8C%81">12.3 Netpoll：特殊网络场景支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#124-so_incoming_cpu">12.4 &lt;code>SO_INCOMING_CPU&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#125-dma-%E5%BC%95%E6%93%8E">12.5 DMA 引擎&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#intels-io-acceleration-technology-ioat">Intel’s I/O Acceleration Technology (IOAT)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%B4%E6%8E%A5%E7%BC%93%E5%AD%98%E8%AE%BF%E9%97%AE-dca-direct-cache-access">直接缓存访问 (DCA, Direct cache access)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#monitoring-ioat-dma-engine">Monitoring IOAT DMA engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-ioat-dma-engine">Tuning IOAT DMA engine&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#13-%E6%80%BB%E7%BB%93">13 总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#14-%E9%A2%9D%E5%A4%96%E8%AE%A8%E8%AE%BA%E5%92%8C%E5%B8%AE%E5%8A%A9">14 额外讨论和帮助&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#15-%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0">15 相关文章&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="1-网络栈监控和调优常规建议">1 网络栈监控和调优：常规建议&lt;a class="td-heading-self-link" href="#1-%e7%bd%91%e7%bb%9c%e6%a0%88%e7%9b%91%e6%8e%a7%e5%92%8c%e8%b0%83%e4%bc%98%e5%b8%b8%e8%a7%84%e5%bb%ba%e8%ae%ae" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>网络栈很复杂，没有一种适用于所有场景的通用方式。如果网络的性能和健康状况对你来说 非常重要，那你别无选择，只能投入大量的时间、精力和资源去深入理解系统的各个部分是 如何工作的。
理想情况下，你应该考虑在网络栈的各个层级测量丢包状况，这样就可以缩小范围，确定哪 个组件需要调优。
&lt;strong>然而，这也是一些网络管理员开始走偏的地方&lt;/strong>：他们想当然地认为通过一波 &lt;code>sysctl&lt;/code> 或 &lt;code>/proc&lt;/code> 操作就可以解决问题，并且认为这些配置适用于所有场景。在某些场景下，可能确实 如此；但是，考虑到整个系统是如此细微而精巧地交织在一起的，如果想做有意义的监控和调优 ，就必须得在更深层次搞清系统是如何工作的。否则，你虽然可以使用默认配置，并在 相当长的时间内运行良好，但终会到某个时间点，你不得不（投时间、精力和资源研究这些 配置，然后）做优化。
本文中的一些示例配置仅为了方便理解（效果），并不作为任何特定配置或默认配置的建议 。在做任何配置改动之前，你应该有一个能够对系统进行监控的框架，以查看变更是否带来 预期的效果。
对远程连接上的机器进行网络变更是相当危险的，机器很可能失联。另外，不要在生产环境 直接调整这些配置；如果可能的话，在新机器上改配置，然后将机器灰度上线到生产。&lt;/p>
&lt;h1 id="2-收包过程俯瞰">2 收包过程俯瞰&lt;a class="td-heading-self-link" href="#2-%e6%94%b6%e5%8c%85%e8%bf%87%e7%a8%8b%e4%bf%af%e7%9e%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>本文将拿 &lt;strong>Intel I350&lt;/strong> 网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下 载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从被网卡接收到进入 socket 接收队列的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>加载网卡驱动，初始化&lt;/li>
&lt;li>包从外部网络进入网卡&lt;/li>
&lt;li>网卡（通过 DMA）将包 copy 到内核内存中的 ring buffer&lt;/li>
&lt;li>产生硬件中断，通知系统收到了一个包&lt;/li>
&lt;li>驱动调用 NAPI，如果轮询（poll）还没开始，就开始轮询&lt;/li>
&lt;li>&lt;code>ksoftirqd&lt;/code> 进程调用 NAPI 的 &lt;code>poll&lt;/code> 函数从 ring buffer 收包（&lt;code>poll&lt;/code> 函数是网卡 驱动在初始化阶段注册的；每个 CPU 上都运行着一个 &lt;code>ksoftirqd&lt;/code> 进程，在系统启动期 间就注册了）&lt;/li>
&lt;li>ring buffer 里包对应的内存区域解除映射（unmapped）&lt;/li>
&lt;li>（通过 DMA 进入）内存的数据包以 &lt;code>skb&lt;/code> 的形式被送至更上层处理&lt;/li>
&lt;li>如果 packet steering 功能打开，或者网卡有多队列，网卡收到的包会被分发到多个 CPU&lt;/li>
&lt;li>包从队列进入协议层&lt;/li>
&lt;li>协议层处理包&lt;/li>
&lt;li>包从协议层进入相应 socket 的接收队列&lt;/li>
&lt;/ol>
&lt;p>接下来会详细介绍这个过程。
协议层分析我们将会关注 IP 和 UDP 层，其他协议层可参考这个过程。&lt;/p>
&lt;h1 id="3-网络设备驱动">3 网络设备驱动&lt;a class="td-heading-self-link" href="#3-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e9%a9%b1%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>本文基于 Linux 3.13。
准确地理解 Linux 内核的收包过程是一件非常有挑战性的事情。我们需要仔细研究网卡驱 动的工作原理，才能对网络栈的相应部分有更加清晰的理解。
本文将拿 &lt;code>ibg&lt;/code> 驱动作为例子，它是常见的 Intel I350 网卡的驱动。先来看网卡 驱动是如何工作的。&lt;/p>
&lt;h2 id="31-初始化">3.1 初始化&lt;a class="td-heading-self-link" href="#31-%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>驱动会使用 &lt;code>module_init&lt;/code> 向内核注册一个初始化函数，当驱动被加载时，内核会调用这个函数。
这个初始化函数（&lt;code>igb_init_module&lt;/code>）的代码见 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L676-L697">&lt;code>drivers/net/ethernet/intel/igb/igb_main.c&lt;/code>&lt;/a>.
过程非常简单直接：&lt;/p>
&lt;pre>&lt;code>/**
* igb_init_module - Driver Registration Routine
*
* igb_init_module is the first routine called when the driver is
* loaded. All it does is register with the PCI subsystem.
**/
static int __init igb_init_module(void)
{
int ret;
pr_info(&amp;quot;%s - version %s\n&amp;quot;, igb_driver_string, igb_driver_version);
pr_info(&amp;quot;%s\n&amp;quot;, igb_copyright);
/* ... */
ret = pci_register_driver(&amp;amp;igb_driver);
return ret;
}
module_init(igb_init_module);
&lt;/code>&lt;/pre>
&lt;p>初始化的大部分工作在 &lt;code>pci_register_driver&lt;/code> 里面完成，下面来细看。&lt;/p>
&lt;h3 id="pci-初始化">PCI 初始化&lt;a class="td-heading-self-link" href="#pci-%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Intel I350 网卡是 &lt;a href="https://en.wikipedia.org/wiki/PCI_Express">PCI express&lt;/a> 设备。 PCI 设备通过 &lt;a href="https://en.wikipedia.org/wiki/PCI_configuration_space#Standardized_registers">PCI Configuration Space&lt;/a> 里面的寄存器识别自己。
当设备驱动编译时，&lt;code>MODULE_DEVICE_TABLE&lt;/code> 宏（定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/module.h#L145-L146">&lt;code>include/module.h&lt;/code>&lt;/a>） 会导出一个 &lt;strong>PCI 设备 ID 列表&lt;/strong>（a table of PCI device IDs），驱动据此识别它可以 控制的设备，内核也会依据这个列表对不同设备加载相应驱动。
&lt;code>igb&lt;/code> 驱动的设备表和 PCI 设备 ID 分别见： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L79-L117">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a> 和&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/e1000_hw.h#L41-L75">drivers/net/ethernet/intel/igb/e1000_hw.h&lt;/a>。&lt;/p>
&lt;pre>&lt;code>static DEFINE_PCI_DEVICE_TABLE(igb_pci_tbl) = {
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS), board_82575 },
/* ... */
};
MODULE_DEVICE_TABLE(pci, igb_pci_tbl);
&lt;/code>&lt;/pre>
&lt;p>前面提到，驱动初始化的时候会调用 &lt;code>pci_register_driver&lt;/code>，这个函数会将该驱动的各 种回调方法注册到一个 &lt;code>struct pci_driver&lt;/code> 变量，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L238-L249">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>static struct pci_driver igb_driver = {
.name = igb_driver_name,
.id_table = igb_pci_tbl,
.probe = igb_probe,
.remove = igb_remove,
/* ... */
};
&lt;/code>&lt;/pre>
&lt;h2 id="32-网络设备初始化">3.2 网络设备初始化&lt;a class="td-heading-self-link" href="#32-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>通过 PCI ID 识别设备后，内核就会为它选择合适的驱动。每个 PCI 驱动注册了一个 &lt;code>probe()&lt;/code> 方法，内核会对每个设备依次调用其驱动的 &lt;code>probe&lt;/code> 方法，一旦找到一个合适的 驱动，就不会再为这个设备尝试其他驱动。
很多驱动都需要大量代码来使得设备 ready，具体做的事情各有差异。典型的过程：&lt;/p>
&lt;ol>
&lt;li>启用 PCI 设备&lt;/li>
&lt;li>请求（requesting）内存范围和 IO 端口&lt;/li>
&lt;li>设置 DMA 掩码&lt;/li>
&lt;li>注册设备驱动支持的 ethtool 方法（后面介绍）&lt;/li>
&lt;li>注册所需的 watchdog（例如，e1000e 有一个检测设备是否僵死的 watchdog）&lt;/li>
&lt;li>其他和具体设备相关的事情，例如一些 workaround，或者特定硬件的非常规处理&lt;/li>
&lt;li>创建、初始化和注册一个 &lt;code>struct net_device_ops&lt;/code> 类型变量，这个变量包含了用于设 备相关的回调函数，例如打开设备、发送数据到网络、设置 MAC 地址等&lt;/li>
&lt;li>创建、初始化和注册一个更高层的 &lt;code>struct net_device&lt;/code> 类型变量（一个变量就代表了 一个设备）&lt;/li>
&lt;/ol>
&lt;p>我们来简单看下 &lt;code>igb&lt;/code> 驱动的 &lt;code>igb_probe&lt;/code> 包含哪些过程。下面的代码来自 &lt;code>igb_probe&lt;/code>，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>err = pci_enable_device_mem(pdev);
/* ... */
err = dma_set_mask_and_coherent(&amp;amp;pdev-&amp;gt;dev, DMA_BIT_MASK(64));
/* ... */
err = pci_request_selected_regions(pdev, pci_select_bars(pdev,
IORESOURCE_MEM),
igb_driver_name);
pci_enable_pcie_error_reporting(pdev);
pci_set_master(pdev);
pci_save_state(pdev);
&lt;/code>&lt;/pre>
&lt;h3 id="更多-pci-驱动信息">更多 PCI 驱动信息&lt;a class="td-heading-self-link" href="#%e6%9b%b4%e5%a4%9a-pci-%e9%a9%b1%e5%8a%a8%e4%bf%a1%e6%81%af" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>详细的 PCI 驱动讨论不在本文范围，如果想进一步了解，推荐如下材料： &lt;a href="http://free-electrons.com/doc/pci-drivers.pdf">分享&lt;/a>， &lt;a href="http://wiki.osdev.org/PCI">wiki&lt;/a>， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/PCI/pci.txt">Linux Kernel Documentation: PCI&lt;/a>。&lt;/p>
&lt;h2 id="33-网络设备启动">3.3 网络设备启动&lt;a class="td-heading-self-link" href="#33-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%90%af%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>igb_probe&lt;/code> 做了很多重要的设备初始化工作。除了 PCI 相关的，还有如下一些通用网络 功能和网络设备相关的工作：&lt;/p>
&lt;ol>
&lt;li>注册 &lt;code>struct net_device_ops&lt;/code> 变量&lt;/li>
&lt;li>注册 ethtool 相关的方法&lt;/li>
&lt;li>从网卡获取默认 MAC 地址&lt;/li>
&lt;li>设置 &lt;code>net_device&lt;/code> 特性标记&lt;/li>
&lt;/ol>
&lt;p>我们逐一看下这些过程，后面会用到。&lt;/p>
&lt;h3 id="331-struct-net_device_ops">3.3.1 &lt;code>struct net_device_ops&lt;/code>&lt;a class="td-heading-self-link" href="#331-struct-net_device_ops" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>网络设备相关的操作函数都注册到这个类型的变量中。&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>static const struct net_device_ops igb_netdev_ops = {
.ndo_open = igb_open,
.ndo_stop = igb_close,
.ndo_start_xmit = igb_xmit_frame,
.ndo_get_stats64 = igb_get_stats64,
.ndo_set_rx_mode = igb_set_rx_mode,
.ndo_set_mac_address = igb_set_mac,
.ndo_change_mtu = igb_change_mtu,
.ndo_do_ioctl = igb_ioctl,
/* ... */
&lt;/code>&lt;/pre>
&lt;p>这个变量会在 &lt;code>igb_probe()&lt;/code>中赋给 &lt;code>struct net_device&lt;/code> 中的 &lt;code>netdev_ops&lt;/code> 字段：&lt;/p>
&lt;pre>&lt;code>static int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
...
netdev-&amp;gt;netdev_ops = &amp;amp;igb_netdev_ops;
}
&lt;/code>&lt;/pre>
&lt;h3 id="332-ethtool-函数注册">3.3.2 &lt;code>ethtool&lt;/code> 函数注册&lt;a class="td-heading-self-link" href="#332-ethtool-%e5%87%bd%e6%95%b0%e6%b3%a8%e5%86%8c" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;a href="https://www.kernel.org/pub/software/network/ethtool/">&lt;code>ethtool&lt;/code>&lt;/a> 是一个命令行工 具，可以查看和修改网络设备的一些配置，常用于收集网卡统计数据。在 Ubuntu 上，可以 通过 &lt;code>apt-get install ethtool&lt;/code> 安装。
&lt;code>ethtool&lt;/code> 通过 &lt;a href="http://man7.org/linux/man-pages/man2/ioctl.2.html">ioctl&lt;/a> 和设备驱 动通信。内核实现了一个通用 &lt;code>ethtool&lt;/code> 接口，网卡驱动实现这些接口，就可以被 &lt;code>ethtool&lt;/code> 调用。当 &lt;code>ethtool&lt;/code> 发起一个系统调用之后，内核会找到对应操作的回调函数 。回调实现了各种简单或复杂的函数，简单的如改变一个 flag 值，复杂的包括调整网卡硬 件如何运行。
相关实现见：&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_ethtool.c">igb_ethtool.c&lt;/a>。&lt;/p>
&lt;h3 id="333-软中断irq">3.3.3 软中断（IRQ）&lt;a class="td-heading-self-link" href="#333-%e8%bd%af%e4%b8%ad%e6%96%adirq" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>当一个数据帧通过 DMA 写到 RAM（内存）后，网卡是如何通知其他系统这个包可以被处理 了呢？
传统的方式是，网卡会产生一个硬件中断（IRQ），通知数据包到了。有&lt;strong>三种常见的硬中 断类型&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>MSI-X&lt;/li>
&lt;li>MSI&lt;/li>
&lt;li>legacy IRQ&lt;/li>
&lt;/ul>
&lt;p>稍后详细介绍到。
先来思考这样一个问题：如果有大量的数据包到达，就会产生大量的硬件中断。CPU 忙于处 理硬件中断的时候，可用于处理其他任务的时间就会减少。
NAPI（New API）是一种新的机制，可以减少产生的硬件中断的数量（但不能完全消除硬中断 ）。&lt;/p>
&lt;h3 id="334-napi">3.3.4 NAPI&lt;a class="td-heading-self-link" href="#334-napi" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="napi">NAPI&lt;a class="td-heading-self-link" href="#napi" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>NAPI 接收数据包的方式和传统方式不同，它允许设备驱动注册一个 &lt;code>poll&lt;/code> 方法，然后调 用这个方法完成收包。
NAPI 的使用方式：&lt;/p>
&lt;ol>
&lt;li>驱动打开 NAPI 功能，默认处于未工作状态（没有在收包）&lt;/li>
&lt;li>数据包到达，网卡通过 DMA 写到内存&lt;/li>
&lt;li>网卡触发一个硬中断，&lt;strong>中断处理函数开始执行&lt;/strong>&lt;/li>
&lt;li>软中断（softirq，稍后介绍），唤醒 NAPI 子系统。这会触发&lt;strong>在一个单独的线程里， 调用驱动注册的 &lt;code>poll&lt;/code> 方法收包&lt;/strong>&lt;/li>
&lt;li>驱动禁止网卡产生新的硬件中断。这样做是为了 NAPI 能够在收包的时候不会被新的中 断打扰&lt;/li>
&lt;li>一旦没有包需要收了，NAPI 关闭，网卡的硬中断重新开启&lt;/li>
&lt;li>转步骤 2&lt;/li>
&lt;/ol>
&lt;p>和传统方式相比，NAPI 一次中断会接收多个包，因此可以减少硬件中断的数量。
&lt;code>poll&lt;/code> 方法是通过调用 &lt;code>netif_napi_add&lt;/code> 注册到 NAPI 的，同时还可以指定权重 &lt;code>weight&lt;/code>，大部分驱动都 hardcode 为 64。后面会进一步解释这个 weight 以及 hardcode 64。
通常来说，驱动在初始化的时候注册 NAPI poll 方法。&lt;/p>
&lt;h3 id="335-igb-驱动的-napi-初始化">3.3.5 &lt;code>igb&lt;/code> 驱动的 NAPI 初始化&lt;a class="td-heading-self-link" href="#335-igb-%e9%a9%b1%e5%8a%a8%e7%9a%84-napi-%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>igb&lt;/code> 驱动的初始化过程是一个很长的调用链：&lt;/p>
&lt;ol>
&lt;li>&lt;code>igb_probe&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_sw_init&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_sw_init&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_init_interrupt_scheme&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_init_interrupt_scheme&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_alloc_q_vectors&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_alloc_q_vectors&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_alloc_q_vector&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_alloc_q_vector&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>netif_napi_add&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>从较高的层面来看，这个调用过程会做以下事情：&lt;/p>
&lt;ol>
&lt;li>如果支持 &lt;code>MSI-X&lt;/code>，调用 &lt;code>pci_enable_msix&lt;/code> 打开它&lt;/li>
&lt;li>计算和初始化一些配置，包括网卡收发队列的数量&lt;/li>
&lt;li>调用 &lt;code>igb_alloc_q_vector&lt;/code> 创建每个发送和接收队列&lt;/li>
&lt;li>&lt;code>igb_alloc_q_vector&lt;/code> 会进一步调用 &lt;code>netif_napi_add&lt;/code> 注册 poll 方法到 NAPI 变量&lt;/li>
&lt;/ol>
&lt;p>我们来看下 &lt;code>igb_alloc_q_vector&lt;/code> 是如何注册 poll 方法和私有数据的： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1145-L1271">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static int igb_alloc_q_vector(struct igb_adapter *adapter,
int v_count, int v_idx,
int txr_count, int txr_idx,
int rxr_count, int rxr_idx)
{
/* ... */
/* allocate q_vector and rings */
q_vector = kzalloc(size, GFP_KERNEL);
if (!q_vector)
return -ENOMEM;
/* initialize NAPI */
netif_napi_add(adapter-&amp;gt;netdev, &amp;amp;q_vector-&amp;gt;napi, igb_poll, 64);
/* ... */
&lt;/code>&lt;/pre>
&lt;p>&lt;code>q_vector&lt;/code> 是新分配的队列，&lt;code>igb_poll&lt;/code> 是 poll 方法，当它收包的时候，会通过 这个接收队列找到关联的 NAPI 变量（&lt;code>q_vector-&amp;gt;napi&lt;/code>）。
这里很重要，后面我们介绍从驱动到网络协议栈的 flow（根据 IP 头信息做哈希，哈希相 同的属于同一个 flow）时会看到。&lt;/p>
&lt;h2 id="34-启用网卡-bring-a-network-device-up">3.4 启用网卡 (Bring A Network Device Up)&lt;a class="td-heading-self-link" href="#34-%e5%90%af%e7%94%a8%e7%bd%91%e5%8d%a1-bring-a-network-device-up" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>回忆前面我们提到的 &lt;code>structure net_device_ops&lt;/code> 变量，它包含网卡启用、发包、设置 mac 地址等回调函数（函数指针）。
当启用一个网卡时（例如，通过 &lt;code>ifconfig eth0 up&lt;/code>），&lt;code>net_device_ops&lt;/code> 的 &lt;code>ndo_open&lt;/code> 方法会被调用。它通常会做以下事情：&lt;/p>
&lt;ol>
&lt;li>分配 RX、TX 队列内存&lt;/li>
&lt;li>打开 NAPI 功能&lt;/li>
&lt;li>注册中断处理函数&lt;/li>
&lt;li>打开（enable）硬中断&lt;/li>
&lt;li>其他&lt;/li>
&lt;/ol>
&lt;p>&lt;code>igb&lt;/code> 驱动中，这个方法对应的是 &lt;code>igb_open&lt;/code> 函数。&lt;/p>
&lt;h3 id="341-准备从网络接收数据">3.4.1 准备从网络接收数据&lt;a class="td-heading-self-link" href="#341-%e5%87%86%e5%a4%87%e4%bb%8e%e7%bd%91%e7%bb%9c%e6%8e%a5%e6%94%b6%e6%95%b0%e6%8d%ae" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>今天的大部分网卡都&lt;strong>使用 DMA 将数据直接写到内存&lt;/strong>，接下来&lt;strong>操作系统可以直接从里 面读取&lt;/strong>。实现这一目的所使用的数据结构是 ring buffer（环形缓冲区）。
要实现这一功能，设备驱动必须和操作系统合作，&lt;strong>预留（reserve）出一段内存来给网卡 使用&lt;/strong>。预留成功后，网卡知道了这块内存的地址，接下来收到的包就会放到这里，进而被 操作系统取走。
由于这块内存区域是有限的，如果数据包的速率非常快，单个 CPU 来不及取走这些包，新 来的包就会被丢弃。这时候，Receive Side Scaling（RSS，接收端扩展）或者多队列（ multiqueue）一类的技术可能就会排上用场。
一些网卡有能力将接收到的包写到&lt;strong>多个不同的内存区域，每个区域都是独立的接收队列&lt;/strong>。这 样操作系统就可以利用多个 CPU（硬件层面）并行处理收到的包。只有部分网卡支持这个功 能。
Intel I350 网卡支持多队列，我们可以在 &lt;code>igb&lt;/code> 的驱动里看出来。&lt;code>igb&lt;/code> 驱动启用的时候 ，最开始做的事情之一就是调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2801-L2804">&lt;code>igb_setup_all_rx_resources&lt;/code>&lt;/a> 函数。这个函数会对每个 RX 队列调用 &lt;code>igb_setup_rx_resources&lt;/code>, 里面会管理 DMA 的内存.
如果对其原理感兴趣，可以进一步查看 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/DMA-API-HOWTO.txt">Linux kernel’s DMA API HOWTO&lt;/a> 。
RX 队列的数量和大小可以通过 ethtool 进行配置，调整这两个参数会对收包或者丢包产生可见影响。
网卡通过对 packet 头（例如源地址、目的地址、端口等）做哈希来决定将 packet 放到 哪个 RX 队列。只有很少的网卡支持调整哈希算法。如果支持的话，那你可以根据算法将特定 的 flow 发到特定的队列，甚至可以做到在硬件层面直接将某些包丢弃。
一些网卡支持调整 RX 队列的权重，你可以有意地将更多的流量发到指定的 queue。
后面会介绍如何对这些参数进行调优。&lt;/p>
&lt;h3 id="342-enable-napi">3.4.2 Enable NAPI&lt;a class="td-heading-self-link" href="#342-enable-napi" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>前面看到了驱动如何注册 NAPI &lt;code>poll&lt;/code> 方法，但是，一般直到网卡被启用之后，NAPI 才被启用。
启用 NAPI 很简单，调用 &lt;code>napi_enable&lt;/code> 函数就行，这个函数会设置 NAPI 变量（&lt;code>struct napi_struct&lt;/code>）中一个表示是否启用的标志位。前面说到，NAPI 启用后并不是立即开始工 作（而是等硬中断触发）。
对于 &lt;code>igb&lt;/code>，驱动初始化或者通过 ethtool 修改 queue 数量或大小的时候，会启用每个 &lt;code>q_vector&lt;/code> 的 NAPI 变量。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2833-L2834">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>for (i = 0; i &amp;lt; adapter-&amp;gt;num_q_vectors; i++)
napi_enable(&amp;amp;(adapter-&amp;gt;q_vector[i]-&amp;gt;napi));
&lt;/code>&lt;/pre>
&lt;h3 id="343-注册中断处理函数">3.4.3 注册中断处理函数&lt;a class="td-heading-self-link" href="#343-%e6%b3%a8%e5%86%8c%e4%b8%ad%e6%96%ad%e5%a4%84%e7%90%86%e5%87%bd%e6%95%b0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>启用 NAPI 之后，下一步就是注册中断处理函数。设备有多种方式触发一个中断：&lt;/p>
&lt;ul>
&lt;li>MSI-X&lt;/li>
&lt;li>MSI&lt;/li>
&lt;li>legacy interrupts&lt;/li>
&lt;/ul>
&lt;p>设备驱动的实现也因此而异。
驱动必须判断出设备支持哪种中断方式，然后注册相应的中断处理函数，这些函数在中断发 生的时候会被执行。
一些驱动，例如 &lt;code>igb&lt;/code>，会试图为每种中断类型注册一个中断处理函数，如果注册失败，就 尝试下一种（没测试过的）类型。
&lt;strong>MSI-X 中断是比较推荐的方式，尤其是对于支持多队列的网卡&lt;/strong>。因为每个 RX 队列有独 立的 MSI-X 中断，因此可以被不同的 CPU 处理（通过 &lt;code>irqbalance&lt;/code> 方式，或者修改 &lt;code>/proc/irq/IRQ_NUMBER/smp_affinity&lt;/code>）。我们后面会看到，处理中断的 CPU 也是随后处 理这个包的 CPU。这样的话，从网卡硬件中断的层面就可以设置让收到的包被不同的 CPU 处理。
如果不支持 MSI-X，那 MSI 相比于传统中断方式仍然有一些优势，驱动仍然会优先考虑它。 这个 &lt;a href="https://en.wikipedia.org/wiki/Message_Signaled_Interrupts">wiki&lt;/a> 介绍了更多 关于 MSI 和 MSI-X 的信息。
在 &lt;code>igb&lt;/code> 驱动中，函数 &lt;code>igb_msix_ring&lt;/code>, &lt;code>igb_intr_msi&lt;/code>, &lt;code>igb_intr&lt;/code> 分别是 MSI-X, MSI, 和传统中断方式的中断处理函数。
这段代码显式了驱动是如何尝试各种中断类型的， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1360-L1413">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static int igb_request_irq(struct igb_adapter *adapter)
{
struct net_device *netdev = adapter-&amp;gt;netdev;
struct pci_dev *pdev = adapter-&amp;gt;pdev;
int err = 0;
if (adapter-&amp;gt;msix_entries) {
err = igb_request_msix(adapter);
if (!err)
goto request_done;
/* fall back to MSI */
/* ... */
}
/* ... */
if (adapter-&amp;gt;flags &amp;amp; IGB_FLAG_HAS_MSI) {
err = request_irq(pdev-&amp;gt;irq, igb_intr_msi, 0,
netdev-&amp;gt;name, adapter);
if (!err)
goto request_done;
/* fall back to legacy interrupts */
/* ... */
}
err = request_irq(pdev-&amp;gt;irq, igb_intr, IRQF_SHARED,
netdev-&amp;gt;name, adapter);
if (err)
dev_err(&amp;amp;pdev-&amp;gt;dev, &amp;quot;Error %d getting interrupt\n&amp;quot;, err);
request_done:
return err;
}
&lt;/code>&lt;/pre>
&lt;p>这就是 &lt;code>igb&lt;/code> 驱动注册中断处理函数的过程，这个函数在一个包到达网卡触发一个硬 件中断时就会被执行。&lt;/p>
&lt;h3 id="344-enable-interrupts">3.4.4 Enable Interrupts&lt;a class="td-heading-self-link" href="#344-enable-interrupts" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>到这里，几乎所有的准备工作都就绪了。唯一剩下的就是打开硬中断，等待数据包进来。 打开硬中断的方式因硬件而异，&lt;code>igb&lt;/code> 驱动是在 &lt;code>__igb_open&lt;/code> 里调用辅助函数 &lt;code>igb_irq_enable&lt;/code> 完成的。
中断通过写寄存器的方式打开：&lt;/p>
&lt;pre>&lt;code>static void igb_irq_enable(struct igb_adapter *adapter)
{
/* ... */
wr32(E1000_IMS, IMS_ENABLE_MASK | E1000_IMS_DRSTA);
wr32(E1000_IAM, IMS_ENABLE_MASK | E1000_IMS_DRSTA);
/* ... */
}
&lt;/code>&lt;/pre>
&lt;p>现在，网卡已经启用了。驱动可能还会做一些额外的事情，例如启动定时器，工作队列（ work queue），或者其他硬件相关的设置。这些工作做完后，网卡就可以收包了。
接下来看一下如何监控和调优网卡。&lt;/p>
&lt;h2 id="35-网卡监控">3.5 网卡监控&lt;a class="td-heading-self-link" href="#35-%e7%bd%91%e5%8d%a1%e7%9b%91%e6%8e%a7" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>监控网络设备有几种不同的方式，每种方式的监控粒度（granularity）和复杂度不同。我 们先从最粗的粒度开始，逐步细化。&lt;/p>
&lt;h3 id="351-using-ethtool--s">3.5.1 Using &lt;code>ethtool -S&lt;/code>&lt;a class="td-heading-self-link" href="#351-using-ethtool--s" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>ethtool -S&lt;/code> 可以查看网卡统计信息（例如丢弃的包数量）：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -S eth0
NIC statistics:
rx_packets: 597028087
tx_packets: 5924278060
rx_bytes: 112643393747
tx_bytes: 990080156714
rx_broadcast: 96
tx_broadcast: 116
rx_multicast: 20294528
....
&lt;/code>&lt;/pre>
&lt;p>监控这些数据比较困难。因为用命令行获取很容易，但是以上字段并没有一个统一的标准。 不同的驱动，甚至同一驱动的不同版本可能字段都会有差异。
你可以先粗略的查看 “drop”, “buffer”, “miss” 等字样。然后，在驱动的源码里找到对应的 更新这些字段的地方，这可能是在软件层面更新的，也有可能是在硬件层面通过寄存器更新 的。如果是通过硬件寄存器的方式，你就得查看网卡的 data sheet（说明书），搞清楚这个 寄存器代表什么。ethtoool 给出的这些字段名，有一些是有误导性的（misleading）。&lt;/p>
&lt;h3 id="352-using-sysfs">3.5.2 Using &lt;code>sysfs&lt;/code>&lt;a class="td-heading-self-link" href="#352-using-sysfs" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>sysfs 也提供了统计信息，但相比于网卡层的统计，要更上层一些。
例如，获取 eth0 的接收端 drop 的数量：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/net/eth0/statistics/rx_dropped
2
&lt;/code>&lt;/pre>
&lt;p>不同类型的统计分别位于&lt;code> /sys/class/net/&amp;lt;NIC&amp;gt;/statistics/&lt;/code> 下面的不同文件，包括 &lt;code>collisions&lt;/code>, &lt;code>rx_dropped&lt;/code>, &lt;code>rx_errors&lt;/code>, &lt;code>rx_missed_errors&lt;/code> 等等。
不幸的是，每种类型代表什么意思，是由驱动来决定的，因此也是由驱动决定何时以及在哪 里更新这些计数的。你可能会发现一些驱动将一些特定类型的错误归类为 drop，而另外 一些驱动可能将它们归类为 miss。
这些值至关重要，因此你需要查看对应的网卡驱动，搞清楚它们真正代表什么。&lt;/p>
&lt;h3 id="353-using-procnetdev">3.5.3 Using &lt;code>/proc/net/dev&lt;/code>&lt;a class="td-heading-self-link" href="#353-using-procnetdev" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>/proc/net/dev&lt;/code> 提供了更高一层的网卡统计。&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/dev
Inter-| Receive | Transmit
face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed
eth0: 110346752214 597737500 0 2 0 0 0 20963860 990024805984 6066582604 0 0 0 0 0 0
lo: 428349463836 1579868535 0 0 0 0 0 0 428349463836 1579868535 0 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>这个文件里显式的统计只是 sysfs 里面的一个子集，但适合作为一个常规的统计参考。
前面的警告（caveat）也适用于此：如果这些数据对你非常重要，那你必须得查看内核源码 、驱动源码和驱动手册，搞清楚每个字段真正代表什么意思，计数是如何以及何时被更新的。&lt;/p>
&lt;h2 id="36-网卡调优">3.6 网卡调优&lt;a class="td-heading-self-link" href="#36-%e7%bd%91%e5%8d%a1%e8%b0%83%e4%bc%98" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="361-查看-rx-队列数量">3.6.1 查看 RX 队列数量&lt;a class="td-heading-self-link" href="#361-%e6%9f%a5%e7%9c%8b-rx-%e9%98%9f%e5%88%97%e6%95%b0%e9%87%8f" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>如果网卡及其驱动支持 RSS/多队列，那你可以会调整 RX queue（也叫 RX channel）的数量。 这可以用 ethtool 完成。
查看 RX queue 数量：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Pre-set maximums:
RX: 0
TX: 0
Other: 0
Combined: 8
Current hardware settings:
RX: 0
TX: 0
Other: 0
Combined: 4
&lt;/code>&lt;/pre>
&lt;p>这里可以看到允许的最大值（网卡及驱动限制），以及当前设置的值。
注意：不是所有网卡驱动都支持这个操作。如果你的网卡不支持，会看到如下类似的错误：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Cannot get device channel parameters
: Operation not supported
&lt;/code>&lt;/pre>
&lt;p>这意味着驱动没有实现 ethtool 的 &lt;code>get_channels&lt;/code> 方法。可能的原因包括：该网卡不支 持调整 RX queue 数量，不支持 RSS/multiqueue，或者驱动没有更新来支持此功能。&lt;/p>
&lt;h3 id="362-调整-rx-queues">3.6.2 调整 RX queues&lt;a class="td-heading-self-link" href="#362-%e8%b0%83%e6%95%b4-rx-queues" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>ethtool -L&lt;/code> 可以修改 RX queue 数量。
注意：一些网卡和驱动只支持 combined queue，这种模式下，RX queue 和 TX queue 是一 对一绑定的，上面的例子我们看到的就是这种。
设置 combined 类型网卡的收发队列为 8 个：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 combined 8
&lt;/code>&lt;/pre>
&lt;p>如果你的网卡支持独立的 RX 和 TX 队列数量，那你可以只修改 RX queue 数量：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 rx 8
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，修改以上配置会使网卡先 down 再 up，因此会造成丢包。请酌情 使用。&lt;/p>
&lt;h2 id="363-调整-rx-queue-的大小">3.6.3 调整 RX queue 的大小&lt;a class="td-heading-self-link" href="#363-%e8%b0%83%e6%95%b4-rx-queue-%e7%9a%84%e5%a4%a7%e5%b0%8f" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>一些网卡和驱动也支持修改 RX queue 的大小。底层是如何工作的，随硬件而异，但幸运的是 ，ethtool 提供了一个通用的接口来做这件事情。增加 RX queue 的大小可以在流量很大的时 候缓解丢包问题，但是，只调整这个还不够，软件层面仍然可能会丢包，因此还需要其他的 一些调优才能彻底的缓解或解决丢包问题。
&lt;code>ethtool -g&lt;/code> 可以查看 queue 的大小。&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX: 4096
RX Mini: 0
RX Jumbo: 0
TX: 4096
Current hardware settings:
RX: 512
RX Mini: 0
RX Jumbo: 0
TX: 512
&lt;/code>&lt;/pre>
&lt;p>以上显式网卡支持最多 4096 个接收和发送 descriptor（描述符，简单理解，存放的是指 向包的指针），但是现在只用到了 512 个。
用 &lt;code>ethtool -G&lt;/code> 修改 queue 大小：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -G eth0 rx 4096
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，修改以上配置会使网卡先 down 再 up，因此会造成丢包。请酌情 使用。&lt;/p>
&lt;h3 id="364-调整-rx-queue-的权重weight">3.6.4 调整 RX queue 的权重（weight）&lt;a class="td-heading-self-link" href="#364-%e8%b0%83%e6%95%b4-rx-queue-%e7%9a%84%e6%9d%83%e9%87%8dweight" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>一些网卡支持给不同的 queue 设置不同的权重，以此分发不同数量的网卡包到不同的队列。
如果你的网卡支持以下功能，那你可以使用：&lt;/p>
&lt;ol>
&lt;li>网卡支持 flow indirection（flow 重定向，flow 是什么前面提到过）&lt;/li>
&lt;li>网卡驱动实现了 &lt;code>get_rxfh_indir_size&lt;/code> 和 &lt;code>get_rxfh_indir&lt;/code> 方法&lt;/li>
&lt;li>使用的 ethtool 版本足够新，支持 &lt;code>-x&lt;/code> 和 &lt;code>-X&lt;/code> 参数来设置 indirection table&lt;/li>
&lt;/ol>
&lt;p>&lt;code>ethtool -x&lt;/code> 检查 flow indirection 设置：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -x eth0
RX flow hash indirection table for eth3 with 2 RX ring(s):
0: 0 1 0 1 0 1 0 1
8: 0 1 0 1 0 1 0 1
16: 0 1 0 1 0 1 0 1
24: 0 1 0 1 0 1 0 1
&lt;/code>&lt;/pre>
&lt;p>第一列是哈希值索引，是该行的第一个哈希值；冒号后面的是每个哈希值对于的 queue，例 如，第一行分别是哈希值 0，1，2，3，4，5，6，7，对应的 packet 应该分别被放到 RX queue 0，1，0，1，0，1，0，1。
例子：在前两个 RX queue 之间均匀的分发（接收到的包）：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -X eth0 equal 2
&lt;/code>&lt;/pre>
&lt;p>例子：用 &lt;code>ethtool -X&lt;/code> 设置自定义权重：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -X eth0 weight 6 2
&lt;/code>&lt;/pre>
&lt;p>以上命令分别给 rx queue 0 和 rx queue 1 不同的权重：6 和 2，因此 queue 0 接收到的数量更 多。注意 queue 一般是和 CPU 绑定的，因此这也意味着相应的 CPU 也会花更多的时间片在收包 上。
一些网卡还支持修改计算 hash 时使用哪些字段。&lt;/p>
&lt;h3 id="365-调整-rx-哈希字段-for-network-flows">3.6.5 调整 RX 哈希字段 for network flows&lt;a class="td-heading-self-link" href="#365-%e8%b0%83%e6%95%b4-rx-%e5%93%88%e5%b8%8c%e5%ad%97%e6%ae%b5-for-network-flows" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>可以用 ethtool 调整 RSS 计算哈希时所使用的字段。
例子：查看 UDP RX flow 哈希所使用的字段：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -n eth0 rx-flow-hash udp4
UDP over IPV4 flows use these fields for computing Hash flow key:
IP SA
IP DA
&lt;/code>&lt;/pre>
&lt;p>可以看到只用到了源 IP（SA：Source Address）和目的 IP。
我们接下来修改一下，加入源端口和目的端口：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -N eth0 rx-flow-hash udp4 sdfn
&lt;/code>&lt;/pre>
&lt;p>&lt;code>sdfn&lt;/code> 的具体含义解释起来有点麻烦，请查看 ethtool 的帮助（man page）。
调整 hash 所用字段是有用的，而 &lt;code>ntuple&lt;/code> 过滤对于更加细粒度的 flow control 更加有用。&lt;/p>
&lt;h3 id="366-ntuple-filtering-for-steering-network-flows">3.6.6 ntuple filtering for steering network flows&lt;a class="td-heading-self-link" href="#366-ntuple-filtering-for-steering-network-flows" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>一些网卡支持 “ntuple filtering” 特性。该特性允许用户（通过 ethtool ）指定一些参数来 在硬件上过滤收到的包，然后将其直接放到特定的 RX queue。例如，用户可以指定到特定目 端口的 TCP 包放到 RX queue 1。
Intel 的网卡上这个特性叫 Intel Ethernet Flow Director，其他厂商可能也有他们的名字 ，这些都是出于市场宣传原因，底层原理是类似的。
我们后面会看到，ntuple filtering 是一个叫 Accelerated Receive Flow Steering (aRFS) 功能的核心部分之一，后者使得 ntuple filtering 的使用更加方便。
这个特性适用的场景：最大化数据本地性（data locality），以增加 CPU 处理网络数据时的 缓存命中率。例如，考虑运行在 80 口的 web 服务器：&lt;/p>
&lt;ol>
&lt;li>webserver 进程运行在 80 口，并绑定到 CPU 2&lt;/li>
&lt;li>和某个 RX queue 关联的硬中断绑定到 CPU 2&lt;/li>
&lt;li>目的端口是 80 的 TCP 流量通过 ntuple filtering 绑定到 CPU 2&lt;/li>
&lt;li>接下来所有到 80 口的流量，从数据包进来到数据到达用户程序的整个过程，都由 CPU 2 处理&lt;/li>
&lt;li>仔细监控系统的缓存命中率、网络栈的延迟等信息，以验证以上配置是否生效&lt;/li>
&lt;/ol>
&lt;p>检查 ntuple filtering 特性是否打开：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -k eth0
Offload parameters for eth0:
...
ntuple-filters: off
receive-hashing: on
&lt;/code>&lt;/pre>
&lt;p>可以看到，上面的 ntuple 是关闭的。
打开：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -K eth0 ntuple on
&lt;/code>&lt;/pre>
&lt;p>打开 ntuple filtering 功能，并确认打开之后，可以用 &lt;code>ethtool -u&lt;/code> 查看当前的 ntuple rules：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -u eth0
40 RX rings available
Total 0 rules
&lt;/code>&lt;/pre>
&lt;p>可以看到当前没有 rules。
我们来加一条：目的端口是 80 的放到 RX queue 2：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -U eth0 flow-type tcp4 dst-port 80 action 2
&lt;/code>&lt;/pre>
&lt;p>也可以用 ntuple filtering 在硬件层面直接 drop 某些 flow 的包。当特定 IP 过来的流 量太大时，这种功能可能会派上用场。更多关于 ntuple 的信息，参考 ethtool man page。
&lt;code>ethtool -S &amp;lt;DEVICE&amp;gt;&lt;/code> 的输出统计里，Intel 的网卡有 &lt;code>fdir_match&lt;/code> 和 &lt;code>fdir_miss&lt;/code> 两项， 是和 ntuple filtering 相关的。关于具体的、详细的统计计数，需要查看相应网卡的设备驱 动和 data sheet。&lt;/p>
&lt;h1 id="4-软中断softirq">4 软中断（SoftIRQ）&lt;a class="td-heading-self-link" href="#4-%e8%bd%af%e4%b8%ad%e6%96%adsoftirq" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>在查看网络栈之前，让我们先开个小差，看下内核里一个叫 SoftIRQ 的东西。&lt;/p>
&lt;h2 id="41-软中断是什么">4.1 软中断是什么&lt;a class="td-heading-self-link" href="#41-%e8%bd%af%e4%b8%ad%e6%96%ad%e6%98%af%e4%bb%80%e4%b9%88" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>内核的软中断系统是一种&lt;strong>在硬中断处理上下文（驱动中）之外执行代码&lt;/strong>的机制。&lt;strong>硬中 断处理函数（handler）执行时，会屏蔽部分或全部（新的）硬中断&lt;/strong>。中断被屏蔽的时间 越长，丢失事件的可能性也就越大。所以，&lt;strong>所有耗时的操作都应该从硬中断处理逻辑中剥 离出来&lt;/strong>，硬中断因此能尽可能快地执行，然后再重新打开硬中断。
内核中也有其他机制将耗时操作转移出去，不过对于网络栈，我们接下来只看软中断这种方 式。
可以把软中断系统想象成一系列&lt;strong>内核线程&lt;/strong>（每个 CPU 一个），这些线程执行针对不同 事件注册的处理函数（handler）。如果你用过 &lt;code>top&lt;/code> 命令，可能会注意到 &lt;code>ksoftirqd/0&lt;/code> 这个内核线程，其表示这个软中断线程跑在 CPU 0 上。
内核子系统（比如网络）能通过 &lt;code>open_softirq()&lt;/code> 注册软中断处理函数。接下来会看到 网络系统是如何注册它的处理函数的。
现在先来学习一下软中断是如何工作的。&lt;/p>
&lt;h2 id="42-ksoftirqd">4.2 &lt;code>ksoftirqd&lt;/code>&lt;a class="td-heading-self-link" href="#42-ksoftirqd" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>软中断对分担硬中断的工作量至关重要，因此软中断线程在&lt;strong>内核启动的很早阶段&lt;/strong>就 &lt;code>spawn&lt;/code> 出来了。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/kernel/softirq.c#L743-L758">&lt;code>kernel/softirq.c&lt;/code>&lt;/a> 展示了 &lt;code>ksoftirqd&lt;/code> 系统是如何初始化的：&lt;/p>
&lt;pre>&lt;code>static struct smp_hotplug_thread softirq_threads = {
.store = &amp;amp;ksoftirqd,
.thread_should_run = ksoftirqd_should_run,
.thread_fn = run_ksoftirqd,
.thread_comm = &amp;quot;ksoftirqd/%u&amp;quot;,
};
static __init int spawn_ksoftirqd(void)
{
register_cpu_notifier(&amp;amp;cpu_nfb);
BUG_ON(smpboot_register_percpu_thread(&amp;amp;softirq_threads));
return 0;
}
early_initcall(spawn_ksoftirqd);
&lt;/code>&lt;/pre>
&lt;p>看到注册了两个回调函数: &lt;code>ksoftirqd_should_run&lt;/code> 和 &lt;code>run_ksoftirqd&lt;/code>。这两个函数都会从 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/kernel/smpboot.c#L94-L163">&lt;code>kernel/smpboot.c&lt;/code>&lt;/a> 里调用，作为事件处理循环的一部分。
&lt;code>kernel/smpboot.c&lt;/code> 里面的代码首先调用 &lt;code>ksoftirqd_should_run&lt;/code> 判断是否有 pending 的软 中断，如果有，就执行 &lt;code>run_ksoftirqd&lt;/code>，后者做一些 bookeeping 工作，然后调用 &lt;code>__do_softirq&lt;/code>。&lt;/p>
&lt;h2 id="43-__do_softirq">4.3 &lt;code>__do_softirq&lt;/code>&lt;a class="td-heading-self-link" href="#43-__do_softirq" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>__do_softirq&lt;/code> 做的几件事情：&lt;/p>
&lt;ul>
&lt;li>判断哪个 softirq 被 pending&lt;/li>
&lt;li>计算 softirq 时间，用于统计&lt;/li>
&lt;li>更新 softirq 执行相关的统计数据&lt;/li>
&lt;li>执行 pending softirq 的处理函数&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>查看 CPU 利用率时，&lt;code>si&lt;/code> 字段对应的就是 softirq&lt;/strong>，度量（从硬中断转移过来的）软 中断的 CPU 使用量。&lt;/p>
&lt;h2 id="44-监控">4.4 监控&lt;a class="td-heading-self-link" href="#44-%e7%9b%91%e6%8e%a7" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>软中断的信息可以从 &lt;code>/proc/softirqs&lt;/code> 读取：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/softirqs
CPU0 CPU1 CPU2 CPU3
HI: 0 0 0 0
TIMER: 2831512516 1337085411 1103326083 1423923272
NET_TX: 15774435 779806 733217 749512
NET_RX: 1671622615 1257853535 2088429526 2674732223
BLOCK: 1800253852 1466177 1791366 634534
BLOCK_IOPOLL: 0 0 0 0
TASKLET: 25 0 0 0
SCHED: 2642378225 1711756029 629040543 682215771
HRTIMER: 2547911 2046898 1558136 1521176
RCU: 2056528783 4231862865 3545088730 844379888
&lt;/code>&lt;/pre>
&lt;p>监控这些数据可以得到软中断的执行频率信息。
例如，&lt;code>NET_RX&lt;/code> 一行显示的是软中断在 CPU 间的分布。如果分布非常不均匀，那某一列的 值就会远大于其他列，这预示着下面要介绍的 Receive Packet Steering / Receive Flow Steering 可能会派上用场。但也要注意：不要太相信这个数值，&lt;code>NET_RX&lt;/code> 太高并不一定都 是网卡触发的，下面会看到其他地方也有可能触发之。
调整其他网络配置时，可以留意下这个指标的变动。
现在，让我们进入网络栈部分，跟踪一个包是如何被接收的。&lt;/p>
&lt;h1 id="5-linux-网络设备子系统">5 Linux 网络设备子系统&lt;a class="td-heading-self-link" href="#5-linux-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%ad%90%e7%b3%bb%e7%bb%9f" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>我们已经知道了网络驱动和软中断是如何工作的，接下来看 Linux 网络设备子系统是如何 初始化的。然后我们就可以从一个包到达网卡开始跟踪它的整个路径。&lt;/p>
&lt;h2 id="51-网络设备子系统的初始化">5.1 网络设备子系统的初始化&lt;a class="td-heading-self-link" href="#51-%e7%bd%91%e7%bb%9c%e8%ae%be%e5%a4%87%e5%ad%90%e7%b3%bb%e7%bb%9f%e7%9a%84%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>网络设备（netdev）的初始化在 &lt;code>net_dev_init&lt;/code>，里面有些东西很有意思。&lt;/p>
&lt;h3 id="struct-softnet_data-变量初始化">&lt;code>struct softnet_data&lt;/code> 变量初始化&lt;a class="td-heading-self-link" href="#struct-softnet_data-%e5%8f%98%e9%87%8f%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>net_dev_init&lt;/code> 为每个 CPU 创建一个 &lt;code>struct softnet_data&lt;/code> 变量。这些变量包含一些 指向重要信息的指针：&lt;/p>
&lt;ul>
&lt;li>需要注册到这个 CPU 的 NAPI 变量列表&lt;/li>
&lt;li>数据处理 backlog&lt;/li>
&lt;li>处理权重&lt;/li>
&lt;li>receive offload 变量列表&lt;/li>
&lt;li>receive packet steering 设置&lt;/li>
&lt;/ul>
&lt;p>接下来随着逐步进入网络栈，我们会一一查看这些功能。&lt;/p>
&lt;h3 id="softirq-handler-初始化">SoftIRQ Handler 初始化&lt;a class="td-heading-self-link" href="#softirq-handler-%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>net_dev_init&lt;/code> 分别为接收和发送数据注册了一个软中断处理函数。&lt;/p>
&lt;pre>&lt;code>static int __init net_dev_init(void)
{
/* ... */
open_softirq(NET_TX_SOFTIRQ, net_tx_action);
open_softirq(NET_RX_SOFTIRQ, net_rx_action);
/* ... */
}
&lt;/code>&lt;/pre>
&lt;p>后面会看到驱动的中断处理函数是如何触发 &lt;code>net_rx_action&lt;/code> 这个为 &lt;code>NET_RX_SOFTIRQ&lt;/code> 软中断注册的处理函数的。&lt;/p>
&lt;h2 id="52-数据来了">5.2 数据来了&lt;a class="td-heading-self-link" href="#52-%e6%95%b0%e6%8d%ae%e6%9d%a5%e4%ba%86" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>终于，网络数据来了！
如果 RX 队列有足够的描述符（descriptors），包会&lt;strong>通过 DMA 写到 RAM&lt;/strong>。设备然后发 起对应于它的中断（或者在 MSI-X 的场景，中断和包达到的 RX 队列绑定）。&lt;/p>
&lt;h3 id="521-中断处理函数">5.2.1 中断处理函数&lt;a class="td-heading-self-link" href="#521-%e4%b8%ad%e6%96%ad%e5%a4%84%e7%90%86%e5%87%bd%e6%95%b0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>一般来说，中断处理函数应该将尽可能多的处理逻辑移出（到软中断），这至关重要，因为 发起一个中断后，其他的中断就会被屏蔽。
我们来看一下 MSI-X 中断处理函数的代码，它展示了中断处理函数是如何尽量简单的。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>static irqreturn_t igb_msix_ring(int irq, void *data)
{
struct igb_q_vector *q_vector = data;
/* Write the ITR value calculated from the previous interrupt. */
igb_write_itr(q_vector);
napi_schedule(&amp;amp;q_vector-&amp;gt;napi);
return IRQ_HANDLED;
}
&lt;/code>&lt;/pre>
&lt;p>这个中断处理函数非常简短，只做了 2 个很快的操作就返回了。
首先，它调用 &lt;code>igb_write_itr&lt;/code> 更新一个硬件寄存器。对这个例子，这个寄存器是记录硬件 中断频率的。
这个寄存器和一个叫 &lt;strong>“Interrupt Throttling”（也叫 “Interrupt Coalescing”）的硬件 特性&lt;/strong>相关，这个特性可以平滑传送到 CPU 的中断数量。我们接下来会看到，ethtool 是 怎么样提供了一个机制用于&lt;strong>调整 IRQ 触发频率&lt;/strong>的。
第二，触发 &lt;code>napi_schedule&lt;/code>，如果 NAPI 的处理循环还没开始的话，这会唤醒它。注意， 这个处理循环是在软中断中执行的，而不是硬中断。
这段代码展示了硬中断尽量简短为何如此重要；为我们接下来理解多核 CPU 的接收逻辑很有 帮助。&lt;/p>
&lt;h3 id="522-napi-和-napi_schedule">5.2.2 NAPI 和 &lt;code>napi_schedule&lt;/code>&lt;a class="td-heading-self-link" href="#522-napi-%e5%92%8c-napi_schedule" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>接下来看从硬件中断中调用的 &lt;code>napi_schedule&lt;/code> 是如何工作的。
注意，NAPI 存在的意义是&lt;strong>无需硬件中断通知就可以接收网络数据&lt;/strong>。前面提到， NAPI 的轮询循环（poll loop）是受硬件中断触发而跑起来的。换句话说，NAPI 功能启用了 ，但是默认是没有工作的，直到第一个包到达的时候，网卡触发的一个硬件将它唤醒。后面 会看到，也还有其他的情况，NAPI 功能也会被关闭，直到下一个硬中断再次将它唤起。
&lt;code>napi_schedule&lt;/code> 只是一个简单的封装，内层调用 &lt;code>__napi_schedule&lt;/code>。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/**
* __napi_schedule - schedule for receive
* @n: entry to schedule
*
* The entry's receive function will be scheduled to run
*/
void __napi_schedule(struct napi_struct *n)
{
unsigned long flags;
local_irq_save(flags);
____napi_schedule(&amp;amp;__get_cpu_var(softnet_data), n);
local_irq_restore(flags);
}
EXPORT_SYMBOL(__napi_schedule);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>__get_cpu_var&lt;/code> 用于获取属于这个 CPU 的 &lt;code>structure softnet_data&lt;/code> 变量。
&lt;code>____napi_schedule&lt;/code>, &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/* Called with irq disabled */
static inline void ____napi_schedule(struct softnet_data *sd,
struct napi_struct *napi)
{
list_add_tail(&amp;amp;napi-&amp;gt;poll_list, &amp;amp;sd-&amp;gt;poll_list);
__raise_softirq_irqoff(NET_RX_SOFTIRQ);
}
&lt;/code>&lt;/pre>
&lt;p>这段代码了做了两个重要的事情：&lt;/p>
&lt;ol>
&lt;li>将（从驱动的中断函数中传来的）&lt;code>napi_struct&lt;/code> 变量，添加到 poll list，后者 attach 到这个 CPU 上的 &lt;code>softnet_data&lt;/code>&lt;/li>
&lt;li>&lt;code>__raise_softirq_irqoff&lt;/code> 触发一个 &lt;code>NET_RX_SOFTIRQ&lt;/code> 类型软中断。这会触发执行 &lt;code>net_rx_action&lt;/code>（如果没有正在执行），后者是网络设备初始化的时候注册的&lt;/li>
&lt;/ol>
&lt;p>接下来会看到，软中断处理函数 &lt;code>net_rx_action&lt;/code> 会调用 NAPI 的 poll 函数来收包。&lt;/p>
&lt;h3 id="523-关于-cpu-和网络数据处理的一点笔记">5.2.3 关于 CPU 和网络数据处理的一点笔记&lt;a class="td-heading-self-link" href="#523-%e5%85%b3%e4%ba%8e-cpu-%e5%92%8c%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e7%9a%84%e4%b8%80%e7%82%b9%e7%ac%94%e8%ae%b0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>注意到目前为止，我们从硬中断处理函数中转移到软中断处理函数的逻辑，都是使用的本 CPU 变量。
驱动的硬中断处理函数做的事情很少，但软中断将会在和硬中断相同的 CPU 上执行。&lt;strong>这就 是为什么给每个 CPU 一个特定的硬中断非常重要：这个 CPU 不仅处理这个硬中断，而且通 过 NAPI 处理接下来的软中断来收包&lt;/strong>。
后面我们会看到，Receive Packet Steering 可以将软中断分给其他 CPU。&lt;/p>
&lt;h3 id="524-监控网络数据到达">5.2.4 监控网络数据到达&lt;a class="td-heading-self-link" href="#524-%e7%9b%91%e6%8e%a7%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e5%88%b0%e8%be%be" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="硬中断请求">硬中断请求&lt;a class="td-heading-self-link" href="#%e7%a1%ac%e4%b8%ad%e6%96%ad%e8%af%b7%e6%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>注意：由于某些驱动在 NAPI 运行时会关闭硬中断，因此只监控硬中断无法得到网络处理健 康状况的全景试图，硬中断监控只是整个监控方案的重要组成部分。
读取硬中断统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/interrupts
CPU0 CPU1 CPU2 CPU3
0: 46 0 0 0 IR-IO-APIC-edge timer
1: 3 0 0 0 IR-IO-APIC-edge i8042
30: 3361234770 0 0 0 IR-IO-APIC-fasteoi aacraid
64: 0 0 0 0 DMAR_MSI-edge dmar0
65: 1 0 0 0 IR-PCI-MSI-edge eth0
66: 863649703 0 0 0 IR-PCI-MSI-edge eth0-TxRx-0
67: 986285573 0 0 0 IR-PCI-MSI-edge eth0-TxRx-1
68: 45 0 0 0 IR-PCI-MSI-edge eth0-TxRx-2
69: 394 0 0 0 IR-PCI-MSI-edge eth0-TxRx-3
NMI: 9729927 4008190 3068645 3375402 Non-maskable interrupts
LOC: 2913290785 1585321306 1495872829 1803524526 Local timer interrupts
&lt;/code>&lt;/pre>
&lt;p>可以看到有多少包进来、硬件中断频率，RX 队列被哪个 CPU 处理等信息。这里只能看到硬中 断数量，不能看出实际多少数据被接收或处理，因为一些驱动在 NAPI 收包时会关闭硬中断。 进一步，使用 Interrupt Coalescing 时也会影响这个统计。监控这个指标能帮你判断出你设 置的 Interrupt Coalescing 是不是在工作。
为了使监控更加完整，需要同时监控 &lt;code>/proc/softirqs&lt;/code> (前面提到)和 &lt;code>/proc&lt;/code>。&lt;/p>
&lt;h3 id="525-数据接收调优">5.2.5 数据接收调优&lt;a class="td-heading-self-link" href="#525-%e6%95%b0%e6%8d%ae%e6%8e%a5%e6%94%b6%e8%b0%83%e4%bc%98" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="中断合并interrupt-coalescing">中断合并（Interrupt coalescing）&lt;a class="td-heading-self-link" href="#%e4%b8%ad%e6%96%ad%e5%90%88%e5%b9%b6interrupt-coalescing" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>中断合并会将多个中断事件放到一起，累积到一定阈值后才向 CPU 发起中断请求。
这可以防止&lt;strong>中断风暴&lt;/strong>，提升吞吐，降低 CPU 使用量，但延迟也变大；中断数量过多则相反。
历史上，早期的 igb、e1000 版本，以及其他的都包含一个叫 InterruptThrottleRate 参数， 最近的版本已经被 ethtool 可配置的参数取代。&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off TX: off
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0
...
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ethtool&lt;/code> 提供了用于中断合并相关的通用接口。但切记，不是所有的设备都支持完整的配 置。你需要查看驱动文档或代码来确定哪些支持，哪些不支持。ethtool 的文档说：&lt;strong>“ 驱动没有实现的接口将会被静默忽略”&lt;/strong>。
某些驱动支持一个有趣的特性：“自适应 RX/TX 硬中断合并”。这个特性一般是在硬件实现的 。驱动通常需要做一些额外的工作来告诉网卡需要打开这个特性（前面的 igb 驱动代码里有 涉及）。
自适应 RX/TX 硬中断合并带来的效果是：带宽比较低时降低延迟，带宽比较高时提升吞吐。
用 &lt;code>ethtool -C&lt;/code> 打开自适应 RX IRQ 合并：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -C eth0 adaptive-rx on
&lt;/code>&lt;/pre>
&lt;p>还可以用 &lt;code>ethtool -C&lt;/code> 更改其他配置。常用的包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>rx-usecs&lt;/code>: How many usecs to delay an RX interrupt after a packet arrives.&lt;/li>
&lt;li>&lt;code>rx-frames&lt;/code>: Maximum number of data frames to receive before an RX interrupt.&lt;/li>
&lt;li>&lt;code>rx-usecs-irq&lt;/code>: How many usecs to delay an RX interrupt while an interrupt is being serviced by the host.&lt;/li>
&lt;li>&lt;code>rx-frames-irq&lt;/code>: Maximum number of data frames to receive before an RX interrupt is generated while the system is servicing an interrupt.&lt;/li>
&lt;/ul>
&lt;p>请注意你的硬件可能只支持以上列表的一个子集，具体请参考相应的驱动说明或源码。
不幸的是，通常并没有一个很好的文档来说明这些选项，最全的文档很可能是头文件。每个选项的解释见 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/ethtool.h#L184-L255">include/uapi/linux/ethtool.h&lt;/a> 。
注意：虽然硬中断合并看起来是个不错的优化项，但需要网络栈的其他一些 部分做针对性调整。只合并硬中断很可能并不会带来多少收益。&lt;/p>
&lt;h4 id="调整硬中断亲和性irq-affinities">调整硬中断亲和性（IRQ affinities）&lt;a class="td-heading-self-link" href="#%e8%b0%83%e6%95%b4%e7%a1%ac%e4%b8%ad%e6%96%ad%e4%ba%b2%e5%92%8c%e6%80%a7irq-affinities" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>If your NIC supports RSS / multiqueue or if you are attempting to optimize for data locality, you may wish to use a specific set of CPUs for handling interrupts generated by your NIC.
Setting specific CPUs allows you to segment which CPUs will be used for processing which IRQs. These changes may affect how upper layers operate, as we’ve seen for the networking stack.
If you do decide to adjust your IRQ affinities, you should first check if you running the irqbalance daemon. This daemon tries to automatically balance IRQs to CPUs and it may overwrite your settings. If you are running irqbalance, you should either disable irqbalance or use the –banirq in conjunction with IRQBALANCE_BANNED_CPUS to let irqbalance know that it shouldn’t touch a set of IRQs and CPUs that you want to assign yourself.
Next, you should check the file /proc/interrupts for a list of the IRQ numbers for each network RX queue for your NIC.
Finally, you can adjust the which CPUs each of those IRQs will be handled by modifying /proc/irq/IRQ_NUMBER/smp_affinity for each IRQ number.
You simply write a hexadecimal bitmask to this file to instruct the kernel which CPUs it should use for handling the IRQ.
Example: Set the IRQ affinity for IRQ 8 to CPU 0&lt;/p>
&lt;pre>&lt;code>$ sudo bash -c 'echo 1 &amp;gt; /proc/irq/8/smp_affinity'
&lt;/code>&lt;/pre>
&lt;h2 id="53-网络数据处理开始">5.3 网络数据处理：开始&lt;a class="td-heading-self-link" href="#53-%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e5%bc%80%e5%a7%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>一旦软中断代码判断出有 softirq 处于 pending 状态，就会开始处理，&lt;strong>执行 &lt;code>net_rx_action&lt;/code>，网络数据处理就此开始&lt;/strong>。
我们来看一下 &lt;code>net_rx_action&lt;/code> 的循环部分，理解它是如何工作的。哪个部分可以调优， 哪个可以监控。&lt;/p>
&lt;h3 id="531-net_rx_action-处理循环">5.3.1 &lt;code>net_rx_action&lt;/code> 处理循环&lt;a class="td-heading-self-link" href="#531-net_rx_action-%e5%a4%84%e7%90%86%e5%be%aa%e7%8e%af" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>&lt;code>net_rx_action&lt;/code> 从包所在的内存开始处理，包是被设备通过 DMA 直接送到内存的。&lt;/strong> 函数遍历本 CPU 队列的 NAPI 变量列表，依次出队并操作之。处理逻辑考虑任务量（work ）和执行时间两个因素：&lt;/p>
&lt;ol>
&lt;li>跟踪记录工作量预算（work budget），预算可以调整&lt;/li>
&lt;li>记录消耗的时间&lt;/li>
&lt;/ol>
&lt;p>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4380-L4383">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>while (!list_empty(&amp;amp;sd-&amp;gt;poll_list)) {
struct napi_struct *n;
int work, weight;
/* If softirq window is exhausted then punt.
* Allow this to run for 2 jiffies since which will allow
* an average latency of 1.5/HZ.
*/
if (unlikely(budget &amp;lt;= 0 || time_after_eq(jiffies, time_limit)))
goto softnet_break;
&lt;/code>&lt;/pre>
&lt;p>这里可以看到内核是如何防止处理数据包过程霸占整个 CPU 的，其中 budget 是该 CPU 的 所有 NAPI 变量的总预算。这也是多队列网卡应该精心调整 IRQ Affinity 的原因。回忆前 面讲的，&lt;strong>处理硬中断的 CPU 接下来会处理相应的软中断&lt;/strong>，进而执行上面包含 budget 的 这段逻辑。
多网卡多队列可能会出现这样的情况：多个 NAPI 变量注册到同一个 CPU 上。每个 CPU 上 的所有 NAPI 变量共享一份 budget。
如果没有足够的 CPU 来分散网卡硬中断，可以考虑增加 &lt;code>net_rx_action&lt;/code> 允许每个 CPU 处理更多包。增加 budget 可以增加 CPU 使用量（&lt;code>top&lt;/code> 等命令看到的 &lt;code>sitime&lt;/code> 或 &lt;code>si&lt;/code> 部分），但可以减少延迟，因为数据处理更加及时。
Note: the CPU will still be bounded by a time limit of 2 jiffies, regardless of the assigned budget.&lt;/p>
&lt;h3 id="532-napi-poll-函数及权重">5.3.2 NAPI poll 函数及权重&lt;a class="td-heading-self-link" href="#532-napi-poll-%e5%87%bd%e6%95%b0%e5%8f%8a%e6%9d%83%e9%87%8d" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>回忆前面，网络设备驱动使用 &lt;code>netif_napi_add&lt;/code> 注册 poll 方法，&lt;code>igb&lt;/code> 驱动有如下代码：&lt;/p>
&lt;pre>&lt;code>/* initialize NAPI */
netif_napi_add(adapter-&amp;gt;netdev, &amp;amp;q_vector-&amp;gt;napi, igb_poll, 64);
&lt;/code>&lt;/pre>
&lt;p>这注册了一个 NAPI 变量，hardcode 64 的权重。我们来看在 &lt;code>net_rx_action&lt;/code> 处理循环 中这个值是如何使用的。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4322-L4338">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>weight = n-&amp;gt;weight;
work = 0;
if (test_bit(NAPI_STATE_SCHED, &amp;amp;n-&amp;gt;state)) {
work = n-&amp;gt;poll(n, weight);
trace_napi_poll(n);
}
WARN_ON_ONCE(work &amp;gt; weight);
budget -= work;
&lt;/code>&lt;/pre>
&lt;p>其中的 &lt;code>n&lt;/code> 是 &lt;code>struct napi&lt;/code> 的变量。其中的 &lt;code>poll&lt;/code> 指向 &lt;code>igb_poll&lt;/code>。&lt;code>poll()&lt;/code> 返回 处理的数据帧数量，budget 会减去这个值。
所以，假设驱动使用 weight 值 64（Linux 3.13.0 的所有驱动都是 hardcode 这个值） ，设置 budget 默认值 300，那系统将在如下条件之一停止数据处理：&lt;/p>
&lt;ol>
&lt;li>&lt;code>igb_poll&lt;/code> 函数被调用了最多 5 次（如果没有数据需要处理，那次数就会很少）&lt;/li>
&lt;li>时间经过了至少 2 个 jiffies&lt;/li>
&lt;/ol>
&lt;h3 id="533-napi-和设备驱动的合约contract">5.3.3 NAPI 和设备驱动的合约（contract）&lt;a class="td-heading-self-link" href="#533-napi-%e5%92%8c%e8%ae%be%e5%a4%87%e9%a9%b1%e5%8a%a8%e7%9a%84%e5%90%88%e7%ba%a6contract" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>NAPI 子系统和设备驱动之间的合约，最重要的一点是关闭 NAPI 的条件。具体如下：&lt;/p>
&lt;ol>
&lt;li>如果驱动的 &lt;code>poll&lt;/code> 方法用完了它的全部 weight（默认 hardcode 64），那 它&lt;strong>不要更改&lt;/strong> NAPI 状态。接下来 &lt;code>net_rx_action&lt;/code> loop 会做的&lt;/li>
&lt;li>如果驱动的 &lt;code>poll&lt;/code> 方法没有用完全部 weight，那它&lt;strong>必须关闭&lt;/strong> NAPI。下次有硬件 中断触发，驱动的硬件处理函数调用 &lt;code>napi_schedule&lt;/code> 时，NAPI 会被重新打开&lt;/li>
&lt;/ol>
&lt;p>接下来先看 &lt;code>net_rx_action&lt;/code> 如何处理合约的第一部分，然后看 &lt;code>poll&lt;/code> 方法如何处理第 二部分。&lt;/p>
&lt;h3 id="534-finishing-the-net_rx_action-loop">5.3.4 Finishing the &lt;code>net_rx_action&lt;/code> loop&lt;a class="td-heading-self-link" href="#534-finishing-the-net_rx_action-loop" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>net_rx_action&lt;/code> 循环的最后一部分代码处理前面提到的 &lt;strong>NAPI 合约的第一部分&lt;/strong>。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4342-L4363">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/* Drivers must not modify the NAPI state if they
* consume the entire weight. In such cases this code
* still &amp;quot;owns&amp;quot; the NAPI instance and therefore can
* move the instance around on the list at-will.
*/
if (unlikely(work == weight)) {
if (unlikely(napi_disable_pending(n))) {
local_irq_enable();
napi_complete(n);
local_irq_disable();
} else {
if (n-&amp;gt;gro_list) {
/* flush too old packets
* If HZ &amp;lt; 1000, flush all packets.
*/
local_irq_enable();
napi_gro_flush(n, HZ &amp;gt;= 1000);
local_irq_disable();
}
list_move_tail(&amp;amp;n-&amp;gt;poll_list, &amp;amp;sd-&amp;gt;poll_list);
}
}
&lt;/code>&lt;/pre>
&lt;p>如果全部 &lt;code>work&lt;/code> 都用完了，&lt;code>net_rx_action&lt;/code> 会面临两种情况：&lt;/p>
&lt;ol>
&lt;li>网络设备需要关闭（例如，用户敲了 &lt;code>ifconfig eth0 down&lt;/code> 命令）&lt;/li>
&lt;li>如果设备不需要关闭，那检查是否有 GRO（后面会介绍）列表。如果时钟 tick rate &lt;code>&amp;gt;= 1000&lt;/code>，所有最近被更新的 GRO network flow 都会被 flush。将这个 NAPI 变量移 到 list 末尾，这个循环下次再进入时，处理的就是下一个 NAPI 变量&lt;/li>
&lt;/ol>
&lt;p>这就是包处理循环如何唤醒驱动注册的 &lt;code>poll&lt;/code> 方法进行包处理的过程。接下来会看到， &lt;code>poll&lt;/code> 方法会收割网络数据，发送到上层栈进行处理。&lt;/p>
&lt;h3 id="535-到达-limit-时退出循环">5.3.5 到达 limit 时退出循环&lt;a class="td-heading-self-link" href="#535-%e5%88%b0%e8%be%be-limit-%e6%97%b6%e9%80%80%e5%87%ba%e5%be%aa%e7%8e%af" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>net_rx_action&lt;/code> 下列条件之一退出循环：&lt;/p>
&lt;ol>
&lt;li>这个 CPU 上注册的 poll 列表已经没有 NAPI 变量需要处理(&lt;code>!list_empty(&amp;amp;sd-&amp;gt;poll_list)&lt;/code>)&lt;/li>
&lt;li>剩余的 &lt;code>budget &amp;lt;= 0&lt;/code>&lt;/li>
&lt;li>已经满足 2 个 jiffies 的时间限制&lt;/li>
&lt;/ol>
&lt;p>代码：&lt;/p>
&lt;pre>&lt;code>/* If softirq window is exhausted then punt.
* Allow this to run for 2 jiffies since which will allow
* an average latency of 1.5/HZ.
*/
if (unlikely(budget &amp;lt;= 0 || time_after_eq(jiffies, time_limit)))
goto softnet_break;
&lt;/code>&lt;/pre>
&lt;p>如果跟踪 &lt;code>softnet_break&lt;/code>，会发现很有意思的东西：
From net/core/dev.c:&lt;/p>
&lt;pre>&lt;code>softnet_break:
sd-&amp;gt;time_squeeze++;
__raise_softirq_irqoff(NET_RX_SOFTIRQ);
goto out;
&lt;/code>&lt;/pre>
&lt;p>&lt;code>softnet_data&lt;/code> 变量更新统计信息，软中断的 &lt;code>NET_RX_SOFTIRQ&lt;/code> 被关闭。
&lt;code>time_squeeze&lt;/code> 字段记录的是满足如下条件的次数：&lt;code>net_rx_action&lt;/code> 有很多 &lt;code>work&lt;/code> 要做但 是 budget 用完了，或者 work 还没做完但时间限制到了。这对理解网络处理的瓶颈至关重要 。我们后面会看到如何监控这个值。关闭 &lt;code>NET_RX_SOFTIRQ&lt;/code> 是为了释放 CPU 时间给其他任务 用。这行代码是有意义的，因为只有我们有更多工作要做（还没做完）的时候才会执行到这里， 我们主动让出 CPU，不想独占太久。
然后执行到了 &lt;code>out&lt;/code> 标签所在的代码。另外还有一种条件也会跳转到 &lt;code>out&lt;/code> 标签：所有 NAPI 变量都处理完了，换言之，budget 数量大于网络包数量，所有驱动都已经关闭 NAPI ，没有什么事情需要 &lt;code>net_rx_action&lt;/code> 做了。
&lt;code>out&lt;/code> 代码段在从 &lt;code>net_rx_action&lt;/code> 返回之前做了一件重要的事情：调用 &lt;code>net_rps_action_and_irq_enable&lt;/code>。Receive Packet Steering 功能打开时这个函数 有重要作用：唤醒其他 CPU 处理网络包。
我们后面会看到 RPS 是如何工作的。现在先看看怎样监控 &lt;code>net_rx_action&lt;/code> 处理循环的 健康状态，以及进入 NAPI &lt;code>poll&lt;/code> 的内部，这样才能更好的理解网络栈。&lt;/p>
&lt;h3 id="536-napi-poll">5.3.6 NAPI &lt;code>poll&lt;/code>&lt;a class="td-heading-self-link" href="#536-napi-poll" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>回忆前文，驱动程序会分配一段内存用于 DMA，将数据包写到内存。就像这段内存是由驱动 程序分配的一样，驱动程序也负责解绑（unmap）这些内存，读取数据，将数据送到网络栈 。
我们看下 &lt;code>igb&lt;/code> 驱动如何实现这一过程的。&lt;/p>
&lt;h4 id="igb_poll">&lt;code>igb_poll&lt;/code>&lt;a class="td-heading-self-link" href="#igb_poll" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>可以看到 &lt;code>igb_poll&lt;/code> 代码其实相当简单。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/**
* igb_poll - NAPI Rx polling callback
* @napi: napi polling structure
* @budget: count of how many packets we should handle
**/
static int igb_poll(struct napi_struct *napi, int budget)
{
struct igb_q_vector *q_vector = container_of(napi,
struct igb_q_vector,
napi);
bool clean_complete = true;
#ifdef CONFIG_IGB_DCA
if (q_vector-&amp;gt;adapter-&amp;gt;flags &amp;amp; IGB_FLAG_DCA_ENABLED)
igb_update_dca(q_vector);
#endif
/* ... */
if (q_vector-&amp;gt;rx.ring)
clean_complete &amp;amp;= igb_clean_rx_irq(q_vector, budget);
/* If all work not completed, return budget and keep polling */
if (!clean_complete)
return budget;
/* If not enough Rx work done, exit the polling mode */
napi_complete(napi);
igb_ring_irq_enable(q_vector);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>几件有意思的事情：&lt;/p>
&lt;ul>
&lt;li>如果内核 &lt;a href="https://lwn.net/Articles/247493/">DCA&lt;/a>（Direct Cache Access）功能打 开了，CPU 缓存是热的，对 RX ring 的访问会命中 CPU cache。更多 DCA 信息见本文 “ Extra” 部分&lt;/li>
&lt;li>然后执行 &lt;code>igb_clean_rx_irq&lt;/code>，这里做的事情非常多，我们后面看&lt;/li>
&lt;li>然后执行 &lt;code>clean_complete&lt;/code>，判断是否仍然有 work 可以做。如果有，就返回 budget（ 回忆，这里是 hardcode 64）。在之前我们已经看到，&lt;code>net_rx_action&lt;/code> 会将这个 NAPI 变量移动到 poll 列表的末尾&lt;/li>
&lt;li>如果所有 &lt;code>work&lt;/code> 都已经完成，驱动通过调用 &lt;code>napi_complete&lt;/code> 关闭 NAPI，并通过调用 &lt;code>igb_ring_irq_enable&lt;/code> 重新进入可中断状态。下次中断到来的时候回重新打开 NAPI&lt;/li>
&lt;/ul>
&lt;p>我们来看 &lt;code>igb_clean_rx_irq&lt;/code> 如何将网络数据送到网络栈。&lt;/p>
&lt;h4 id="igb_clean_rx_irq">&lt;code>igb_clean_rx_irq&lt;/code>&lt;a class="td-heading-self-link" href="#igb_clean_rx_irq" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>&lt;code>igb_clean_rx_irq&lt;/code> 方法是一个循环，每次处理一个包，直到 budget 用完，或者没有数 据需要处理了。
做的几件重要事情：&lt;/p>
&lt;ol>
&lt;li>分配额外的 buffer 用于接收数据，因为已经用过的 buffer 被 clean out 了。一次分配 &lt;code>IGB_RX_BUFFER_WRITE (16)&lt;/code>个。&lt;/li>
&lt;li>从 RX 队列取一个 buffer，保存到一个 &lt;code>skb&lt;/code> 类型的变量中&lt;/li>
&lt;li>判断这个 buffer 是不是一个包的最后一个 buffer。如果是，继续处理；如果不是，继续 从 buffer 列表中拿出下一个 buffer，加到 skb。当数据帧的大小比一个 buffer 大的时候， 会出现这种情况&lt;/li>
&lt;li>验证数据的 layout 和头信息是正确的&lt;/li>
&lt;li>更新 &lt;code>skb-&amp;gt;len&lt;/code>，表示这个包已经处理的字节数&lt;/li>
&lt;li>设置 &lt;code>skb&lt;/code> 的 hash, checksum, timestamp, VLAN id, protocol 字段。hash， checksum，timestamp，VLAN ID 信息是硬件提供的，如果硬件报告 checksum error， &lt;code>csum_error&lt;/code> 统计就会增加。如果 checksum 通过了，数据是 UDP 或者 TCP 数据，&lt;code>skb&lt;/code> 就会 被标记成 &lt;code>CHECKSUM_UNNECESSARY&lt;/code>&lt;/li>
&lt;li>构建的 skb 经 &lt;code>napi_gro_receive()&lt;/code>进入协议栈&lt;/li>
&lt;li>更新处理过的包的统计信息&lt;/li>
&lt;li>循环直至处理的包数量达到 budget&lt;/li>
&lt;/ol>
&lt;p>循环结束的时候，这个函数设置收包的数量和字节数统计信息。
接下来在进入协议栈之前，我们先开两个小差：首先是看一些如何监控和调优软中断，其次 是介绍 GRO。有了这个两个背景，后面（通过 &lt;code>napi_gro_receive&lt;/code> 进入）协议栈部分会更容易理解。&lt;/p>
&lt;h3 id="537-监控网络数据处理">5.3.7 监控网络数据处理&lt;a class="td-heading-self-link" href="#537-%e7%9b%91%e6%8e%a7%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="procnetsoftnet_stat">&lt;code>/proc/net/softnet_stat&lt;/code>&lt;a class="td-heading-self-link" href="#procnetsoftnet_stat" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>前面看到，如果 budget 或者 time limit 到了而仍有包需要处理，那 &lt;code>net_rx_action&lt;/code> 在退出 循环之前会更新统计信息。这个信息存储在该 CPU 的 &lt;code>struct softnet_data&lt;/code> 变量中。
这些统计信息打到了&lt;code>/proc/net/softnet_stat&lt;/code>，但不幸的是，关于这个的文档很少。每一 列代表什么并没有标题，而且列的内容会随着内核版本可能发生变化。
在内核 3.13.0 中，你可以阅读内核源码，查看每一列分别对应什么。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/net-procfs.c#L161-L165">net/core/net-procfs.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>seq_printf(seq,
&amp;quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&amp;quot;,
sd-&amp;gt;processed, sd-&amp;gt;dropped, sd-&amp;gt;time_squeeze, 0,
0, 0, 0, 0, /* was fastroute */
sd-&amp;gt;cpu_collision, sd-&amp;gt;received_rps, flow_limit_count);
&lt;/code>&lt;/pre>
&lt;p>其中一些的名字让人很困惑，而且在你意想不到的地方更新。在接下来的网络栈分析说，我 们会举例说明其中一些字段是何时、在哪里被更新的。前面我们已经看到了 &lt;code>squeeze_time&lt;/code> 是在 &lt;code>net_rx_action&lt;/code> 在被更新的，到此时，如下数据你应该能看懂了：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/softnet_stat
6dcad223 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6f0e1565 00000000 00000002 00000000 00000000 00000000 00000000 00000000 00000000 00000000
660774ec 00000000 00000003 00000000 00000000 00000000 00000000 00000000 00000000 00000000
61c99331 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6794b1b3 00000000 00000005 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6488cb92 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
&lt;/code>&lt;/pre>
&lt;p>关于&lt;code>/proc/net/softnet_stat&lt;/code> 的重要细节:&lt;/p>
&lt;ol>
&lt;li>每一行代表一个 &lt;code>struct softnet_data&lt;/code> 变量。因为每个 CPU 只有一个该变量，所以每行 其实代表一个 CPU&lt;/li>
&lt;li>每列用空格隔开，数值用 16 进制表示&lt;/li>
&lt;li>第一列 &lt;code>sd-&amp;gt;processed&lt;/code>，是处理的网络帧的数量。如果你使用了 ethernet bonding， 那这个值会大于总的网络帧的数量，因为 ethernet bonding 驱动有时会触发网络数据被 重新处理（re-processed）&lt;/li>
&lt;li>第二列，&lt;code>sd-&amp;gt;dropped&lt;/code>，是因为处理不过来而 drop 的网络帧数量。后面会展开这一话题&lt;/li>
&lt;li>第三列，&lt;code>sd-&amp;gt;time_squeeze&lt;/code>，前面介绍过了，由于 budget 或 time limit 用完而退出 &lt;code>net_rx_action&lt;/code> 循环的次数&lt;/li>
&lt;li>接下来的 5 列全是 0&lt;/li>
&lt;li>第九列，&lt;code>sd-&amp;gt;cpu_collision&lt;/code>，是为了发送包而获取锁的时候有冲突的次数&lt;/li>
&lt;li>第十列，&lt;code>sd-&amp;gt;received_rps&lt;/code>，是这个 CPU 被其他 CPU 唤醒去收包的次数&lt;/li>
&lt;li>最后一列，&lt;code>flow_limit_count&lt;/code>，是达到 flow limit 的次数。flow limit 是 RPS 的特性， 后面会稍微介绍一下&lt;/li>
&lt;/ol>
&lt;p>如果你要画图监控这些数据，确保你的列和相应的字段是对的上的，最保险的方式是阅读相 应版本的内核代码。&lt;/p>
&lt;h3 id="538-网络数据处理调优">5.3.8 网络数据处理调优&lt;a class="td-heading-self-link" href="#538-%e7%bd%91%e7%bb%9c%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e8%b0%83%e4%bc%98" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="调整-net_rx_action-budget">调整 &lt;code>net_rx_action&lt;/code> budget&lt;a class="td-heading-self-link" href="#%e8%b0%83%e6%95%b4-net_rx_action-budget" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>&lt;code>net_rx_action&lt;/code> budget 表示一个 CPU 单次轮询（&lt;code>poll&lt;/code>）所允许的最大收包数量。单次 poll 收包时，所有注册到这个 CPU 的 NAPI 变量收包数量之和不能大于这个阈值。 调整：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.netdev_budget=600
&lt;/code>&lt;/pre>
&lt;p>如果要保证重启仍然生效，需要将这个配置写到&lt;code>/etc/sysctl.conf&lt;/code>。
Linux 3.13.0 的默认配置是 300。&lt;/p>
&lt;h2 id="54-grogeneric-receive-offloading">5.4 GRO（Generic Receive Offloading）&lt;a class="td-heading-self-link" href="#54-grogeneric-receive-offloading" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;strong>Large Receive Offloading (LRO) 是一个硬件优化，GRO 是 LRO 的一种软件实现。&lt;/strong>
两种方案的主要思想都是：&lt;strong>通过合并“足够类似”的包来减少传送给网络栈的包数，这有 助于减少 CPU 的使用量&lt;/strong>。例如，考虑大文件传输的场景，包的数量非常多，大部分包都是一 段文件数据。相比于每次都将小包送到网络栈，可以将收到的小包合并成一个很大的包再送 到网络栈。GRO &lt;strong>使协议层只需处理一个 header&lt;/strong>，而将包含大量数据的整个大包送到用 户程序。
这类优化方式的缺点是 &lt;strong>信息丢失&lt;/strong>：包的 option 或者 flag 信息在合并时会丢 失。这也是为什么大部分人不使用或不推荐使用 LRO 的原因。
LRO 的实现，一般来说，对合并包的规则非常宽松。GRO 是 LRO 的软件实现，但是对于包合并 的规则更严苛。
顺便说一下，&lt;strong>如果用 tcpdump 抓包，有时会看到机器收到了看起来不现实的、非常大的包&lt;/strong>， 这很可能是你的系统开启了 GRO。接下来会看到，&lt;strong>tcpdump 的抓包点（捕获包的 tap ）在整个栈的更后面一些，在 GRO 之后&lt;/strong>。&lt;/p>
&lt;h3 id="使用-ethtool-修改-gro-配置">使用 ethtool 修改 GRO 配置&lt;a class="td-heading-self-link" href="#%e4%bd%bf%e7%94%a8-ethtool-%e4%bf%ae%e6%94%b9-gro-%e9%85%8d%e7%bd%ae" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>-k&lt;/code> 查看 GRO 配置：&lt;/p>
&lt;pre>&lt;code>$ ethtool -k eth0 | grep generic-receive-offload
generic-receive-offload: on
&lt;/code>&lt;/pre>
&lt;p>&lt;code>-K&lt;/code> 修改 GRO 配置：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -K eth0 gro on
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，修改 GRO 配置会涉及先 down 再 up 这个网卡，因此这个网卡上的连接 都会中断。&lt;/p>
&lt;h2 id="55-napi_gro_receive">5.5 &lt;code>napi_gro_receive&lt;/code>&lt;a class="td-heading-self-link" href="#55-napi_gro_receive" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果开启了 GRO，&lt;code>napi_gro_receive&lt;/code> 将负责处理网络数据，并将数据送到协议栈，大部分 相关的逻辑在函数 &lt;code>dev_gro_receive&lt;/code> 里实现。&lt;/p>
&lt;h3 id="dev_gro_receive">&lt;code>dev_gro_receive&lt;/code>&lt;a class="td-heading-self-link" href="#dev_gro_receive" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这个函数首先检查 GRO 是否开启了，如果是，就准备做 GRO。GRO 首先遍历一个 offload filter 列表，如果高层协议认为其中一些数据属于 GRO 处理的范围，就会允许其对数据进行 操作。
协议层以此方式让网络设备层知道，这个 packet 是不是当前正在处理的一个需要做 GRO 的 network flow 的一部分，而且也可以通过这种方式传递一些协议相关的信息。例如，TCP 协 议需要判断是否以及合适应该将一个 ACK 包合并到其他包里。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3844-L3856">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>list_for_each_entry_rcu(ptype, head, list) {
if (ptype-&amp;gt;type != type || !ptype-&amp;gt;callbacks.gro_receive)
continue;
skb_set_network_header(skb, skb_gro_offset(skb));
skb_reset_mac_len(skb);
NAPI_GRO_CB(skb)-&amp;gt;same_flow = 0;
NAPI_GRO_CB(skb)-&amp;gt;flush = 0;
NAPI_GRO_CB(skb)-&amp;gt;free = 0;
pp = ptype-&amp;gt;callbacks.gro_receive(&amp;amp;napi-&amp;gt;gro_list, skb);
break;
}
&lt;/code>&lt;/pre>
&lt;p>如果协议层提示是时候 flush GRO packet 了，那就到下一步处理了。这发生在 &lt;code>napi_gro_complete&lt;/code>，会进一步调用相应协议的 &lt;code>gro_complete&lt;/code> 回调方法，然后调用 &lt;code>netif_receive_skb&lt;/code> 将包送到协议栈。 这个过程见&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3862-L3872">net/core/dev.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>if (pp) {
struct sk_buff *nskb = *pp;
*pp = nskb-&amp;gt;next;
nskb-&amp;gt;next = NULL;
napi_gro_complete(nskb);
napi-&amp;gt;gro_count--;
}
&lt;/code>&lt;/pre>
&lt;p>接下来，如果协议层将这个包合并到一个已经存在的 flow，&lt;code>napi_gro_receive&lt;/code> 就没什么事 情需要做，因此就返回了。如果 packet 没有被合并，而且 GRO 的数量小于 &lt;code>MAX_GRO_SKBS&lt;/code>（ 默认是 8），就会创建一个新的 entry 加到本 CPU 的 NAPI 变量的 &lt;code>gro_list&lt;/code>。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3877-L3886">net/core/dev.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>if (NAPI_GRO_CB(skb)-&amp;gt;flush || napi-&amp;gt;gro_count &amp;gt;= MAX_GRO_SKBS)
goto normal;
napi-&amp;gt;gro_count++;
NAPI_GRO_CB(skb)-&amp;gt;count = 1;
NAPI_GRO_CB(skb)-&amp;gt;age = jiffies;
skb_shinfo(skb)-&amp;gt;gso_size = skb_gro_len(skb);
skb-&amp;gt;next = napi-&amp;gt;gro_list;
napi-&amp;gt;gro_list = skb;
ret = GRO_HELD;
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>这就是 Linux 网络栈中 GRO 的工作原理。&lt;/strong>&lt;/p>
&lt;h2 id="56-napi_skb_finish">5.6 &lt;code>napi_skb_finish&lt;/code>&lt;a class="td-heading-self-link" href="#56-napi_skb_finish" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>一旦 &lt;code>dev_gro_receive&lt;/code> 完成，&lt;code>napi_skb_finish&lt;/code> 就会被调用，其如果一个 packet 被合并了 ，就释放不用的变量；或者调用 &lt;code>netif_receive_skb&lt;/code> 将数据发送到网络协议栈（因为已经 有 &lt;code>MAX_GRO_SKBS&lt;/code> 个 flow 了，够 GRO 了）。
接下来，是看 &lt;code>netif_receive_skb&lt;/code> 如何将数据交给协议层。但在此之前，我们先看一下 RPS。&lt;/p>
&lt;h1 id="6-rps-receive-packet-steering">6 RPS (Receive Packet Steering)&lt;a class="td-heading-self-link" href="#6-rps-receive-packet-steering" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>回忆前面我们讨论了网络设备驱动是如何注册 NAPI &lt;code>poll&lt;/code> 方法的。每个 NAPI 变量都会运 行在相应 CPU 的软中断的上下文中。而且，触发硬中断的这个 CPU 接下来会负责执行相应的软 中断处理函数来收包。
换言之，同一个 CPU 既处理硬中断，又处理相应的软中断。
一些网卡（例如 Intel I350）在硬件层支持多队列。这意味着收进来的包会被通过 DMA 放到 位于不同内存的队列上，而不同的队列有相应的 NAPI 变量管理软中断 &lt;code>poll()&lt;/code>过程。因此， 多个 CPU 同时处理从网卡来的中断，处理收包过程。
这个特性被称作 RSS（Receive Side Scaling，接收端扩展）。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222">RPS&lt;/a> （Receive Packet Steering，接收包控制，接收包引导）是 RSS 的一种软件实现。因为是软 件实现的，意味着任何网卡都可以使用这个功能，即便是那些只有一个接收队列的网卡。但 是，因为它是软件实现的，这意味着 RPS 只能在 packet 通过 DMA 进入内存后，RPS 才能开始工 作。
这意味着，RPS 并不会减少 CPU 处理硬件中断和 NAPI &lt;code>poll&lt;/code>（软中断最重要的一部分）的时 间，但是可以在 packet 到达内存后，将 packet 分到其他 CPU，从其他 CPU 进入协议栈。
RPS 的工作原理是对个 packet 做 hash，以此决定分到哪个 CPU 处理。然后 packet 放到每个 CPU 独占的接收后备队列（backlog）等待处理。这个 CPU 会触发一个进程间中断（ &lt;a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">IPI&lt;/a>，Inter-processor Interrupt）向对端 CPU。如果当时对端 CPU 没有在处理 backlog 队列收包，这个进程间中断会 触发它开始从 backlog 收包。&lt;code>/proc/net/softnet_stat&lt;/code> 其中有一列是记录 &lt;code>softnet_data&lt;/code> 变量（也即这个 CPU）收到了多少 IPI（&lt;code>received_rps&lt;/code> 列）。
因此，&lt;code>netif_receive_skb&lt;/code> 或者继续将包送到协议栈，或者交给 RPS，后者会转交给其他 CPU 处理。&lt;/p>
&lt;h2 id="rps-调优">RPS 调优&lt;a class="td-heading-self-link" href="#rps-%e8%b0%83%e4%bc%98" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>使用 RPS 需要在内核做配置（Ubuntu + Kernel 3.13.0 支持），而且需要一个掩码（ bitmask）指定哪些 CPU 可以处理那些 RX 队列。相关的一些信息可以在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L138-L164">内核文档  &lt;/a>里找到。
bitmask 配置位于：&lt;code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus&lt;/code>。
例如，对于 eth0 的 queue 0，你需要更改&lt;code>/sys/class/net/eth0/queues/rx-0/rps_cpus&lt;/code>。&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L160-L164">  内核文档  &lt;/a>里说，对一些特定的配置下，RPS 没必要了。
注意：打开 RPS 之后，原来不需要处理软中断（softirq）的 CPU 这时也会参与处理。因此相 应 CPU 的 &lt;code>NET_RX&lt;/code> 数量，以及 &lt;code>si&lt;/code> 或 &lt;code>sitime&lt;/code> 占比都会相应增加。你可以对比启用 RPS 前后的 数据，以此来确定你的配置是否生效，以及是否符合预期（哪个 CPU 处理哪个网卡的哪个中 断）。&lt;/p>
&lt;h1 id="7-rfs-receive-flow-steering">7 RFS (Receive Flow Steering)&lt;a class="td-heading-self-link" href="#7-rfs-receive-flow-steering" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>RFS（Receive flow steering）和 RPS 配合使用。RPS 试图在 CPU 之间平衡收包，但是没考虑 数据的本地性问题，如何最大化 CPU 缓存的命中率。RFS 将属于相同 flow 的包送到相同的 CPU 进行处理，可以提高缓存命中率。&lt;/p>
&lt;h2 id="调优打开-rfs">调优：打开 RFS&lt;a class="td-heading-self-link" href="#%e8%b0%83%e4%bc%98%e6%89%93%e5%bc%80-rfs" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>RPS 记录一个全局的 hash table，包含所有 flow 的信息。这个 hash table 的大小可以在 &lt;code>net.core.rps_sock_flow_entries&lt;/code>：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.rps_sock_flow_entries=32768
&lt;/code>&lt;/pre>
&lt;p>其次，你可以设置每个 RX queue 的 flow 数量，对应着 &lt;code>rps_flow_cnt&lt;/code>：
例如，eth0 的 RX queue0 的 flow 数量调整到 2048：&lt;/p>
&lt;pre>&lt;code>$ sudo bash -c 'echo 2048 &amp;gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt'
&lt;/code>&lt;/pre>
&lt;h1 id="8-arfs-hardware-accelerated-rfs">8 aRFS (Hardware accelerated RFS)&lt;a class="td-heading-self-link" href="#8-arfs-hardware-accelerated-rfs" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>RFS 可以用硬件加速，网卡和内核协同工作，判断哪个 flow 应该在哪个 CPU 上处理。这需要网 卡和网卡驱动的支持。
如果你的网卡驱动里对外提供一个 &lt;code>ndo_rx_flow_steer&lt;/code> 函数，那就是支持 RFS。&lt;/p>
&lt;h2 id="调优-启用-arfs">调优: 启用 aRFS&lt;a class="td-heading-self-link" href="#%e8%b0%83%e4%bc%98-%e5%90%af%e7%94%a8-arfs" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>假如你的网卡支持 aRFS，你可以开启它并做如下配置：&lt;/p>
&lt;ul>
&lt;li>打开并配置 RPS&lt;/li>
&lt;li>打开并配置 RFS&lt;/li>
&lt;li>内核中编译期间指定了 &lt;code>CONFIG_RFS_ACCEL&lt;/code> 选项。Ubuntu kernel 3.13.0 是有的&lt;/li>
&lt;li>打开网卡的 ntuple 支持。可以用 ethtool 查看当前的 ntuple 设置&lt;/li>
&lt;li>配置 IRQ（硬中断）中每个 RX 和 CPU 的对应关系&lt;/li>
&lt;/ul>
&lt;p>以上配置完成后，aRFS 就会自动将 RX queue 数据移动到指定 CPU 的内存，每个 flow 的包都会 到达同一个 CPU，不需要你再通过 ntuple 手动指定每个 flow 的配置了。&lt;/p>
&lt;h1 id="9-从-netif_receive_skb-进入协议栈">9 从 &lt;code>netif_receive_skb&lt;/code> 进入协议栈&lt;a class="td-heading-self-link" href="#9-%e4%bb%8e-netif_receive_skb-%e8%bf%9b%e5%85%a5%e5%8d%8f%e8%ae%ae%e6%a0%88" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>重新捡起我们前面已经几次提到过的 &lt;code>netif_receive_skb&lt;/code>，这个函数在好几个地方被调用 。两个最重要的地方（前面都看到过了）：&lt;/p>
&lt;ul>
&lt;li>&lt;code>napi_skb_finish&lt;/code>：当 packet 不需要被合并到已经存在的某个 GRO flow 的时候&lt;/li>
&lt;li>&lt;code>napi_gro_complete&lt;/code>：协议层提示需要 flush 当前的 flow 的时候&lt;/li>
&lt;/ul>
&lt;p>提示：&lt;code>netif_receive_skb&lt;/code> 和它调用的函数都运行在软中断处理循环（softirq processing loop）的上下文中，因此这里的时间会记录到 &lt;code>top&lt;/code> 命令看到的 &lt;code>si&lt;/code> 或者 &lt;code>sitime&lt;/code> 字段。
&lt;code>netif_receive_skb&lt;/code> 首先会检查用户有没有设置一个接收时间戳选项（sysctl），这个选 项决定在 packet 在到达 backlog queue 之前还是之后打时间戳。如果启用，那立即打时间戳 ，在 RPS 之前（CPU 和 backlog queue 绑定）；如果没有启用，那只有在它进入到 backlog queue 之后才会打时间戳。如果 RPS 开启了，那这个选项可以将打时间戳的任务分散个其他 CPU，但会带来一些延迟。&lt;/p>
&lt;h2 id="91-调优-收包打时间戳rx-packet-timestamping">9.1 调优: 收包打时间戳（RX packet timestamping）&lt;a class="td-heading-self-link" href="#91-%e8%b0%83%e4%bc%98-%e6%94%b6%e5%8c%85%e6%89%93%e6%97%b6%e9%97%b4%e6%88%b3rx-packet-timestamping" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>你可以调整包被收到后，何时给它打时间戳。
关闭收包打时间戳：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.netdev_tstamp_prequeue=0
&lt;/code>&lt;/pre>
&lt;p>默认是 1。&lt;/p>
&lt;h1 id="10-netif_receive_skb">10 &lt;code>netif_receive_skb&lt;/code>&lt;a class="td-heading-self-link" href="#10-netif_receive_skb" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>处理完时间戳后，&lt;code>netif_receive_skb&lt;/code> 会根据 RPS 是否启用来做不同的事情。我们先来看简 单情况，RPS 未启用。&lt;/p>
&lt;h2 id="101-不使用-rps默认">10.1 不使用 RPS（默认）&lt;a class="td-heading-self-link" href="#101-%e4%b8%8d%e4%bd%bf%e7%94%a8-rps%e9%bb%98%e8%ae%a4" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果 RPS 没启用，会调用&lt;code>__netif_receive_skb&lt;/code>，它做一些 bookkeeping 工作，进而调用 &lt;code>__netif_receive_skb_core&lt;/code>，将数据移动到离协议栈更近一步。
&lt;code>__netif_receive_skb_core&lt;/code> 工作的具体细节我们稍后再看，先看一下 RPS 启用的情况下的 代码调用关系，它也会调到这个函数的。&lt;/p>
&lt;h2 id="102-使用-rps">10.2 使用 RPS&lt;a class="td-heading-self-link" href="#102-%e4%bd%bf%e7%94%a8-rps" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果 RPS 启用了，它会做一些计算，判断使用哪个 CPU 的 backlog queue，这个过程由 &lt;code>get_rps_cpu&lt;/code> 函数完成。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>cpu = get_rps_cpu(skb-&amp;gt;dev, skb, &amp;amp;rflow);
if (cpu &amp;gt;= 0) {
ret = enqueue_to_backlog(skb, cpu, &amp;amp;rflow-&amp;gt;last_qtail);
rcu_read_unlock();
return ret;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>get_rps_cpu&lt;/code> 会考虑前面提到的 RFS 和 aRFS 设置，以此选出一个合适的 CPU，通过调用 &lt;code>enqueue_to_backlog&lt;/code> 将数据放到它的 backlog queue。&lt;/p>
&lt;h2 id="103-enqueue_to_backlog">10.3 &lt;code>enqueue_to_backlog&lt;/code>&lt;a class="td-heading-self-link" href="#103-enqueue_to_backlog" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>首先从远端 CPU 的 &lt;code>struct softnet_data&lt;/code> 变量获取 backlog queue 长度。如果 backlog 大于 &lt;code>netdev_max_backlog&lt;/code>，或者超过了 flow limit，直接 drop，并更新 &lt;code>softnet_data&lt;/code> 的 drop 统计。注意这是远端 CPU 的统计。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>qlen = skb_queue_len(&amp;amp;sd-&amp;gt;input_pkt_queue);
if (qlen &amp;lt;= netdev_max_backlog &amp;amp;&amp;amp; !skb_flow_limit(skb, qlen)) {
if (skb_queue_len(&amp;amp;sd-&amp;gt;input_pkt_queue)) {
enqueue:
__skb_queue_tail(&amp;amp;sd-&amp;gt;input_pkt_queue, skb);
input_queue_tail_incr_save(sd, qtail);
return NET_RX_SUCCESS;
}
/* Schedule NAPI for backlog device */
if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;amp;sd-&amp;gt;backlog.state)) {
if (!rps_ipi_queued(sd))
____napi_schedule(sd, &amp;amp;sd-&amp;gt;backlog);
}
goto enqueue;
}
sd-&amp;gt;dropped++;
kfree_skb(skb);
return NET_RX_DROP;
&lt;/code>&lt;/pre>
&lt;p>&lt;code>enqueue_to_backlog&lt;/code> 被调用的地方很少。在基于 RPS 处理包的地方，以及 &lt;code>netif_rx&lt;/code>，会 调用到它。大部分驱动都不应该使用 &lt;code>netif_rx&lt;/code>，而应该是用 &lt;code>netif_receive_skb&lt;/code>。如果 你没用到 RPS，你的驱动也没有使用 &lt;code>netif_rx&lt;/code>，那增大 &lt;code>backlog&lt;/code> 并不会带来益处，因为它 根本没被用到。
注意：检查你的驱动，如果它调用了 &lt;code>netif_receive_skb&lt;/code>，而且你没用 RPS，那增大 &lt;code>netdev_max_backlog&lt;/code> 并不会带来任何性能提升，因为没有数据包会被送到 &lt;code>input_pkt_queue&lt;/code>。
如果 &lt;code>input_pkt_queue&lt;/code> 足够小，而 flow limit（后面会介绍）也还没达到（或者被禁掉了 ），那数据包将会被放到队列。这里的逻辑有点 funny，但大致可以归为为：&lt;/p>
&lt;ul>
&lt;li>如果 backlog 是空的：如果远端 CPU NAPI 变量没有运行，并且 IPI 没有被加到队列，那就 触发一个 IPI 加到队列，然后调用&lt;code>____napi_schedule&lt;/code> 进一步处理&lt;/li>
&lt;li>如果 backlog 非空，或者远端 CPU NAPI 变量正在运行，那就 enqueue 包&lt;/li>
&lt;/ul>
&lt;p>这里使用了 &lt;code>goto&lt;/code>，所以代码看起来有点 tricky。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3201-L3218">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>if (skb_queue_len(&amp;amp;sd-&amp;gt;input_pkt_queue)) {
enqueue:
__skb_queue_tail(&amp;amp;sd-&amp;gt;input_pkt_queue, skb);
input_queue_tail_incr_save(sd, qtail);
rps_unlock(sd);
local_irq_restore(flags);
return NET_RX_SUCCESS;
}
/* Schedule NAPI for backlog device
* We can use non atomic operation since we own the queue lock
*/
if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;amp;sd-&amp;gt;backlog.state)) {
if (!rps_ipi_queued(sd))
____napi_schedule(sd, &amp;amp;sd-&amp;gt;backlog);
}
goto enqueue;
&lt;/code>&lt;/pre>
&lt;h3 id="flow-limits">Flow limits&lt;a class="td-heading-self-link" href="#flow-limits" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>RPS 在不同 CPU 之间分发 packet，但是，如果一个 flow 特别大，会出现单个 CPU 被打爆，而 其他 CPU 无事可做（饥饿）的状态。因此引入了 flow limit 特性，放到一个 backlog 队列的属 于同一个 flow 的包的数量不能超过一个阈值。这可以保证即使有一个很大的 flow 在大量收包 ，小 flow 也能得到及时的处理。
检查 flow limit 的代码，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200">net/core/dev.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>if (qlen &amp;lt;= netdev_max_backlog &amp;amp;&amp;amp; !skb_flow_limit(skb, qlen)) {
&lt;/code>&lt;/pre>
&lt;p>默认，flow limit 功能是关掉的。要打开 flow limit，你需要指定一个 bitmap（类似于 RPS 的 bitmap）。&lt;/p>
&lt;h3 id="监控由于-input_pkt_queue-打满或-flow-limit-导致的丢包">监控：由于 &lt;code>input_pkt_queue&lt;/code> 打满或 flow limit 导致的丢包&lt;a class="td-heading-self-link" href="#%e7%9b%91%e6%8e%a7%e7%94%b1%e4%ba%8e-input_pkt_queue-%e6%89%93%e6%bb%a1%e6%88%96-flow-limit-%e5%af%bc%e8%87%b4%e7%9a%84%e4%b8%a2%e5%8c%85" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>在&lt;code>/proc/net/softnet_stat&lt;/code> 里面的 dropped 列计数，包含本节提到的原因导致的 drop。&lt;/p>
&lt;h3 id="调优">调优&lt;a class="td-heading-self-link" href="#%e8%b0%83%e4%bc%98" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h4 id="tuning-adjusting-netdev_max_backlog-to-prevent-drops">Tuning: Adjusting netdev_max_backlog to prevent drops&lt;a class="td-heading-self-link" href="#tuning-adjusting-netdev_max_backlog-to-prevent-drops" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>在调整这个值之前，请先阅读前面的“注意”。
如果你使用了 RPS，或者你的驱动调用了 &lt;code>netif_rx&lt;/code>，那增加 &lt;code>netdev_max_backlog&lt;/code> 可以改 善在 &lt;code>enqueue_to_backlog&lt;/code> 里的丢包：
例如：increase backlog to 3000 with sysctl.&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.netdev_max_backlog=3000
&lt;/code>&lt;/pre>
&lt;p>默认值是 1000。&lt;/p>
&lt;h4 id="tuning-adjust-the-napi-weight-of-the-backlog-poll-loop">Tuning: Adjust the NAPI weight of the backlog poll loop&lt;a class="td-heading-self-link" href="#tuning-adjust-the-napi-weight-of-the-backlog-poll-loop" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>&lt;code>net.core.dev_weight&lt;/code> 决定了 backlog poll loop 可以消耗的整体 budget（参考前面更改 &lt;code>net.core.netdev_budget&lt;/code> 的章节）：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.dev_weight=600
&lt;/code>&lt;/pre>
&lt;p>默认值是 64。
记住，backlog 处理逻辑和设备驱动的 &lt;code>poll&lt;/code> 函数类似，都是在软中断（softirq）的上下文 中执行，因此受整体 budget 和处理时间的限制，前面已经分析过了。&lt;/p>
&lt;h4 id="tuning-enabling-flow-limits-and-tuning-flow-limit-hash-table-size">Tuning: Enabling flow limits and tuning flow limit hash table size&lt;a class="td-heading-self-link" href="#tuning-enabling-flow-limits-and-tuning-flow-limit-hash-table-size" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.flow_limit_table_len=8192
&lt;/code>&lt;/pre>
&lt;p>默认值是 4096.
这只会影响新分配的 flow hash table。所以，如果你想增加 table size 的话，应该在打开 flow limit 功能之前设置这个值。
打开 flow limit 功能的方式是，在&lt;code>/proc/sys/net/core/flow_limit_cpu_bitmap&lt;/code> 中指定一 个 bitmask，和通过 bitmask 打开 RPS 的操作类似。&lt;/p>
&lt;h2 id="104-处理-backlog-队列napi-poller">10.4 处理 backlog 队列：NAPI poller&lt;a class="td-heading-self-link" href="#104-%e5%a4%84%e7%90%86-backlog-%e9%98%9f%e5%88%97napi-poller" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>每个 CPU 都有一个 backlog queue，其加入到 NAPI 变量的方式和驱动差不多，都是注册一个 &lt;code>poll&lt;/code> 方法，在软中断的上下文中处理包。此外，还提供了一个 &lt;code>weight&lt;/code>，这也和驱动类似 。
注册发生在网络系统初始化的时候。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L6952-L6955">net/core/dev.c&lt;/a>的 &lt;code>net_dev_init&lt;/code> 函数：&lt;/p>
&lt;pre>&lt;code>sd-&amp;gt;backlog.poll = process_backlog;
sd-&amp;gt;backlog.weight = weight_p;
sd-&amp;gt;backlog.gro_list = NULL;
sd-&amp;gt;backlog.gro_count = 0;
&lt;/code>&lt;/pre>
&lt;p>backlog NAPI 变量和设备驱动 NAPI 变量的不同之处在于，它的 weight 是可以调节的，而设备 驱动是 hardcode 64。在下面的调优部分，我们会看到如何用 sysctl 调整这个设置。&lt;/p>
&lt;h2 id="105-process_backlog">10.5 &lt;code>process_backlog&lt;/code>&lt;a class="td-heading-self-link" href="#105-process_backlog" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>process_backlog&lt;/code> 是一个循环，它会一直运行直至 &lt;code>weight&lt;/code>（前面介绍了）用完，或者 backlog 里没有数据了。
backlog queue 里的数据取出来，传递给&lt;code>__netif_receive_skb&lt;/code>。这个函数做的事情和 RPS 关闭的情况下做的事情一样。即，&lt;code>__netif_receive_skb&lt;/code> 做一些 bookkeeping 工作，然后调 用&lt;code>__netif_receive_skb_core&lt;/code> 将数据发送给更上面的协议层。
&lt;code>process_backlog&lt;/code> 和 NAPI 之间遵循的合约，和驱动和 NAPI 之间的合约相同：NAPI is disabled if the total weight will not be used. The poller is restarted with the call to &lt;code>____napi_schedule&lt;/code> from &lt;code>enqueue_to_backlog&lt;/code> as described above.
函数返回接收完成的数据帧数量（在代码中是变量 &lt;code>work&lt;/code>），&lt;code>net_rx_action&lt;/code>（前面介绍了 ）将会从 budget（通过 &lt;code>net.core.netdev_budget&lt;/code> 可以调整，前面介绍了）里减去这个值。&lt;/p>
&lt;h2 id="106-__netif_receive_skb_core将数据送到抓包点tap或协议层">10.6 &lt;code>__netif_receive_skb_core&lt;/code>：将数据送到抓包点（tap）或协议层&lt;a class="td-heading-self-link" href="#106-__netif_receive_skb_core%e5%b0%86%e6%95%b0%e6%8d%ae%e9%80%81%e5%88%b0%e6%8a%93%e5%8c%85%e7%82%b9tap%e6%88%96%e5%8d%8f%e8%ae%ae%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>__netif_receive_skb_core&lt;/code> 完成&lt;strong>将数据送到协议栈&lt;/strong>这一繁重工作（the heavy lifting of delivering the data)。在此之前，它会先&lt;strong>检查是否插入了 packet tap（探 测点），这些 tap 是抓包用的&lt;/strong>。例如，&lt;code>AF_PACKET&lt;/code> 地址族就可以插入这些抓包指令， 一般通过 &lt;code>libpcap&lt;/code> 库。
&lt;strong>如果存在抓包点（tap），数据就会先到抓包点，然后才到协议层。&lt;/strong>&lt;/p>
&lt;h2 id="107-送到抓包点tap">10.7 送到抓包点（tap）&lt;a class="td-heading-self-link" href="#107-%e9%80%81%e5%88%b0%e6%8a%93%e5%8c%85%e7%82%b9tap" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果有 packet tap（通常通过 &lt;code>libpcap&lt;/code>），packet 会送到那里。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>list_for_each_entry_rcu(ptype, &amp;amp;ptype_all, list) {
if (!ptype-&amp;gt;dev || ptype-&amp;gt;dev == skb-&amp;gt;dev) {
if (pt_prev)
ret = deliver_skb(skb, pt_prev, orig_dev);
pt_prev = ptype;
}
}
&lt;/code>&lt;/pre>
&lt;p>如果对 packet 如何经过 pcap 有兴趣，可以阅读 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/packet/af_packet.c">net/packet/af_packet.c&lt;/a>。&lt;/p>
&lt;h2 id="108-送到协议层">10.8 送到协议层&lt;a class="td-heading-self-link" href="#108-%e9%80%81%e5%88%b0%e5%8d%8f%e8%ae%ae%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>处理完 taps 之后，&lt;code>__netif_receive_skb_core&lt;/code> 将数据发送到协议层。它会从数据包中取出 协议信息，然后遍历注册在这个协议上的回调函数列表。
可以看&lt;code>__netif_receive_skb_core&lt;/code> 函数，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>type = skb-&amp;gt;protocol;
list_for_each_entry_rcu(ptype,
&amp;amp;ptype_base[ntohs(type) &amp;amp; PTYPE_HASH_MASK], list) {
if (ptype-&amp;gt;type == type &amp;amp;&amp;amp;
(ptype-&amp;gt;dev == null_or_dev || ptype-&amp;gt;dev == skb-&amp;gt;dev ||
ptype-&amp;gt;dev == orig_dev)) {
if (pt_prev)
ret = deliver_skb(skb, pt_prev, orig_dev);
pt_prev = ptype;
}
}
&lt;/code>&lt;/pre>
&lt;p>上面的 &lt;code>ptype_base&lt;/code> 是一个 hash table，定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L146">net/core/dev.c&lt;/a>中:&lt;/p>
&lt;pre>&lt;code>struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;
&lt;/code>&lt;/pre>
&lt;p>每种协议在上面的 hash table 的一个 slot 里，添加一个过滤器到列表里。这个列表的头用如 下函数获取：&lt;/p>
&lt;pre>&lt;code>static inline struct list_head *ptype_head(const struct packet_type *pt)
{
if (pt-&amp;gt;type == htons(ETH_P_ALL))
return &amp;amp;ptype_all;
else
return &amp;amp;ptype_base[ntohs(pt-&amp;gt;type) &amp;amp; PTYPE_HASH_MASK];
}
&lt;/code>&lt;/pre>
&lt;p>添加的时候用 &lt;code>dev_add_pack&lt;/code> 这个函数。这就是协议层如何注册自身，用于处理相应协议的 网络数据的。
现在，你已经知道了数据是如何从卡进入到协议层的了。&lt;/p>
&lt;h1 id="11-协议层注册">11 协议层注册&lt;a class="td-heading-self-link" href="#11-%e5%8d%8f%e8%ae%ae%e5%b1%82%e6%b3%a8%e5%86%8c" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>接下来我们看协议层注册自身的实现。
本文会拿 IP 层作为例子，因为它最常用，大部分读者都很熟悉。&lt;/p>
&lt;h2 id="111-ip-协议层">11.1 IP 协议层&lt;a class="td-heading-self-link" href="#111-ip-%e5%8d%8f%e8%ae%ae%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>IP 层在函数 &lt;code>inet_init&lt;/code> 中将自身注册到 &lt;code>ptype_base&lt;/code> 哈希表。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1788">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>dev_add_pack(&amp;amp;ip_packet_type);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>struct packet_type&lt;/code> 的变量 &lt;code>ip_packet_type&lt;/code> 定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1673-L1676">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static struct packet_type ip_packet_type __read_mostly = {
.type = cpu_to_be16(ETH_P_IP),
.func = ip_rcv,
};
&lt;/code>&lt;/pre>
&lt;p>&lt;code>__netif_receive_skb_core&lt;/code> 会调用 &lt;code>deliver_skb&lt;/code> (前面介绍过了), 后者会调用&lt;code>.func&lt;/code> 方法(这个例子中就是 &lt;code>ip_rcv&lt;/code>)。&lt;/p>
&lt;h3 id="1111-ip_rcv">11.1.1 &lt;code>ip_rcv&lt;/code>&lt;a class="td-heading-self-link" href="#1111-ip_rcv" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>ip_rcv&lt;/code> 方法的核心逻辑非常简单直接，此外就是一些数据合法性验证，统计计数器更新等 等。它在最后会以 netfilter 的方式调用 &lt;code>ip_rcv_finish&lt;/code> 方法。这样做的目的是，任何 iptables 规则都能在 packet 刚进入 IP 层协议的时候被应用，在其他处理之前。
我们可以在 &lt;code>ip_rcv&lt;/code> 结束的时候看到交给 netfilter 的代码： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L453-L454">net/ipv4/ip_input.c&lt;/a>&lt;/p>
&lt;pre>&lt;code>return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL, ip_rcv_finish);
&lt;/code>&lt;/pre>
&lt;h4 id="netfilter-and-iptables">netfilter and iptables&lt;a class="td-heading-self-link" href="#netfilter-and-iptables" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>这里简单介绍下 &lt;code>netfilter&lt;/code>, &lt;code>iptables&lt;/code> 和 &lt;code>conntrack&lt;/code>。
&lt;code>NF_HOOK_THRESH&lt;/code> 会检查是否有 filter 被安装，并会适时地返回到 IP 协议层，避免过深的进 入 netfilter 处理，以及在 netfilter 下面再做 hook 的 iptables 和 conntrack。
注意：&lt;strong>netfilter 或 iptables 规则都是在软中断上下文中执行的&lt;/strong>，数量很多或规则很 复杂时会导致&lt;strong>网络延迟&lt;/strong>。但如果你就是需要一些规则的话，那这个性能损失看起来是无 法避免的。&lt;/p>
&lt;h3 id="1112-ip_rcv_finish">11.1.2 &lt;code>ip_rcv_finish&lt;/code>&lt;a class="td-heading-self-link" href="#1112-ip_rcv_finish" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>netfilter 完成对数据的处理之后，就会调用 &lt;code>ip_rcv_finish&lt;/code> —— 当然，前提是 netfilter 没有丢掉这个包。
&lt;code>ip_rcv_finish&lt;/code> 开始的地方做了一个优化。为了能将包送到合适的目的地，需要一个路由 子系统的 &lt;code>dst_entry&lt;/code> 变量。为了获取这个变量，早期的代码调用了 &lt;code>early_demux&lt;/code> 函数，从 这个数据的目的端的高层协议中。
&lt;code>early_demux&lt;/code> 是一个优化项，通过检查相应的变量是否缓存在 &lt;code>socket&lt;/code> 变量上，来路由 这个包所需要的 &lt;code>dst_entry&lt;/code> 变量。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L317-L327">net/ipv4/ip_input.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>if (sysctl_ip_early_demux &amp;amp;&amp;amp; !skb_dst(skb) &amp;amp;&amp;amp; skb-&amp;gt;sk == NULL) {
const struct net_protocol *ipprot;
int protocol = iph-&amp;gt;protocol;
ipprot = rcu_dereference(inet_protos[protocol]);
if (ipprot &amp;amp;&amp;amp; ipprot-&amp;gt;early_demux) {
ipprot-&amp;gt;early_demux(skb);
/* must reload iph, skb-&amp;gt;head might have changed */
iph = ip_hdr(skb);
}
}
&lt;/code>&lt;/pre>
&lt;p>可以看到，这个函数只有在 &lt;code>sysctl_ip_early_demux&lt;/code> 为 &lt;code>true&lt;/code> 的时候才有可能被执行。默 认 &lt;code>early_demux&lt;/code> 是打开的。下一节会介绍如何关闭它，以及为什么你可能会需要关闭它。
如果这个优化打开了，但是并没有命中缓存（例如，这是第一个包），这个包就会被送到内 核的路由子系统，在那里将会计算出一个 &lt;code>dst_entry&lt;/code> 并赋给相应的字段。
路由子系统完成工作后，会更新计数器，然后调用 &lt;code>dst_input(skb)&lt;/code>，后者会进一步调用 &lt;code>dst_entry&lt;/code> 变量中的 &lt;code>input&lt;/code> 方法，这个方法是一个函数指针，有路由子系统初始化。例如 ，如果 packet 的最终目的地是本机（local system），路由子系统会将 &lt;code>ip_local_deliver&lt;/code> 赋 给 &lt;code>input&lt;/code>。&lt;/p>
&lt;h4 id="调优-打开或关闭-ip-协议的-early-demux-选项">调优: 打开或关闭 IP 协议的 early demux 选项&lt;a class="td-heading-self-link" href="#%e8%b0%83%e4%bc%98-%e6%89%93%e5%bc%80%e6%88%96%e5%85%b3%e9%97%ad-ip-%e5%8d%8f%e8%ae%ae%e7%9a%84-early-demux-%e9%80%89%e9%a1%b9" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>关闭 &lt;code>early_demux&lt;/code> 优化：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.ipv4.ip_early_demux=0
&lt;/code>&lt;/pre>
&lt;p>默认是 1，即该功能默认是打开的。
添加这个 &lt;code>sysctl&lt;/code> 开关的原因是，一些用户报告说，在某些场景下 &lt;code>early_demux&lt;/code> 优化会导 致 ~5% 左右的吞吐量下降。&lt;/p>
&lt;h3 id="1113-ip_local_deliver">11.1.3 &lt;code>ip_local_deliver&lt;/code>&lt;a class="td-heading-self-link" href="#1113-ip_local_deliver" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>回忆我们看到的 IP 协议层过程：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>ip_rcv&lt;/code> 做一些初始的 bookkeeping&lt;/li>
&lt;li>将包交给 netfilter 处理，同时还有一个回调函数，netfilter 处理完毕后会调用这个函 数&lt;/li>
&lt;li>处理结束的时候，调用 &lt;code>ip_rcv_finish&lt;/code>，将数据包送到协议栈的更上层&lt;/li>
&lt;/ol>
&lt;p>&lt;code>ip_local_deliver&lt;/code> 的逻辑与此类似： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L241-L258">net/ipv4/ip_input.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/*
* Deliver IP Packets to the higher protocol layers.
*/
int ip_local_deliver(struct sk_buff *skb)
{
/*
* Reassemble IP fragments.
*/
if (ip_is_fragment(ip_hdr(skb))) {
if (ip_defrag(skb, IP_DEFRAG_LOCAL_DELIVER))
return 0;
}
return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN, skb, skb-&amp;gt;dev, NULL,
ip_local_deliver_finish);
}
&lt;/code>&lt;/pre>
&lt;p>只要 packet 没有在 netfilter 被 drop，就会调用 &lt;code>ip_local_deliver_finish&lt;/code> 函数。&lt;/p>
&lt;h3 id="1114-ip_local_deliver_finish">11.1.4 &lt;code>ip_local_deliver_finish&lt;/code>&lt;a class="td-heading-self-link" href="#1114-ip_local_deliver_finish" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>ip_local_deliver_finish&lt;/code> 从数据包中读取协议，寻找注册在这个协议上的 &lt;code>struct net_protocol&lt;/code> 变量，并调用该变量中的回调方法。这样将包送到协议栈的更上层。&lt;/p>
&lt;h4 id="monitoring-ip-protocol-layer-statistics">Monitoring: IP protocol layer statistics&lt;a class="td-heading-self-link" href="#monitoring-ip-protocol-layer-statistics" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>读取&lt;code>/proc/net/snmp&lt;/code> 获取详细的 IP 协议统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/snmp
Ip: Forwarding DefaultTTL InReceives InHdrErrors InAddrErrors ForwDatagrams InUnknownProtos InDiscards InDelivers OutRequests OutDiscards OutNoRoutes ReasmTimeout ReasmReqds ReasmOKs ReasmFails FragOKs FragFails FragCreates
Ip: 1 64 25922988125 0 0 15771700 0 0 25898327616 22789396404 12987882 51 1 10129840 2196520 1 0 0 0
...
&lt;/code>&lt;/pre>
&lt;p>这个文件包含几个协议层的统计信息。先是 IP 层。
与这些列相关的，IP 层的统计类型都定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/snmp.h#L10-L59">include/uapi/linux/snmp.h&lt;/a>：&lt;/p>
&lt;pre>&lt;code>enum
{
IPSTATS_MIB_NUM = 0,
/* frequently written fields in fast path, kept in same cache line */
IPSTATS_MIB_INPKTS, /* InReceives */
IPSTATS_MIB_INOCTETS, /* InOctets */
IPSTATS_MIB_INDELIVERS, /* InDelivers */
IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */
IPSTATS_MIB_OUTPKTS, /* OutRequests */
IPSTATS_MIB_OUTOCTETS, /* OutOctets */
/* ... */
&lt;/code>&lt;/pre>
&lt;p>读取&lt;code>/proc/net/netstat&lt;/code> 获取更详细的 IP 层统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/netstat | grep IpExt
IpExt: InNoRoutes InTruncatedPkts InMcastPkts OutMcastPkts InBcastPkts OutBcastPkts InOctets OutOctets InMcastOctets OutMcastOctets InBcastOctets OutBcastOctets InCsumErrors InNoECTPkts InECT0Pktsu InCEPkts
IpExt: 0 0 0 0 277959 0 14568040307695 32991309088496 0 0 58649349 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>格式和&lt;code>/proc/net/snmp&lt;/code> 类似，除了每列的命字都一 &lt;code>IpExt&lt;/code> 开头之外。
一些有趣的统计：&lt;/p>
&lt;ul>
&lt;li>&lt;code>InReceives&lt;/code>: The total number of IP packets that reached ip_rcv before any data integrity checks.&lt;/li>
&lt;li>&lt;code>InHdrErrors&lt;/code>: Total number of IP packets with corrupted headers. The header was too short, too long, non-existent, had the wrong IP protocol version number, etc.&lt;/li>
&lt;li>&lt;code>InAddrErrors&lt;/code>: Total number of IP packets where the host was unreachable.&lt;/li>
&lt;li>&lt;code>ForwDatagrams&lt;/code>: Total number of IP packets that have been forwarded.&lt;/li>
&lt;li>&lt;code>InUnknownProtos&lt;/code>: Total number of IP packets with unknown or unsupported protocol specified in the header.&lt;/li>
&lt;li>&lt;code>InDiscards&lt;/code>: Total number of IP packets discarded due to memory allocation failure or checksum failure when packets are trimmed.&lt;/li>
&lt;li>&lt;code>InDelivers&lt;/code>: Total number of IP packets successfully delivered to higher protocol layers. Keep in mind that those protocol layers may drop data even if the IP layer does not.&lt;/li>
&lt;li>InCsumErrors: Total number of IP Packets with checksum errors.&lt;/li>
&lt;/ul>
&lt;p>注意这些计数分别在 IP 层的不同地方被更新。由于代码一直在更新，重复计数或者计数错 误的 bug 可能会引入。如果这些计数对你非常重要，强烈建议阅读内核的相应源码，确定 它们是在哪里更新的，以及更新的对不对，是不是有 bug 等等。&lt;/p>
&lt;h2 id="112-高层协议注册">11.2 高层协议注册&lt;a class="td-heading-self-link" href="#112-%e9%ab%98%e5%b1%82%e5%8d%8f%e8%ae%ae%e6%b3%a8%e5%86%8c" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>本文介绍 UDP 处理函数的注册过程，TCP 的注册过程与此一样，并且是在相同的时间注册的。
在 &lt;code>net/ipv4/af_inet.c&lt;/code> 中定义了 UDP、TCP 和 ICMP 协议的回调函数相关的数据结构，IP 层处 理完毕之后会调用相应的回调. From &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1526-L1547">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static const struct net_protocol tcp_protocol = {
.early_demux = tcp_v4_early_demux,
.handler = tcp_v4_rcv,
.err_handler = tcp_v4_err,
.no_policy = 1,
.netns_ok = 1,
};
static const struct net_protocol udp_protocol = {
.early_demux = udp_v4_early_demux,
.handler = udp_rcv,
.err_handler = udp_err,
.no_policy = 1,
.netns_ok = 1,
};
static const struct net_protocol icmp_protocol = {
.handler = icmp_rcv,
.err_handler = icmp_err,
.no_policy = 1,
.netns_ok = 1,
};
&lt;/code>&lt;/pre>
&lt;p>这些变量在 &lt;code>inet&lt;/code> 地址族初始化的时候被注册。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1720-L1725">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/*
* Add all the base protocols.
*/
if (inet_add_protocol(&amp;amp;icmp_protocol, IPPROTO_ICMP) &amp;lt; 0)
pr_crit(&amp;quot;%s: Cannot add ICMP protocol\n&amp;quot;, __func__);
if (inet_add_protocol(&amp;amp;udp_protocol, IPPROTO_UDP) &amp;lt; 0)
pr_crit(&amp;quot;%s: Cannot add UDP protocol\n&amp;quot;, __func__);
if (inet_add_protocol(&amp;amp;tcp_protocol, IPPROTO_TCP) &amp;lt; 0)
pr_crit(&amp;quot;%s: Cannot add TCP protocol\n&amp;quot;, __func__);
&lt;/code>&lt;/pre>
&lt;p>接下来我们详细查看 UDP 协议。上面可以看到，UDP 的回调函数是 &lt;code>udp_rcv&lt;/code>。这是从 IP 层进 入 UDP 层的入口。我们就从这里开始探索。&lt;/p>
&lt;h2 id="113-udp-协议层">11.3 UDP 协议层&lt;a class="td-heading-self-link" href="#113-udp-%e5%8d%8f%e8%ae%ae%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>UDP 协议层的实现见&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c">net/ipv4/udp.c&lt;/a>。&lt;/p>
&lt;h3 id="1131-udp_rcv">11.3.1 &lt;code>udp_rcv&lt;/code>&lt;a class="td-heading-self-link" href="#1131-udp_rcv" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这个函数只要一行，调用&lt;code>__udp4_lib_rcv&lt;/code> 接收 UDP 报文。&lt;/p>
&lt;h3 id="1132-__udp4_lib_rcv">11.3.2 &lt;code>__udp4_lib_rcv&lt;/code>&lt;a class="td-heading-self-link" href="#1132-__udp4_lib_rcv" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;code>__udp4_lib_rcv&lt;/code> 首先对包数据进行合法性检查，获取 UDP 头、UDP 数据报长度、源地址、目 标地址等信息。然后进行其他一些完整性检测和 checksum 验证。
回忆前面的 IP 层内容，在送到更上面一层协议（这里是 UDP）之前，会将一个 &lt;code>dst_entry&lt;/code> 会关联到 &lt;code>skb&lt;/code>。
如果对应的 &lt;code>dst_entry&lt;/code> 找到了，并且有对应的 socket，&lt;code>__udp4_lib_rcv&lt;/code> 会将 packet 放到 &lt;code>socket&lt;/code> 的接收队列：&lt;/p>
&lt;pre>&lt;code>sk = skb_steal_sock(skb);
if (sk) {
struct dst_entry *dst = skb_dst(skb);
int ret;
if (unlikely(sk-&amp;gt;sk_rx_dst != dst))
udp_sk_rx_dst_set(sk, dst);
ret = udp_queue_rcv_skb(sk, skb);
sock_put(sk);
/* a return value &amp;gt; 0 means to resubmit the input, but
* it wants the return to be -protocol, or 0
*/
if (ret &amp;gt; 0)
return -ret;
return 0;
} else {
&lt;/code>&lt;/pre>
&lt;p>如果 &lt;code>early_demux&lt;/code> 中没有关联 socket 信息，那此时会调用&lt;code>__udp4_lib_lookup_skb&lt;/code> 查找对应的 socket。
以上两种情况，最后都会将 packet 放到 socket 的接收队列：&lt;/p>
&lt;pre>&lt;code>ret = udp_queue_rcv_skb(sk, skb);
sock_put(sk);
&lt;/code>&lt;/pre>
&lt;p>如果 socket 没有找到，数据报(datagram)会被丢弃：&lt;/p>
&lt;pre>&lt;code>/* No socket. Drop packet silently, if checksum is wrong */
if (udp_lib_checksum_complete(skb))
goto csum_error;
UDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);
icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
/*
* Hmm. We got an UDP packet to a port to which we
* don't wanna listen. Ignore it.
*/
kfree_skb(skb);
return 0;
&lt;/code>&lt;/pre>
&lt;h3 id="1133-udp_queue_rcv_skb">11.3.3 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;a class="td-heading-self-link" href="#1133-udp_queue_rcv_skb" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这个函数的前面部分所做的工作：&lt;/p>
&lt;ol>
&lt;li>判断和这个数据报关联的 socket 是不是 &lt;a href="https://tools.ietf.org/html/rfc3948">encapsulation&lt;/a> socket。如果是，将 packet 送到该层的处理函数&lt;/li>
&lt;li>判断这个数据报是不是 UDP-Lite 数据报，做一些完整性检测&lt;/li>
&lt;li>验证 UDP 数据报的校验和，如果校验失败，就丢弃&lt;/li>
&lt;/ol>
&lt;p>最后，我们来到了 socket 的接收队列逻辑，判断队列是不是满了： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1548-L1549">net/ipv4/udp.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>if (sk_rcvqueues_full(sk, skb, sk-&amp;gt;sk_rcvbuf))
goto drop;
&lt;/code>&lt;/pre>
&lt;h3 id="1334-sk_rcvqueues_full">13.3.4 &lt;code>sk_rcvqueues_full&lt;/code>&lt;a class="td-heading-self-link" href="#1334-sk_rcvqueues_full" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>定义如下：&lt;/p>
&lt;pre>&lt;code>/*
* Take into account size of receive queue and backlog queue
* Do not take into account this skb truesize,
* to allow even a single big packet to come.
*/
static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb,
unsigned int limit)
{
unsigned int qsize = sk-&amp;gt;sk_backlog.len + atomic_read(&amp;amp;sk-&amp;gt;sk_rmem_alloc);
return qsize &amp;gt; limit;
}
&lt;/code>&lt;/pre>
&lt;p>Tuning these values is a bit tricky as there are many things that can be adjusted.&lt;/p>
&lt;h4 id="调优-socket-receive-queue-memory">调优: Socket receive queue memory&lt;a class="td-heading-self-link" href="#%e8%b0%83%e4%bc%98-socket-receive-queue-memory" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>上面看到，判断 socket 接收队列是否满了是和 &lt;code>sk-&amp;gt;sk_rcvbuf&lt;/code> 做比较。 这个值可以被两个 sysctl 参数控制：最大值和默认值：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.rmem_max=8388608
$ sudo sysctl -w net.core.rmem_default=8388608
&lt;/code>&lt;/pre>
&lt;p>你也可以在你的应用里调用 &lt;code>setsockopt&lt;/code> 带上 &lt;code>SO_RCVBUF&lt;/code> 来修改这个值(&lt;code>sk-&amp;gt;sk_rcvbuf&lt;/code>) ，能设置的最大值不能超过 &lt;code>net.core.rmem_max&lt;/code>。
但是，你也可以 &lt;code>setsockopt&lt;/code> 带上 &lt;code>SO_RCVBUFFORCE&lt;/code> 来覆盖 &lt;code>net.core.rmem_max&lt;/code>，但是执 行应用的用户要有 &lt;code>CAP_NET_ADMIN&lt;/code> 权限。
&lt;code>skb_set_owner_r&lt;/code> 函数设置 UDP 数据包的 owner，并会更新计数器 &lt;code>sk-&amp;gt;sk_rmem_alloc&lt;/code>。 我们接下来会看到。
&lt;code>sk_add_backlog&lt;/code> 函数会更新 &lt;code>sk-&amp;gt;sk_backlog.len&lt;/code> 计数，后面看。&lt;/p>
&lt;h3 id="1135-udp_queue_rcv_skb">11.3.5 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;a class="td-heading-self-link" href="#1135-udp_queue_rcv_skb" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>判断 queue 未满之后，就会将数据报放到里面： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1554-L1561">net/ipv4/udp.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>bh_lock_sock(sk);
if (!sock_owned_by_user(sk))
rc = __udp_queue_rcv_skb(sk, skb);
else if (sk_add_backlog(sk, skb, sk-&amp;gt;sk_rcvbuf)) {
bh_unlock_sock(sk);
goto drop;
}
bh_unlock_sock(sk);
return rc;
&lt;/code>&lt;/pre>
&lt;p>第一步先判断有没有用户空间的程序正在这个 socket 上进行系统调用。如果没有，就可以调用&lt;code>__udp_queue_rcv_skb&lt;/code> 将数据报放到接收队列；如果有，就调用 &lt;code>sk_add_backlog&lt;/code> 将它放到 backlog 队列。
当用户空间程序释放在这个 socket 上的系统调用时（通过向内核调用 &lt;code>release_sock&lt;/code>），这 个数据报就从 backlog 移动到了接收队列。&lt;/p>
&lt;h3 id="1137-__udp_queue_rcv_skb">11.3.7 &lt;code>__udp_queue_rcv_skb&lt;/code>&lt;a class="td-heading-self-link" href="#1137-__udp_queue_rcv_skb" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这个函数调用 &lt;code>sock_queue_rcv_skb&lt;/code> 将数据报送到 socket 接收队列；如果失败，更新统计计数并释放 skb。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1431-L1443">net/ipv4/udp.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>rc = sock_queue_rcv_skb(sk, skb);
if (rc &amp;lt; 0) {
int is_udplite = IS_UDPLITE(sk);
/* Note that an ENOMEM error is charged twice */
if (rc == -ENOMEM)
UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,is_udplite);
UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);
kfree_skb(skb);
trace_udp_fail_queue_rcv_skb(rc, sk);
return -1;
}
&lt;/code>&lt;/pre>
&lt;h3 id="1138-monitoring-udp-protocol-layer-statistics">11.3.8 Monitoring: UDP protocol layer statistics&lt;a class="td-heading-self-link" href="#1138-monitoring-udp-protocol-layer-statistics" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>以下文件可以获取非常有用的 UDP 统计：&lt;/p>
&lt;pre>&lt;code>/proc/net/snmp
/proc/net/udp
/proc/net/snmp
&lt;/code>&lt;/pre>
&lt;h4 id="监控-udp-协议统计procnetsnmp">监控 UDP 协议统计：&lt;code>/proc/net/snmp&lt;/code>&lt;a class="td-heading-self-link" href="#%e7%9b%91%e6%8e%a7-udp-%e5%8d%8f%e8%ae%ae%e7%bb%9f%e8%ae%a1procnetsnmp" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;pre>&lt;code>$ cat /proc/net/snmp | grep Udp\:
Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors
Udp: 16314 0 0 17161 0 0
&lt;/code>&lt;/pre>
&lt;p>Much like the detailed statistics found in this file for the IP protocol, you will need to read the protocol layer source to determine exactly when and where these values are incremented.&lt;/p>
&lt;pre>&lt;code>InDatagrams: Incremented when recvmsg was used by a userland program to read datagram. Also incremented when a UDP packet is encapsulated and sent back for processing.
NoPorts: Incremented when UDP packets arrive destined for a port where no program is listening.
InErrors: Incremented in several cases: no memory in the receive queue, when a bad checksum is seen, and if sk_add_backlog fails to add the datagram.
OutDatagrams: Incremented when a UDP packet is handed down without error to the IP protocol layer to be sent.
RcvbufErrors: Incremented when sock_queue_rcv_skb reports that no memory is available; this happens if sk-&amp;gt;sk_rmem_alloc is greater than or equal to sk-&amp;gt;sk_rcvbuf.
SndbufErrors: Incremented if the IP protocol layer reported an error when trying to send the packet and no error queue has been setup. Also incremented if no send queue space or kernel memory are available.
InCsumErrors: Incremented when a UDP checksum failure is detected. Note that in all cases I could find, InCsumErrors is incrememnted at the same time as InErrors. Thus, InErrors - InCsumErros should yield the count of memory related errors on the receive side.
&lt;/code>&lt;/pre>
&lt;h4 id="监控-udp-socket-统计procnetudp">监控 UDP socket 统计：&lt;code>/proc/net/udp&lt;/code>&lt;a class="td-heading-self-link" href="#%e7%9b%91%e6%8e%a7-udp-socket-%e7%bb%9f%e8%ae%a1procnetudp" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;pre>&lt;code>$ cat /proc/net/udp
sl local_address rem_address st tx_queue rx_queue tr tm-&amp;gt;when retrnsmt uid timeout inode ref pointer drops
515: 00000000:B346 00000000:0000 07 00000000:00000000 00:00000000 00000000 104 0 7518 2 0000000000000000 0
558: 00000000:0371 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7408 2 0000000000000000 0
588: 0100007F:038F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7511 2 0000000000000000 0
769: 00000000:0044 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7673 2 0000000000000000 0
812: 00000000:006F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7407 2 0000000000000000 0
&lt;/code>&lt;/pre>
&lt;p>The first line describes each of the fields in the lines following:&lt;/p>
&lt;ul>
&lt;li>&lt;code>sl&lt;/code>: Kernel hash slot for the socket&lt;/li>
&lt;li>&lt;code>local_address&lt;/code>: Hexadecimal local address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>rem_address&lt;/code>: Hexadecimal remote address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>st&lt;/code>: The state of the socket. Oddly enough, the UDP protocol layer seems to use some TCP socket states. In the example above, 7 is TCP_CLOSE.&lt;/li>
&lt;li>&lt;code>tx_queue&lt;/code>: The amount of memory allocated in the kernel for outgoing UDP datagrams.&lt;/li>
&lt;li>&lt;code>rx_queue&lt;/code>: The amount of memory allocated in the kernel for incoming UDP datagrams.&lt;/li>
&lt;li>&lt;code>tr&lt;/code>, tm-&amp;gt;when, retrnsmt: These fields are unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>uid&lt;/code>: The effective user id of the user who created this socket.&lt;/li>
&lt;li>&lt;code>timeout&lt;/code>: Unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>inode&lt;/code>: The inode number corresponding to this socket. You can use this to help you determine which user process has this socket open. Check /proc/[pid]/fd, which will contain symlinks to socket[:inode].&lt;/li>
&lt;li>&lt;code>ref&lt;/code>: The current reference count for the socket.&lt;/li>
&lt;li>&lt;code>pointer&lt;/code>: The memory address in the kernel of the struct sock.&lt;/li>
&lt;li>&lt;code>drops&lt;/code>: The number of datagram drops associated with this socket. Note that this does not include any drops related to sending datagrams (on corked UDP sockets or otherwise); this is only incremented in receive paths as of the kernel version examined by this blog post.&lt;/li>
&lt;/ul>
&lt;p>打印这些信息的代码见&lt;a href="https://github.com/torvalds/linux/blob/master/net/ipv4/udp.c#L2396-L2431">net/ipv4/udp.c&lt;/a>.&lt;/p>
&lt;h2 id="114-将数据放到-socket-队列">11.4 将数据放到 socket 队列&lt;a class="td-heading-self-link" href="#114-%e5%b0%86%e6%95%b0%e6%8d%ae%e6%94%be%e5%88%b0-socket-%e9%98%9f%e5%88%97" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>网络数据通过 &lt;code>sock_queue_rcv&lt;/code> 进入 socket 的接收队列。这个函数在将数据报最终送到接收 队列之前，会做几件事情：&lt;/p>
&lt;ol>
&lt;li>检查 socket 已分配的内存，如果超过了 receive buffer 的大小，丢弃这个包并更新计数&lt;/li>
&lt;li>应用 &lt;code>sk_filter&lt;/code>，这允许 BPF（Berkeley Packet Filter）过滤器在 socket 上被应用&lt;/li>
&lt;li>执行 &lt;code>sk_rmem_scedule&lt;/code>，确保有足够大的 receive buffer 接收这个数据报&lt;/li>
&lt;li>执行 &lt;code>skb_set_owner_r&lt;/code>，这会计算数据报的长度并更新 &lt;code>sk-&amp;gt;sk_rmem_alloc&lt;/code> 计数&lt;/li>
&lt;li>调用&lt;code>__skb_queue_tail&lt;/code> 将数据加到队列尾端&lt;/li>
&lt;/ol>
&lt;p>最后，所有在这个 socket 上等待数据的进程都收到一个通知通过 &lt;code>sk_data_ready&lt;/code> 通知处理 函数。
&lt;strong>这就是一个数据包从到达机器开始，依次穿过协议栈，到达 socket，最终被用户程序读取 的过程。&lt;/strong>&lt;/p>
&lt;h1 id="12-其他">12 其他&lt;a class="td-heading-self-link" href="#12-%e5%85%b6%e4%bb%96" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>还有一些值得讨论的地方，放在前面哪里都不太合适，故统一放到这里。&lt;/p>
&lt;h2 id="121-打时间戳-timestamping">12.1 打时间戳 (timestamping)&lt;a class="td-heading-self-link" href="#121-%e6%89%93%e6%97%b6%e9%97%b4%e6%88%b3-timestamping" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>前面提到，网络栈可以收集包的时间戳信息。如果使用了 RPS 功能，有相应的 &lt;code>sysctl&lt;/code> 参数 可以控制何时以及如何收集时间戳；更多关于 RPS、时间戳，以及网络栈在哪里完成这些工 作的内容，请查看前面的章节。一些网卡甚至支持在硬件上打时间戳。
如果你想看内核网络栈给收包增加了多少延迟，那这个特性非常有用。
内核&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/timestamping.txt">关于时间戳的文档&lt;/a> 非常优秀，甚至还包括一个&lt;a href="https://github.com/torvalds/linux/tree/v3.13/Documentation/networking/timestamping">示例程序和相应的 Makefile&lt;/a>，有兴趣的话可以上手试试。
使用 &lt;code>ethtool -T&lt;/code> 可以查看网卡和驱动支持哪种打时间戳方式：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -T eth0
Time stamping parameters for eth0:
Capabilities:
software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
PTP Hardware Clock: none
Hardware Transmit Timestamp Modes: none
Hardware Receive Filter Modes: none
&lt;/code>&lt;/pre>
&lt;p>从上面这个信息看，该网卡不支持硬件打时间戳。但这个系统上的软件打时间戳，仍然可以 帮助我判断内核在接收路径上到底带来多少延迟。&lt;/p>
&lt;h2 id="122-socket-低延迟选项busy-polling">12.2 socket 低延迟选项：busy polling&lt;a class="td-heading-self-link" href="#122-socket-%e4%bd%8e%e5%bb%b6%e8%bf%9f%e9%80%89%e9%a1%b9busy-polling" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>socket 有个 &lt;code>SO_BUSY_POLL&lt;/code> 选项，可以让内核在&lt;strong>阻塞式接收&lt;/strong>（blocking receive） 的时候做 busy poll。这个选项会减少延迟，但会增加 CPU 使用量和耗电量。
&lt;strong>重要提示&lt;/strong>：要使用此功能，首先要检查你的设备驱动是否支持。Linux 内核 3.13.0 的 &lt;code>igb&lt;/code> 驱动不支持，但 &lt;code>ixgbe&lt;/code> 驱动支持。如果你的驱动实现(并注册)了 &lt;code>struct net_device_ops&lt;/code>(前面介绍过了)的 &lt;code>ndo_busy_poll&lt;/code> 方法，那它就是支持 &lt;code>SO_BUSY_POLL&lt;/code>。
Intel 有一篇非常好的&lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/open-source-kernel-enhancements-paper.pdf">文章&lt;/a>介绍其原理。
对单个 socket 设置此选项，需要传一个以微秒（microsecond）为单位的时间，内核会 在这个时间内对设备驱动的接收队列做 busy poll。当在这个 socket 上触发一个阻塞式读请 求时，内核会 busy poll 来收数据。
全局设置此选项，可以修改 &lt;code>net.core.busy_poll&lt;/code> 配置（毫秒，microsecond），当 &lt;code>poll&lt;/code> 或 &lt;code>select&lt;/code> 方 法以阻塞方式调用时，busy poll 的时长就是这个值。&lt;/p>
&lt;h2 id="123-netpoll特殊网络场景支持">12.3 Netpoll：特殊网络场景支持&lt;a class="td-heading-self-link" href="#123-netpoll%e7%89%b9%e6%ae%8a%e7%bd%91%e7%bb%9c%e5%9c%ba%e6%99%af%e6%94%af%e6%8c%81" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Linux 内核提供了一种方式，在内核挂掉（crash）的时候，设备驱动仍然可以接收和发送数 据，相应的 API 被称作 &lt;code>Netpoll&lt;/code>。这个功能在一些特殊的网络场景有用途，比如最著名的两个例子： &lt;a href="http://sysprogs.com/VisualKernel/kgdboe/launch/">&lt;code>kgdb&lt;/code>&lt;/a>和 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/netconsole.txt">&lt;code>netconsole&lt;/code>&lt;/a>。
大部分驱动都支持 &lt;code>Netpoll&lt;/code> 功能。支持此功能的驱动需要实现 &lt;code>struct net_device_ops&lt;/code> 的 &lt;code>ndo_poll_controller&lt;/code> 方法（回调函数，探测驱动模块的时候注册的，前面介绍过）。
当网络设备子系统收包或发包的时候，会首先检查这个包的目的端是不是 &lt;code>netpoll&lt;/code>。
例如，我们来看下&lt;code>__netif_receive_skb_core&lt;/code>，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3511-L3514">net/dev/core.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
{
/* ... */
/* if we've gotten here through NAPI, check netpoll */
if (netpoll_receive_skb(skb))
goto out;
/* ... */
}
&lt;/code>&lt;/pre>
&lt;p>设备驱动收发包相关代码里，关于 &lt;code>netpoll&lt;/code> 的判断逻辑在很前面。
Netpoll API 的消费者可以通过 &lt;code>netpoll_setup&lt;/code> 函数注册 &lt;code>struct netpoll&lt;/code> 变量，后者有收 包和发包的 hook 方法（函数指针）。
如果你对使用 Netpoll API 感兴趣，可以看看 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/netconsole.c">netconsole&lt;/a> 的&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/netconsole.c">驱动&lt;/a> ，Netpoll API 的头文件 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netpoll.h">&lt;code>include/linux/netpoll.h&lt;/code>&lt;/a> ，以及&lt;a href="http://people.redhat.com/~jmoyer/netpoll-linux_kongress-2005.pdf">这个&lt;/a>精 彩的分享。&lt;/p>
&lt;h2 id="124-so_incoming_cpu">12.4 &lt;code>SO_INCOMING_CPU&lt;/code>&lt;a class="td-heading-self-link" href="#124-so_incoming_cpu" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;code>SO_INCOMING_CPU&lt;/code> 直到 Linux 3.19 才添加, 但它非常有用，所以这里讨论一下。
使用 &lt;code>getsockopt&lt;/code> 带 &lt;code>SO_INCOMING_CPU&lt;/code> 选项，可以判断当前哪个 CPU 在处理这个 socket 的网 络包。你的应用程序可以据此将 socket 交给在期望的 CPU 上运行的线程，增加数据本地性（ data locality）和 CPU 缓存命中率。
在提出 &lt;code>SO_INCOMING_CPU&lt;/code> 的&lt;a href="https://patchwork.ozlabs.org/patch/408257/">邮件列表&lt;/a> 里有一个简单示例框架，展示在什么场景下使用这个功能。&lt;/p>
&lt;h2 id="125-dma-引擎">12.5 DMA 引擎&lt;a class="td-heading-self-link" href="#125-dma-%e5%bc%95%e6%93%8e" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>DMA engine (直接内存访问引擎)是一个硬件，允许 CPU 将&lt;strong>很大的复制操作&lt;/strong>（large copy operations）offload（下放）给它。这样 CPU 就从数据拷贝中解放出来，去做其他工作，而 拷贝就交由硬件完成。合理的使用 DMA 引擎（代码要利用到 DMA 特性）可以减少 CPU 的使用量 。
Linux 内核有一个通用的 DMA 引擎接口，DMA engine 驱动实现这个接口即可。更多关于这个接 口的信息可以查看内核源码的&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/dmaengine.txt">文档  &lt;/a>。
内核支持的 DMA 引擎很多，这里我们拿 Intel 的&lt;a href="https://en.wikipedia.org/wiki/I/O_Acceleration_Technology">IOAT DMA engine&lt;/a>为例来看一下。&lt;/p>
&lt;h3 id="intels-io-acceleration-technology-ioat">Intel’s I/O Acceleration Technology (IOAT)&lt;a class="td-heading-self-link" href="#intels-io-acceleration-technology-ioat" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>很多服务器都安装了&lt;a href="http://www.intel.com/content/www/us/en/wireless-network/accel-technology.html">Intel I/O AT bundle&lt;/a> ，其中包含了一系列性能优化相关的东西，包括一个硬件 DMA 引擎。可以查看 &lt;code>dmesg&lt;/code> 里面有 没有 &lt;code>ioatdma&lt;/code>，判断这个模块是否被加载，以及它是否找到了支持的硬件。
DMA 引擎在很多地方有用到，例如 TCP 协议栈。
Intel IOAT DMA engine 最早出现在 Linux 2.6.18，但随后 3.13.11.10 就禁用掉了，因为有一些 bug，会导致数据损坏。
&lt;code>3.13.11.10&lt;/code> 版本之前的内核默认是开启的，将来这些版本的内核如果有更新，可能也会禁用掉。&lt;/p>
&lt;h4 id="直接缓存访问-dca-direct-cache-access">直接缓存访问 (DCA, Direct cache access)&lt;a class="td-heading-self-link" href="#%e7%9b%b4%e6%8e%a5%e7%bc%93%e5%ad%98%e8%ae%bf%e9%97%ae-dca-direct-cache-access" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>&lt;a href="http://www.intel.com/content/www/us/en/wireless-network/accel-technology.html">Intel I/O AT bundle&lt;/a> 中的另一个有趣特性是直接缓存访问（DCA）。
该特性允许网络设备（通过各自的驱动）直接将网络数据放到 CPU 缓存上。至于是如何实现 的，随各家驱动而异。对于 &lt;code>igb&lt;/code> 的驱动，你可以查看 &lt;code>igb_update_dca&lt;/code> 和 &lt;code>igb_update_rx_dca&lt;/code> 这两个函数的实现。&lt;code>igb&lt;/code> 驱动使用 DCA，直接写硬件网卡的一个 寄存器。
要使用 DCA 功能，首先检查你的 BIOS 里是否打开了此功能，然后确保 &lt;code>dca&lt;/code> 模块加载了， 还要确保你的网卡和驱动支持 DCA。&lt;/p>
&lt;h4 id="monitoring-ioat-dma-engine">Monitoring IOAT DMA engine&lt;a class="td-heading-self-link" href="#monitoring-ioat-dma-engine" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>如上所说，如果你不怕数据损坏的风险，那你可以使用 &lt;code>ioatdma&lt;/code> 模块。监控上，可以看几 个 sysfs 参数。
例如，监控一个 DMA 通道（channel）总共 offload 的 &lt;code>memcpy&lt;/code> 操作次数：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/dma/dma0chan0/memcpy_count
123205655
&lt;/code>&lt;/pre>
&lt;p>类似的，一个 DMA 通道总共 offload 的字节数：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/dma/dma0chan0/bytes_transferred
131791916307
&lt;/code>&lt;/pre>
&lt;h4 id="tuning-ioat-dma-engine">Tuning IOAT DMA engine&lt;a class="td-heading-self-link" href="#tuning-ioat-dma-engine" aria-label="Heading self-link">&lt;/a>&lt;/h4>
&lt;p>IOAT DMA engine 只有在包大小超过一定的阈值之后才会使用，这个阈值叫 &lt;code>copybreak&lt;/code>。 之所以要设置阈值是因为，对于小包，设置和使用 DMA 的开销要大于其收益。
调整 DMA engine &lt;code>copybreak&lt;/code> 参数：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.ipv4.tcp_dma_copybreak=2048
&lt;/code>&lt;/pre>
&lt;p>默认值是 4096。&lt;/p>
&lt;h1 id="13-总结">13 总结&lt;a class="td-heading-self-link" href="#13-%e6%80%bb%e7%bb%93" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Linux 网络栈很复杂。
对于这样复杂的系统（以及类似的其他系统）， 如果不能在更深的层次理解它正在做什么，就不可能做监控和调优。 当遇到网络问题时，你可能会在网上搜到一些 &lt;code>sysctl.conf&lt;/code> 最优实践一类的东西，然后应 用在自己的系统上，但这并不是网络栈调优的最佳方式。
监控网络栈需要从驱动开始，逐步往上，仔细地在每一层统计网络数据。 这样你才能清楚地看出哪里有丢包（drop），哪里有收包错误（errors），然后根据导致错 误的原因做相应的配置调整。
&lt;strong>不幸的是，这项工作并没有捷径。&lt;/strong>&lt;/p>
&lt;h1 id="14-额外讨论和帮助">14 额外讨论和帮助&lt;a class="td-heading-self-link" href="#14-%e9%a2%9d%e5%a4%96%e8%ae%a8%e8%ae%ba%e5%92%8c%e5%b8%ae%e5%8a%a9" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>需要一些额外的关于网络栈的指导(navigating the network stack)？对本文有疑问，或有 相关内容本文没有提到？以上问题，都可以发邮件给&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/support@packagecloud.io">我们&lt;/a>， 以便我们知道如何提供帮助。&lt;/p>
&lt;h1 id="15-相关文章">15 相关文章&lt;a class="td-heading-self-link" href="#15-%e7%9b%b8%e5%85%b3%e6%96%87%e7%ab%a0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>如果你喜欢本文，你可能对下面这些底层技术文章也感兴趣：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">Monitoring and Tuning the Linux Networking Stack: Sending Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/">The Definitive Guide to Linux System Calls&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/02/29/how-does-strace-work/">How does strace work?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/03/14/how-does-ltrace-work/">How does ltrace work?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/03/21/apt-hash-sum-mismatch/">APT Hash sum mismatch&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2014/10/28/howto-gpg-sign-verify-deb-packages-apt-repositories/">HOWTO: GPG sign and verify deb packages and APT repositories&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2014/11/24/howto-gpg-sign-verify-rpm-packages-yum-repositories/">HOWTO: GPG sign and verify RPM packages and yum repositories&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>