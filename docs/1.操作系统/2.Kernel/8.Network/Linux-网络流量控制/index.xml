<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – _index</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</link><description>Recent content in _index on 断念梦</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Netfilter 流量控制系统</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/</guid><description/></item><item><title>Docs: Connnection Tracking(连接跟踪)</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Connnection-Tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Connnection-Tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">arthurchiao，连接跟踪：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Connection Tracking(连接跟踪系统，简称 ConnTrack、CT)&lt;/strong>，用于跟踪并且记录连接状态。CT 是 &lt;a href="docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux%20%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Linux%20%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6.md">Linux 网络流量控制&lt;/a>的基础&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617860207674-43ea3c6d-0d0f-4fac-bccb-90e752e75a47.png" alt="">
例如，上图是一台 IP 地址为 &lt;code>10.1.1.2&lt;/code> 的 Linux 机器，我们能看到这台机器上有三条 连接：&lt;/p>
&lt;ol>
&lt;li>机器访问外部 HTTP 服务的连接（目的端口 80）&lt;/li>
&lt;li>外部访问机器内 FTP 服务的连接（目的端口 21）&lt;/li>
&lt;li>机器访问外部 DNS 服务的连接（目的端口 53）&lt;/li>
&lt;/ol>
&lt;p>连接跟踪所做的事情就是发现并跟踪这些连接的状态，具体包括：&lt;/p>
&lt;ul>
&lt;li>从数据包中提取&lt;strong>元组&lt;/strong>（tuple）信息，辨别&lt;strong>数据流&lt;/strong>（flow）和对应的&lt;strong>连接&lt;/strong>（connection）&lt;/li>
&lt;li>为所有连接维护一个&lt;strong>状态数据库&lt;/strong>（conntrack table），例如连接的创建时间、发送 包数、发送字节数等等&lt;/li>
&lt;li>回收过期的连接（GC）&lt;/li>
&lt;li>为更上层的功能（例如 NAT）提供服务&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，&lt;strong>连接跟踪中所说的“连接”，概念和 TCP/IP 协议中“面向连接”（ connection oriented）的“连接”并不完全相同&lt;/strong>，简单来说：&lt;/p>
&lt;ul>
&lt;li>TCP/IP 协议中，连接是一个四层（Layer 4）的概念。
&lt;ul>
&lt;li>TCP 是有连接的，或称面向连接的（connection oriented），发送出去的包都要求对端应答（ACK），并且有重传机制&lt;/li>
&lt;li>UDP 是无连接的，发送的包无需对端应答，也没有重传机制&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CT 中，一个元组（tuple）定义的一条数据流（flow ）就表示一条连接（connection）。
&lt;ul>
&lt;li>后面会看到 UDP 甚至是 &lt;strong>ICMP 这种三层协议在 CT 中也都是有连接记录的&lt;/strong>&lt;/li>
&lt;li>但&lt;strong>不是所有协议都会被连接跟踪&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>本文中用到“连接”一词时，大部分情况下指的都是后者，即“连接跟踪”中的“连接”。&lt;/p>
&lt;h1 id="原理">原理&lt;/h1>
&lt;p>要跟踪一台机器的所有连接状态，就需要：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>拦截（或称过滤）流经这台机器的每一个数据包，并进行分析&lt;/strong>。&lt;/li>
&lt;li>根据这些信息&lt;strong>建立&lt;/strong>起这台机器上的&lt;strong>连接信息数据库&lt;/strong>（conntrack table）。&lt;/li>
&lt;li>根据拦截到的包信息，不断更新数据库&lt;/li>
&lt;/ul>
&lt;p>例如&lt;/p>
&lt;ul>
&lt;li>拦截到一个 TCP SYNC 包时，说明正在尝试建立 TCP 连接，需要创建一条新 conntrack entry 来记录这条连接&lt;/li>
&lt;li>拦截到一个属于已有 conntrack entry 的包时，需要更新这条 conntrack entry 的收发包数等统计信息&lt;/li>
&lt;/ul>
&lt;p>除了以上两点功能需求，还要考虑&lt;strong>性能问题&lt;/strong>，因为连接跟踪要对每个包进行过滤和分析 。性能问题非常重要，但不是本文重点，后面介绍实现时会进一步提及。
之外，这些功能最好还有配套的管理工具来更方便地使用。&lt;/p>
&lt;h2 id="connection-tracking-的实现">Connection Tracking 的实现&lt;/h2>
&lt;p>现在(2021 年 4 月 9 日)提到连接跟踪（conntrack），可能首先都会想到 Netfilter。但由上节讨论可知， 连接跟踪概念是独立于 Netfilter 的，&lt;strong>Netfilter 只是 Linux 内核中的一种连接跟踪实现&lt;/strong>。&lt;/p>
&lt;p>换句话说，&lt;strong>只要具备了 hook 能力，能拦截到进出主机的每个包，完全可以在此基础上自 己实现一套连接跟踪&lt;/strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861067581-3b23cb80-cd1f-4d7d-9767-57581c62233b.png" alt="">&lt;/p>
&lt;p>云原生网络方案 Cilium 在 &lt;code>1.7.4+&lt;/code> 版本就实现了这样一套独立的连接跟踪和 NAT 机制 （完备功能需要 Kernel &lt;code>4.19+&lt;/code>）。其基本原理是：&lt;/p>
&lt;ol>
&lt;li>基于 BPF hook 实现数据包的拦截功能（等价于 netfilter 里面的 hook 机制）&lt;/li>
&lt;li>在 BPF hook 的基础上，实现一套全新的 conntrack 和 NAT&lt;/li>
&lt;/ol>
&lt;p>因此，即便&lt;a href="https://github.com/cilium/cilium/issues/12879">卸载 Netfilter&lt;/a> ，也不会影响 Cilium 对 Kubernetes ClusterIP、NodePort、ExternalIPs 和 LoadBalancer 等功能的支持 [2]。
由于这套连接跟踪机制是独立于 Netfilter 的，因此它的 conntrack 和 NAT 信息也没有 存储在内核的（也就是 Netfilter 的）conntrack table 和 NAT table。所以常规的 &lt;code>conntrack/netstats/ss/lsof&lt;/code> 等工具是看不到的，要使用 Cilium 的命令，例如：&lt;/p>
&lt;pre>&lt;code>$ cilium bpf nat list
$ cilium bpf ct list global
&lt;/code>&lt;/pre>
&lt;p>配置也是独立的，需要在 Cilium 里面配置，例如命令行选项 &lt;code>--bpf-ct-tcp-max&lt;/code>。
另外，本文会多次提到连接跟踪模块和 NAT 模块独立，但&lt;strong>出于性能考虑，具体实现中 二者代码可能是有耦合的&lt;/strong>。例如 Cilium 做 conntrack 的垃圾回收（GC）时就会顺便把 NAT 里相应的 entry 回收掉，而非为 NAT 做单独的 GC。&lt;/p>
&lt;h3 id="netfilter-中的-connection-tracking">Netfilter 中的 Connection Tracking&lt;/h3>
&lt;p>详见 Netifilter 中 &lt;a href="docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux%20%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Netfilter%20%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/Connection%20Tracking(%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA)%E6%9C%BA%E5%88%B6.md">Connection Tracking(连接跟踪)机制&lt;/a>&lt;/p>
&lt;h3 id="bpf-中的-connection-tracking">BPF 中的 Connection Tracking&lt;/h3>
&lt;h2 id="connection-tracking-的应用实践">Connection Tracking 的应用实践&lt;/h2>
&lt;p>来看几个 conntrack 的具体应用。&lt;/p>
&lt;h3 id="151-网络地址转换nat">1.5.1 网络地址转换（NAT）&lt;/h3>
&lt;p>网络地址转换（NAT），名字表达的意思也比较清楚：对（数据包的）网络地址（&lt;code>IP + Port&lt;/code>）进行转换。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861112761-712ea43a-0ed4-4c94-8758-3f593fc3a1b6.png" alt="">
Fig 1.4. NAT 及其内核位置示意图
例如上图中，机器自己的 IP &lt;code>10.1.1.2&lt;/code> 是能与外部正常通信的，但 &lt;code>192.168&lt;/code> 网段是私有 IP 段，外界无法访问，也就是说源 IP 地址是 &lt;code>192.168&lt;/code> 的包，其&lt;strong>应答包是无 法回来的&lt;/strong>。因此&lt;/p>
&lt;ul>
&lt;li>当源地址为 &lt;code>192.168&lt;/code> 网段的包要出去时，机器会先将源 IP 换成机器自己的 &lt;code>10.1.1.2&lt;/code> 再发送出去；&lt;/li>
&lt;li>收到应答包时，再进行相反的转换。&lt;/li>
&lt;/ul>
&lt;p>这就是 NAT 的基本过程。
Docker 默认的 &lt;code>bridge&lt;/code> 网络模式就是这个原理 [4]。每个容器会分一个私有网段的 IP 地址，这个 IP 地址可以在宿主机内的不同容器之间通信，但容器流量出宿主机时要进行 NAT。
NAT 又可以细分为几类：&lt;/p>
&lt;ul>
&lt;li>SNAT：对源地址（source）进行转换&lt;/li>
&lt;li>DNAT：对目的地址（destination）进行转换&lt;/li>
&lt;li>Full NAT：同时对源地址和目的地址进行转换&lt;/li>
&lt;/ul>
&lt;p>以上场景属于 SNAT，将不同私有 IP 都映射成同一个“公有 IP”，以使其能访问外部网络服 务。这种场景也属于正向代理。
NAT 依赖连接跟踪的结果。连接跟踪&lt;strong>最重要的使用场景&lt;/strong>就是 NAT。&lt;/p>
&lt;h4 id="四层负载均衡l4lb">四层负载均衡（L4LB）&lt;/h4>
&lt;p>再将范围稍微延伸一点，讨论一下 NAT 模式的四层负载均衡。
四层负载均衡是根据包的四层信息（例如 &lt;code>src/dst ip, src/dst port, proto&lt;/code>）做流量分发。
VIP（Virtual IP）是四层负载均衡的一种实现方式：&lt;/p>
&lt;ul>
&lt;li>多个后端真实 IP（Real IP）挂到同一个虚拟 IP（VIP）上&lt;/li>
&lt;li>客户端过来的流量先到达 VIP，再经负载均衡算法转发给某个特定的后端 IP&lt;/li>
&lt;/ul>
&lt;p>如果在 VIP 和 Real IP 节点之间使用的 NAT 技术（也可以使用其他技术），那客户端访 问服务端时，L4LB 节点将做双向 NAT（Full NAT），数据流如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861112756-297f87b7-f40e-4886-899a-65629964fd2c.png" alt="">
Fig 1.5. L4LB: Traffic path in NAT mode [3]&lt;/p>
&lt;h3 id="152-有状态防火墙">1.5.2 有状态防火墙&lt;/h3>
&lt;p>有状态防火墙（stateful firewall）是相对于早期的&lt;strong>无状态防火墙&lt;/strong>（stateless firewall）而言的：早期防火墙只能写 &lt;code>drop syn&lt;/code> 或者 &lt;code>allow syn&lt;/code> 这种非常简单直接 的规则，&lt;strong>没有 flow 的概念&lt;/strong>，因此无法实现诸如 &lt;strong>“如果这个 ack 之前已经有 syn， 就 allow，否则 drop”&lt;/strong> 这样的规则，使用非常受限 [6]。
显然，要实现有状态防火墙，就必须记录 flow 和状态，这正是 conntrack 做的事情。
来看个更具体的防火墙应用：OpenStack 主机防火墙解决方案 —— 安全组（security group）。&lt;/p>
&lt;h4 id="openstack-安全组">OpenStack 安全组&lt;/h4>
&lt;p>简单来说，安全组实现了&lt;strong>虚拟机级别&lt;/strong>的安全隔离，具体实现是：在 node 上连接 VM 的 网络设备上做有状态防火墙。在当时，最能实现这一功能的可能就是 Netfilter/iptables。
回到宿主机内网络拓扑问题： OpenStack 使用 OVS bridge 来连接一台宿主机内的所有 VM。 如果只从网络连通性考虑，那每个 VM 应该直接连到 OVS bridge &lt;code>br-int&lt;/code>。但这里问题 就来了 [7]：&lt;/p>
&lt;ul>
&lt;li>OVS 没有 conntrack 模块，&lt;/li>
&lt;li>Linux 中有 conntrack 模块，但基于 conntrack 的防火墙&lt;strong>工作在 IP 层&lt;/strong>（L3），通过 iptables 控制，&lt;/li>
&lt;li>而 &lt;strong>OVS 是 L2 模块&lt;/strong>，无法使用 L3 模块的功能，&lt;/li>
&lt;/ul>
&lt;p>因此无法在 OVS （连接虚拟机）的设备上做防火墙。
所以，2016 之前 OpenStack 的解决方案是，在每个 OVS 和 VM 之间再加一个 Linux bridge ，如下图所示，
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861113322-0f9c4ca7-ffca-43ab-840d-78db13d23008.png" alt="">
Fig 1.6. Network topology within an OpenStack compute node, picture from &lt;a href="https://thesaitech.wordpress.com/2017/09/24/how-to-trace-the-tap-interfaces-and-linux-bridges-on-the-hypervisor-your-openstack-vm-is-on/">Sai&amp;rsquo;s Blog&lt;/a>
Linux bridge 也是 L2 模块，按道理也无法使用 iptables。但是，&lt;strong>它有一个 L2 工具 ebtables，能够跳转到 iptables&lt;/strong>，因此间接支持了 iptables，也就能用到 Netfilter/iptables 防火墙的功能。
这种 workaround 不仅丑陋、增加网络复杂性，而且会导致性能问题。因此， RedHat 在 2016 年提出了一个 OVS conntrack 方案 [7]，从那以后，才有可能干掉 Linux bridge 而仍然具备安全组的功能。&lt;/p></description></item><item><title>Docs: Linux 网络流量控制</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Network_traffic_control">Wiki-Network Traffic Control&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>：
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/lartc-qdisc-zh/#91-%E9%98%9F%E5%88%97queues%E5%92%8C%E6%8E%92%E9%98%9F%E8%A7%84%E5%88%99queueing-disciplines">[译] 《Linux 高级路由与流量控制手册（2012）》第九章：用 tc qdisc 管理 Linux 网络带宽&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://lartc.org/LARTC-zh_CN.GB2312.pdf">《Linux 高级路由与流量控制手册（2003）》 中文翻译&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在计算机网络中，&lt;strong>Traffic Control(流量控制，简称 TC)&lt;/strong> 系统可以让服务器，像路由器一样工作，这也是 &lt;a href="docs/4.%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1/SDN(%E8%BD%AF%E4%BB%B6%E5%AE%9A%E4%B9%89%E7%BD%91%E8%B7%AF).md">SDN(软件定义网路)&lt;/a> 中重要的组成部分。通过精准的流量控制，可以让服务器减少拥塞、延迟、数据包丢失；实现 NAT 功能、控制带宽、阻止入侵；等等等等。&lt;/p>
&lt;p>Traffic Control(流量控制) 在不同的语境中有不同的含义，可以表示一整套完整功能的系统、也可以表示为一种处理网络数据包的行为&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>众所周知，在互联网诞生之初都是各个高校和科研机构相互通讯，并没有网络流量控制方面的考虑和设计，TCP/IP 协议的原则是尽可能好地为所有数据流服务，不同的数据流之间是平等的。然而多年的实践表明，这种原则并不是最理想的，有些数据流应该得到特别的照顾， 比如，远程登录的交互数据流应该比数据下载有更高的优先级。&lt;/p>
&lt;p>针对不同的数据流采取不同的策略，这种可能性是存在的。并且，随着研究的发展和深入， 人们已经提出了各种不同的管理模式。&lt;a href="docs/x_%E6%A0%87%E5%87%86%E5%8C%96%E6%9C%AF%E8%AF%AD/Internet(%E4%BA%92%E8%81%94%E7%BD%91)/IETF.md">IETF&lt;/a> 已经发布了几个标准， 如综合服务(Integrated Services)、区分服务(Diferentiated Services)等。其实，Linux 内核从 2 2 开始，就已经实现了相关的 &lt;strong>Traffic Control(流量控制)&lt;/strong> 功能。&lt;/p>
&lt;p>实际上，流量控制系统可以想象成 &lt;strong>Message Queue(消息队列)&lt;/strong> 的功能。都是为了解决数据量瞬间太大导致处理不过来的问题。&lt;/p>
&lt;h1 id="traffic-control-的实现">Traffic Control 的实现&lt;/h1>
&lt;p>想要实现 Traffic Control(流量控制) 系统，通常需要以下功能中的一个或多个：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Queuing(队列)&lt;/strong> # 每个进出服务器的数据包，都排好队逐一处理。&lt;/li>
&lt;li>&lt;strong>Hook(钩子)&lt;/strong> # 也可以称为** DataPath**。用于拦截进出服务器的每个数据包，并对数据包进行处理。
&lt;ul>
&lt;li>每种实现流量控制的程序，在内核中添加的 Hook 的功能各不相同，Hook 的先后顺序也各不相同，甚至可以多个 Traffic Control 共存，然后在各自的 Hook 上处理数据包&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Connection Tracking(连接跟踪)&lt;/strong> # 每个被拦截到的数据包，都需要记录其信息以跟踪他们。&lt;/li>
&lt;/ul>
&lt;p>通过对数据包进行 &lt;strong>Queuing(排队)&lt;/strong>，我们可以决定数据的发送方式。我们只能对发送的数据进行整形。&lt;/p>
&lt;p>&lt;strong>互联网的工作机制&lt;/strong>决定了&lt;strong>接收端无法直接控制发送端的行为&lt;/strong>。这就像你家的邮箱一样：除非能联系到所有人（告诉他们未经同意不要寄信给你），否则 你无法控制别人寄多少东西过来。&lt;/p>
&lt;p>但与实际生活不同的是，互联网基于 TCP/IP 协议栈，这多少会带来一些帮助。TCP/IP 无法提前知道两台主机之间的网络带宽，因此开始时它会以越来越快的速度发送数据，直到开始出现丢包，这时它知道已经没有可用空间来存储这些待发送的包了，因此就会 降低发送速度。TCP/IP 的实际工作过程比这个更智能一点，后面会再讨论。&lt;/p>
&lt;p>这就好比你留下一半的信件在实体邮箱里不取，期望别人知道这个状况后会停止给你寄新的信件。 但不幸的是，&lt;strong>这种方式只对互联网管用，对你的实体邮箱无效&lt;/strong> :-)&lt;/p>
&lt;p>如果内网有一台路由器，你希望&lt;strong>限制某几台主机的下载速度&lt;/strong>，那你应该找到发送数据到这些主机的路由器内部的接口，然后在这些 &lt;strong>路由器内部接口&lt;/strong>上做 &lt;strong>整流&lt;/strong>（traffic shaping，流量整形）。&lt;/p>
&lt;p>此外，还要确保链路瓶颈（bottleneck of the link）也在你的控制范围内。例如，如果网卡是 100Mbps，但路由器的链路带宽是 256Kbps，那首先应该确保不要发送过多数据给路由 器，因为它扛不住。否则，&lt;strong>链路控制和带宽整形的决定权就不在主机侧而到路由器侧了&lt;/strong>。要达到限速目的，需要对**“发送队列”&lt;strong>有完全的把控（”own the queue”），这里的“发送队列”也就是&lt;/strong>整条链路上最慢的一段**（slowest link in the chain）。 幸运的是，大多数情况下这个条件都是能满足的。&lt;/p>
&lt;p>再用白话一点的描述：其实所谓的控制发送端行为，这种描述中的 发送端 是一个相对概念，在 Linux 每个 Hook 发给下一个 Hook 的时候，前一个 Hook 就是下一个 Hook 的发送端，所以，控制发送端行为，就是在第一个 Hook 收到数据包时，控制他发給下一个 Hook 或应用程序的数据包的行为。&lt;/p>
&lt;h2 id="实现流量控制系统的具体方式">实现流量控制系统的具体方式&lt;/h2>
&lt;p>流量控制系统的行为通常都是在内核中完成的，所有一般都是将将官代码直接写进内核，或者使用模块加载进内核，还有新时代的以 BPF 模式加在进内核。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux%20%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/Netfilter%20%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/Netfilter%20%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F.md">Netfilter 流量控制系统&lt;/a>
&lt;ul>
&lt;li>通过 iptables、nftables 控制 Netfilter 框架中的 Hook 行为&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux%20%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/TC%20%E6%A8%A1%E5%9D%97/TC%20%E6%A8%A1%E5%9D%97.md">TC 模块&lt;/a>
&lt;ul>
&lt;li>通过 tc 二进制程序控制 Hook 行为&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/BPF/BPF%20%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/BPF%20%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6.md">BPF 流量控制机制&lt;/a>
&lt;ul>
&lt;li>待整理，暂时不知道 Linux 中有什么会基于 BPF 的应用程序。
&lt;ul>
&lt;li>但是有一个 Cilium 程序，是基于 BPF 做的，只不过只能部署在 Kubernetes 集群中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>如下图所示，每种实现方式，都具有不同的 Hook：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1616164826770-1d929135-1194-44e1-91a9-3dd4e99c34ca.png" alt="">&lt;/p>
&lt;ul>
&lt;li>其中 Netfilter 框架具有最庞大的 Hook 以及 DataPath，上图中间带颜色的部分，基本都是 Netfilter 框架可以处理流量的地方
&lt;ul>
&lt;li>包括 prerouting、input、forward、output、postrouting 这几个默认的 Hook&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ingress(qdisc)、egress(qdisc) 属于 TC 模块的 Hook&lt;/li>
&lt;li>其他的则是 eBPF 添加的新 Hook&lt;/li>
&lt;/ul>
&lt;p>当然，随着 eBPF 的兴起，Netfilter 这冗长的流量处理过程，被历史淘汰也是必然的趋势~~~~&lt;/p>
&lt;h1 id="各种流量控制方法的区别">各种流量控制方法的区别&lt;/h1>
&lt;h2 id="kube-proxy-包转发路径">kube-proxy 包转发路径&lt;/h2>
&lt;p>从网络角度看，使用传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241861-a7af19e7-ea7f-49ec-ac8d-3ed6979e1f9e.png" alt="">
步骤：&lt;/p>
&lt;ol>
&lt;li>网卡收到一个包（通过 DMA 放到 ring-buffer）。&lt;/li>
&lt;li>包经过 XDP hook 点。&lt;/li>
&lt;li>内核&lt;strong>给包分配内存&lt;/strong>，此时才有了大家熟悉的 &lt;code>skb&lt;/code>（包的内核结构体表示），然后 送到内核协议栈。&lt;/li>
&lt;li>包经过 GRO 处理，对分片包进行重组。&lt;/li>
&lt;li>包进入 tc（traffic control）的 ingress hook。接下来，&lt;strong>所有橙色的框都是 Netfilter 处理点&lt;/strong>。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>raw&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>包经过内核的&lt;strong>连接跟踪&lt;/strong>（conntrack）模块。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>nat&lt;/code> table 的 iptables 规则。&lt;/li>
&lt;li>进行&lt;strong>路由判断&lt;/strong>（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。&lt;/li>
&lt;li>Netfilter：在 &lt;code>FORWARD&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>FORWARD&lt;/code> hook 点处理 &lt;code>filter&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>POSTROUTING&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>POSTROUTING&lt;/code> hook 点处理 &lt;code>nat&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。&lt;/li>
&lt;li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：&lt;/li>
&lt;li>发送到一个本机 veth 设备，或者一个本机 service endpoint，&lt;/li>
&lt;li>或者，如果目的 IP 是主机外，就通过网卡发出去。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>相关阅读，有助于理解以上过程：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2019-11-30-cracking-k8s-node-proxy.md%20%25%7D">Cracking Kubernetes Node Proxy (aka kube-proxy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2019-02-18-deep-dive-into-iptables-and-netfilter-arch-zh.md%20%25%7D">(译) 深入理解 iptables 和 netfilter 架构&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2020-08-05-conntrack-design-and-implementation-zh.md%20%25%7D">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">(译) 深入理解 Cilium 的 eBPF 收发包路径（datapath）&lt;/a>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>译者注。&lt;/p>
&lt;h2 id="cilium-ebpf-包转发路径">Cilium eBPF 包转发路径&lt;/h2>
&lt;p>作为对比，再来看下 Cilium eBPF 中的包转发路径：&lt;/p>
&lt;blockquote>
&lt;p>建议和 &lt;a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">(译) 深入理解 Cilium 的 eBPF 收发包路径（datapath）&lt;/a> 对照看。
译者注。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241799-94b516d7-7bf7-4b37-adf5-1c2defbac27c.png" alt="">
对比可以看出，&lt;strong>Cilium eBPF datapath 做了短路处理&lt;/strong>：从 tc ingress 直接 shortcut 到 tc egress，节省了 9 个中间步骤（总共 17 个）。更重要的是：这个 datapath &lt;strong>绕过了 整个 Netfilter 框架&lt;/strong>（橘黄色的框们），Netfilter 在大流量情况下性能是很差的。
去掉那些不用的框之后，Cilium eBPF datapath 长这样：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936242480-85cfc77b-217e-44d9-936a-b4d982cf1e7f.png" alt="">
&lt;strong>Cilium/eBPF 还能走的更远&lt;/strong>。例如，如果包的目的端是另一台主机上的 service endpoint，那你可以直接在 XDP 框中完成包的重定向（收包 &lt;code>1-&amp;gt;2&lt;/code>，在步骤 &lt;code>2&lt;/code> 中对包 进行修改，再通过 &lt;code>2-&amp;gt;1&lt;/code> 发送出去），将其发送出去，如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241746-6f9a6415-8747-49ca-9a74-cbf06c7a7be8.png" alt="">
可以看到，这种情况下包都**没有进入内核协议栈（准确地说，都没有创建 skb）**就被转 发出去了，性能可想而知。&lt;/p>
&lt;blockquote>
&lt;p>XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码，而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: TC 模块</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/TC-%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/8.Network/Linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/TC-%E6%A8%A1%E5%9D%97/</guid><description/></item></channel></rss>