<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – I_O Multiplexing(输入_输出多路复用)</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/6.Filesystem/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</link><description>Recent content in I_O Multiplexing(输入_输出多路复用) on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/6.Filesystem/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: I/O Multiplexing(输入/输出多路复用)</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/6.Filesystem/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/6.Filesystem/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/tXEfsLqdePjcPS6FKa-qzg">原文链接&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.zhihu.com/question/28594409">知乎问答&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>这是高性能、高并发系列的第三篇，承接上文&lt;a href="https://mp.weixin.qq.com/s/maTzszO4Y2dyxgm7g2YdXw">《读取文件时程序经历了什么》&lt;/a>&lt;/p>
&lt;h1 id="背景">背景&lt;a class="td-heading-self-link" href="#%e8%83%8c%e6%99%af" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="文件描述符太多了怎么办">文件描述符太多了怎么办&lt;a class="td-heading-self-link" href="#%e6%96%87%e4%bb%b6%e6%8f%8f%e8%bf%b0%e7%ac%a6%e5%a4%aa%e5%a4%9a%e4%ba%86%e6%80%8e%e4%b9%88%e5%8a%9e" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>经过了这么多的铺垫，终于到高性能、高并发这一主题了。&lt;/p>
&lt;p>从前几节我们知道，所有 I/O 操作都可以通过文件样的概念来进行，这当然包括网络通信。&lt;/p>
&lt;p>如果你是一个 web 服务器，当三次握手成功以后，我们通过调用 accept 同样会得到一个文件描述符，只不过这个文件描述符是用来进行网络通信的，通过读写该文件描述符你就可以同客户端通信。在这里为了概念上好理解，我们称之为链接描述符，通过这个描述符我们就可以读写客户端的数据了。&lt;/p>
&lt;pre>&lt;code>int conn_fd = accept(...);
&lt;/code>&lt;/pre>
&lt;p>server 的处理逻辑通常是读取客户端请求数据，然后执行某些特定逻辑：&lt;/p>
&lt;pre>&lt;code>if(read(conn_fd, request_buff) &amp;gt; 0) {
do_something(request_buff);
}
&lt;/code>&lt;/pre>
&lt;p>是不是非常简单，然而世界终归是复杂的，也不是这么简单的。&lt;/p>
&lt;p>接下来就是比较复杂的了。&lt;/p>
&lt;p>既然我们的主题是高并发，那么 server 端就不可能只和一个客户端通信，而是成千上万个客户端。这时你需要处理不再是一个描述符这么简单，而是有可能要处理成千上万个描述符。&lt;/p>
&lt;p>为了不让问题一上来就过于复杂，我们先简单化，假设只同时处理两个客户端的请求。&lt;/p>
&lt;p>有的同学可能会说，这还不简单，这样写不就行了：&lt;/p>
&lt;pre>&lt;code>if(read(socket_fd1, buff) &amp;gt; 0) { // 处理第一个
do_something();
}
if(read(socket_fd2, buff) &amp;gt; 0) {
do_something();
&lt;/code>&lt;/pre>
&lt;p>在本篇第二节中我们讨论过这是非常典型的阻塞式 I/O，如果读取第一个请求进程被阻塞而暂停运行，那么这时我们就无法处理第二个请求了，即使第二个请求的数据已经就位，这也就意味着所有其它客户端必须等待，而且通常情况下也不会只有两个客户端而是成千上万个，上万个连接也要这样串行处理吗。&lt;/p>
&lt;p>聪明的你一定会想到使用多线程，为每个请求开启一个线程，这样一个线程被阻塞不会影响到其它线程了，注意，既然是高并发，那么我们要为成千上万个请求开启成千上万个线程吗，大量创建销毁线程会严重影响系统性能。&lt;/p>
&lt;p>那么这个问题该怎么解决呢？&lt;/p>
&lt;p>&lt;strong>这里的关键点在于在进行 I/O 时，我们并不知道该文件描述对应的 I/O 设备是否是可读的、是否是可写的&lt;/strong>，在外设的不可读或不可写的状态下进行 I/O 只会导致进程阻塞被暂停运行。&lt;/p>
&lt;p>因此要优雅的解决这个问题，就要从其它角度来思考这个问题了。&lt;/p>
&lt;h3 id="不要打电话给我有需要我会打给你">不要打电话给我，有需要我会打给你&lt;a class="td-heading-self-link" href="#%e4%b8%8d%e8%a6%81%e6%89%93%e7%94%b5%e8%af%9d%e7%bb%99%e6%88%91%e6%9c%89%e9%9c%80%e8%a6%81%e6%88%91%e4%bc%9a%e6%89%93%e7%bb%99%e4%bd%a0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>大家生活中肯定会接到过推销电话，而且不止一个，一天下来接上十个八个推销电话你的身体会被掏空的。&lt;/p>
&lt;p>这个场景的关键点在于打电话的人并不知道你是不是要买东西，只能来一遍遍问你，因此一种更好的策略是不要让他们打电话给你，记下他们的电话，有需要的话打给他们。&lt;/p>
&lt;p>也就是&lt;strong>不要打电话给我，有需要我会打给你&lt;/strong>。&lt;/p>
&lt;p>在这个例子中，你，就好比内核，推销者就好比应用程序，电话号码就好比文件描述符，和你用电话沟通就好比 I/O。&lt;/p>
&lt;p>现在你应该明白了吧，处理多个文件描述符的更好方法其实就存在于推销电话中。&lt;/p>
&lt;p>因此相比上一节中我们主动通过 I/O 接口&lt;strong>主动&lt;/strong>问内核这些文件描述符对应的外设是不是已经就绪了，一种更好的方法是，我们把这些内核一股脑扔给内核，并霸气的告诉内核：“&lt;strong>我这里有 1 万个文件描述符，你替我监视着它们，有可以读写的文件描述符时你就告诉我，我好处理&lt;/strong>”。而不是弱弱的问内核：“第一个文件描述可以读写了吗？第二个文件描述符可以读写吗？第三个文件描述符可以读写了吗？”&lt;/p>
&lt;p>这样应用程序就从“繁忙”的&lt;strong>主动&lt;/strong>变为清闲的&lt;strong>被动&lt;/strong>了，反正哪些设备 ok 了内核会通知我， 能偷懒我才不要那么勤奋。&lt;/p>
&lt;p>这是一种不同的处理 I/O 的机制，同样需要起一个名字，再次祭出“弄不懂原则”，就叫 I/O 多路复用吧，这就是 I/O multiplexing。&lt;/p>
&lt;h1 id="io-多路复用io-multiplexing">I/O 多路复用，I/O multiplexing&lt;a class="td-heading-self-link" href="#io-%e5%a4%9a%e8%b7%af%e5%a4%8d%e7%94%a8io-multiplexing" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>multiplexing 一词其实多用于通信领域，为了充分利用通信线路，希望在一个信道中传输多路信号，要想在一个信道中传输多路信号就需要把这多路信号结合为一路，将多路信号组合成一个信号的设备被称为 multiplexer，显然接收方接收到这一路组合后的信号后要恢复原先的多路信号，这个设备被称为 demultiplexer，如图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/blqckm/1616167656134-de0aac96-7af9-404c-9aab-534046313177.jpeg" alt="">&lt;/p>
&lt;p>回到我们的主题。&lt;/p>
&lt;p>所谓 I/O 多路复用指的是这样一个过程：&lt;/p>
&lt;ol>
&lt;li>我们拿到了一堆文件描述符(不管是网络相关的、还是磁盘文件相关等等，任何文件描述符都可以)&lt;/li>
&lt;li>通过调用&lt;strong>某个函数&lt;/strong>告诉内核：“&lt;strong>这个函数你先不要返回，你替我监视着这些描述符，当这堆文件描述符中有可以进行 I/O 读写操作的时候你再返回&lt;/strong>”&lt;/li>
&lt;li>当调用的这个函数返回后我们就能知道哪些文件描述符可以进行 I/O 操作了。&lt;/li>
&lt;/ol>
&lt;p>那么有哪些函数可以用来进行 I/O 多路复用呢？&lt;/p>
&lt;p>在 Linux 世界中有这样三种机制可以用来进行 I/O 多路复用：&lt;/p>
&lt;ul>
&lt;li>select&lt;/li>
&lt;li>poll&lt;/li>
&lt;li>epoll&lt;/li>
&lt;/ul>
&lt;p>接下来我们就简单介绍一下牛掰的 I/O 多路复用 三剑客。本质上 select、poll、epoll 都是阻塞式 I/O，也就是我们常说的同步 I/O。&lt;/p>
&lt;h2 id="select初出茅庐">select：初出茅庐&lt;a class="td-heading-self-link" href="#select%e5%88%9d%e5%87%ba%e8%8c%85%e5%ba%90" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>在 select 这种 I/O 多路复用机制下，我们需要把想监控的文件描述集合通过函数参数的形式告诉 select，然后 select 会将这些文件描述符集合&lt;strong>拷贝&lt;/strong>到内核中，我们知道数据拷贝是有性能损耗的，因此为了减少这种数据拷贝带来的性能损耗，Linux 内核对集合的大小做了限制，并规定用户监控的文件描述集合不能超过 1024 个，同时当 select 返回后我们仅仅能知道有些文件描述符可以读写了，但是我们不知道是哪一个，因此程序员必须再遍历一边找到具体是哪个文件描述符可以读写了。&lt;/p>
&lt;p>因此，总结下来 select 有这样几个特点：&lt;/p>
&lt;ul>
&lt;li>我能照看的文件描述符数量有限，不能超过 1024 个&lt;/li>
&lt;li>用户给我的文件描述符需要拷贝的内核中&lt;/li>
&lt;li>我只能告诉你有文件描述符满足要求了，但是我不知道是哪个，你自己一个一个去找吧(遍历)&lt;/li>
&lt;/ul>
&lt;p>因此我们可以看到，select 机制的特性在高性能网络服务器动辄几万几十万并发链接的场景下无疑是低效的。&lt;/p>
&lt;h2 id="poll小有所成">poll：小有所成&lt;a class="td-heading-self-link" href="#poll%e5%b0%8f%e6%9c%89%e6%89%80%e6%88%90" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>poll 和 select 是非常相似的，poll 相对于 select 的优化仅仅在于解决了文件描述符不能超过 1024 个的限制，select 和 poll 都会随着监控的文件描述增加而出现性能下降，因此不适合高并发场景。&lt;/p>
&lt;h2 id="epoll独步天下">epoll：独步天下&lt;a class="td-heading-self-link" href="#epoll%e7%8b%ac%e6%ad%a5%e5%a4%a9%e4%b8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>在 select 面临的三个问题中，文件描述数量限制已经在 poll 中解决了，剩下的两个问题呢？&lt;/p>
&lt;p>针对第一个 epoll 使用的策略是&lt;strong>各个击破&lt;/strong>与&lt;strong>共享内存&lt;/strong>。&lt;/p>
&lt;p>实际上文件描述符集合变化的频率比较低，select 和 poll 频繁的拷贝整个集合，内核都快要烦死了，epoll 通过引入 epoll_ctl 很体贴的做到了只操作那些有变化的文件描述符，同时 epoll 和内核还成为了好朋友，共享了同一块内存，这块内存中保存的就是那些已经可读或者可写的的文件描述符集合，这样就减少了内核和程序的内存拷贝开销。&lt;/p>
&lt;p>针对第二点，epoll 使用的策略是“当小弟”。&lt;/p>
&lt;p>在 select 和 poll 机制下，进程要亲自下场去各个文件描述符上等待，任何一个文件描述可读或者可写就唤醒进程，但是进程被唤醒后也是一脸懵逼并不知道到底是哪个文件描述符可读或可写，还要再从头到尾检查一遍。&lt;/p>
&lt;p>但 epoll 就懂事多了，主动找到进程要当小弟替大哥出头。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/blqckm/1616167656131-12fb40de-2e45-48ff-8ff4-fa29ffe99d1e.jpeg" alt="">&lt;/p>
&lt;p>在这种机制下，进程不需要亲自下场了，进程只要等待在 epoll 上，epoll 代替进程去各个文件描述符上等待，当哪个文件描述符可读或者可写的时候就告诉 epoll，epoll 用小本本认真记录下来然后唤醒大哥：“进程大哥，快醒醒，你要处理的文件描述符我都记下来了”，这样进程被唤醒后就无需自己从头到尾检查一遍，因为 epoll 都已经记下来了。&lt;/p>
&lt;p>因此我们可以看到，在这种机制下，实际上利用的就是“不要打电话给我，有需要我会打给你”，这就不需要一遍一遍像孙子一样问各个文件描述符了，而是翻身做主人当大爷了，“你们那个文件描述符可读或者可写了主动报上来”，这中机制实际上就是大名鼎鼎的事件驱动，event-driven，这也是我们下一篇的主题。&lt;/p>
&lt;p>实际上在 Linux 平台，&lt;strong>epoll 基本上就是高并发的代名词&lt;/strong>。&lt;/p>
&lt;p>限于篇幅，关于 epoll 的详细使用方法就不在这里讲解了。&lt;/p>
&lt;h2 id="总结">总结&lt;a class="td-heading-self-link" href="#%e6%80%bb%e7%bb%93" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>基于一切皆文件的设计哲学，I/O 也可以通过文件的形式实现，显然高并发要与多个文件交互，这就离不开高效的 I/O 多路复用技术，本文我们详细讲解了什么是 I/O 多路复用以及使用方法，这其中以 epoll 为代表的 I/O 多路复用(基于事件驱动)技术使用非常广泛，实际上你会发现但凡涉及到高并发、高性能都能见到事件驱动的编程方法，这也是下一篇的主题。&lt;/p></description></item><item><title>Docs: 这次答应我，一举拿下 I/O 多路复用！</title><link>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/6.Filesystem/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E8%BF%99%E6%AC%A1%E7%AD%94%E5%BA%94%E6%88%91%E4%B8%80%E4%B8%BE%E6%8B%BF%E4%B8%8B-I_O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel/6.Filesystem/I_O-Multiplexing%E8%BE%93%E5%85%A5_%E8%BE%93%E5%87%BA%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E8%BF%99%E6%AC%A1%E7%AD%94%E5%BA%94%E6%88%91%E4%B8%80%E4%B8%BE%E6%8B%BF%E4%B8%8B-I_O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</guid><description>
&lt;p>参考：&lt;a href="https://mp.weixin.qq.com/s/Qpa0qXxuIM8jrBqDaXmVNA">原文链接&lt;/a>
这次，我们以最简单 socket 网络模型，一步一步的过度到 I/O 多路复用。
但我不会具体细节说到每个系统调用的参数，这方面书上肯定比我说的详细。
好了，&lt;strong>发车！&lt;/strong>
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990621920-d7609fbc-f5b6-4459-8d6e-8b9611df8b46.png" alt="">&lt;/p>
&lt;hr>
&lt;h3 id="最基本的-socket-模型">最基本的 Socket 模型&lt;a class="td-heading-self-link" href="#%e6%9c%80%e5%9f%ba%e6%9c%ac%e7%9a%84-socket-%e6%a8%a1%e5%9e%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>要想客户端和服务器能在网络中通信，那必须得使用 Socket   编程，它是进程间通信里比较特别的方式，特别之处在于它是可以跨主机间通信。
Socket   的中文名叫作插口，咋一看还挺迷惑的。事实上，双方要进行网络通信前，各自得创建一个 Socket，这相当于客户端和服务器都开了一个“口子”，双方读取和发送数据的时候，都通过这个“口子”。这样一看，是不是觉得很像弄了一根网线，一头插在客户端，一头插在服务端，然后进行通信。
创建 Socket 的时候，可以指定网络层使用的是 IPv4 还是 IPv6，传输层使用的是 TCP 还是 UDP。
UDP 的 Socket 编程相对简单些，这里我们只介绍基于 TCP 的 Socket 编程。
服务器的程序要先跑起来，然后等待客户端的连接和数据，我们先来看看服务端的 Socket 编程过程是怎样的。
服务端首先调用 &lt;code>socket()&lt;/code> 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调用 &lt;code>bind()&lt;/code> 函数，给这个 Socket 绑定一个 &lt;strong>IP 地址和端口&lt;/strong>，绑定这两个的目的是什么？&lt;/p>
&lt;ul>
&lt;li>
&lt;p>绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>绑定完 IP 地址和端口后，就可以调用 &lt;code>listen()&lt;/code> 函数进行监听，此时对应 TCP 状态图中的 &lt;code>listen&lt;/code>，如果我们要判定服务器中一个网络程序有没有启动，可以通过 &lt;code>netstate&lt;/code> 命令查看对应的端口号是否有被监听。
服务端进入了监听状态后，通过调用 &lt;code>accept()&lt;/code> 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。
那客户端是怎么发起连接的呢？客户端在创建好 Socket 后，调用 &lt;code>connect()&lt;/code> 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。
在  TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>一个是还没完全建立连接的队列，称为 &lt;strong>TCP 半连接队列&lt;/strong>，这个队列都是没有完成三次握手的连接，此时服务端处于 &lt;code>syn_rcvd&lt;/code> 的状态；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一个是一件建立连接的队列，称为 &lt;strong>TCP 全连接队列&lt;/strong>，这个队列都是完成了三次握手的连接，此时服务端处于 &lt;code>established&lt;/code> 状态；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>当 TCP 全连接队列不为空后，服务端的 &lt;code>accept()&lt;/code> 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的  Socket 返回应用程序，后续数据传输都用这个 Socket。
注意，监听的 Socket 和真正用来传数据的 Socket 是两个：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>一个叫作&lt;strong>监听 Socket&lt;/strong>；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一个叫作&lt;strong>已连接 Socket&lt;/strong>；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 &lt;code>read()&lt;/code> 和 &lt;code>write()&lt;/code> 函数来读写数据。
至此， TCP 协议的 Socket 程序的调用过程就结束了，整个过程如下图：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990622049-a792e7bb-309e-431b-8760-dc932121f5b7.webp" alt="">
看到这，不知道你有没有觉得读写 Socket   的方式，好像读写文件一样。
是的，基于 Linux 一切皆文件的理念，在内核中 Socket 也是以「文件」的形式存在的，也是有对应的文件描述符。&lt;/p>
&lt;blockquote>
&lt;p>PS : 下面会说到内核里的数据结构，不感兴趣的可以跳过这一部分，不会对后续的内容有影响。&lt;/p>
&lt;/blockquote>
&lt;p>文件描述符的作用是什么？每一个进程都有一个数据结构 &lt;code>task_struct&lt;/code>，该结构体里有一个指向「文件描述符数组」的成员指针。该数组里列出这个进程打开的所有文件的文件描述符。数组的下标是文件描述符，是一个整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，也就是说内核可以通过文件描述符找到对应打开的文件。
然后每个文件都有一个 inode，Socket 文件的 inode 指向了内核中的 Socket 结构，在这个结构体里有两个队列，分别是&lt;strong>发送队列&lt;/strong>和&lt;strong>接收队列&lt;/strong>，这个两个队列里面保存的是一个个 &lt;code>struct sk_buff&lt;/code>，用链表的组织形式串起来。
sk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。
你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。
于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 &lt;code>data&lt;/code> 的指针，比如：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb-&amp;gt;data 的值，来逐步剥离协议首部。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb-&amp;gt;data 的值来增加协议首部。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>你可以从下面这张图看到，当发送报文时，data 指针的移动过程。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990622097-8cb14955-a6de-429f-9116-0cd54a18505e.webp" alt="">&lt;/p>
&lt;hr>
&lt;h3 id="如何服务更多的用户">如何服务更多的用户？&lt;a class="td-heading-self-link" href="#%e5%a6%82%e4%bd%95%e6%9c%8d%e5%8a%a1%e6%9b%b4%e5%a4%9a%e7%9a%84%e7%94%a8%e6%88%b7" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是同步阻塞的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。
可如果我们服务器只能服务一个客户，那这样就太浪费资源了，于是我们要改进这个网络 I/O 模型，以支持更多的客户端。
在改进网络 I/O 模型前，我先来提一个问题，你知道服务器单机理论最大能连接多少个客户端？
相信你知道 TCP 连接是由四元组唯一确认的，这个四元组就是：&lt;strong>本机 IP, 本机端口, 对端 IP, 对端端口&lt;/strong>。
服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以&lt;strong>最大 TCP 连接数 = 客户端 IP 数 × 客户端端口数&lt;/strong>。
对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是&lt;strong>服务端单机最大 TCP 连接数约为 2 的 48 次方&lt;/strong>。
这个理论值相当“丰满”，但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>文件描述符&lt;/strong>，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>系统内存&lt;/strong>，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>那如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗？
并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。
从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。
不过，要想真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。&lt;/p>
&lt;hr>
&lt;h3 id="多进程模型">多进程模型&lt;a class="td-heading-self-link" href="#%e5%a4%9a%e8%bf%9b%e7%a8%8b%e6%a8%a1%e5%9e%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>基于最原始的阻塞网络 I/O， 如果服务器要支持多个客户端，其中比较传统的方式，就是使用&lt;strong>多进程模型&lt;/strong>，也就是为每个客户端分配一个进程来处理请求。
服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 &lt;code>fork()&lt;/code> 函数创建一个子进程，实际上就把父进程所有相关的东西都&lt;strong>复制&lt;/strong>一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。
这两个进程刚复制完的时候，几乎一摸一样。不过，会根据&lt;strong>返回值&lt;/strong>来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。
正因为子进程会&lt;strong>复制父进程的文件描述符&lt;/strong>，于是就可以直接使用「已连接 Socket 」和客户端通信了，
可以发现，子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」。
下面这张图描述了从连接请求到连接建立，父进程创建生子进程为客户服务。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990621984-4aa06dbc-8c34-4cc9-8b36-6026f265e4dc.webp" alt="">
另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成&lt;strong>僵尸进程&lt;/strong>，随着僵尸进程越多，会慢慢耗尽我们的系统资源。
因此，父进程要“善后”好自己的孩子，怎么善后呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 &lt;code>wait()&lt;/code> 和 &lt;code>waitpid()&lt;/code> 函数。
这种用多个进程来应付多个客户端的方式，在应对 100 个客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣。
进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。&lt;/p>
&lt;hr>
&lt;h3 id="多线程模型">多线程模型&lt;a class="td-heading-self-link" href="#%e5%a4%9a%e7%ba%bf%e7%a8%8b%e6%a8%a1%e5%9e%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>既然进程间上下文切换的“包袱”很重，那我们就搞个比较轻量级的模型来应对多用户的请求 —— &lt;strong>多线程模型&lt;/strong>。
线程是运行在进程中的一个“逻辑流”，单进程中可以运行多个线程，同进程里的线程可以共享进程的部分资源的，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时是不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。
当服务器与客户端 TCP 完成连接后，通过 &lt;code>pthread_create()&lt;/code> 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。
如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。
那么，我们可以使用&lt;strong>线程池&lt;/strong>的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 Socket 进程处理。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990621981-9b743905-ca45-40e2-adff-7d9ea8d7e49c.gif" alt="image.gif">
需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。
上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。&lt;/p>
&lt;hr>
&lt;h3 id="io-多路复用">I/O 多路复用&lt;a class="td-heading-self-link" href="#io-%e5%a4%9a%e8%b7%af%e5%a4%8d%e7%94%a8" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 &lt;strong>I/O 多路复用&lt;/strong>技术。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990621980-b0ae9e69-e0eb-4993-a7a1-3e8e131b665a.gif" alt="image.gif">
一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。
我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，&lt;strong>进程可以通过一个系统调用函数从内核中获取多个事件&lt;/strong>。
select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。
select/poll/epoll 这是三个多路复用接口，都能实现 C10K 吗？接下来，我们分别说说它们。&lt;/p>
&lt;hr>
&lt;h3 id="selectpoll">select/poll&lt;a class="td-heading-self-link" href="#selectpoll" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>select 实现多路复用的方式是，将已连接的 Socket 都放到一个&lt;strong>文件描述符集合&lt;/strong>，然后调用 select 函数将文件描述符集合&lt;strong>拷贝&lt;/strong>到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过&lt;strong>遍历&lt;/strong>文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合&lt;strong>拷贝&lt;/strong>回用户态里，然后用户态还需要再通过&lt;strong>遍历&lt;/strong>的方法找到可读或可写的 Socket，然后再对其处理。
所以，对于 select 这种方式，需要进行 &lt;strong>2 次「遍历」文件描述符集合&lt;/strong>，一次是在内核态里，一个次是在用户态里 ，而且还会发生 &lt;strong>2 次「拷贝」文件描述符集合&lt;/strong>，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。
select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 &lt;code>1024&lt;/code>，只能监听 0~1023 的文件描述符。
poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。
但是 poll 和 select 并没有太大的本质区别，&lt;strong>都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合&lt;/strong>，这种方式随着并发数上来，性能的损耗会呈指数级增长。&lt;/p>
&lt;hr>
&lt;h3 id="epoll">epoll&lt;a class="td-heading-self-link" href="#epoll" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>epoll 通过两个方面，很好解决了 select/poll 的问题。
&lt;em>第一点&lt;/em>，epoll 在内核里使用&lt;strong>红黑树来跟踪进程所有待检测的文件描述字&lt;/strong>，把需要监控的 socket 通过 &lt;code>epoll_ctl()&lt;/code> 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 &lt;code>O(logn)&lt;/code>，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。
&lt;em>第二点&lt;/em>， epoll 使用事件驱动的机制，内核里&lt;strong>维护了一个链表来记录就绪事件&lt;/strong>，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 &lt;code>epoll_wait()&lt;/code> 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。
从下图你可以看到 epoll 相关的接口作用：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990621994-518cf2ab-1b3e-4a13-8e65-7b8a35d589c1.gif" alt="image.gif">
epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，&lt;strong>epoll 被称为解决 C10K 问题的利器&lt;/strong>。
插个题外话，网上文章不少说，&lt;code>epoll_wait&lt;/code> 返回时，对于就绪的事件，epoll 使用的是共享内存的方式，即用户态和内核态都指向了就绪链表，所以就避免了内存拷贝消耗。
这是错的！看过 epoll 内核源码的都知道，&lt;strong>压根就没有使用共享内存这个玩意&lt;/strong>。你可以从下面这份代码看到， epoll&lt;em>wait 实现的内核代码中调用了 &lt;code>__put_user&lt;/code> 函数，这个函数就是将数据从内核拷贝到用户空间。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iobvql/1616990621987-74f6140d-47d5-4af3-8119-872c9656ebe2.gif" alt="image.gif">
好了，这个题外话就说到这了，我们继续！
epoll 支持两种事件触发模式，分别是**边缘触发（_edge-triggered，ET&lt;/em>）&lt;strong>和&lt;/strong>水平触发（&lt;em>level-triggered，LT&lt;/em>）**。
这两个术语还挺抽象的，其实它们的区别还是很好理解的。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，&lt;strong>服务器端只会从 epoll_wait 中苏醒一次&lt;/strong>，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，&lt;strong>服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束&lt;/strong>，目的是告诉我们有数据需要读取；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。
这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。
如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。
如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会&lt;strong>循环&lt;/strong>从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，&lt;strong>边缘触发模式一般和非阻塞 I/O 搭配使用&lt;/strong>，程序会一直执行 I/O 操作，直到系统调用（如  &lt;code>read&lt;/code>  和  &lt;code>write&lt;/code>）返回错误，错误类型为  &lt;code>EAGAIN&lt;/code>  或  &lt;code>EWOULDBLOCK&lt;/code>。
一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。
select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。
另外，使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用，Linux 手册关于 select 的内容中有如下说明：&lt;/p>
&lt;blockquote>
&lt;p>Under Linux, select() may report a socket file descriptor as &amp;ldquo;ready for reading&amp;rdquo;, while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block.&lt;/p>
&lt;/blockquote>
&lt;p>我谷歌翻译的结果：&lt;/p>
&lt;blockquote>
&lt;p>在 Linux 下，select() 可能会将一个 socket 文件描述符报告为 &amp;ldquo;准备读取&amp;rdquo;，而后续的读取块却没有。例如，当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。因此，在不应该阻塞的 socket 上使用 O_NONBLOCK 可能更安全。&lt;/p>
&lt;/blockquote>
&lt;p>简单点理解，就是&lt;strong>多路复用 API 返回的事件并不一定可读写的&lt;/strong>，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此最好搭配非阻塞 I/O，以便应对极少数的特殊情况。&lt;/p>
&lt;hr>
&lt;h3 id="总结">总结&lt;a class="td-heading-self-link" href="#%e6%80%bb%e7%bb%93" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。
比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000   个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。
为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的  I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。
select 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。
在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。
很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。
epoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。&lt;/p>
&lt;hr>
&lt;h6 id="参考资料">参考资料&lt;a class="td-heading-self-link" href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" aria-label="Heading self-link">&lt;/a>&lt;/h6>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://www.zhihu.com/question/39792257">https://www.zhihu.com/question/39792257&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://journey-c.github.io/io-multiplexing/#25-io-multiplexing">https://journey-c.github.io/io-multiplexing/#25-io-multiplexing&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://panqiincs.me/2015/08/01/io-multiplexing-with-epoll/">https://panqiincs.me/2015/08/01/io-multiplexing-with-epoll/&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h6 id="推荐阅读">推荐阅读&lt;a class="td-heading-self-link" href="#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb" aria-label="Heading self-link">&lt;/a>&lt;/h6>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&amp;amp;mid=2247484005&amp;amp;idx=1&amp;amp;sn=cb07ee1c891a7bdd0af3859543190202&amp;amp;scene=21#wechat_redirect">图解 TCP 三次握手和四次挥手&lt;/a>
&lt;a href="https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&amp;amp;mid=2247484774&amp;amp;idx=1&amp;amp;sn=fa9e67e60c5f9d2e9d2aa6ea8ab2a441&amp;amp;scene=21#wechat_redirect">图解 TCP 内核参数&lt;/a>&lt;/p>
&lt;hr></description></item></channel></rss>