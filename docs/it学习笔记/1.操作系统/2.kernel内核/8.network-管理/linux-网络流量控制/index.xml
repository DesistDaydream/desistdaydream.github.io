<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Linux 网络流量控制</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</link><description>Recent content in Linux 网络流量控制 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Connnection Tracking(连接跟踪)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/connnection-tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/connnection-tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">arthurchiao,连接跟踪：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Connection Tracking(连接跟踪系统，简称 ConnTrack、CT)&lt;/strong>，用于跟踪并且记录连接状态。CT 是&lt;a href="https://www.yuque.com/go/doc/34346416">流量控制系统&lt;/a>的基础
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617860207674-43ea3c6d-0d0f-4fac-bccb-90e752e75a47.png" alt="">
例如，上图是一台 IP 地址为 &lt;code>10.1.1.2&lt;/code> 的 Linux 机器，我们能看到这台机器上有三条 连接：&lt;/p>
&lt;ol>
&lt;li>机器访问外部 HTTP 服务的连接（目的端口 80）&lt;/li>
&lt;li>外部访问机器内 FTP 服务的连接（目的端口 21）&lt;/li>
&lt;li>机器访问外部 DNS 服务的连接（目的端口 53）&lt;/li>
&lt;/ol>
&lt;p>连接跟踪所做的事情就是发现并跟踪这些连接的状态，具体包括：&lt;/p>
&lt;ul>
&lt;li>从数据包中提取&lt;strong>元组&lt;/strong>（tuple）信息，辨别&lt;strong>数据流&lt;/strong>（flow）和对应的&lt;strong>连接&lt;/strong>（connection）&lt;/li>
&lt;li>为所有连接维护一个&lt;strong>状态数据库&lt;/strong>（conntrack table），例如连接的创建时间、发送 包数、发送字节数等等&lt;/li>
&lt;li>回收过期的连接（GC）&lt;/li>
&lt;li>为更上层的功能（例如 NAT）提供服务&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，&lt;strong>连接跟踪中所说的“连接”，概念和 TCP/IP 协议中“面向连接”（ connection oriented）的“连接”并不完全相同&lt;/strong>，简单来说：&lt;/p>
&lt;ul>
&lt;li>TCP/IP 协议中，连接是一个四层（Layer 4）的概念。
&lt;ul>
&lt;li>TCP 是有连接的，或称面向连接的（connection oriented），发送出去的包都要求对端应答（ACK），并且有重传机制&lt;/li>
&lt;li>UDP 是无连接的，发送的包无需对端应答，也没有重传机制&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CT 中，一个元组（tuple）定义的一条数据流（flow ）就表示一条连接（connection）。
&lt;ul>
&lt;li>后面会看到 UDP 甚至是 &lt;strong>ICMP 这种三层协议在 CT 中也都是有连接记录的&lt;/strong>&lt;/li>
&lt;li>但&lt;strong>不是所有协议都会被连接跟踪&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>本文中用到“连接”一词时，大部分情况下指的都是后者，即“连接跟踪”中的“连接”。&lt;/p>
&lt;h1 id="原理">原理&lt;/h1>
&lt;p>要跟踪一台机器的所有连接状态，就需要：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>拦截（或称过滤）流经这台机器的每一个数据包，并进行分析&lt;/strong>。&lt;/li>
&lt;li>根据这些信息&lt;strong>建立&lt;/strong>起这台机器上的&lt;strong>连接信息数据库&lt;/strong>（conntrack table）。&lt;/li>
&lt;li>根据拦截到的包信息，不断更新数据库&lt;/li>
&lt;/ul>
&lt;p>例如&lt;/p>
&lt;ul>
&lt;li>拦截到一个 TCP SYNC 包时，说明正在尝试建立 TCP 连接，需要创建一条新 conntrack entry 来记录这条连接&lt;/li>
&lt;li>拦截到一个属于已有 conntrack entry 的包时，需要更新这条 conntrack entry 的收发包数等统计信息&lt;/li>
&lt;/ul>
&lt;p>除了以上两点功能需求，还要考虑&lt;strong>性能问题&lt;/strong>，因为连接跟踪要对每个包进行过滤和分析 。性能问题非常重要，但不是本文重点，后面介绍实现时会进一步提及。
之外，这些功能最好还有配套的管理工具来更方便地使用。&lt;/p>
&lt;h2 id="connection-tracking-的实现">Connection Tracking 的实现&lt;/h2>
&lt;p>现在(2021 年 4 月 9 日)提到连接跟踪（conntrack），可能首先都会想到 Netfilter。但由上节讨论可知， 连接跟踪概念是独立于 Netfilter 的，&lt;strong>Netfilter 只是 Linux 内核中的一种连接跟踪实现&lt;/strong>。&lt;/p>
&lt;p>换句话说，&lt;strong>只要具备了 hook 能力，能拦截到进出主机的每个包，完全可以在此基础上自 己实现一套连接跟踪&lt;/strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861067581-3b23cb80-cd1f-4d7d-9767-57581c62233b.png" alt="">&lt;/p>
&lt;p>云原生网络方案 Cilium 在 &lt;code>1.7.4+&lt;/code> 版本就实现了这样一套独立的连接跟踪和 NAT 机制 （完备功能需要 Kernel &lt;code>4.19+&lt;/code>）。其基本原理是：&lt;/p>
&lt;ol>
&lt;li>基于 BPF hook 实现数据包的拦截功能（等价于 netfilter 里面的 hook 机制）&lt;/li>
&lt;li>在 BPF hook 的基础上，实现一套全新的 conntrack 和 NAT&lt;/li>
&lt;/ol>
&lt;p>因此，即便&lt;a href="https://github.com/cilium/cilium/issues/12879">卸载 Netfilter&lt;/a> ，也不会影响 Cilium 对 Kubernetes ClusterIP、NodePort、ExternalIPs 和 LoadBalancer 等功能的支持 [2]。
由于这套连接跟踪机制是独立于 Netfilter 的，因此它的 conntrack 和 NAT 信息也没有 存储在内核的（也就是 Netfilter 的）conntrack table 和 NAT table。所以常规的 &lt;code>conntrack/netstats/ss/lsof&lt;/code> 等工具是看不到的，要使用 Cilium 的命令，例如：&lt;/p>
&lt;pre>&lt;code>$ cilium bpf nat list
$ cilium bpf ct list global
&lt;/code>&lt;/pre>
&lt;p>配置也是独立的，需要在 Cilium 里面配置，例如命令行选项 &lt;code>--bpf-ct-tcp-max&lt;/code>。
另外，本文会多次提到连接跟踪模块和 NAT 模块独立，但&lt;strong>出于性能考虑，具体实现中 二者代码可能是有耦合的&lt;/strong>。例如 Cilium 做 conntrack 的垃圾回收（GC）时就会顺便把 NAT 里相应的 entry 回收掉，而非为 NAT 做单独的 GC。&lt;/p>
&lt;h3 id="netfilter-中的-connection-tracking">Netfilter 中的 Connection Tracking&lt;/h3>
&lt;p>详见 &lt;a href="https://www.yuque.com/go/doc/33221811">Netifilter 中 Connection Tracking 章节&lt;/a>&lt;/p>
&lt;h3 id="bpf-中的-connection-tracking">BPF 中的 Connection Tracking&lt;/h3>
&lt;h2 id="connection-tracking-的应用实践">Connection Tracking 的应用实践&lt;/h2>
&lt;p>来看几个 conntrack 的具体应用。&lt;/p>
&lt;h3 id="151-网络地址转换nat">1.5.1 网络地址转换（NAT）&lt;/h3>
&lt;p>网络地址转换（NAT），名字表达的意思也比较清楚：对（数据包的）网络地址（&lt;code>IP + Port&lt;/code>）进行转换。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861112761-712ea43a-0ed4-4c94-8758-3f593fc3a1b6.png" alt="">
Fig 1.4. NAT 及其内核位置示意图
例如上图中，机器自己的 IP &lt;code>10.1.1.2&lt;/code> 是能与外部正常通信的，但 &lt;code>192.168&lt;/code> 网段是私有 IP 段，外界无法访问，也就是说源 IP 地址是 &lt;code>192.168&lt;/code> 的包，其&lt;strong>应答包是无 法回来的&lt;/strong>。因此&lt;/p>
&lt;ul>
&lt;li>当源地址为 &lt;code>192.168&lt;/code> 网段的包要出去时，机器会先将源 IP 换成机器自己的 &lt;code>10.1.1.2&lt;/code> 再发送出去；&lt;/li>
&lt;li>收到应答包时，再进行相反的转换。&lt;/li>
&lt;/ul>
&lt;p>这就是 NAT 的基本过程。
Docker 默认的 &lt;code>bridge&lt;/code> 网络模式就是这个原理 [4]。每个容器会分一个私有网段的 IP 地址，这个 IP 地址可以在宿主机内的不同容器之间通信，但容器流量出宿主机时要进行 NAT。
NAT 又可以细分为几类：&lt;/p>
&lt;ul>
&lt;li>SNAT：对源地址（source）进行转换&lt;/li>
&lt;li>DNAT：对目的地址（destination）进行转换&lt;/li>
&lt;li>Full NAT：同时对源地址和目的地址进行转换&lt;/li>
&lt;/ul>
&lt;p>以上场景属于 SNAT，将不同私有 IP 都映射成同一个“公有 IP”，以使其能访问外部网络服 务。这种场景也属于正向代理。
NAT 依赖连接跟踪的结果。连接跟踪&lt;strong>最重要的使用场景&lt;/strong>就是 NAT。&lt;/p>
&lt;h4 id="四层负载均衡l4lb">四层负载均衡（L4LB）&lt;/h4>
&lt;p>再将范围稍微延伸一点，讨论一下 NAT 模式的四层负载均衡。
四层负载均衡是根据包的四层信息（例如 &lt;code>src/dst ip, src/dst port, proto&lt;/code>）做流量分发。
VIP（Virtual IP）是四层负载均衡的一种实现方式：&lt;/p>
&lt;ul>
&lt;li>多个后端真实 IP（Real IP）挂到同一个虚拟 IP（VIP）上&lt;/li>
&lt;li>客户端过来的流量先到达 VIP，再经负载均衡算法转发给某个特定的后端 IP&lt;/li>
&lt;/ul>
&lt;p>如果在 VIP 和 Real IP 节点之间使用的 NAT 技术（也可以使用其他技术），那客户端访 问服务端时，L4LB 节点将做双向 NAT（Full NAT），数据流如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861112756-297f87b7-f40e-4886-899a-65629964fd2c.png" alt="">
Fig 1.5. L4LB: Traffic path in NAT mode [3]&lt;/p>
&lt;h3 id="152-有状态防火墙">1.5.2 有状态防火墙&lt;/h3>
&lt;p>有状态防火墙（stateful firewall）是相对于早期的&lt;strong>无状态防火墙&lt;/strong>（stateless firewall）而言的：早期防火墙只能写 &lt;code>drop syn&lt;/code> 或者 &lt;code>allow syn&lt;/code> 这种非常简单直接 的规则，&lt;strong>没有 flow 的概念&lt;/strong>，因此无法实现诸如 &lt;strong>“如果这个 ack 之前已经有 syn， 就 allow，否则 drop”&lt;/strong> 这样的规则，使用非常受限 [6]。
显然，要实现有状态防火墙，就必须记录 flow 和状态，这正是 conntrack 做的事情。
来看个更具体的防火墙应用：OpenStack 主机防火墙解决方案 —— 安全组（security group）。&lt;/p>
&lt;h4 id="openstack-安全组">OpenStack 安全组&lt;/h4>
&lt;p>简单来说，安全组实现了&lt;strong>虚拟机级别&lt;/strong>的安全隔离，具体实现是：在 node 上连接 VM 的 网络设备上做有状态防火墙。在当时，最能实现这一功能的可能就是 Netfilter/iptables。
回到宿主机内网络拓扑问题： OpenStack 使用 OVS bridge 来连接一台宿主机内的所有 VM。 如果只从网络连通性考虑，那每个 VM 应该直接连到 OVS bridge &lt;code>br-int&lt;/code>。但这里问题 就来了 [7]：&lt;/p>
&lt;ul>
&lt;li>OVS 没有 conntrack 模块，&lt;/li>
&lt;li>Linux 中有 conntrack 模块，但基于 conntrack 的防火墙&lt;strong>工作在 IP 层&lt;/strong>（L3），通过 iptables 控制，&lt;/li>
&lt;li>而 &lt;strong>OVS 是 L2 模块&lt;/strong>，无法使用 L3 模块的功能，&lt;/li>
&lt;/ul>
&lt;p>因此无法在 OVS （连接虚拟机）的设备上做防火墙。
所以，2016 之前 OpenStack 的解决方案是，在每个 OVS 和 VM 之间再加一个 Linux bridge ，如下图所示，
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861113322-0f9c4ca7-ffca-43ab-840d-78db13d23008.png" alt="">
Fig 1.6. Network topology within an OpenStack compute node, picture from &lt;a href="https://thesaitech.wordpress.com/2017/09/24/how-to-trace-the-tap-interfaces-and-linux-bridges-on-the-hypervisor-your-openstack-vm-is-on/">Sai&amp;rsquo;s Blog&lt;/a>
Linux bridge 也是 L2 模块，按道理也无法使用 iptables。但是，&lt;strong>它有一个 L2 工具 ebtables，能够跳转到 iptables&lt;/strong>，因此间接支持了 iptables，也就能用到 Netfilter/iptables 防火墙的功能。
这种 workaround 不仅丑陋、增加网络复杂性，而且会导致性能问题。因此， RedHat 在 2016 年提出了一个 OVS conntrack 方案 [7]，从那以后，才有可能干掉 Linux bridge 而仍然具备安全组的功能。&lt;/p></description></item><item><title>Docs: Linux 网络流量控制</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Network_traffic_control">Wiki-Network Traffic Control&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>：
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/lartc-qdisc-zh/#91-%E9%98%9F%E5%88%97queues%E5%92%8C%E6%8E%92%E9%98%9F%E8%A7%84%E5%88%99queueing-disciplines">[译] 《Linux 高级路由与流量控制手册（2012）》第九章：用 tc qdisc 管理 Linux 网络带宽&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://lartc.org/LARTC-zh_CN.GB2312.pdf">《Linux 高级路由与流量控制手册（2003）》 中文翻译&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在计算机网络中，&lt;strong>Traffic Control(流量控制，简称 TC)&lt;/strong> 系统可以让服务器，像路由器一样工作，这也是 &lt;a href="https://www.yuque.com/go/doc/33218298">SDN&lt;/a> 中重要的组成部分。通过精准的流量控制，可以让服务器减少拥塞、延迟、数据包丢失；实现 NAT 功能、控制带宽、阻止入侵；等等等等。&lt;/p>
&lt;p>Traffic Control(流量控制) 在不同的语境中有不同的含义，可以表示一整套完整功能的系统、也可以表示为一种处理网络数据包的行为&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>众所周知，在互联网诞生之初都是各个高校和科研机构相互通讯，并没有网络流量控制方面的考虑和设计，TCP/IP 协议的原则是尽可能好地为所有数据流服务，不同的数据流之间是平等的。然而多年的实践表明，这种原则并不是最理想的，有些数据流应该得到特别的照顾， 比如，远程登录的交互数据流应该比数据下载有更高的优先级。&lt;/p>
&lt;p>针对不同的数据流采取不同的策略，这种可能性是存在的。并且，随着研究的发展和深入， 人们已经提出了各种不同的管理模式。&lt;a href="https://www.yuque.com/go/doc/34208492">IETF&lt;/a> 已经发布了几个标准， 如综合服务(Integrated Services)、区分服务(Diferentiated Services)等。其实，Linux 内核从 2 2 开始，就已经实现了相关的 &lt;strong>Traffic Control(流量控制)&lt;/strong> 功能。&lt;/p>
&lt;p>实际上，流量控制系统可以想象成 &lt;strong>Message Queue(消息队列)&lt;/strong> 的功能。都是为了解决数据量瞬间太大导致处理不过来的问题。&lt;/p>
&lt;h1 id="traffic-control-的实现">Traffic Control 的实现&lt;/h1>
&lt;p>想要实现 Traffic Control(流量控制) 系统，通常需要以下功能中的一个或多个：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Queuing(队列)&lt;/strong> # 每个进出服务器的数据包，都排好队逐一处理。&lt;/li>
&lt;li>&lt;strong>Hook(钩子)&lt;/strong> # 也可以称为** DataPath**。用于拦截进出服务器的每个数据包，并对数据包进行处理。
&lt;ul>
&lt;li>每种实现流量控制的程序，在内核中添加的 Hook 的功能各不相同，Hook 的先后顺序也各不相同，甚至可以多个 Traffic Control 共存，然后在各自的 Hook 上处理数据包&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Connection Tracking(连接跟踪)&lt;/strong> # 每个被拦截到的数据包，都需要记录其信息以跟踪他们。&lt;/li>
&lt;/ul>
&lt;p>通过对数据包进行 &lt;strong>Queuing(排队)&lt;/strong>，我们可以决定数据的发送方式。我们只能对发送的数据进行整形。&lt;/p>
&lt;p>&lt;strong>互联网的工作机制&lt;/strong>决定了&lt;strong>接收端无法直接控制发送端的行为&lt;/strong>。这就像你家的邮箱一样：除非能联系到所有人（告诉他们未经同意不要寄信给你），否则 你无法控制别人寄多少东西过来。&lt;/p>
&lt;p>但与实际生活不同的是，互联网基于 TCP/IP 协议栈，这多少会带来一些帮助。TCP/IP 无法提前知道两台主机之间的网络带宽，因此开始时它会以越来越快的速度发送数据，直到开始出现丢包，这时它知道已经没有可用空间来存储这些待发送的包了，因此就会 降低发送速度。TCP/IP 的实际工作过程比这个更智能一点，后面会再讨论。&lt;/p>
&lt;p>这就好比你留下一半的信件在实体邮箱里不取，期望别人知道这个状况后会停止给你寄新的信件。 但不幸的是，&lt;strong>这种方式只对互联网管用，对你的实体邮箱无效&lt;/strong> :-)&lt;/p>
&lt;p>如果内网有一台路由器，你希望&lt;strong>限制某几台主机的下载速度&lt;/strong>，那你应该找到发送数据到这些主机的路由器内部的接口，然后在这些 &lt;strong>路由器内部接口&lt;/strong>上做 &lt;strong>整流&lt;/strong>（traffic shaping，流量整形）。&lt;/p>
&lt;p>此外，还要确保链路瓶颈（bottleneck of the link）也在你的控制范围内。例如，如果网卡是 100Mbps，但路由器的链路带宽是 256Kbps，那首先应该确保不要发送过多数据给路由 器，因为它扛不住。否则，&lt;strong>链路控制和带宽整形的决定权就不在主机侧而到路由器侧了&lt;/strong>。要达到限速目的，需要对**“发送队列”&lt;strong>有完全的把控（”own the queue”），这里的“发送队列”也就是&lt;/strong>整条链路上最慢的一段**（slowest link in the chain）。 幸运的是，大多数情况下这个条件都是能满足的。&lt;/p>
&lt;p>再用白话一点的描述：其实所谓的控制发送端行为，这种描述中的 发送端 是一个相对概念，在 Linux 每个 Hook 发给下一个 Hook 的时候，前一个 Hook 就是下一个 Hook 的发送端，所以，控制发送端行为，就是在第一个 Hook 收到数据包时，控制他发給下一个 Hook 或应用程序的数据包的行为。&lt;/p>
&lt;h2 id="实现流量控制系统的具体方式">实现流量控制系统的具体方式&lt;/h2>
&lt;p>流量控制系统的行为通常都是在内核中完成的，所有一般都是将将官代码直接写进内核，或者使用模块加载进内核，还有新时代的以 BPF 模式加在进内核。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/34346353">&lt;strong>Netfilter 框架&lt;/strong>&lt;/a>
&lt;ul>
&lt;li>通过 iptables、nftables 控制 Netfilter 框架中的 Hook 行为&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/34380573">&lt;strong>tc 模块&lt;/strong>&lt;/a>
&lt;ul>
&lt;li>通过 tc 二进制程序控制 Hook 行为&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/33144610">&lt;strong>BPF 接口&lt;/strong>&lt;/a>
&lt;ul>
&lt;li>待整理，暂时不知道 Linux 中有什么会基于 BPF 的应用程序。
&lt;ul>
&lt;li>但是有一个 Cilium 程序，是基于 BPF 做的，只不过只能部署在 Kubernetes 集群中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>如下图所示，每种实现方式，都具有不同的 Hook：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1616164826770-1d929135-1194-44e1-91a9-3dd4e99c34ca.png" alt="">&lt;/p>
&lt;ul>
&lt;li>其中 Netfilter 框架具有最庞大的 Hook 以及 DataPath，上图中间带颜色的部分，基本都是 Netfilter 框架可以处理流量的地方
&lt;ul>
&lt;li>包括 prerouting、input、forward、output、postrouting 这几个默认的 Hook&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ingress(qdisc)、egress(qdisc) 属于 TC 模块的 Hook&lt;/li>
&lt;li>其他的则是 eBPF 添加的新 Hook&lt;/li>
&lt;/ul>
&lt;p>当然，随着 eBPF 的兴起，Netfilter 这冗长的流量处理过程，被历史淘汰也是必然的趋势~~~~&lt;/p>
&lt;h1 id="各种流量控制方法的区别">各种流量控制方法的区别&lt;/h1>
&lt;h2 id="kube-proxy-包转发路径">kube-proxy 包转发路径&lt;/h2>
&lt;p>从网络角度看，使用传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241861-a7af19e7-ea7f-49ec-ac8d-3ed6979e1f9e.png" alt="">
步骤：&lt;/p>
&lt;ol>
&lt;li>网卡收到一个包（通过 DMA 放到 ring-buffer）。&lt;/li>
&lt;li>包经过 XDP hook 点。&lt;/li>
&lt;li>内核&lt;strong>给包分配内存&lt;/strong>，此时才有了大家熟悉的 &lt;code>skb&lt;/code>（包的内核结构体表示），然后 送到内核协议栈。&lt;/li>
&lt;li>包经过 GRO 处理，对分片包进行重组。&lt;/li>
&lt;li>包进入 tc（traffic control）的 ingress hook。接下来，&lt;strong>所有橙色的框都是 Netfilter 处理点&lt;/strong>。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>raw&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>包经过内核的&lt;strong>连接跟踪&lt;/strong>（conntrack）模块。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>nat&lt;/code> table 的 iptables 规则。&lt;/li>
&lt;li>进行&lt;strong>路由判断&lt;/strong>（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。&lt;/li>
&lt;li>Netfilter：在 &lt;code>FORWARD&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>FORWARD&lt;/code> hook 点处理 &lt;code>filter&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>POSTROUTING&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>POSTROUTING&lt;/code> hook 点处理 &lt;code>nat&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。&lt;/li>
&lt;li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：&lt;/li>
&lt;li>发送到一个本机 veth 设备，或者一个本机 service endpoint，&lt;/li>
&lt;li>或者，如果目的 IP 是主机外，就通过网卡发出去。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>相关阅读，有助于理解以上过程：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2019-11-30-cracking-k8s-node-proxy.md%20%25%7D">Cracking Kubernetes Node Proxy (aka kube-proxy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2019-02-18-deep-dive-into-iptables-and-netfilter-arch-zh.md%20%25%7D">(译) 深入理解 iptables 和 netfilter 架构&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2020-08-05-conntrack-design-and-implementation-zh.md%20%25%7D">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">(译) 深入理解 Cilium 的 eBPF 收发包路径（datapath）&lt;/a>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>译者注。&lt;/p>
&lt;h2 id="cilium-ebpf-包转发路径">Cilium eBPF 包转发路径&lt;/h2>
&lt;p>作为对比，再来看下 Cilium eBPF 中的包转发路径：&lt;/p>
&lt;blockquote>
&lt;p>建议和 &lt;a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">(译) 深入理解 Cilium 的 eBPF 收发包路径（datapath）&lt;/a> 对照看。
译者注。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241799-94b516d7-7bf7-4b37-adf5-1c2defbac27c.png" alt="">
对比可以看出，&lt;strong>Cilium eBPF datapath 做了短路处理&lt;/strong>：从 tc ingress 直接 shortcut 到 tc egress，节省了 9 个中间步骤（总共 17 个）。更重要的是：这个 datapath &lt;strong>绕过了 整个 Netfilter 框架&lt;/strong>（橘黄色的框们），Netfilter 在大流量情况下性能是很差的。
去掉那些不用的框之后，Cilium eBPF datapath 长这样：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936242480-85cfc77b-217e-44d9-936a-b4d982cf1e7f.png" alt="">
&lt;strong>Cilium/eBPF 还能走的更远&lt;/strong>。例如，如果包的目的端是另一台主机上的 service endpoint，那你可以直接在 XDP 框中完成包的重定向（收包 &lt;code>1-&amp;gt;2&lt;/code>，在步骤 &lt;code>2&lt;/code> 中对包 进行修改，再通过 &lt;code>2-&amp;gt;1&lt;/code> 发送出去），将其发送出去，如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241746-6f9a6415-8747-49ca-9a74-cbf06c7a7be8.png" alt="">
可以看到，这种情况下包都**没有进入内核协议栈（准确地说，都没有创建 skb）**就被转 发出去了，性能可想而知。&lt;/p>
&lt;blockquote>
&lt;p>XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码，而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Netfilter 流量控制系统</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.netfilter.org/index.html">Netfilter 官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.netfilter.org/documentation/index.html">Netfilter 官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Netfilter">Wiki-Netfilter&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>：
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/">[译] 深入理解 iptables 和 netfilter 架构&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/nat-zh/">[译] NAT - 网络地址转换（2016）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="netfilter">Netfilter&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512374-db897dd5-0704-42f2-a1d8-441af05f247c.jpeg" alt="">&lt;/p>
&lt;p>Netfilter 是 Linux 操作系统核心层内部的一个数据包处理模块集合的统称。一种网络筛选系统，对数据包进入以及出去本机进行的一些控制与管理。该功能的所有模块可以通过下图所示的目录进行查找，其中还包括 ipvs 等。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512330-720231f3-a4f2-4a51-96cf-137a36724b74.jpeg" alt="">
Netfilter 项目支持如下功能&lt;/p>
&lt;ul>
&lt;li>网络地址转换(Network Address Translate)&lt;/li>
&lt;li>数据包过滤&lt;/li>
&lt;li>数据包日志记录&lt;/li>
&lt;li>用户空间数据包队列&lt;/li>
&lt;li>其他数据包处理&lt;/li>
&lt;li>等等&lt;/li>
&lt;/ul>
&lt;p>Netfilter Hooks 是 Linux 内核中的一个框架，它会让 Netfilter 的模块在 Linux 网络堆栈的不同位置注册回调函数。然后，为遍历 Linux 网络堆栈中相应 Hook 的每个数据包调用已注册的回调函数。&lt;/p>
&lt;ul>
&lt;li>用白话说：内核加入了 Netfilter 模块后，每个数据包进来之后，都会经过五个 Hooks 点来处理，以便决定每个数据包的走向。&lt;/li>
&lt;/ul>
&lt;h2 id="hooks">Hooks&lt;/h2>
&lt;p>hooks function(钩子函数) 是 Linux 网络栈中的流量检查点。所有流量通过网卡进入内核或从内核出去都会调用 Hook 函数来进行检查，并根据其规则进行过滤。Netfilter 框架中一共有 5 个 Hook，就是下文定义的“五链”。&lt;/p>
&lt;ul>
&lt;li>当一个数据包在其中一个 Hooks 中匹配到自己的规则后，则会进入下一个 Hook 寻找匹配自身的规则，直到将 5 个 Hook 挨个匹配一遍。&lt;/li>
&lt;li>可以把 Hook 想象成地铁站的闸机，通过闸机的人，就是数据流量，这个能不能从闸机过去，则看闸机对这个人身份验证的结果，是放行还是阻止&lt;/li>
&lt;/ul>
&lt;h2 id="iptabelesnftables">iptabeles/nftables&lt;/h2>
&lt;p>工作于用户空间的管理工具，对 5 个 hook 进行规则管理，iptabels 或 nftables 进程，开机后，只是把设定好的规则写进 hook 中&lt;/p>
&lt;p>Netfilter 所设置的规则是存放在内核内存中的，Iptables 是一个应用层(Ring3)的应用程序，它通过 Netfilter 放出的接口来对存放在内核内存中的 Xtables(Netfilter 的配置表)进行修改(这是一个典型的 Ring3 和 Ring0 配合的架构)&lt;/p>
&lt;h1 id="五链chain">五链(Chain)&lt;/h1>
&lt;p>把每个 Hook 上的规则都串起来类似于一条链子，所以称为链，一共 5 个 Hook，所以有 5 个 Chain。每个规则都是由“源 IP、目标 IP、端口、目标”等信息组合起来的。(i.e 对从哪来的或者到哪去的 IP 的哪个端口，要执行什么动作或‘引用什么 Chain 来对这个数据包执行什么动作’)&lt;/p>
&lt;ol>
&lt;li>&lt;strong>PREROUTING 链&lt;/strong> # 路由前，处理刚到达本机并在路由转发前的数据包。它会转换数据包中的目标 IP 地址（destination ip address），通常用于 DNAT(destination NAT)。处理完成之后分成两种情况，目的 IP 为本机网口则 INPUT，目的 IP 非本机网口则 FORWARD&lt;/li>
&lt;li>&lt;strong>INPUT 链&lt;/strong> # 进入，处理来自外部的数据。&lt;/li>
&lt;li>&lt;strong>FORWARD 链&lt;/strong> # 转发，将数据转发到本机的其他网络设备上。(需要开启 linux 的 IP 转发功能 net.ipv4.ip_forward=1 才会进入该流程；就算 ping 的是本机的其余网络设备上的 IP，也是由接收该数据包的网络设备进行回应)，FORWARD 的行为类似于路由器，系统中每个网络设备就是路由器上的每个端口，只有打开转发功能，才可以把数据包路由到其余端口上。
&lt;ol>
&lt;li>虚拟化或容器技术中，如果一台设备中有多个网段，一般都会打开转发功能，以实现不同网段路由互通的效果。&lt;/li>
&lt;li>或者服务器作为 VPN 使用时，由于不同网络设备所属网段不同，也需要打开转发功能。&lt;/li>
&lt;li>等等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>**OUTPUT 链 **# 出去，处理向外发送的数据。&lt;/li>
&lt;li>**POSTROUTING 链 **# 路由后，处理即将离开本机的数据包。它会转换数据包中的源 IP 地址（source ip address），通常用于 SNAT（source NAT）。(该路由是通过 Linux 中定义的 route 规则发送的，与内核的 ip_forward 无关)&lt;/li>
&lt;li>**自定义链 **# 用户自己定义的链，不会调用系统 Hook，而是由系统默认的 5 个链在 target 中定义引用&lt;/li>
&lt;/ol>
&lt;h2 id="规则rule匹配match规则的匹配条件匹配的用法详见iptables-框架工具介绍">规则(Rule)匹配(Match)：(规则的匹配条件)匹配的用法详见：iptables 框架工具介绍&lt;/h2>
&lt;p>规则，需要有具体的内容才能称为规则，所以 Match 就是规则中的具体内容。&lt;/p>
&lt;p>每条链上的规则，需要对流量进行匹配后才能对该流量进行相应的处理，匹配内容包括“数据包的源地址、目标地址、协议、目标等”，(e.g.这个数据使用哪个协议从哪来的到哪去的目标是什么)
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512349-f2e6f4c5-d617-4b04-a432-f9a7389120df.jpeg" alt="">
Match 功能的实现依赖于模块(类似于内核的模块)，比如右图，可以使用命令 rpm -ql iptables | grep &amp;ldquo;.so&amp;quot;查看都有哪些模块，其中的 XXX.so 就是各个功能的模块，大写字母是 target 所用的模块，小写字母是基本匹配与扩展匹配所用的模块&lt;/p>
&lt;ol>
&lt;li>基本匹配：源地址、目标地址、协议、入流网卡、出流网卡&lt;/li>
&lt;li>扩展匹配：用于对基本匹配的内容扩充，包括两类，普通的扩展匹配和基于
&lt;ol>
&lt;li>通用扩展匹配，可以直接使用。&lt;/li>
&lt;li>基于基本匹配的扩展匹配。需要有基本匹配规则才可以使用。
&lt;ol>
&lt;li>e.g.需要匹配某些端口，这类匹配必须基于 tcp 匹配规则上使用，否则无效(e.g.-p tcp -m tcp -m multiport &amp;ndash;dport22,23,24)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>目标(target)：每个规则中的目标。即在每条链上对每个进出流量匹配上之后应该执行什么动作，Target 包括以下几种
&lt;ol>
&lt;li>ACCEPT #允许流量通过&lt;/li>
&lt;li>REJECT #拒绝流量通过&lt;/li>
&lt;li>DROP #丢弃，不响应，发送方无法判断是被拒绝&lt;/li>
&lt;li>RETURN #返回调用链&lt;/li>
&lt;li>MARK #做防火墙标记&lt;/li>
&lt;li>用于 nat 表的 target
&lt;ol>
&lt;li>DNAT|SNAT #{目的|源}地址转换&lt;/li>
&lt;li>REDIRECT #端口重定向&lt;/li>
&lt;li>MASQUERADE #地址伪装类似于 SNAT，但是不用指明要转换的地址，而是自动选择要转换的地址，用于外部地址不固定的情况&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>用于 raw 表的 target
&lt;ol>
&lt;li>NOTRACK #raw 表专用的 target，用于对匹配规则进行 notrack(不跟踪)处理&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>LOG #将数据包的相关信息记录日志，执行完该目标后，会继续匹配后面的规则&lt;/li>
&lt;li>引用自定义链 #直接使用“-j 自定义链的名称”即可，让基本 5 个 Chain 上匹配成功的数据包继续执行自定义链上的规则。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>注意，这里面的路由指数据包在 Linux 本机内部路由&lt;/p>
&lt;h2 id="linux-数据包路由原理iptablesnetfilter-入门学习">Linux 数据包路由原理、Iptables/netfilter 入门学习&lt;/h2>
&lt;p>数据流处理流程简介&lt;/p>
&lt;p>注意：每个数据包在 CHAIN 中匹配到适用于自己的规则之后，则直接进入下一个 CHAIN，而不会遍历 CHAIN 中每条规则去挨个匹配适用于自己的规则。比如下面两种情况&lt;/p>
&lt;p>INPUT 链默认 DROP，匹配第一条：目的端口是 9090 的数据 DROP，然后不再检查下一项，那么 9090 无法访问&lt;/p>
&lt;pre>&lt;code>-P INPUT DROP
-A INPUT -p tcp -m tcp --dport 9090 -j DROP
-A INPUT -p tcp -m tcp --dport 9090 -j ACCEPT
&lt;/code>&lt;/pre>
&lt;p>INPUT 链默认 DROP，匹配第一条目的端口是 9090 的数据 ACCEPT，然后不再检查下一条规则，则 9090 可以访问&lt;/p>
&lt;pre>&lt;code>-P INPUT DROP
-A INPUT -p tcp -m tcp --dport 9090 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 9090 -j DROP
&lt;/code>&lt;/pre>
&lt;p>匹配条件：根据协议报文特征指定&lt;/p>
&lt;ol>
&lt;li>基本匹配条件&lt;/li>
&lt;li>扩展匹配条件&lt;/li>
&lt;/ol>
&lt;p>处理动作：&lt;/p>
&lt;ol>
&lt;li>内建处理机制&lt;/li>
&lt;li>自定义处理机制&lt;/li>
&lt;li>注意：自定义的链不会有流量经过，而是在主要的 5 链中引用自定义链上的规则，来实现对流量的处理&lt;/li>
&lt;/ol>
&lt;p>下图是从服务器外部进入网卡，再进入网络栈的数据流走向，如果直接是服务器内部服务生成的数据包进入网络栈，则不适用于该图
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512341-aeeeff06-b602-4340-bc4f-cd582144f85f.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>当一个数据包进入网卡时，数据包首先进入 PREROUTING 链，在 PREROUTING 链中我们有机会修改数据包的 DestIP(目的 IP)，然后内核的&amp;quot;路由模块&amp;quot;根据&amp;quot;数据包目的 IP&amp;quot;以及&amp;quot;内核中的路由表&amp;quot;判断是否需要转送出去(注意，这个时候数据包的 DestIP 有可能已经被我们修改过了)&lt;/li>
&lt;li>如果数据包就是进入本机的(即数据包的目的 IP 是本机的网口 IP)，数据包就会沿着图向下移动，到达 INPUT 链。数据包到达 INPUT 链后，任何进程都会收到它&lt;/li>
&lt;li>本机上运行的程序也可以发送数据包，这些数据包经过 OUTPUT 链，然后到达 POSTROTING 链输出(注意，这个时候数据包的 SrcIP 有可能已经被我们修改过了)&lt;/li>
&lt;li>如果数据包是要转发出去的(即目的 IP 地址不再当前子网中)，且内核允许转发，数据包就会向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出(选择对应子网的网口发送出去)&lt;/li>
&lt;/ol>
&lt;p>出于安全考虑，Linux 系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的 ip 地址将包发往本机另一网卡，该网卡根据路由表继续发送数据包。这通常就是路由器所要实现的功能。&lt;/p>
&lt;p>配置 Linux 系统的 ip 转发功能，首先保证硬件连通，然后打开系统的转发功能，less /proc/sys/net/ipv4/ip_forward，该文件内容为 0，表示禁止数据包转发，1 表示允许，将其修改为 1。可使用命令 echo &amp;ldquo;1&amp;rdquo; &amp;gt; /proc/sys/net/ipv4/ip_forward 修改文件内容，重启网络服务或主机后效果不再。若要其自动执行，可将命令 echo &amp;ldquo;1&amp;rdquo; &amp;gt; /proc/sys/net/ipv4/ip_forward 写入脚本/etc/rc.d/rc.local 或者 在/etc/sysconfig/network 脚本中添加 FORWARD_IPV4=&amp;ldquo;YES&amp;rdquo;&lt;/p>
&lt;h1 id="natnetwork-address-translation网络地址转换">NAT(Network Address Translation)网络地址转换&lt;/h1>
&lt;p>NAT 为了安全性而产生的，主要用来隐藏本地主机的 IP 地址&lt;/p>
&lt;h2 id="snatsource-源地址转换针对请求报文的源地址而言">SNAT：Source 源地址转换，针对请求报文的源地址而言&lt;/h2>
&lt;p>当想访问外网的时候，把源地址转换，作用于 POSTROUTING 链&lt;/p>
&lt;p>常用于内网私网地址转换成公网地址，比如家用路由器&lt;/p>
&lt;h2 id="dnatdestination-目的地址转换针对请求报文的目标地址而言">DNAT：Destination 目的地址转换，针对请求报文的目标地址而言&lt;/h2>
&lt;p>当从外部访问某 IP 时，把目的 IP 转换，作用于 PREROUTING、FORWARD 链&lt;/p>
&lt;p>把内网中的服务器发布到外网中去，&lt;/p>
&lt;p>常用于公网访问一个公司的公网 IP，但是由私网 IP 来提供服务，比如 LVS 的 nat 模型&lt;/p>
&lt;p>比如在公司内网中提供一个 web 服务，但是由于是私网地址，来自互联网的任何请求无法送达这台 web 服务器，这时候我们可以对外宣称公司的 web 服务在一个公网的 IP 地址上，但是公网的 IP 地址所在服务器上又没有提供 web 服务，这时候，来自外网访问的请求，全部 DNAT 成私网 IP，即可对外提供请求。&lt;/p>
&lt;h2 id="注意">注意：&lt;/h2>
&lt;p>由于 SNAT 与 DNAT 在描述的时候主要是都是针对请求报文而言的，那么当地址转换以后，响应报文响应的是转换后的地址，这时候就无法把响应请求送还给发起请求的设备了，这怎么办呢？这时候，同样需要一个地址转换，只不过通过 NAT 机制自行完成的，如何自动完成呢？这里面会有一个连接追踪机制，跟踪每一个数据连接（详见：&lt;a href="https://www.yuque.com/go/doc/33221811">ConnTrack 连接跟踪机制&lt;/a>），当响应报文到来的时候，根据连接追踪表中的信息记录的请求报文是怎么转换的相关信息，来对响应报文进行 NAT 转换。&lt;/p></description></item><item><title>Docs: TC 模块</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/</guid><description/></item><item><title>Docs: TC 模块</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/</guid><description/></item></channel></rss>