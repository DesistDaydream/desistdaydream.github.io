<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 8.Network 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/</link><description>Recent content in 8.Network 管理 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Bond 与 Team</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/bond-%E4%B8%8E-team/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/bond-%E4%B8%8E-team/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Link_aggregation">Wiki,Link Aggregation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Media-independent_interface">Wiki,MII&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/networking/bonding.html">Linux 内核文档,Linux 网络文档-Linux 以太网 Bonding 驱动入门指南&lt;/a>(这里可以看到所有 Bonding 参数)&lt;/li>
&lt;li>&lt;a href="https://wiki.linuxfoundation.org/networking/bonding">Linux 基金会 Wiki,网络-bonding&lt;/a>&lt;/li>
&lt;li>红帽官方的 bond 说明文档：&lt;a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/networking_guide/ch-configure_network_bonding">https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/networking_guide/ch-configure_network_bonding&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.ibm.com/docs/en/linux-on-systems?topic=recommendations-link-monitoring">https://www.ibm.com/docs/en/linux-on-systems?topic=recommendations-link-monitoring&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Link Aggregation(链路聚合)&lt;/strong> 技术就是将多条物理链路聚合成一条带宽更高的逻辑链路，该逻辑链路的带宽等于被聚合在一起的多条物理链路的带宽之和。聚合在一起的物理链路的条数可以根据业务的带宽需求来配置。因此链路聚合具有成本低，配置灵活的优点，此外，链路聚合还具有链路冗余备份的功能，聚合在一起的链路彼此动态备份，提高了网络的稳定性。早期链路聚合技术的实现没有统一的标准，各厂商都有自己私有的解决方案，功能不完全相同，也互不兼容。因此，IEEE 专门制定了链路聚合的标准，目前链路聚合技术的正式标准为 IEEE Standard 802.3ad，而 &lt;strong>Link Aggregation Control Protocol(链路汇聚控制协议,LACP)&lt;/strong> 是该标准的主要内容之一，是一种实现链路动态聚合的协议。&lt;/p>
&lt;h2 id="link-aggregation-control-protocol">Link Aggregation Control Protocol&lt;/h2>
&lt;p>**Link Aggregation Control Protocol(链路汇聚控制协议，简称 LACP) **在 IEEE 以太网标准中，提供了一种方法，可以将多个物理链路捆绑在一起以形成单个逻辑链路。LACP 允许网络设备通过将 LACP 数据包发送到 &lt;strong>Peer(对等方)&lt;/strong> 以 &lt;strong>negotiate(协商)&lt;/strong> 链路状态，并实现自动捆绑。&lt;/p>
&lt;blockquote>
&lt;p>Peer(对等方) 指的是与本网络设备直连的可以实现 LACP 的对端网络设备
LACP 数据包通常称为 &lt;strong>Link Aggregation Control Protocol Data Unit(链路汇聚控制协议数据单元，简称 LACPDU)&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;h1 id="bond网卡绑定">Bond，网卡绑定&lt;/h1>
&lt;p>Bond 类型的网络设备是通过把多个网络设备绑定为一个逻辑网络设备，实现本地网络设备的冗余、带宽扩容和负载均衡。在应用部署中是一种常用的技术。&lt;/p>
&lt;p>Linux 中使用 bonding 模块实现 bonding 驱动程序。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>lichenhao@hw-cloud-xngy-jump-server-linux-2 ~&lt;span style="color:#f92672">]&lt;/span>$ modinfo bonding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>filename: /lib/modules/5.4.0-88-generic/kernel/drivers/net/bonding/bonding.ko
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>author: Thomas Davis, tadavis@lbl.gov and many others
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>description: Ethernet Channel Bonding Driver, v3.7.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>version: 3.7.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>license: GPL
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias: rtnl-link-bond
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>srcversion: B95AF01257E8C745F584C8F
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>depends:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>retpoline: Y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>intree: Y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>name: bonding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>vermagic: 5.4.0-88-generic SMP mod_unload modversions
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sig_id: PKCS#7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>signer: Build time autogenerated kernel key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sig_key: 2D:2D:71:A0:22:44:6D:60:C8:49:CB:0E:D7:43:D0:D2:7A:5C:0E:F1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sig_hashalgo: sha512
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>signature: AE:16:69:2D:17:C0:36:10:F4:52:73:EB:A4:CB:CB:FC:68:78:DE:3A:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: max_bonds:Max number of bonded devices &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: tx_queues:Max number of transmit queues &lt;span style="color:#f92672">(&lt;/span>default &lt;span style="color:#f92672">=&lt;/span> 16&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: num_grat_arp:Number of peer notifications to send on failover event &lt;span style="color:#f92672">(&lt;/span>alias of num_unsol_na&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: num_unsol_na:Number of peer notifications to send on failover event &lt;span style="color:#f92672">(&lt;/span>alias of num_grat_arp&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: miimon:Link check interval in milliseconds &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: updelay:Delay before considering link up, in milliseconds &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: downdelay:Delay before considering link down, in milliseconds &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: use_carrier:Use netif_carrier_ok &lt;span style="color:#f92672">(&lt;/span>vs MII ioctls&lt;span style="color:#f92672">)&lt;/span> in miimon; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> off, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> on &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: mode:Mode of operation; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> balance-rr, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> active-backup, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> balance-xor, &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> broadcast, &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> 802.3ad, &lt;span style="color:#ae81ff">5&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> balance-tlb, &lt;span style="color:#ae81ff">6&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> balance-alb &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: primary:Primary network device to use &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: primary_reselect:Reselect primary slave once it comes up; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> always &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> only &lt;span style="color:#66d9ef">if&lt;/span> speed of primary is better, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> only on active slave failure &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: lacp_rate:LACPDU tx rate to request from 802.3ad partner; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> slow, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> fast &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: ad_select:802.3ad aggregation selection logic; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> stable &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> bandwidth, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> count &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: min_links:Minimum number of available links before turning on carrier &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: xmit_hash_policy:balance-alb, balance-tlb, balance-xor, 802.3ad hashing method; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> layer &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> layer 3+4, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> layer 2+3, &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> encap layer 2+3, &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> encap layer 3+4 &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: arp_interval:arp interval in milliseconds &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: arp_ip_target:arp targets in n.n.n.n form &lt;span style="color:#f92672">(&lt;/span>array of charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: arp_validate:validate src/dst of ARP probes; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> none &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> active, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> backup, &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> all &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: arp_all_targets:fail on any/all arp targets timeout; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> any &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> all &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: fail_over_mac:For active-backup, &lt;span style="color:#66d9ef">do&lt;/span> not set all slaves to the same MAC; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> none &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> active, &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> follow &lt;span style="color:#f92672">(&lt;/span>charp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: all_slaves_active:Keep all frames received on an interface by setting active flag &lt;span style="color:#66d9ef">for&lt;/span> all slaves; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> never &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> always. &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: resend_igmp:Number of IGMP membership reports to send on link failure &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: packets_per_slave:Packets to send per slave in balance-rr mode; &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> a random slave, &lt;span style="color:#ae81ff">1&lt;/span> packet per slave &lt;span style="color:#f92672">(&lt;/span>default&lt;span style="color:#f92672">)&lt;/span>, &amp;gt;1 packets per slave. &lt;span style="color:#f92672">(&lt;/span>int&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parm: lp_interval:The number of seconds between instances where the bonding driver sends learning packets to each slaves peer switch. The default is 1. &lt;span style="color:#f92672">(&lt;/span>uint&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="bond-参数">Bond 参数&lt;/h1>
&lt;h2 id="arp-监控参数">ARP 监控参数&lt;/h2>
&lt;p>ARP 监控参数与 MII 监控参数不可同时使用&lt;/p>
&lt;p>&lt;strong>arp_interval&lt;/strong> # ARP 监控模式的监控频率，单位 毫秒。&lt;code>默认值：0&lt;/code>。0 值表示禁用 ARP 监控
&lt;strong>arp_ip_target&lt;/strong> #&lt;/p>
&lt;h2 id="mii-监控参数">MII 监控参数&lt;/h2>
&lt;p>&lt;strong>Media Independent Interface(介质无关接口，简称 MII)&lt;/strong>，通过该接口可以检测聚合链路的状态，当某个网络设备故障时，bonding 驱动会将这个故障设备标记为关闭。虽然不会将设备踢出聚合组，但是数据不在通过故障设备传输
MII 监控参数与 ARP 监控参数不可同时使用&lt;/p>
&lt;p>&lt;strong>miimon&lt;/strong> # MII 监控模式的监控频率，单位 毫秒。这决定了每个备链路状态的故障检查频率。&lt;code>默认值：0&lt;/code>。0 值表示禁用 MII 链路监控&lt;/p>
&lt;ul>
&lt;li>通常设置为 100&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>use_carrier&lt;/strong> # 指定 miimon 是否应使用 MII 或 ETHTOOL ioctls 与 netif_carrier_ok() 来确定链接状态。 默认值是 1，这使得可以使用 netif_carrier_ok（）。 这由 Linux on Z 上的 qeth 设备驱动程序支持。
**downdelay **# 检测到网络设备故障后，持续 downdelay 毫秒后，关闭该设备。
&lt;strong>updelay&lt;/strong> # 检测到网络设备恢复后，持续 updelay 毫秒后，启用该设备&lt;/p>
&lt;h2 id="bond-模式参数">Bond 模式参数&lt;/h2>
&lt;p>&lt;strong>mode&lt;/strong> # 指定 bonding 策略。&lt;code>默认值：balance-rr&lt;/code>。常见的 bond 模式有七种：括号中是该模式所对应的数字，使用 nmcli 命令时，不要使用数字代替，NetworkManager 无法识别数字。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>balance-rr(0)&lt;/strong> # 表示负载分担 round-robin，和交换机的聚合强制不协商的方式配合。&lt;/li>
&lt;li>&lt;strong>active-backup(1)&lt;/strong> # 表示主备模式，只有一块网卡是 active,另外一块是备的 standby，这时如果交换机配的是捆绑，将不能正常工作，因为交换机往两块网卡发包，有一半包是丢弃的。
&lt;ul>
&lt;li>注意：vmwork 的虚拟机中只能做 mode=1 的实验，其它的工作模式得用真机来实践，并且需要添加额外参数(fail_over_mac=1)才能实现主备模式&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>balance-xor(2)&lt;/strong> # 表示 XOR Hash 负载分担，和交换机的聚合强制不协商方式配合(需要 xmit_hash_policy)
&lt;ul>
&lt;li>推荐 bond 参数：mode=balance-xor,miimon=100,xmit_hash_policy=layer3+4&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>broadcast(3)&lt;/strong> # 表示所有包从所有 interface 发出，这个不均衡，只有冗余机制&amp;hellip;和交换机的聚合强制不协商方式配合。&lt;/li>
&lt;li>&lt;strong>802.3ad(4)&lt;/strong> # 表示支持 802.3ad 协议，动态链路聚合，需要和交换机的聚合 LACP 方式配合(需要 xmit_hash_policy)
&lt;ul>
&lt;li>推荐 bond 参数：mode=802.3ad,miimon=100,lacp_rate=1,xmit_hash_policy=layer3+4&lt;/li>
&lt;li>802.3ad 模式的 Bond 网络设备的最大带宽是所有 Slave 设备最大带宽之和&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>balance-tlb(5)&lt;/strong> # 是根据每个 slave 的负载情况选择 slave 进行发送，接收时使用当前轮到的 slave&lt;/li>
&lt;li>&lt;strong>balance-alb(6)&lt;/strong> # 在 5 的 tlb 基础上增加了 rlb。Adaptive Load Balancing(简称 ALB) 协议，可以根据网络状态和物理网卡的带宽来动态地将数据包分配到每个物理网卡上。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>注意：&lt;/p>
&lt;ul>
&lt;li>active-backup、balance-tlb 和 balance-alb 模式不需要交换机的任何特殊配置。其他绑定模式需要配置交换机以便整合链接。例如：Cisco 交换机需要在模式 0、2 和 3 中使用 EtherChannel，但在模式 4 中需要 LACP 和 EtherChannel。有关交换机附带文档，请查看 &lt;a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt">https://www.kernel.org/doc/Documentation/networking/bonding.txt&lt;/a>。&lt;/li>
&lt;li>若想 bond 功能生效，关闭 NetworkManager 服务 或者 在配置文件中的 BONDING_OPTS 中 MODE 的值不要用数字，而是直接使用模式名称来作为值&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>每种模式的理论最大带宽：&lt;/p>
&lt;ul>
&lt;li>0 和 2 模式是所有网络设备的带宽之和&lt;/li>
&lt;li>1 和 3 模式是单个网络设备的带宽&lt;/li>
&lt;li>4 模式也是带宽之和，但是会动态分配，有时候最大值可能会超过带宽之和的上限&lt;/li>
&lt;li>5 和 6 与 4 类似，都是带宽之和，且可能超过上限。&lt;/li>
&lt;/ul>
&lt;h2 id="其他参数">其他参数&lt;/h2>
&lt;p>&lt;strong>lacp_rate&lt;/strong> # 作用于 802.3ad 模式。向聚合链路的对端(通常来说都是交换机)传输** LACPDU** 包的速率。可用的值有如下几个：&lt;/p>
&lt;ul>
&lt;li>slow # 每 30 秒传输一次 LACPDU，即每 30 秒协商一次&lt;/li>
&lt;li>fast # 每秒传输一次 LACPDU，即每秒协商一次&lt;/li>
&lt;li>max_bonds # 指定要为此绑定驱动程序实例创建的绑定设备数。例如，如果 max_bonds 为 3，并且绑定驱动程序尚未加载，则将创建 bond0、bond1 和 bond2。默认值为 1&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>xmit_hash_policy&lt;/strong> # 作用于 balance-xor 和 802.3ad 模式。配置传输 hash 策略。&lt;code>默认值：layer2&lt;/code>。&lt;/p>
&lt;ul>
&lt;li>layer2 # 该策略支持 802.3ad。使用 XOR 或硬件 MAC 地址生成 hash。&lt;/li>
&lt;li>layer2+3 # 该策略支持 802.3ad。使用 XOR 或硬件 MAC 地址与 IP 地址一起生成 hash。&lt;/li>
&lt;li>layer3+4 # 该策略不完全支持 802.3ad&lt;/li>
&lt;li>说明：
&lt;ul>
&lt;li>这里面的 2，3，4 其实就是指的 ISO 模型里的层，2 层就是用 MAC 进行计算，2+3 就是用 MAC 加 IP 进行计算，3+4 就是用 IP 加 PORT 进行计算。&lt;/li>
&lt;li>只使用 2 层的 MAC 进行计算时，会导致同一个网关的数据流将完全从一个端口发送，但是如果使用 2+3 或 3+4，虽然负载更均衡了，但是由于使用了上层协议进行计算，则增加了 hash 的开销。&lt;/li>
&lt;li>计算越负责，负载均衡效果越好，但是资源开销越大。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="bond-配置">Bond 配置&lt;/h1>
&lt;p>/sys/class/net/bonding_masters # 当前系统下已经启用的 Bond 名称
/sys/class/net/BondName/* # Bond 类型网络设备的运行时信息，这里面某些信息是可以被修改的。&lt;/p>
&lt;ul>
&lt;li>./statistics/* # 网络设备的状态信息，比如 发送/接受 了多少数据包、多少数据量 等等，&lt;code>ip -s&lt;/code> 参数可以从这里获取到信息&lt;/li>
&lt;li>./bonding/* # Bond 参数&lt;/li>
&lt;/ul>
&lt;p>/proc/net/bonding/* # bond 运行时状态信息。其内文件名为 Bond 的名称。查看文件内容解析详见&lt;a href="#cvrU1">《Bonding 在内核中的信息解析》&lt;/a>部分&lt;/p>
&lt;h2 id="bond-基本配置文件详解">Bond 基本配置文件详解&lt;/h2>
&lt;p>首先是配置一个 bond 网络设备，IP 等相关信息配置在 bond 网络设备上。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao network-scripts&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /etc/sysconfig/network-scripts/ifcfg-bond0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BONDING_OPTS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;mode=balance-rr&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TYPE&lt;span style="color:#f92672">=&lt;/span>Bond
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BONDING_MASTER&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BOOTPROTO&lt;span style="color:#f92672">=&lt;/span>dhcp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DEFROUTE&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME&lt;span style="color:#f92672">=&lt;/span>bond0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DEVICE&lt;span style="color:#f92672">=&lt;/span>bond0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ONBOOT&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>其次配置让一个物理网络设备绑定到该 bond 设备上&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao network-scripts&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /etc/sysconfig/network-scripts/ifcfg-bond-slave-em1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TYPE&lt;span style="color:#f92672">=&lt;/span>Ethernet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME&lt;span style="color:#f92672">=&lt;/span>bond-slave-em1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DEVICE&lt;span style="color:#f92672">=&lt;/span>em1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ONBOOT&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MASTER&lt;span style="color:#f92672">=&lt;/span>bond0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SLAVE&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="bond-在内核中的信息解析">Bond 在内核中的信息解析&lt;/h1>
&lt;p>/proc/net/bonding/BondNAME 文件中保存了当前系统中已启动的 Bond 类型的网络设备的信息
这些信息分为多个部分&lt;/p>
&lt;ul>
&lt;li>Bond 类型的网络设备通用信息&lt;/li>
&lt;li>Bond 类型的网络设备特定信息，比如 802.3ad 类型的 Bond 就有独自的信息&lt;/li>
&lt;li>Bond 下 Slave 网络设备通用信息&lt;/li>
&lt;/ul>
&lt;h2 id="balance-rr">balance-rr&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Bonding Mode: load balancing &lt;span style="color:#f92672">(&lt;/span>round-robin&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Polling Interval &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">100&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Up Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Down Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave Interface: enp6s0f0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Speed: &lt;span style="color:#ae81ff">10000&lt;/span> Mbps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Duplex: full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Link Failure Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Permanent HW addr: 40:a6:b7:25:f2:3c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave queue ID: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave Interface: enp6s0f1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Speed: &lt;span style="color:#ae81ff">10000&lt;/span> Mbps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Duplex: full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Link Failure Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Permanent HW addr: 40:a6:b7:25:f2:3d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave queue ID: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Bond 设备&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Bonding Mode&lt;/strong> # Bond 模式的名称&lt;/li>
&lt;li>&lt;strong>MII Status&lt;/strong> # 链路监控状态&lt;/li>
&lt;li>&lt;strong>MII Polling Interval&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>Up Delay&lt;/strong> # 检测到 Slave 网络设备恢复后，持续 updelay 毫秒后，启用 Slave 设备&lt;/li>
&lt;li>&lt;strong>Down Delay&lt;/strong> # 检测到 Slave 网络设备故障后，持续 downdelay 毫秒后，关闭 Slave 设备。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Slave 设备&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Slave Interface&lt;/strong> # 与 Bond 设备关联的 Slave 网络设备的名称&lt;/li>
&lt;li>&lt;strong>MII Status&lt;/strong> # 链路监控状态&lt;/li>
&lt;li>**Speed **# 最大传输速度，即网卡的带宽&lt;/li>
&lt;li>&lt;strong>Duplex&lt;/strong> # 双工模式&lt;/li>
&lt;li>&lt;strong>Link Failure Count&lt;/strong> # 失联总次数&lt;/li>
&lt;li>&lt;strong>Permanent HW addr&lt;/strong> # 关联的物理网卡的硬件地址，即网卡的 MAC 地址&lt;/li>
&lt;li>&lt;strong>Slave queue ID&lt;/strong> # 队列 ID&lt;/li>
&lt;/ul>
&lt;h2 id="balance-xor">balance-xor&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@vs-7 bonding&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/net/bonding/bond0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Ethernet Channel Bonding Driver: v3.7.1 &lt;span style="color:#f92672">(&lt;/span>April 27, 2011&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Bonding Mode: load balancing &lt;span style="color:#f92672">(&lt;/span>xor&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#75715e"># 此 bond 的模式&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Transmit Hash Policy: layer3+4 &lt;span style="color:#f92672">(&lt;/span>1&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#75715e"># 此 bond 模式的参数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Polling Interval &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">100&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Up Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Down Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Peer Notification Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave Interface: eno1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Speed: &lt;span style="color:#ae81ff">1000&lt;/span> Mbps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Duplex: full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Link Failure Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Permanent HW addr: f0:d4:e2:ea:28:54
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave queue ID: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave Interface: eno2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Speed: &lt;span style="color:#ae81ff">1000&lt;/span> Mbps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Duplex: full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Link Failure Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Permanent HW addr: f0:d4:e2:ea:28:55
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave queue ID: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="8023ad">802.3ad&lt;/h2>
&lt;blockquote>
&lt;ul>
&lt;li>&lt;a href="https://stackoverflow.com/questions/62173444/churn-state-meaning-in-lacp-bonding">StackOverflow,bonding LACP 模式下 Churn 的含义&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mlog.club/article/2693580">https://mlog.club/article/2693580&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1295423">https://bugzilla.redhat.com/show_bug.cgi?id=1295423&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?id=ea53abfab960909d622ca37bcfb8e1c5378d21cc">https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?id=ea53abfab960909d622ca37bcfb8e1c5378d21cc&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/solutions/4122011">https://access.redhat.com/solutions/4122011&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://github.com/systemd/systemd/issues/15208">https://github.com/systemd/systemd/issues/15208&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@yihualu-33 test_dir&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/net/bonding/bond1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Ethernet Channel Bonding Driver: v3.7.1 &lt;span style="color:#f92672">(&lt;/span>April 27, 2011&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Bonding Mode: IEEE 802.3ad Dynamic link aggregation
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Transmit Hash Policy: layer3+4 &lt;span style="color:#f92672">(&lt;/span>1&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Polling Interval &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">100&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Up Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Down Delay &lt;span style="color:#f92672">(&lt;/span>ms&lt;span style="color:#f92672">)&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>802.3ad info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LACP rate: slow
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Min links: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Aggregator selection policy &lt;span style="color:#f92672">(&lt;/span>ad_select&lt;span style="color:#f92672">)&lt;/span>: stable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>System priority: &lt;span style="color:#ae81ff">65535&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>System MAC address: 32:1c:0d:e9:ca:e9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Active Aggregator Info:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Aggregator ID: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Number of ports: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Actor Key: &lt;span style="color:#ae81ff">15&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Partner Key: &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Partner Mac Address: 00:00:00:00:00:06
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave Interface: enp6s0f0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Speed: &lt;span style="color:#ae81ff">10000&lt;/span> Mbps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Duplex: full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Link Failure Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Permanent HW addr: 40:a6:b7:26:60:b4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave queue ID: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Aggregator ID: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Actor Churn State: none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Partner Churn State: none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Actor Churned Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Partner Churned Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>details actor lacp pdu:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system priority: &lt;span style="color:#ae81ff">65535&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system mac address: 32:1c:0d:e9:ca:e9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port key: &lt;span style="color:#ae81ff">15&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port priority: &lt;span style="color:#ae81ff">255&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port number: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port state: &lt;span style="color:#ae81ff">61&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>details partner lacp pdu:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system priority: &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system mac address: 00:00:00:00:00:06
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oper key: &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port priority: &lt;span style="color:#ae81ff">32768&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port number: &lt;span style="color:#ae81ff">33862&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port state: &lt;span style="color:#ae81ff">63&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave Interface: enp6s0f1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MII Status: up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Speed: &lt;span style="color:#ae81ff">10000&lt;/span> Mbps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Duplex: full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Link Failure Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Permanent HW addr: 40:a6:b7:26:60:b5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Slave queue ID: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Aggregator ID: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Actor Churn State: none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Partner Churn State: none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Actor Churned Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Partner Churned Count: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>details actor lacp pdu:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system priority: &lt;span style="color:#ae81ff">65535&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system mac address: 32:1c:0d:e9:ca:e9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port key: &lt;span style="color:#ae81ff">15&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port priority: &lt;span style="color:#ae81ff">255&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port number: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port state: &lt;span style="color:#ae81ff">61&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>details partner lacp pdu:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system priority: &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system mac address: 00:00:00:00:00:06
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oper key: &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port priority: &lt;span style="color:#ae81ff">32768&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port number: &lt;span style="color:#ae81ff">33862&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port state: &lt;span style="color:#ae81ff">63&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>802.3ad 模式的 Bond 需要与交换机交互 LACP 信息，这里对 服务器 和 交换机 的称呼如下：&lt;/p>
&lt;ul>
&lt;li>**Actor **# 指主机，即服务器&lt;/li>
&lt;li>&lt;strong>Partner&lt;/strong> # 指交换机&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Aggregator ID&lt;/strong> #
&lt;strong>Actor Churn State 与 Partner Churn State&lt;/strong> # 务器与交换机的 Churn 状态&lt;/p>
&lt;ul>
&lt;li>Churn State # Churn 状态共有三种，先是 monitoring，然后是 churned，最后 none 就正常了。
&lt;ul>
&lt;li>monitoring # 等待其他 PDUs 达成共识
&lt;ul>
&lt;li>The bond slave interface is in the process of the initial LACP communication with the LACP peer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>churned # 扰动、流逝
&lt;ul>
&lt;li>One of the peer&amp;rsquo;s LACP (etherchannel) interfaces is suspended or is otherwise no longer active as an LACP interface&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>none # 链路已同步&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Actor Churned Count 与 Partner Churned Count&lt;/strong> #
&lt;strong>details actor lacp pdu 与 details partner lacp pdu&lt;/strong> # 服务器与交换机的 LACPDU 信息细节&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggzysy/1644663197947-87aaf3c3-d762-4682-8747-059014f1a7bc.png" alt="image.png">&lt;/p>
&lt;h1 id="team类似于-bond比-bond-更优秀">Team，类似于 Bond，比 Bond 更优秀&lt;/h1>
&lt;h1 id="rhel7-中网卡绑定-team-和-bond-的区别">RHEL7 中网卡绑定 team 和 bond 的区别&lt;/h1>
&lt;p>red hat 官方给出的 team 和 bond 特性对比&lt;/p>
&lt;p>A Comparison of Features in Bonding and Team&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Feature&lt;/td>
&lt;td>Bonding&lt;/td>
&lt;td>Team&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>broadcast Tx policy&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>round-robin Tx policy&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>active-backup Tx policy&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LACP (802.3ad) support&lt;/td>
&lt;td>Yes (active only)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hash-based Tx policy&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>User can set hash function&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tx load-balancing support (TLB)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LACP hash port select&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>load-balancing for LACP support&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ethtool link monitoring&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ARP link monitoring&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NS/NA (IPv6) link monitoring&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ports up/down delays&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>port priorities and stickiness (“primary”option enhancement)&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>separate per-port link monitoring setup&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>multiple link monitoring setup&lt;/td>
&lt;td>Limited&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lockless Tx/Rx path&lt;/td>
&lt;td>No (rwlock)&lt;/td>
&lt;td>Yes (RCU)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VLAN support&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>user-space runtime control&lt;/td>
&lt;td>Limited&lt;/td>
&lt;td>Full&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Logic in user-space&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Extensibility&lt;/td>
&lt;td>Hard&lt;/td>
&lt;td>Easy&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Modular design&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Performance overhead&lt;/td>
&lt;td>Low&lt;/td>
&lt;td>Very Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>D-Bus interface&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>multiple device stacking&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>zero config using LLDP&lt;/td>
&lt;td>No&lt;/td>
&lt;td>(in planning)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NetworkManager support&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Bridge</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/bridge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/bridge/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://mp.weixin.qq.com/s/JnKz1fUgZmGdvfxOm2ehZg">聊聊 Linux 上软件实现的 “交换机” - Bridge！&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>大家好，我是飞哥！&lt;/p>
&lt;p>Linux 中的 veth 是一对儿能互相连接、互相通信的虚拟网卡。通过使用它，我们可以让 Docker 容器和母机通信，或者是在两个 Docker 容器中进行交流。参见&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247486424&amp;amp;idx=1&amp;amp;sn=d66fe4ebf1cd9e5079606f71a0169697&amp;amp;scene=21#wechat_redirect">《轻松理解 Docker 网络虚拟化基础之 veth 设备！》&lt;/a>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>不过在实际中，我们会想在一台物理机上我们虚拟出来几个、甚至几十个容器，以求得充分压榨物理机的硬件资源。但这样带来的问题是大量的容器之间的网络互联。很明显上面简单的 veth 互联方案是没有办法直接工作的，我们该怎么办？？？&lt;/p>
&lt;p>回头想一下，在物理机的网络环境中，多台不同的物理机之间是如何连接一起互相通信的呢？没错，那就是以太网交换机。同一网络内的多台物理机通过交换机连在一起，然后它们就可以相互通信了。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>在我们的网络虚拟化环境里，和物理网络中的交换机一样，也需要这样的一个软件实现的设备。它需要有很多个虚拟端口，能把更多的虚拟网卡连接在一起，通过自己的转发功能让这些虚拟网卡之间可以通信。在 Linux 下这个软件实现交换机的技术就叫做 bridge（再强调下，这是纯软件实现的）。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>各个 Docker 容器都通过 veth 连接到 bridge 上，bridge 负责在不同的 “端口” 之间转发数据包。这样各个 Docker 之间就可以互相通信了！&lt;/p>
&lt;p>今天我们来展开聊聊 bridge 的详细工作过程。&lt;/p>
&lt;h2 id="一如何使用-bridge">一、如何使用 bridge&lt;/h2>
&lt;p>在分析它的工作原理之前，很有必要先来看一看网桥是如何使用的。&lt;/p>
&lt;p>为了方便大家理解，接下来我们通过动手实践的方式，在一台 Linux 上创建一个小型的虚拟网络出来，并让它们之间互相通信。&lt;/p>
&lt;h3 id="11-创建两个不同的网络">1.1 创建两个不同的网络&lt;/h3>
&lt;p>Bridge 是用来连接两个不同的虚拟网络的，所以在准备实验 bridge 之前我们得先需要用 net namespace 构建出两个不同的网络空间来。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>具体的创建过程如下。我们通过 ip netns 命令创建 net namespace。首先创建一个 net1：&lt;/p>
&lt;p>&lt;code># ip netns add net1&lt;/code>&lt;/p>
&lt;p>接下来创建一对儿 veth 出来，设备名分别是 veth1 和 veth1_p。并把其中的一头 veth1 放到这个新的 netns 中。&lt;/p>
&lt;p>&lt;code># ip link add veth1 type veth peer name veth1_p # ip link set veth1 netns net1&lt;/code>&lt;/p>
&lt;p>因为我们打算是用这个 veth1 来通信，所以需要为其配置上 ip，并把它启动起来。&lt;/p>
&lt;p>&lt;code># ip netns exec net1 ip addr add 192.168.0.101/24 dev veth1 # ip netns exec net1 ip link set veth1 up&lt;/code>&lt;/p>
&lt;p>查看一下，上述的配置是否成功。&lt;/p>
&lt;p>&lt;code># ip netns exec net1 ip link list # ip netns exec net1 ifconfig&lt;/code>&lt;/p>
&lt;p>重复上述步骤，在创建一个新的 netns 出来，命名分别为。&lt;/p>
&lt;ul>
&lt;li>netns: net2&lt;/li>
&lt;li>veth pair: veth2, veth2_p&lt;/li>
&lt;li>ip: 192.168.0.102&lt;/li>
&lt;/ul>
&lt;p>好了，这样我们就在一台 Linux 就创建出来了两个虚拟的网络环境。&lt;/p>
&lt;h3 id="12-把两个网络连接到一起">1.2 把两个网络连接到一起&lt;/h3>
&lt;p>在上一个步骤中，我们只是创建出来了两个独立的网络环境而已。这个时候这两个环境之间还不能互相通信。我们需要创建一个虚拟交换机 - bridge， 来把这两个网络环境连起来。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>创建过程如下。创建一个 bridge 设备, 把刚刚创建的两对儿 veth 中剩下的两头 “插” 到 bridge 上来。&lt;/p>
&lt;p>&lt;code># brctl addbr br0 # ip link set dev veth1_p master br0 # ip link set dev veth2_p master br0 # ip addr add 192.168.0.100/24 dev br0&lt;/code>&lt;/p>
&lt;p>再为 bridge 配置上 IP，并把 bridge 以及插在其上的 veth 启动起来。&lt;/p>
&lt;p>&lt;code># ip link set veth1_p up # ip link set veth2_p up # ip link set br0 up&lt;/code>&lt;/p>
&lt;p>查看一下当前 bridge 的状态，确认刚刚的操作是成功了的。&lt;/p>
&lt;p>&lt;code># brctl show bridge name     bridge id               STP enabled     interfaces br0             8000.4e931ecf02b1       no              veth1_p                                                         veth2_p&lt;/code>&lt;/p>
&lt;h3 id="13-网络连通测试">1.3 网络连通测试&lt;/h3>
&lt;p>激动人心的时刻就要到了，我们在 net1 里（通过指定 ip netns exec net1 以及 -I veth1），ping 一下 net2 里的 IP（192.168.0.102）试试。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;pre>&lt;code># ip netns exec net1 ping 192.168.0.102 -I veth1PING 192.168.0.102 (192.168.0.102) from 192.168.0.101 veth1: 56(84) bytes of data.64 bytes from 192.168.0.102: icmp_seq=1 ttl=64 time=0.037 ms64 bytes from 192.168.0.102: icmp_seq=2 ttl=64 time=0.008 ms64 bytes from 192.168.0.102: icmp_seq=3 ttl=64 time=0.005 ms
&lt;/code>&lt;/pre>
&lt;p>哇塞，通了通了！！&lt;/p>
&lt;p>这样，我们就在一台 Linux 上虚拟出了 net1 和 net2 两个不同的网络环境。我们还可以按照这种方式创建更多的网络，都可以通过一个 bridge 连接到一起。这就是 Docker 中网络系统工作的基本原理。&lt;/p>
&lt;h2 id="二bridge-是如何创建出来的">二、Bridge 是如何创建出来的&lt;/h2>
&lt;p>在内核中，bridge 是由两个相邻存储的内核对象来表示的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>我们先看下它是如何被创建出来的。内核中创建 bridge 的关键代码在 br_add_bridge 这个函数里。&lt;/p>
&lt;p>`//file:net/bridge/br_if.c
int br_add_bridge(struct net _net, const char _name)
{
 // 申请网桥设备，并用  br_dev_setup  来启动它
 dev = alloc_netdev(sizeof(struct net_bridge), name,
      br_dev_setup);&lt;/p>
&lt;p>dev_net_set(dev, net);
 dev-&amp;gt;rtnl_link_ops = &amp;amp;br_link_ops;&lt;/p>
&lt;p>// 注册网桥设备
 res = register_netdev(dev);
 if (res)
  free_netdev(dev);
 return res;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>上述代码中注册网桥的关键代码是 alloc_netdev 这一行。在这个函数里，将申请网桥的内核对象 net_device。在这个函数调用里要注意两点。&lt;/p>
&lt;ul>
&lt;li>
&lt;ol>
&lt;li>第一个参数传入了 struct net_bridge 的大小&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>第三个参数传入的 br_dev_setup 是一个函数。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>带着这两点注意事项，我们进入到 alloc_netdev 的实现中。&lt;/p>
&lt;p>&lt;code>//file: include/linux/netdevice.h #define alloc_netdev(sizeof_priv, name, setup) \  alloc_netdev_mqs(sizeof_priv, name, setup, 1, 1)&lt;/code>&lt;/p>
&lt;p>好吧，竟然是个宏。那就得看 alloc_netdev_mqs 了。&lt;/p>
&lt;p>`//file: net/core/dev.c
struct net_device _alloc_netdev_mqs(int sizeof_priv, &amp;hellip;，void (_setup)(struct net_device *))
{
 // 申请网桥设备
 alloc_size = sizeof(struct net_device);
 if (sizeof_priv) {
  alloc_size = ALIGN(alloc_size, NETDEV_ALIGN);
  alloc_size += sizeof_priv;
 }&lt;/p>
&lt;p>p = kzalloc(alloc_size, GFP_KERNEL);
 dev = PTR_ALIGN(p, NETDEV_ALIGN);&lt;/p>
&lt;p>// 网桥设备初始化
 dev-&amp;gt;&amp;hellip; = &amp;hellip;;
 setup(dev); //setup 是一个函数指针，实际使用的是  br_dev_setup&lt;/p>
&lt;p>&amp;hellip;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在上述代码中。kzalloc 是用来在内核态申请内核内存的。需要注意的是，申请的内存大小是一个 struct net_device 再加上一个 struct net_bridge（第一个参数传进来的）。一次性就申请了两个内核对象，这说明&lt;strong>bridge 在内核中是由两个内核数据结构来表示的，分别是 struct net_device 和 struct net_bridge。&lt;/strong>&lt;/p>
&lt;p>申请完了一家紧接着调用 setup，这实际是外部传入的 br_dev_setup 函数。在这个函数内部进行进一步的初始化。&lt;/p>
&lt;p>&lt;code>//file: net/bridge/br_device.c void br_dev_setup(struct net_device *dev) {  struct net_bridge *br = netdev_priv(dev);  dev-&amp;gt;... = ...;  br-&amp;gt;... = ...;  ... }&lt;/code>&lt;/p>
&lt;p>&lt;strong>总之，brctl addbr br0 命令主要就是完成了 bridge 内核对象（struct net_device 和 struct net_bridge）的申请以及初始化。&lt;/strong>&lt;/p>
&lt;h2 id="三添加设备">三、添加设备&lt;/h2>
&lt;p>调用 &lt;code>brctl addif br0 veth0&lt;/code> 给网桥添加设备的时候，会将 veth 设备以虚拟的方式连到网桥上。当添加了若干个 veth 以后，内核中对象的大概逻辑图如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>其中 veth 是由 struct net_device 来表示，bridge 的虚拟插口是由 struct net_bridge_port 来表示。我们接下来看看源码，是如何达成上述的逻辑结果的。&lt;/p>
&lt;p>添加设备会调用到 net/bridge/br_if.c 下面的 br_add_if。&lt;/p>
&lt;p>`//file: net/bridge/br_if.c
int br_add_if(struct net_bridge _br, struct net_device _dev)
{
 //  申请一个  net_bridge_port
 struct net_bridge_port *p;
 p = new_nbp(br, dev);&lt;/p>
&lt;p>//  注册设备帧接收函数
 err = netdev_rx_handler_register(dev, br_handle_frame, p);&lt;/p>
&lt;p>//  添加到  bridge  的已用端口列表里
 list_add_rcu(&amp;amp;p-&amp;gt;list, &amp;amp;br-&amp;gt;port_list);&lt;/p>
&lt;p>&amp;hellip;&amp;hellip;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>这个函数中的第二个参数 dev 传入的是要添加的设备。在本文中，就可以认为是 veth 的其中一头。比较关键的是 net_bridge_port 这个结构体，它模拟的是物理交换机上的一个插口。它起到一个连接的作用，把 veth 和 bridge 给连接了起来。见 new_nbp 源码如下：&lt;/p>
&lt;p>`//file: net/bridge/br_if.c
static struct net_bridge_port _new_nbp(struct net_bridge _br,
           struct net_device _dev)
{
 // 申请插口对象
 struct net_bridge_port _p;
 p = kzalloc(sizeof(*p), GFP_KERNEL);&lt;/p>
&lt;p>// 初始化插口
 index = find_portno(br);
 p-&amp;gt;br = br;
 p-&amp;gt;dev = dev;
 p-&amp;gt;port_no = index;
 &amp;hellip;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 new_nbp 中，先是申请了代表插口的内核对象。find_portno 是在当前 bridge 下寻找一个可用的端口号。接下来插口对象通过 &lt;code>p-&amp;gt;br = br&lt;/code> 和 bridge 设备关联了起来，通过 &lt;code>p-&amp;gt;dev = dev&lt;/code> 和代表 veth 设备的 dev 对象也建立了联系。&lt;/p>
&lt;p>在 br_add_if 中还调用 netdev_rx_handler_register 注册了设备帧接收函数，设置 veth 上的 rx_handler 为 br_handle_frame。&lt;strong>后面在接收包的时候会回调到它&lt;/strong>。&lt;/p>
&lt;p>&lt;code>//file: int netdev_rx_handler_register(struct net_device *dev,           rx_handler_func_t *rx_handler,           void *rx_handler_data) {  ...   rcu_assign_pointer(dev-&amp;gt;rx_handler_data, rx_handler_data);  rcu_assign_pointer(dev-&amp;gt;rx_handler, rx_handler); }&lt;/code>&lt;/p>
&lt;h2 id="四数据包处理过程">四、数据包处理过程&lt;/h2>
&lt;p>在&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">图解 Linux 网络包接收过程&lt;/a>中我们讲到过接收包的完整流程。数据包会被网卡先从到 RingBuffer 中，然后依次经过硬中断、软中断处理。在软中断中再依次把包送到设备层、协议栈，最后唤醒应用程序。&lt;/p>
&lt;p>不过，拿 veth 设备来举例，如果它连接到了网桥上的话，在设备层的 __netif_receive_skb_core 函数中和上述过程有所不同。连在 bridge 上的 veth 在收到数据包的时候，不会进入协议栈，而是会进入网桥处理。网桥找到合适的转发口（另一个 veth），通过这个 veth 把数据转发出去。工作流程如下图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>我们从 veth1_p 设备的接收看起，所有的设备的接收都一样，都会进入 __netif_receive_skb_core 设备层的关键函数。&lt;/p>
&lt;p>`//file: net/core/dev.c
static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
{
 &amp;hellip;&lt;/p>
&lt;p>// tcpdump  抓包点
 list_for_each_entry_rcu(&amp;hellip;);&lt;/p>
&lt;p>//  执行设备的  rx_handler（也就是  br_handle_frame）
 rx_handler = rcu_dereference(skb-&amp;gt;dev-&amp;gt;rx_handler);
 if (rx_handler) {
  switch (rx_handler(&amp;amp;skb)) { 
  case RX_HANDLER_CONSUMED:
   ret = NET_RX_SUCCESS;
   goto unlock;
  }
 }&lt;/p>
&lt;p>//  送往协议栈
 //&amp;hellip;&lt;/p>
&lt;p>unlock:
 rcu_read_unlock();
out:
 return ret;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 __netif_receive_skb_core 中先是过了 tcpdump 的抓包点，然后查找和执行了 rx_handler。在上面小节中我们看到，把 veth 连接到网桥上的时候，veth 对应的内核对象 dev 中的 rx_handler 被设置成了 br_handle_frame。&lt;strong>所以连接到网桥上的 veth 在收到包的时候，会将帧送入到网桥处理函数 br_handle_frame 中&lt;/strong>。&lt;/p>
&lt;p>另外要注意的是网桥函数处理完的话，一般来说就 goto unlock 退出了。和普通的网卡数据包接收相比，并不会往下再送到协议栈了。&lt;/p>
&lt;p>接着来看下网桥是咋工作的吧，进入到 br_handle_frame 中来搜寻。&lt;/p>
&lt;p>`//file: net/bridge/br_input.c
rx_handler_result_t br_handle_frame(struct sk_buff **pskb)
{
 &amp;hellip;&lt;/p>
&lt;p>forward:
 NF_HOOK(NFPROTO_BRIDGE, NF_BR_PRE_ROUTING, skb, skb-&amp;gt;dev, NULL,
   br_handle_frame_finish);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>上面我对 br_handle_frame 的逻辑进行了充分的简化，简化后它的核心就是调用 br_handle_frame_finish。同样 br_handle_frame_finish 也有点小复杂。本文中，我们主要想了解的 Docker 场景下 bridge 上的 veth 设备转发。所以根据这个场景，我又对该函数进行了充分的简化。&lt;/p>
&lt;p>`//file: net/bridge/br_input.c
int br_handle_frame_finish(struct sk_buff _skb)
{  
 //  获取  veth  所连接的网桥端口、以及网桥设备
 struct net_bridge_port _p = br_port_get_rcu(skb-&amp;gt;dev);
 br = p-&amp;gt;br;&lt;/p>
&lt;p>//  更新和查找转发表
 struct net_bridge_fdb_entry *dst;
 br_fdb_update(br, p, eth_hdr(skb)-&amp;gt;h_source, vid);
 dst = __br_fdb_get(br, dest, vid)&lt;/p>
&lt;p>//  转发
 if (dst) {
  br_forward(dst-&amp;gt;dst, skb, skb2);
 } 
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在硬件中，交换机和集线器的主要区别就是它会智能地把数据送到正确的端口上去，而不会像集线器那样给所有的端口都群发一遍。所以在上面的函数中，我们看到了更新和查找转发表的逻辑。这就是网桥在学习，它会根据它的自学习结果来工作。&lt;/p>
&lt;p>在找到要送往的端口后，下一步就是调用 br_forward =&amp;gt; __br_forward 进入真正的转发流程。&lt;/p>
&lt;p>`//file: net/bridge/br_forward.c
static void __br_forward(const struct net_bridge_port _to, struct sk_buff _skb)
{
 //  将  skb  中的  dev  改成新的目的  dev
 skb-&amp;gt;dev = to-&amp;gt;dev;&lt;/p>
&lt;p>NF_HOOK(NFPROTO_BRIDGE, NF_BR_FORWARD, skb, indev, skb-&amp;gt;dev,
  br_forward_finish);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 __br_forward 中，将 skb 上的设备 dev 改为了新的目的 dev。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>然后调用 br_forward_finish 进入发送流程。在 br_forward_finish 里会依次调用 br_dev_queue_push_xmit、dev_queue_xmit。&lt;/p>
&lt;p>&lt;code>//file: net/bridge/br_forward.c int br_forward_finish(struct sk_buff *skb) {  return NF_HOOK(NFPROTO_BRIDGE, NF_BR_POST_ROUTING, skb, NULL, skb-&amp;gt;dev,          br_dev_queue_push_xmit); } int br_dev_queue_push_xmit(struct sk_buff *skb) {  dev_queue_xmit(skb);  ... }&lt;/code>&lt;/p>
&lt;p>dev_queue_xmit 就是发送函数，在上一篇&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247486424&amp;amp;idx=1&amp;amp;sn=d66fe4ebf1cd9e5079606f71a0169697&amp;amp;scene=21#wechat_redirect">《轻松理解 Docker 网络虚拟化基础之 veth 设备！》&lt;/a>中我们介绍过，后续的发送过程就是 dev_queue_xmit =&amp;gt; dev_hard_start_xmit =&amp;gt; veth_xmit。在 veth_xmit 中会获取到当前 veth 的对端，然后把数据给它发送过去。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>至此，bridge 上的转发流程就算是完毕了。要注意到的是，整个 bridge 的工作的源码都是在 net/core/dev.c 或 net/bridge 目录下。都是在设备层工作的。这也就充分印证了我们经常说的 bridge（物理交换机也一样） 是二层上的设备。&lt;/p>
&lt;p>接下来，收到网桥发过来数据的 veth 会把数据包发送给它的对端 veth2，veth2 再开始自己的数据包接收流程。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;h2 id="五总结">五、总结&lt;/h2>
&lt;p>所谓网络虚拟化，其实用一句话来概括就是&lt;strong>用软件来模拟实现真实的物理网络连接&lt;/strong>。&lt;/p>
&lt;p>Linux 内核中的 bridge 模拟实现了物理网络中的交换机的角色。和物理网络类似，可以将虚拟设备插入到 bridge 上。不过和物理网络有点不一样的是，一对儿 veth 插入 bridge 的那端其实就不是设备了，可以理解为退化成了一个网线插头。&lt;/p>
&lt;p>当 bridge 接入了多对儿 veth 以后，就可以通过自身实现的网络包转发的功能来让不同的 veth 之间互相通信了。&lt;/p>
&lt;p>回到 Docker 的使用场景上来举例，完整的 Docker 1 和 Docker 2 通信的过程是这样的：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>大致步骤是：&lt;/p>
&lt;ul>
&lt;li>1.Docker1 往 veth1 上发送数据&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>由于 veth1_p 是 veth1 的 pair， 所以这个虚拟设备上可以收到包&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>3.veth 收到包以后发现自己是连在网桥上的，于是乎进入网桥处理。在网桥设备上寻找要转发到的端口，这时找到了 veth2_p 开始发送。网桥完成了自己的转发工作&lt;/li>
&lt;li>4.veth2 作为 veth2_p 的对端，收到了数据包&lt;/li>
&lt;li>5.Docker2 里的就可以从 veth2 设备上收到数据了&lt;/li>
&lt;/ul>
&lt;p>觉得这个流程图还不过瘾？那我们再继续拉大视野，从两个 Docker 的用户态来开始看一看。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p>
&lt;p>Docker 1 在需要发送数据的时候，先通过 send 系统调用发送，这个发送会执行到协议栈进行协议头的封装等处理。经由邻居子系统找到要使用的设备（veth1）后，从这个设备将数据发送出去，veth1 的对端 veth1_p 会收到数据包。&lt;/p>
&lt;p>收到数据的 veth1_p 是一个连接在 bridge 上的设备，这时候 bridge 会接管该 veth 的数据接收过程。从自己连接的所有设备中查找目的设备。找到 veth2_p 以后，调用该设备的发送函数将数据发送出去。同样 veth2_p 的对端 veth2 即将收到数据。&lt;/p>
&lt;p>其中 veth2 收到数据后，将和 lo、eth0 等设备一样，进入正常的数据接收处理过程。Docker 2 中的用户态进程将能够收到 Docker 1 发送过来的数据了就。&lt;/p>
&lt;p>怎么样，今天你有没有更深入地理解了 Docker 的工作原理呢？最后转发到朋友圈，让你的朋友们也一起来学学吧~~~&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/50a2c4de-0e0a-4411-b386-112edd0a3fdf/640" alt="">&lt;/p></description></item><item><title>Docs: Connection Tracking(连接跟踪)机制</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/connection-tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/connection-tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA%E6%9C%BA%E5%88%B6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://conntrack-tools.netfilter.org/manual.html">Netfilter 官方文档，连接跟踪工具用户手册&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/">云计算基层技术-netfilter 框架研究&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>,&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Connection Tracking(连接跟踪系统，简称 ConnTrack、CT)&lt;/strong>，用于跟踪并且记录连接状态。Linux 为每一个经过网络堆栈的数据包，生成一个** ConnTrack Entry(连接跟踪条目，简称 Entry)**，并把该条目记录在一个 &lt;strong>ConnnTrack Table(连接跟踪表)&lt;/strong> 中，条目中主要是包含该连接的协议、源 IP 和 PORT、目标 IP 和 PORT、协议号、数据包的大小等等等信息。此后，在处理数据包时读取该文件，在文件中所有属于此连接的数据包都被唯一地分配给这个连接，并标识连接的状态。该文件中的每一个条目都有一个持续时间，当持续时间结束后，该连接会被自动清除，再有相同的连接进来的时候，则按照新连接来处理。Netfilter 中定义了如下几个连接状态以便对具有这些状态的连接进行处理：&lt;/p>
&lt;p>可跟踪的连接状态有以下几个&lt;/p>
&lt;ol>
&lt;li>NEW：新发出的请求。在连接跟踪文件中(nf_conntrack)不存在此连接。&lt;/li>
&lt;li>ESTABLISHED：已建立的。NEW 状态之后，在 nf_conntrack 文件中为其建立的条目失效之前所进行的通信的状态&lt;/li>
&lt;li>RELATED：有关联的。某个已经建立的连接所建立的新连接；e.g.FTP 的数据传输连接就是控制连接所 RELATED 出来的连接。–icmp-type 8(ping 请求)就是–icmp-type 0(ping 应答) 所 RELATED 出来的。&lt;/li>
&lt;li>INVALIED：无法识别的连接。&lt;/li>
&lt;li>UNTRACKED：不跟踪的链接状态，仅在使用 raw 表的时候该状态才有用，即 raw 不进行链接跟踪的时候，则连接跟踪表中没有记录的数据包就是此状态&lt;/li>
&lt;li>其他：
&lt;ol>
&lt;li>NEW 与 ESTABLISHED 的定义：只要第一次请求就算 NEW(e.g.本机往外第一次发送，外部第一次发往本机的请求)，哪怕对第一个 NEW 请求再回应的都算 ESTABLISHED。注意在 INPUT 和 OUTPUT 链上定义 NEW 的情况，INPUT 是对外部访问本机来说第一次是 NEW；OUTPUT 是对本机访问外部来说第一次是 NEW。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>注意：ConnTrack 中所定义的状态与 TCP 等协议所定义的状态不一样，这里面定义的状态只是为了可以通过一种新的方式来处理每一个数据包，并进行过滤，这是 Netfilter 中所定义的状态&lt;/p>
&lt;p>ConnTrack 功能依靠** nf_conntrack **模块来实现的，当启用 iptables 功能时(比如 firewalld)会自动加载该模块&lt;/p>
&lt;p>连接跟踪是防火墙模块的状态检测的基础，同时也是地址转换中实现 SNAT 和 DNAT 的基础，如果在 nat 表上没有连接跟踪，那么则没法进行 nat 转换(比如通过 raw 表来关闭连接跟踪)。&lt;/p>
&lt;h1 id="conntrack-table连接跟踪表">ConnTrack Table(连接跟踪表)&lt;/h1>
&lt;p>ConnTrack 将连接跟踪表保存于系统的内存当中，可以通过 &lt;strong>cat /proc/net/nf_conntrack &lt;strong>或&lt;/strong> conntrack -L &lt;strong>命令查看到当前已跟踪的所有&lt;/strong> ConnTrack Entry(连接跟踪条目)&lt;/strong>。不同的协议，条目的内容也不太一样，下面是一个 tcp 协议的条目内容：&lt;/p>
&lt;ul>
&lt;li>&lt;code>ipv4 2 tcp 6 299 ESTABLISHED src=192.168.2.40 dst=172.38.40.250 sport=61758 dport=22 src=172.38.40.250 dst=192.168.2.40 sport=22 dport=61758 [ASSURED] mark=0 zone=0 use=2&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>nf_conntrack 文件中，每个条目占用单独一行。条目中包含了数据包的原始方向信息(蓝色部分)，和期望的响应包信息(红色部分)，这样内核能够在后续到来的数据包中识别出属于此连接的双向数据包，并更新此连接的状态。&lt;/p>
&lt;p>在内核中，&lt;strong>ConnTrackTable(连接跟踪表)&lt;/strong> 实际上是一个 &lt;strong>hash table(哈希表)&lt;/strong>。收到一个数据包，通过如下步骤判断该数据包是否署一个已有连接(即定位连接跟踪条目)：&lt;/p>
&lt;ul>
&lt;li>第一步：内核提取数据包信息(源 IP、目的 IP、port，协议号)进行 hash 计算得到一个 hash 值，在哈希表中以此 hash 值做索引，索引结果为数据包所属的 &lt;strong>Bucket(储存区)&lt;/strong>。这一步 hash 计算时间固定并且很短。
&lt;ul>
&lt;li>一个 **Bucket(储存区) &lt;strong>里包含一个&lt;/strong> linked list(**已链接的列表，简称 &lt;strong>链表)&lt;/strong>，即已经追踪的条目的列表。也就是说，每个 Bucket 里可以存放多个 ConnTrack Entry。所谓 Bucket 的大小，就是指一个 Bucket 中可以存放多少个 ConnTrack Entry。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>第二步：遍历第一步获取的 Bucket 中的所有条目，查找是否有匹配的条目。这一步是比较耗时的操作，所以说 Bucket 越大，遍历时间越长&lt;/li>
&lt;/ul>
&lt;h2 id="bucket储存区">Bucket(储存区)&lt;/h2>
&lt;p>在 Connection Tracking 系统中的 hash table 中，有若干个 &lt;strong>Bucket(储存区)&lt;/strong>，Bucket 的个数通过两个内核参数计算而来&lt;/p>
&lt;ul>
&lt;li>net.netfilter.nf_conntrack_buckets # 一个表最大的 Bucket 数量。默认通过内存计算得来，内存越高，Bucket 越多。也可以通过设置模块参数指定具体的数值
&lt;ul>
&lt;li>无法通过 sysctl 修改 nf_conntrack_buckets 的值，该值只能通过加载 nf_conntrack 模块时的参数来决定。使用 &lt;code>echo &amp;quot;options nf_conntrack hashsize=16384&amp;quot; &amp;gt; /etc/modprobe.d/nf_conntrack.conf&lt;/code> 命令即可设置该内核参数为 16384&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>net.netfilter.nf_conntrack_max # 一个表最大的 Entry 数量。默认为 nf_conntrack_buckets 值的 4 倍。也就是说，&lt;strong>Bucket 的大小默认为 4&lt;/strong>，即系统默认每个 Bucket 中包含 4 个 ConnTrack Entry。
&lt;ul>
&lt;li>当不使用系统默认的 nf_conntrack_buckets 值时，则 nf_conntrack_max 的值为 nf_conntrack_buckets 的 8 倍&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>如果把一个 Bucket 的大小称为 &lt;code>BucketSize&lt;/code> 的话，那么&lt;code>BucketSize = nf_conntrack_max / nf_conntrack_buckets&lt;/code>(这意思就是说 &lt;code>储存区的大小=条目总数 / 储存区的总数&lt;/code>，所以储存区大小就是指能装多少条目)&lt;/p>
&lt;h2 id="conntrack-entry连接跟踪条目">ConnTrack Entry(连接跟踪条目)&lt;/h2>
&lt;p>conntrack 从经过它的数据包中提取详细的，唯一的信息，因此能保持对每一个连接的跟踪。关于 conntrack 如何确定一个连接，对于 tcp/udp，连接由他们的源目地址，源目端口唯一确定。对于 icmp，由 type，code 和 id 字段确定。&lt;/p>
&lt;pre>&lt;code>ipv4 2 tcp 6 33 SYN_SENT src=172.16.200.119 dst=172.16.202.12 sport=54786 dport=10051 [UNREPLIED] src=172.16.202.12 dst=172.16.200.119 sport=10051 dport=54786 mark=0 zone=0 use=2
&lt;/code>&lt;/pre>
&lt;p>如上是一条 conntrack 条目，它代表当前已跟踪到的某个连接，conntrack 维护的所有信息都包含在这个条目中，通过它就可以知道某个连接处于什么状态&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ipv4&lt;/strong> # 此连接使用 ipv4 协议，是一条 tcp 连接(tcp 的协议类型代码是 6)&lt;/li>
&lt;li>&lt;strong>33&lt;/strong> # 这条 conntrack 条目在当前时间点的生存时间(每个 conntrack 条目都会有生存时间，从设置值开始倒计时，倒计时完后此条目将被清除)，可以使用&lt;code>sysctl -a |grep conntrack | grep timeout&lt;/code>查看不同协议不同状态下生存时间设置值，当然这些设置值都可以调整，注意若后续有收到属于此连接的数据包，则此生存时间将被重置(重新从设置值开始倒计时)，并且状态改变，生存时间设置值也会响应改为新状态的值&lt;/li>
&lt;li>&lt;strong>SYN_SENT&lt;/strong> # 到此刻为止 conntrack 跟踪到的这个连接的状态(内核角度)，&lt;code>SYN_SENT&lt;/code>表示这个连接只在一个方向发送了一初始 TCP SYN 包，还未看到响应的 SYN+ACK 包(只有 tcp 才会有这个字段)。&lt;/li>
&lt;li>&lt;strong>src=172.16.200.119 dst=172.16.202.12 sport=54786 dport=10051&lt;/strong> #从数据包中提取的此连接的源目地址、源目端口，是 conntrack 首次看到此数据包时候的信息。&lt;/li>
&lt;li>&lt;strong>[UNREPLIED]&lt;/strong> # 说明此刻为止这个连接还没有收到任何响应，当一个连接已收到响应时，[UNREPLIED]标志就会被移除&lt;/li>
&lt;li>&lt;strong>src=172.16.202.12 dst=172.16.200.119 sport=10051 dport=54786&lt;/strong> # 地址和端口和前面是相反的，这部分不是数据包中带有的信息，是 conntrack 填充的信息，代表 conntrack 希望收到的响应包信息。意思是若后续 conntrack 跟踪到某个数据包信息与此部分匹配，则此数据包就是此连接的响应数据包。注意这部分确定了 conntrack 如何判断响应包(tcp/udp)，icmp 是依据另外几个字段&lt;/li>
&lt;/ul>
&lt;p>上面是 tcp 连接的条目，而 udp 和 icmp 没有连接建立和关闭过程，因此条目字段会有所不同，后面 iptables 状态匹配部分我们会看到处于各个状态的 conntrack 条目&lt;/p>
&lt;p>注意：conntrack 机制并不能够修改或过滤数据包，它只是跟踪网络连接并维护连接跟踪表，以提供给 iptables 做状态匹配使用，也就是说，如果你 iptables 中用不到状态匹配，那就没必要启用 conntrack&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>所以，一个 ConnTrack Table 就类似于下面的表：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>Hash Table&lt;/strong>&lt;/th>
&lt;th>Bucket 1&lt;/th>
&lt;th>Bucket 2&lt;/th>
&lt;th>Bucket 3&lt;/th>
&lt;th>&amp;hellip;&amp;hellip;.&lt;/th>
&lt;th>Bucket N&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Entry 1&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Entry 2&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&amp;hellip;&amp;hellip;..&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Entry N&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;td>条目内容&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="计算连接跟踪表所占内存">计算连接跟踪表所占内存&lt;/h1>
&lt;p>&lt;strong>total&lt;em>mem_used(单位为 Bytes) = nf_conntrack_max * sizeof(struct ip&lt;/em>conntrack) + nf_conntrack_buckets * sizeof(struct list_head)&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>sizeof(struct ip_conntrack) 连接跟踪对象大小，默认 376&lt;/li>
&lt;li>sizeof(struct list_head) 链表项大小，默认为 16&lt;/li>
&lt;/ol>
&lt;p>上述两个值可以通过如下 python 代码计算出来&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>import ctypes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#不同系统可能此库名不一样，需要修改&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LIBNETFILTER_CONNTRACK &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;libnetfilter_conntrack.so.3.6.0&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nfct &lt;span style="color:#f92672">=&lt;/span> ctypes.CDLL&lt;span style="color:#f92672">(&lt;/span>LIBNETFILTER_CONNTRACK&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print &lt;span style="color:#e6db74">&amp;#39;sizeof(struct nf_conntrack):&amp;#39;&lt;/span>, nfct.nfct_maxsize&lt;span style="color:#f92672">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print &lt;span style="color:#e6db74">&amp;#39;sizeof(struct list_head):&amp;#39;&lt;/span>, ctypes.sizeof&lt;span style="color:#f92672">(&lt;/span>ctypes.c_void_p&lt;span style="color:#f92672">)&lt;/span> * &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>假如，我系统信息如下：&lt;/p>
&lt;pre>&lt;code>[root@node-1 ~]# cat /proc/sys/net/netfilter/nf_conntrack_max
524288
[root@node-1 ~]# cat /proc/sys/net/netfilter/nf_conntrack_buckets
131072
&lt;/code>&lt;/pre>
&lt;p>那么，此系统下，连接跟踪表所占内存即为：&lt;/p>
&lt;pre>&lt;code>(524288 * 376 + 131072 * 16) / 1024 / 1024 = 190MiB
&lt;/code>&lt;/pre>
&lt;h1 id="conntrack-关联文件与配置">ConnTrack 关联文件与配置&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt">内核官方文档，网络-nf_conntrack-sysctl&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>连接跟踪系统的配置大部分都可以通过修改内核参数来进行，还有一部分需要通过指定 模块的参数 来配置。&lt;/p>
&lt;ul>
&lt;li>**/proc/net/nf_conntrack **# 连接跟踪表，该文件用于记录每一个连接跟踪条目
&lt;ul>
&lt;li>注意：Ubuntu 中没有该文件，可以通过 &lt;code>conntrack -L&lt;/code> 命令获取连接跟踪条目。据说该文件已 deprecated(弃用)，但是未找到官方说明&lt;/li>
&lt;li>&lt;a href="https://forum.ubuntu.com.cn/viewtopic.php?t=480072">https://forum.ubuntu.com.cn/viewtopic.php?t=480072&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://askubuntu.com/questions/266991/in-ubuntu-12-10-how-to-enable-proc-net-ip-conntrack">https://askubuntu.com/questions/266991/in-ubuntu-12-10-how-to-enable-proc-net-ip-conntrack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://patchwork.ozlabs.org/project/ubuntu-kernel/patch/1341986947-28300-3-git-send-email-bryan.wu@canonical.com/">https://patchwork.ozlabs.org/project/ubuntu-kernel/patch/1341986947-28300-3-git-send-email-bryan.wu@canonical.com/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/69589/files#r418929810">https://github.com/kubernetes/kubernetes/pull/69589/files#r418929810&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>/proc/sys/net/nf_conntrack_max&lt;/strong> # 等于 /proc/sys/net/netfilter/nf_conntrack_max 的值。修改这俩参数任意一个值，都会互相同步。&lt;/li>
&lt;li>**/proc/sys/net/netfilter/* **# 网络栈的运行时属性所在的目录
&lt;ul>
&lt;li>&lt;strong>./nf_conntrack_count&lt;/strong> # 当前连接跟踪数。&lt;/li>
&lt;li>**./nf_conntrack_max **# 连接跟踪表的大小，即一个表中有可以存放多少个条目。默认值为 nf_conntrack_buckets *4 。等于 /proc/sys/net/nf_conntrack_max 的值。&lt;/li>
&lt;li>&lt;strong>./nf_conntrack_buckets&lt;/strong> # hash 表的大小，即一个 hash 表中有多少个 Buckets。&lt;/li>
&lt;li>&lt;strong>./nf_conntrack_tcp_timeout_time_wait&lt;/strong> # timewait 状态的条目超时时间。 默认 120 秒&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="应用实例">应用实例&lt;/h1>
&lt;p>第一次 TCP 或 UDP 或 ICMP 等协议请求建立连接后，有一个持续时间，在持续时间内，这个连接信息会保存在连接跟踪表(记录在 nf_conntrack 文件中)中，当同一个 IP 再次请求的时候，这个请求的数据包则不会被当成 NEW 状态的数据包来处理(具体的状态有几种详见下文)，这个概念可以用在这么一个真实环境当中。&lt;/p>
&lt;h1 id="conntrack-命令行工具">conntrack 命令行工具&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote></description></item><item><title>Docs: Connection 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/connection-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/connection-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-nmcli.html">Manual(手册),nm-settings-nmcli(5)&lt;/a> # 这个 man 手册中，可以看到每个 Setting 中都有哪些 Property 以及这些 Property 的作用。&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-dbus.html">Manual(手册),nm-settings-dbus(5)&lt;/a> # 这里有 Property 的默认值&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-keyfile.html">Manual(手册),nm-settings-keyfile(5)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-ifcfg-rh.html">Manual(手册),nm-settings-ifcfg-rh(5)&lt;/a>&lt;/li>
&lt;li>在 &lt;a href="https://developer-old.gnome.org/NetworkManager/">GNOME 开发者中心官网&lt;/a>中，也可以查到 Manual&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Connection 配置文件默认由 keyfile 插件管理，是类似 INI 格式的。同时配置文件还会保存在 D-Bus 中。&lt;/p>
&lt;p>在 D-Bus 中，NetworkManager 将 INI 中的 Sections(部分) 称为** Settings(设置)**，Setting 多个是 &lt;strong>Properties(属性)&lt;/strong> 的集合。所以，很多文档，都将 Connection 表示为一组特定的、封装好的、独立的 &lt;strong>Settings(集合)&lt;/strong>。Connection 由一个或多个 Settings 组成。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 我启动了一个 连接&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nmcli con up bridge-slave-bond0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Connection successfully activated &lt;span style="color:#f92672">(&lt;/span>master waiting &lt;span style="color:#66d9ef">for&lt;/span> slaves&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">(&lt;/span>D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/16&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 从 D-Bus 的路径中可以看到这些信息&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># busctl get-property org.freedesktop.NetworkManager /org/freedesktop/NetworkManager/ActiveConnection/16 org.freedesktop.NetworkManager.Connection.Active&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Connection Default6 Dhcp4Config Id Ip6Config SpecificObject StateFlags Uuid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Default Devices Dhcp6Config Ip4Config Master State Type Vpn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 上面的信息是按 TAB 补全出来的&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># busctl get-property org.freedesktop.NetworkManager /org/freedesktop/NetworkManager/ActiveConnection/16 org.freedesktop.NetworkManager.Connection.Active Id&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>s &lt;span style="color:#e6db74">&amp;#34;bridge-slave-bond0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>注意：&lt;/p>
&lt;ul>
&lt;li>在&lt;a href="https://www.yuque.com/go/doc/33221856"> nmcli&lt;/a> 命令中使用 SETTING.PROPERTY 时，如果 SETTING 和 PROPERTY 是 唯一的，则可以使用 &lt;code>Alias(别名)&lt;/code>。
&lt;ul>
&lt;li>比如 connection.typ 的别名为 type。其实就是缩写，简化操作。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>802-3-ethernet 类型就是一般物理网卡，通常使用 ethernet 别名表示。不管 ethernet 是作为 SETTING 还是作为 VALUE，都可以使用别名。但是在 -f 选项中，不能使用别名。&lt;/li>
&lt;li>&lt;strong>在 RedHad 中，是无法从 /etc/NetworkManager/system-connections/ 目录中找到连接配置文件，这是因为 RedHad 系发行版使用的是 ifcfg-rh 插件&lt;/strong>&lt;/li>
&lt;li>配置文件可以通过命令行修改，比如 &lt;code>nmcli con add connection.type XXX&lt;/code>，其中 connection 就是 connection 这个 SETTING，type 就是 该 SETTING 下的一个属性
&lt;ul>
&lt;li>所以，命令行中的 &lt;strong>SETTING.PROPERTY&lt;/strong> 与配置文件是&lt;strong>一一对应的&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="connection-setting">connection SETTING&lt;/h1>
&lt;p>&lt;strong>autoconnect=&lt;!-- raw HTML omitted -->&lt;/strong> # 别名 autoconnect。该连接是否自动连接。
&lt;strong>id=&lt;!-- raw HTML omitted -->&lt;/strong> # 别名 con-name。该连接的名称。若不指定，则会默认生成一个
**interface-name STRING **# 别名 ifname。该连接绑定的网络设备名称。
**master=&lt;!-- raw HTML omitted --> **# 别名 master。该连接的主设备的 name 或 UUID。具有 master 属性的连接将会降级为从设备.常用于向 bond 或者 brdige 设备中添加从设备时使用。&lt;/p>
&lt;ul>
&lt;li>在使用 nmcli 命令时如果使用 master 别名，则会自动为连接添加 slave-type 属性，属性根据主设备的类型决定。如果不使用别名，则需要显式得使用 connection.slave-type 来指定该连接的从属类型。&lt;/li>
&lt;li>若主设备状态 down，则该从设备状态变为 lowerlayerdown&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>type=&lt;!-- raw HTML omitted -->&lt;/strong> # 别名 type。连接类型，常用的有 ethernet、bridge、bridge-slave、bond、bond-slave、tun 等等。其实就是要添加的连接的网络设备的类型。&lt;strong>必选，每个连接必须有一个 type&lt;/strong>&lt;/p>
&lt;h1 id="ipv4-setting">ipv4 SETTING&lt;/h1>
&lt;p>&lt;strong>address=&amp;lt;[]UNIT32&amp;gt;&lt;/strong> # 别名 ip4。指定该连接的 IP 地址。可以使用 192.168.0.0/24 这种格式&lt;/p>
&lt;ul>
&lt;li>若有多个 IP 地址，则&lt;/li>
&lt;/ul>
&lt;p>**dns=&amp;lt;[]UINT32&amp;gt; **# DNS 服务端 IP 地址列表。多个地址以 &lt;code>;&lt;/code> 分割。
&lt;strong>dns-search=&lt;!-- raw HTML omitted -->&lt;/strong> # 待补充
**gateway=&lt;!-- raw HTML omitted --> **# 别名 gw4。指定该连接的网关地址。
**method=&lt;!-- raw HTML omitted --> **# 该连接的 ipv4 获取方法。即通过 dhcp 获取还是手动指定等等&lt;/p>
&lt;ul>
&lt;li>auto # ipv4 地址可以自动获取(通过 dhcp、pp 等)。默认值&lt;/li>
&lt;li>disabled # 在此连接上不使用 ipv4。Note：如果在配置文件中不指定 ipv4 信息，那么在 reload 时，连接的 ipv4.method 也会变成该值&lt;/li>
&lt;li>link-local # 为该连接分配在 169.254/16 范围的本地地址&lt;/li>
&lt;li>manual # 手动指定 IP。如果使用此方法，则必须指定 ipv4.address 属性。&lt;/li>
&lt;li>shared # 表示此连接将提过对其他计算机的网络访问权限，为接口分配一个 10.42.x.1/24 的地址，并启动 dhcp 和 dns 转发服务，并且该接口为 Nat-ed 到当前的默认网络连接&lt;/li>
&lt;/ul>
&lt;p>**routes=&lt;!-- raw HTML omitted --> **# 设定作用在该连接上的路由条目，多个条目以逗号分割
&lt;strong>routing-rule=&lt;!-- raw HTML omitted -->&lt;/strong> # 设定路由策略。&lt;/p>
&lt;h1 id="ipv6-setting">ipv6 SETTING&lt;/h1>
&lt;p>&lt;strong>method=&lt;!-- raw HTML omitted -->&lt;/strong> # 与 ipv4.method 基本相同&lt;/p>
&lt;h1 id="bond-setting">bond SETTING&lt;/h1>
&lt;p>&lt;strong>options=&amp;lt;map[STRING]STRING&amp;gt;&lt;/strong> # bond 选项。以逗号分隔的键值对，每个键值对都对应 bond 的一个选项和其值。&lt;code>默认值：{'mode':'balance-rr'}&lt;/code>&lt;/p>
&lt;h1 id="vlan-setting">vlan SETTING&lt;/h1>
&lt;p>&lt;strong>id=&lt;!-- raw HTML omitted -->&lt;/strong> # 别名 id。此连接关联的网络设备的 VLAN 标识符。有效范围是 0 到 4094
&lt;strong>parent=&lt;!-- raw HTML omitted -->&lt;/strong> # 别名 dev。VLAN 网络设备的父设备的 name 或 UUID。即指定在哪个网络设备上附加 VLAN 标识符。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nmcli con add type vlan con-name vlan2-eth0 dev eth0 id 2 ipv4.method disabled ipv6.method disabled&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Connection &lt;span style="color:#e6db74">&amp;#39;vlan2-eth0&amp;#39;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>b54bd8d9-de8a-4e26-a579-8e9ff95f126f&lt;span style="color:#f92672">)&lt;/span> successfully added.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nmcli c s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME UUID TYPE DEVICE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>....
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>vlan2-eth0 0a557c85-f98a-44c3-a2a5-31645efb98b9 vlan eth0.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ip a s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>....
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>165: eth0.2@eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span style="color:#ae81ff">1500&lt;/span> qdisc noqueue state UP group default qlen &lt;span style="color:#ae81ff">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether 52:54:00:6a:86:89 brd ff:ff:ff:ff:ff:ff
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="802-3-ethernet-setting">802-3-ethernet SETTING&lt;/h1>
&lt;p>别名 ethernet&lt;/p>
&lt;p>&lt;strong>mtu=&lt;!-- raw HTML omitted -->&lt;/strong> # 别名 mtu。连接关联的物理设备的 MTU 的值。&lt;code>默认值：auto&lt;/code>&lt;/p>
&lt;h1 id="redhad-中-connection-与-老式配置-对应关系">RedHad 中 Connection 与 老式配置 对应关系&lt;/h1>
&lt;h1 id="imagepnghttpsnotes-learningoss-cn-beijingaliyuncscomqpdcnf1624352815778-2fd45e29-fc7b-495f-90e7-0bc0c2691a27png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qpdcnf/1624352815778-2fd45e29-fc7b-495f-90e7-0bc0c2691a27.png" alt="image.png">&lt;/h1>
&lt;h2 id="看一下老式配置的内容">看一下老式配置的内容&lt;/h2>
&lt;p>DEVICE=eth0 #指明与此配置文件相关联的网络设备名称
TYPE=Ethernet #指明该网络设备的类型
BOOTPROTO=none #指明设备获取 ip 的方法，有 atuo、none、disabled 等&lt;/p>
&lt;ol>
&lt;li>auto # 自动获取 ip(通过 dhcp 等方法)&lt;/li>
&lt;li>none # 没有方法，也就是说手动指定 IP。可以手动指定 IPADDR、PREFIX、GATEWAY 等&lt;/li>
&lt;li>disabled # 不获取 ip，如果不指定 IPADDR、NETMASK、GATEWAY 的话，默认就是 disabled&lt;/li>
&lt;/ol>
&lt;p>IPADDR=192.168.10.22 #指定该设备 IP
NETMASK=255.255.255.0 #指定该设备的掩码&lt;/p>
&lt;ol>
&lt;li>PREFIX=24 #也可是使用这种方式来表示掩码&lt;/li>
&lt;/ol>
&lt;p>GATEWAY=192.168.10.2 #指定该设备的网关
DEFROUTE=yes #该网卡的路由是否设置为默认路由
ONBOOT=yes|no #启动网卡时是否自动加载该配置文件
USERCTL={yes|no} #非 root 用户是否可以控制该设备
PROXY_METHOD=none
BROWSER_ONLY=no
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=eth0 # 用于 NetworkManager 服务中，作为连接名字。
UUID=74b13ff5-697f-43cf-abdc-27389b57ecbe
ZONE=public&lt;/p>
&lt;h1 id="keyfile-配置文件示例">keyfile 配置文件示例&lt;/h1>
&lt;h2 id="一个以太网的配置">一个以太网的配置&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>connection&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>id&lt;span style="color:#f92672">=&lt;/span>Main eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uuid&lt;span style="color:#f92672">=&lt;/span>27afa607-ee36-43f0-b8c3-9d245cdc4bb3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type&lt;span style="color:#f92672">=&lt;/span>802-3-ethernet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>autoconnect&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>ipv4&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>method&lt;span style="color:#f92672">=&lt;/span>auto
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>802-3-ethernet&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mac-address&lt;span style="color:#f92672">=&lt;/span>00:23:5a:47:1f:71
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="简单的桥设备配置">简单的桥设备配置&lt;/h2>
&lt;p>桥设备&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>connection&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>id&lt;span style="color:#f92672">=&lt;/span>MainBridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uuid&lt;span style="color:#f92672">=&lt;/span>171ae855-a0ab-42b6-bd0c-60f5812eea9d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>interface-name&lt;span style="color:#f92672">=&lt;/span>MainBridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type&lt;span style="color:#f92672">=&lt;/span>bridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>bridge&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>interface-name&lt;span style="color:#f92672">=&lt;/span>MainBridge
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>桥的从设备&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>connection&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>id&lt;span style="color:#f92672">=&lt;/span>br-port-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uuid&lt;span style="color:#f92672">=&lt;/span>d6e8ae98-71f8-4b3d-9d2d-2e26048fe794
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>interface-name&lt;span style="color:#f92672">=&lt;/span>em1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type&lt;span style="color:#f92672">=&lt;/span>ethernet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>master&lt;span style="color:#f92672">=&lt;/span>MainBridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>slave-type&lt;span style="color:#f92672">=&lt;/span>bridge
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>VLAN 设备&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>connection&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>id&lt;span style="color:#f92672">=&lt;/span>VLAN &lt;span style="color:#66d9ef">for&lt;/span> building 4A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uuid&lt;span style="color:#f92672">=&lt;/span>8ce1c9e0-ce7a-4d2c-aa28-077dda09dd7e
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>interface-name&lt;span style="color:#f92672">=&lt;/span>VLAN-4A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type&lt;span style="color:#f92672">=&lt;/span>vlan
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>vlan&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>interface-name&lt;span style="color:#f92672">=&lt;/span>VLAN-4A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>parent&lt;span style="color:#f92672">=&lt;/span>eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>id&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Connnection Tracking(连接跟踪)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/connnection-tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/connnection-tracking%E8%BF%9E%E6%8E%A5%E8%B7%9F%E8%B8%AA/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">arthurchiao,连接跟踪：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Connection Tracking(连接跟踪系统，简称 ConnTrack、CT)&lt;/strong>，用于跟踪并且记录连接状态。CT 是&lt;a href="https://www.yuque.com/go/doc/34346416">流量控制系统&lt;/a>的基础
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617860207674-43ea3c6d-0d0f-4fac-bccb-90e752e75a47.png" alt="">
例如，上图是一台 IP 地址为 &lt;code>10.1.1.2&lt;/code> 的 Linux 机器，我们能看到这台机器上有三条 连接：&lt;/p>
&lt;ol>
&lt;li>机器访问外部 HTTP 服务的连接（目的端口 80）&lt;/li>
&lt;li>外部访问机器内 FTP 服务的连接（目的端口 21）&lt;/li>
&lt;li>机器访问外部 DNS 服务的连接（目的端口 53）&lt;/li>
&lt;/ol>
&lt;p>连接跟踪所做的事情就是发现并跟踪这些连接的状态，具体包括：&lt;/p>
&lt;ul>
&lt;li>从数据包中提取&lt;strong>元组&lt;/strong>（tuple）信息，辨别&lt;strong>数据流&lt;/strong>（flow）和对应的&lt;strong>连接&lt;/strong>（connection）&lt;/li>
&lt;li>为所有连接维护一个&lt;strong>状态数据库&lt;/strong>（conntrack table），例如连接的创建时间、发送 包数、发送字节数等等&lt;/li>
&lt;li>回收过期的连接（GC）&lt;/li>
&lt;li>为更上层的功能（例如 NAT）提供服务&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，&lt;strong>连接跟踪中所说的“连接”，概念和 TCP/IP 协议中“面向连接”（ connection oriented）的“连接”并不完全相同&lt;/strong>，简单来说：&lt;/p>
&lt;ul>
&lt;li>TCP/IP 协议中，连接是一个四层（Layer 4）的概念。
&lt;ul>
&lt;li>TCP 是有连接的，或称面向连接的（connection oriented），发送出去的包都要求对端应答（ACK），并且有重传机制&lt;/li>
&lt;li>UDP 是无连接的，发送的包无需对端应答，也没有重传机制&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CT 中，一个元组（tuple）定义的一条数据流（flow ）就表示一条连接（connection）。
&lt;ul>
&lt;li>后面会看到 UDP 甚至是 &lt;strong>ICMP 这种三层协议在 CT 中也都是有连接记录的&lt;/strong>&lt;/li>
&lt;li>但&lt;strong>不是所有协议都会被连接跟踪&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>本文中用到“连接”一词时，大部分情况下指的都是后者，即“连接跟踪”中的“连接”。&lt;/p>
&lt;h1 id="原理">原理&lt;/h1>
&lt;p>要跟踪一台机器的所有连接状态，就需要：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>拦截（或称过滤）流经这台机器的每一个数据包，并进行分析&lt;/strong>。&lt;/li>
&lt;li>根据这些信息&lt;strong>建立&lt;/strong>起这台机器上的&lt;strong>连接信息数据库&lt;/strong>（conntrack table）。&lt;/li>
&lt;li>根据拦截到的包信息，不断更新数据库&lt;/li>
&lt;/ul>
&lt;p>例如&lt;/p>
&lt;ul>
&lt;li>拦截到一个 TCP SYNC 包时，说明正在尝试建立 TCP 连接，需要创建一条新 conntrack entry 来记录这条连接&lt;/li>
&lt;li>拦截到一个属于已有 conntrack entry 的包时，需要更新这条 conntrack entry 的收发包数等统计信息&lt;/li>
&lt;/ul>
&lt;p>除了以上两点功能需求，还要考虑&lt;strong>性能问题&lt;/strong>，因为连接跟踪要对每个包进行过滤和分析 。性能问题非常重要，但不是本文重点，后面介绍实现时会进一步提及。
之外，这些功能最好还有配套的管理工具来更方便地使用。&lt;/p>
&lt;h2 id="connection-tracking-的实现">Connection Tracking 的实现&lt;/h2>
&lt;p>现在(2021 年 4 月 9 日)提到连接跟踪（conntrack），可能首先都会想到 Netfilter。但由上节讨论可知， 连接跟踪概念是独立于 Netfilter 的，&lt;strong>Netfilter 只是 Linux 内核中的一种连接跟踪实现&lt;/strong>。&lt;/p>
&lt;p>换句话说，&lt;strong>只要具备了 hook 能力，能拦截到进出主机的每个包，完全可以在此基础上自 己实现一套连接跟踪&lt;/strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861067581-3b23cb80-cd1f-4d7d-9767-57581c62233b.png" alt="">&lt;/p>
&lt;p>云原生网络方案 Cilium 在 &lt;code>1.7.4+&lt;/code> 版本就实现了这样一套独立的连接跟踪和 NAT 机制 （完备功能需要 Kernel &lt;code>4.19+&lt;/code>）。其基本原理是：&lt;/p>
&lt;ol>
&lt;li>基于 BPF hook 实现数据包的拦截功能（等价于 netfilter 里面的 hook 机制）&lt;/li>
&lt;li>在 BPF hook 的基础上，实现一套全新的 conntrack 和 NAT&lt;/li>
&lt;/ol>
&lt;p>因此，即便&lt;a href="https://github.com/cilium/cilium/issues/12879">卸载 Netfilter&lt;/a> ，也不会影响 Cilium 对 Kubernetes ClusterIP、NodePort、ExternalIPs 和 LoadBalancer 等功能的支持 [2]。
由于这套连接跟踪机制是独立于 Netfilter 的，因此它的 conntrack 和 NAT 信息也没有 存储在内核的（也就是 Netfilter 的）conntrack table 和 NAT table。所以常规的 &lt;code>conntrack/netstats/ss/lsof&lt;/code> 等工具是看不到的，要使用 Cilium 的命令，例如：&lt;/p>
&lt;pre>&lt;code>$ cilium bpf nat list
$ cilium bpf ct list global
&lt;/code>&lt;/pre>
&lt;p>配置也是独立的，需要在 Cilium 里面配置，例如命令行选项 &lt;code>--bpf-ct-tcp-max&lt;/code>。
另外，本文会多次提到连接跟踪模块和 NAT 模块独立，但&lt;strong>出于性能考虑，具体实现中 二者代码可能是有耦合的&lt;/strong>。例如 Cilium 做 conntrack 的垃圾回收（GC）时就会顺便把 NAT 里相应的 entry 回收掉，而非为 NAT 做单独的 GC。&lt;/p>
&lt;h3 id="netfilter-中的-connection-tracking">Netfilter 中的 Connection Tracking&lt;/h3>
&lt;p>详见 &lt;a href="https://www.yuque.com/go/doc/33221811">Netifilter 中 Connection Tracking 章节&lt;/a>&lt;/p>
&lt;h3 id="bpf-中的-connection-tracking">BPF 中的 Connection Tracking&lt;/h3>
&lt;h2 id="connection-tracking-的应用实践">Connection Tracking 的应用实践&lt;/h2>
&lt;p>来看几个 conntrack 的具体应用。&lt;/p>
&lt;h3 id="151-网络地址转换nat">1.5.1 网络地址转换（NAT）&lt;/h3>
&lt;p>网络地址转换（NAT），名字表达的意思也比较清楚：对（数据包的）网络地址（&lt;code>IP + Port&lt;/code>）进行转换。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861112761-712ea43a-0ed4-4c94-8758-3f593fc3a1b6.png" alt="">
Fig 1.4. NAT 及其内核位置示意图
例如上图中，机器自己的 IP &lt;code>10.1.1.2&lt;/code> 是能与外部正常通信的，但 &lt;code>192.168&lt;/code> 网段是私有 IP 段，外界无法访问，也就是说源 IP 地址是 &lt;code>192.168&lt;/code> 的包，其&lt;strong>应答包是无 法回来的&lt;/strong>。因此&lt;/p>
&lt;ul>
&lt;li>当源地址为 &lt;code>192.168&lt;/code> 网段的包要出去时，机器会先将源 IP 换成机器自己的 &lt;code>10.1.1.2&lt;/code> 再发送出去；&lt;/li>
&lt;li>收到应答包时，再进行相反的转换。&lt;/li>
&lt;/ul>
&lt;p>这就是 NAT 的基本过程。
Docker 默认的 &lt;code>bridge&lt;/code> 网络模式就是这个原理 [4]。每个容器会分一个私有网段的 IP 地址，这个 IP 地址可以在宿主机内的不同容器之间通信，但容器流量出宿主机时要进行 NAT。
NAT 又可以细分为几类：&lt;/p>
&lt;ul>
&lt;li>SNAT：对源地址（source）进行转换&lt;/li>
&lt;li>DNAT：对目的地址（destination）进行转换&lt;/li>
&lt;li>Full NAT：同时对源地址和目的地址进行转换&lt;/li>
&lt;/ul>
&lt;p>以上场景属于 SNAT，将不同私有 IP 都映射成同一个“公有 IP”，以使其能访问外部网络服 务。这种场景也属于正向代理。
NAT 依赖连接跟踪的结果。连接跟踪&lt;strong>最重要的使用场景&lt;/strong>就是 NAT。&lt;/p>
&lt;h4 id="四层负载均衡l4lb">四层负载均衡（L4LB）&lt;/h4>
&lt;p>再将范围稍微延伸一点，讨论一下 NAT 模式的四层负载均衡。
四层负载均衡是根据包的四层信息（例如 &lt;code>src/dst ip, src/dst port, proto&lt;/code>）做流量分发。
VIP（Virtual IP）是四层负载均衡的一种实现方式：&lt;/p>
&lt;ul>
&lt;li>多个后端真实 IP（Real IP）挂到同一个虚拟 IP（VIP）上&lt;/li>
&lt;li>客户端过来的流量先到达 VIP，再经负载均衡算法转发给某个特定的后端 IP&lt;/li>
&lt;/ul>
&lt;p>如果在 VIP 和 Real IP 节点之间使用的 NAT 技术（也可以使用其他技术），那客户端访 问服务端时，L4LB 节点将做双向 NAT（Full NAT），数据流如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861112756-297f87b7-f40e-4886-899a-65629964fd2c.png" alt="">
Fig 1.5. L4LB: Traffic path in NAT mode [3]&lt;/p>
&lt;h3 id="152-有状态防火墙">1.5.2 有状态防火墙&lt;/h3>
&lt;p>有状态防火墙（stateful firewall）是相对于早期的&lt;strong>无状态防火墙&lt;/strong>（stateless firewall）而言的：早期防火墙只能写 &lt;code>drop syn&lt;/code> 或者 &lt;code>allow syn&lt;/code> 这种非常简单直接 的规则，&lt;strong>没有 flow 的概念&lt;/strong>，因此无法实现诸如 &lt;strong>“如果这个 ack 之前已经有 syn， 就 allow，否则 drop”&lt;/strong> 这样的规则，使用非常受限 [6]。
显然，要实现有状态防火墙，就必须记录 flow 和状态，这正是 conntrack 做的事情。
来看个更具体的防火墙应用：OpenStack 主机防火墙解决方案 —— 安全组（security group）。&lt;/p>
&lt;h4 id="openstack-安全组">OpenStack 安全组&lt;/h4>
&lt;p>简单来说，安全组实现了&lt;strong>虚拟机级别&lt;/strong>的安全隔离，具体实现是：在 node 上连接 VM 的 网络设备上做有状态防火墙。在当时，最能实现这一功能的可能就是 Netfilter/iptables。
回到宿主机内网络拓扑问题： OpenStack 使用 OVS bridge 来连接一台宿主机内的所有 VM。 如果只从网络连通性考虑，那每个 VM 应该直接连到 OVS bridge &lt;code>br-int&lt;/code>。但这里问题 就来了 [7]：&lt;/p>
&lt;ul>
&lt;li>OVS 没有 conntrack 模块，&lt;/li>
&lt;li>Linux 中有 conntrack 模块，但基于 conntrack 的防火墙&lt;strong>工作在 IP 层&lt;/strong>（L3），通过 iptables 控制，&lt;/li>
&lt;li>而 &lt;strong>OVS 是 L2 模块&lt;/strong>，无法使用 L3 模块的功能，&lt;/li>
&lt;/ul>
&lt;p>因此无法在 OVS （连接虚拟机）的设备上做防火墙。
所以，2016 之前 OpenStack 的解决方案是，在每个 OVS 和 VM 之间再加一个 Linux bridge ，如下图所示，
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ynfo7m/1617861113322-0f9c4ca7-ffca-43ab-840d-78db13d23008.png" alt="">
Fig 1.6. Network topology within an OpenStack compute node, picture from &lt;a href="https://thesaitech.wordpress.com/2017/09/24/how-to-trace-the-tap-interfaces-and-linux-bridges-on-the-hypervisor-your-openstack-vm-is-on/">Sai&amp;rsquo;s Blog&lt;/a>
Linux bridge 也是 L2 模块，按道理也无法使用 iptables。但是，&lt;strong>它有一个 L2 工具 ebtables，能够跳转到 iptables&lt;/strong>，因此间接支持了 iptables，也就能用到 Netfilter/iptables 防火墙的功能。
这种 workaround 不仅丑陋、增加网络复杂性，而且会导致性能问题。因此， RedHat 在 2016 年提出了一个 OVS conntrack 方案 [7]，从那以后，才有可能干掉 Linux bridge 而仍然具备安全组的功能。&lt;/p></description></item><item><title>Docs: firewalld 命令行工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/firewalldiptables-%E7%9A%84%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/firewalld-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/firewalldiptables-%E7%9A%84%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/firewalld-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="firewall-cmd">firewall-cmd&lt;/h1>
&lt;p>所有命令都是对当前默认 ZONE(通过 firewall-cmd &amp;ndash;get-default-zone 命令获得当前默认 zone)进行操作，如果想要对指定 ZONE 操作，需要使用&amp;ndash;zone=XXX&lt;/p>
&lt;p>OPTIONS&lt;/p>
&lt;ul>
&lt;li>
&lt;p>--reload #重新加载防火墙规则并保留连接状态信息。注意：reload 会删除所有 runtime 模式的配置并应用 permanent 模式的配置。但是已经建立的连接不受影响(e.g.已经在对本机长 ping 的设备不会断开连接)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--complete-reload #重新加载防火墙规则并丢弃连接状态信息。注意：与 reload 不同，已经建立的连接会被丢弃(e.g.已经在对本机长 ping 的设备会断开连接)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--permanent #开启永久模式，在该模式的配置都会永久保留&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--add-source=&lt;!-- raw HTML omitted --> #绑定 SOURCE 到一个 ZONE。SOURCE 可以使 MASK、MAC、ipset&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查&lt;/p>
&lt;ul>
&lt;li>
&lt;p>--get-default-zone #打印出当前默认的 ZONE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--list-services #列出一个 ZONE 中已经添加了的 service&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--list-interfaces #列出一个 ZONE 中已经绑定的网络设备&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--list-rich-rules #列出一个 ZONE 中已经添加的丰富语言规则&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>EXAMPLE&lt;/p>
&lt;ul>
&lt;li>
&lt;p>firewall-cmd &amp;ndash;zone=drop &amp;ndash;change-interface=eth1 #将经由 eth1 网卡的所有流量放在 drop 区域中进行处理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>firewall-cmd &amp;ndash;zone=public &amp;ndash;add-service=cockpit &amp;ndash;permanent #在永久模式下，允许 cockpit 服务的流量通过 public 这个 ZONE 内的网络设备&lt;/p>
&lt;/li>
&lt;li>
&lt;p>增&lt;/p>
&lt;ul>
&lt;li>
&lt;p>firewall-cmd &amp;ndash;add-source=10.10.10.0/24 #绑定 10.10.10.0/24 网段的 IP 到默认 ZONE 中&lt;/p>
&lt;/li>
&lt;li>
&lt;p>firewall-cmd &amp;ndash;add-port=9100/tcp &amp;ndash;permanent # 永久添加 9100 端口到默认 zone 中。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>查&lt;/p>
&lt;ul>
&lt;li>firewall-cmd &amp;ndash;list-services #列出当前默认 ZONE 中已经添加了的 service&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>rich-rule(丰富语言规则)i.e.高级模式，可以配置更详细的规则&lt;/p>
&lt;ul>
&lt;li>
&lt;p>firewall-cmd &amp;ndash;add-rich-rule=&amp;lsquo;rule family=ipv4 source address=&amp;ldquo;10.10.10.10&amp;rdquo; port port=1234 protocol=tcp accept&amp;rsquo;&lt;/p>
&lt;ul>
&lt;li>-A IN_public_allow -s 10.10.10.10/32 -p tcp -m tcp &amp;ndash;dport 1234 -m conntrack &amp;ndash;ctstate NEW -j ACCEPT&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: firewalld(Iptables 的管理工具)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/firewalldiptables-%E7%9A%84%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/firewalldiptables-%E7%9A%84%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;p>firewalld 与 iptabels 一样，是管理 Linux 内核中的 Netfilter 功能的工具。&lt;/p>
&lt;p>FirewallD 使用两个配置模式：“runtime 运行时”和“permanent 持久”。&lt;/p>
&lt;ol>
&lt;li>runtime 模式：默认模式。所有配置即时生效，在重启系统、重新启动 FirewallD 时、使用&amp;ndash;reload 重载配置等操作是，在该模式下的配置都将被清除。&lt;/li>
&lt;li>permanent 模式：需要使用 &amp;ndash;permanent 选项生效，配置才会永久保存。如果想让 permanetn 模式下的配置立即生效，需要使用&amp;ndash;reload 命令或者重启 firewalld 服务。&lt;/li>
&lt;/ol>
&lt;h2 id="firewalld-中-zone区域的概念">firewalld 中 zone(区域)的概念&lt;/h2>
&lt;p>“区域”是针对给定位置或场景（例如家庭、公共、受信任等）可能具有的各种信任级别的预构建规则集。不同的区域允许不同的网络服务和入站流量类型，而拒绝其他任何流量。 首次启用 FirewallD 后，public 将是默认区域。&lt;/p>
&lt;p>区域也可以用于不同的网络接口。例如，要分离内部网络和互联网的接口，你可以在 internal 区域上允许 DHCP，但在 external 区域仅允许 HTTP 和 SSH。未明确设置为特定区域的任何接口将添加到默认区域。&lt;/p>
&lt;p>所以，一般情况下，所有区域都是拒绝所有新的入站流量，对已经建立连接的不再阻止。在使用 firewall-cmd 命令添加某 service、port、ip 等属性时，相当于允许对应属性的流量入站。i.e.添加进去就表示允许&lt;/p>
&lt;h2 id="zone-的种类与说明">zone 的种类与说明：&lt;/h2>
&lt;ol>
&lt;li>public（公共） #默认的 zone。在公共区域内使用，不能相信网络内的其他计算机不会对您的计算机造成危害，只能接收经过选取的连接。&lt;/li>
&lt;li>block（限制） #任何接收的网络连接都被 IPv4 的 icmp-host-prohibited 信息和 IPv6 的 icmp6-adm-prohibited 信息所拒绝。&lt;/li>
&lt;li>dmz（非军事区） #用于您的非军事区内的电脑，此区域内可公开访问，可以有限地进入您的内部网络，仅仅接收经过选择的连接。&lt;/li>
&lt;li>drop（丢弃） #任何接收的网络数据包都被丢弃，没有任何回复。仅能有发送出去的网络连接。&lt;/li>
&lt;li>external（外部） #特别是为路由器启用了伪装功能的外部网。您不能信任来自网络的其他计算，不能相信它们不会对您的计算机造成危害，只能接收经过选择的连接。&lt;/li>
&lt;li>home（家庭） #用于家庭网络。您可以基本信任网络内的其他计算机不会危害您的计算机。仅仅接收经过选择的连接。&lt;/li>
&lt;li>internal（内部） #用于内部网络。您可以基本上信任网络内的其他计算机不会威胁您的计算机。仅仅接受经过选择的连接。&lt;/li>
&lt;li>trusted（信任） #可接受所有的网络连接。&lt;/li>
&lt;li>work（工作） #用于工作区。您可以基本相信网络内的其他电脑不会危害您的电脑。仅仅接收经过选择的连接。&lt;/li>
&lt;/ol>
&lt;p>用实际举例：将设备某个网卡放在区域中，则流经该网卡的流量都会遵循该区域中所定义的规则。&lt;/p>
&lt;h1 id="firewalld-关联文件与配置">Firewalld 关联文件与配置&lt;/h1>
&lt;p>/usr/lib/firewalld # 保存默认配置，如默认区域和公用服务。 避免修改它们，因为每次 firewall 软件包更新时都会覆盖这些文件。
/etc/firewalld # 保存系统配置文件。这些文件将覆盖默认配置。&lt;/p>
&lt;ul>
&lt;li>firewalld.conf #&lt;/li>
&lt;/ul>
&lt;h1 id="firewall-安装完成后的-iptables-模式的默认配置">firewall 安装完成后的 iptables 模式的默认配置&lt;/h1>
&lt;p>以下是 public 区域下 filter 表的默认配置，大部分都是对于自定义链的规则&lt;/p>
&lt;ul>
&lt;li>
&lt;h1 id="设置-3-个基本链的默认-target">设置 3 个基本链的默认 target&lt;/h1>
&lt;/li>
&lt;li>-P INPUT ACCEPT&lt;/li>
&lt;li>-P FORWARD ACCEPT&lt;/li>
&lt;li>-P OUTPUT ACCEPT&lt;/li>
&lt;li>
&lt;h1 id="默认会创建多个自定义链">默认会创建多个自定义链&lt;/h1>
&lt;/li>
&lt;li>-N FORWARD_IN_ZONES&lt;/li>
&lt;li>-N FORWARD_IN_ZONES_SOURCE&lt;/li>
&lt;li>-N FORWARD_OUT_ZONES&lt;/li>
&lt;li>-N FORWARD_OUT_ZONES_SOURCE&lt;/li>
&lt;li>-N FORWARD_direct&lt;/li>
&lt;li>-N FWDI_public&lt;/li>
&lt;li>-N FWDI_public_allow&lt;/li>
&lt;li>-N FWDI_public_deny&lt;/li>
&lt;li>-N FWDI_public_log&lt;/li>
&lt;li>-N FWDO_public&lt;/li>
&lt;li>-N FWDO_public_allow&lt;/li>
&lt;li>-N FWDO_public_deny&lt;/li>
&lt;li>-N FWDO_public_log&lt;/li>
&lt;li>-N INPUT_ZONES&lt;/li>
&lt;li>-N INPUT_ZONES_SOURCE&lt;/li>
&lt;li>-N INPUT_direct&lt;/li>
&lt;li>-N IN_public&lt;/li>
&lt;li>-N IN_public_allow&lt;/li>
&lt;li>-N IN_public_deny&lt;/li>
&lt;li>-N IN_public_log&lt;/li>
&lt;li>-N OUTPUT_direct&lt;/li>
&lt;li>
&lt;h1 id="设置-input-链基本规则所有流量直接交给-input_directinput_zones_sourceinput_zones-这-3-个自定义链来继续进行规则匹配">设置 INPUT 链基本规则，所有流量直接交给 INPUT_direct、INPUT_ZONES_SOURCE、INPUT_ZONES 这 3 个自定义链来继续进行规则匹配。&lt;/h1>
&lt;/li>
&lt;li>-A INPUT -m conntrack &amp;ndash;ctstate RELATED,ESTABLISHED -j ACCEPT&lt;/li>
&lt;li>-A INPUT -i lo -j ACCEPT&lt;/li>
&lt;li>-A INPUT -j INPUT_direct&lt;/li>
&lt;li>-A INPUT -j INPUT_ZONES_SOURCE&lt;/li>
&lt;li>-A INPUT -j INPUT_ZONES #流量转给 INPUT 的 ZONES&lt;/li>
&lt;li>-A INPUT -m conntrack &amp;ndash;ctstate INVALID -j DROP&lt;/li>
&lt;li>-A INPUT -j REJECT &amp;ndash;reject-with icmp-host-prohibited #在 INPUT 链上拒绝所有流量，并通过 icmp 协议提示客户端 prohibited&lt;/li>
&lt;li>
&lt;h1 id="设置-forward-链基本规则所有流量直接交给">设置 FORWARD 链基本规则，所有流量直接交给&lt;/h1>
&lt;/li>
&lt;li>-A FORWARD -m conntrack &amp;ndash;ctstate RELATED,ESTABLISHED -j ACCEPT&lt;/li>
&lt;li>-A FORWARD -i lo -j ACCEPT&lt;/li>
&lt;li>-A FORWARD -j FORWARD_direct&lt;/li>
&lt;li>-A FORWARD -j FORWARD_IN_ZONES_SOURCE&lt;/li>
&lt;li>-A FORWARD -j FORWARD_IN_ZONES&lt;/li>
&lt;li>-A FORWARD -j FORWARD_OUT_ZONES_SOURCE&lt;/li>
&lt;li>-A FORWARD -j FORWARD_OUT_ZONES&lt;/li>
&lt;li>-A FORWARD -m conntrack &amp;ndash;ctstate INVALID -j DROP&lt;/li>
&lt;li>-A FORWARD -j REJECT &amp;ndash;reject-with icmp-host-prohibited #在 FORWARD 链上拒绝所有流量，并通过 icmp 协议提示客户端 prohibited&lt;/li>
&lt;li>
&lt;h1 id="设置-output-链基本规则直接把后续检查转交给-output_direct-这个自定义-chain-进行规则匹配">设置 OUTPUT 链基本规则，直接把后续检查转交给 OUTPUT_direct 这个自定义 chain 进行规则匹配&lt;/h1>
&lt;/li>
&lt;li>-A OUTPUT -j OUTPUT_direct&lt;/li>
&lt;li>
&lt;h1 id="forward-相关的自定义-chain-规则">FORWARD 相关的自定义 chain 规则&lt;/h1>
&lt;/li>
&lt;li>-A FORWARD_IN_ZONES -i bond0 -g FWDI_public&lt;/li>
&lt;li>-A FORWARD_IN_ZONES -i eth2 -g FWDI_public&lt;/li>
&lt;li>-A FORWARD_IN_ZONES -i eth1 -g FWDI_public&lt;/li>
&lt;li>-A FORWARD_IN_ZONES -i eth0 -g FWDI_public&lt;/li>
&lt;li>-A FORWARD_IN_ZONES -g FWDI_public&lt;/li>
&lt;li>-A FORWARD_OUT_ZONES -o bond0 -g FWDO_public&lt;/li>
&lt;li>-A FORWARD_OUT_ZONES -o eth2 -g FWDO_public&lt;/li>
&lt;li>-A FORWARD_OUT_ZONES -o eth1 -g FWDO_public&lt;/li>
&lt;li>-A FORWARD_OUT_ZONES -o eth0 -g FWDO_public&lt;/li>
&lt;li>-A FORWARD_OUT_ZONES -g FWDO_public&lt;/li>
&lt;li>-A FWDI_public -j FWDI_public_log&lt;/li>
&lt;li>-A FWDI_public -j FWDI_public_deny&lt;/li>
&lt;li>-A FWDI_public -j FWDI_public_allow&lt;/li>
&lt;li>-A FWDI_public -p icmp -j ACCEPT&lt;/li>
&lt;li>-A FWDO_public -j FWDO_public_log&lt;/li>
&lt;li>-A FWDO_public -j FWDO_public_deny&lt;/li>
&lt;li>-A FWDO_public -j FWDO_public_allow&lt;/li>
&lt;li>
&lt;h1 id="input-相关的自定义-chain-规则">INPUT 相关的自定义 chain 规则&lt;/h1>
&lt;/li>
&lt;li>
&lt;h1 id="input_zones-用来将各个网络设备区分到指定的-zone-中">INPUT_ZONES 用来将各个网络设备区分到指定的 ZONE 中&lt;/h1>
&lt;/li>
&lt;li>-A INPUT_ZONES -i bond0 -g IN_public&lt;/li>
&lt;li>-A INPUT_ZONES -i eth2 -g IN_public&lt;/li>
&lt;li>-A INPUT_ZONES -i eth1 -g IN_public&lt;/li>
&lt;li>-A INPUT_ZONES -i eth0 -g IN_public #将 eth0 的流量放到 public 区域中继续进行匹配&lt;/li>
&lt;li>-A INPUT_ZONES -g IN_public&lt;/li>
&lt;li>
&lt;h1 id="input-链上的-public-区域的规则">INPUT 链上的 public 区域的规则&lt;/h1>
&lt;/li>
&lt;li>-A IN_public -j IN_public_log&lt;/li>
&lt;li>-A IN_public -j IN_public_deny&lt;/li>
&lt;li>-A IN_public -j IN_public_allow&lt;/li>
&lt;li>-A IN_public -p icmp -j ACCEPT&lt;/li>
&lt;li>-A IN_public_allow -p tcp -m tcp &amp;ndash;dport 22 -m conntrack &amp;ndash;ctstate NEW -j ACCEPT&lt;/li>
&lt;li>
&lt;h1 id="xxxx-区域的规则">XXXX 区域的规则&lt;/h1>
&lt;/li>
&lt;li>。。。。每当一个网络设备被放到某个区域中，这个区域就会激活，会在整个 iptables 表中显示，可以使用 firewall-cmd &amp;ndash;zone=drop &amp;ndash;change-interface=eth1 进行验证&lt;/li>
&lt;/ul>
&lt;p>从下往上看的话，firewalld 会默认方通 22 端口(i.e.方通 sshd 服务)和 icmp 协议，并且自定义规则链的数据结构详见脑图 firewalld 之 filter 表基本配置图.mindmap&lt;/p></description></item><item><title>Docs: ftp 协议的实现工具，chroot说明</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/ftp-%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%B7%A5%E5%85%B7chroot%E8%AF%B4%E6%98%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/ftp-%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%B7%A5%E5%85%B7chroot%E8%AF%B4%E6%98%8E/</guid><description>
&lt;h1 id="vsftp实现-ftp-协议的工具">vsftp：实现 ftp 协议的工具&lt;/h1>
&lt;h2 id="vsftp-配置">vsftp 配置&lt;/h2>
&lt;p>/etc/vsftpd.conf #vsftpd 程序的配置文件
/etc/ftpusers #此文件包含&lt;em>禁止&lt;/em>FTP 登录的用户名，通常有&amp;quot;root&amp;quot;，&amp;ldquo;uucp&amp;rdquo;，&amp;ldquo;news&amp;rdquo; 之类，因为这些用户权限太高，登录 FTP 误操作危险性大。&lt;/p>
&lt;ol>
&lt;li>User NAME&lt;/li>
&lt;li>User NAME&lt;/li>
&lt;li>User NAME&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;.&lt;/li>
&lt;li>User NAME&lt;/li>
&lt;/ol>
&lt;p>配置文件 keywords 说明&lt;/p>
&lt;p>最小基本配置&lt;/p>
&lt;ol>
&lt;li>write_enable=YES #对 ftp 服务器是否有写的权限&lt;/li>
&lt;li>local_enable=YES #是否允许本地用户登录(本地用户即 ftp 服务器自身的用户)&lt;/li>
&lt;li>anonymous_enable=NO #是否允许匿名登录&lt;/li>
&lt;/ol>
&lt;p>扩展配置&lt;/p>
&lt;ol>
&lt;li>chroot_local_user=YES #是否启动本地用户 chroot 规则，chroot 改变登录 ftp 的本地用户根的目录位置&lt;/li>
&lt;li>allow_writeable_chroot=YES #允许在限定目录有写权限&lt;/li>
&lt;li>chroot_list_enable=YES #是否启动不受 chroot 规则的用户名单&lt;/li>
&lt;li>chroot_list_file=/etc/vsftpd.chroot_list #定义不受限制的用户名单在哪个文件中&lt;/li>
&lt;li>pam_service_name=vsftpd 改为 pam_service_name=ftp #如果始终登录时候提示密码错误，则修改此项&lt;/li>
&lt;/ol>
&lt;p>注：chroot 的意思是改变根路径的位置(linux 系统中以/为根目录位置，但是对于执行 chroot 的用户或者程序来说，是 chroot(PATH)后 PATH 的位置是新的根目录位置)，比如 Telnet，ssh，如果都定义了 chroot(PATH)规则，那么远程登录的用户将无法访问到该 linux 系统中除了定义的 PATH 外的其余目录&lt;/p>
&lt;p>例：第一个图是不启动 chroot 规则的情况，第二张图是启用 chroot 规则的情况，可以看到当使用 chroot 时，对于 linux 系统中/srv/ftp 目录在 ftp 软件中，是作为/来存在的，由于这个原因，所以启动 chroot 的时候，ftp 工具无法访问所设定的/目录以外的其他目录+
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pvqe8m/1616165219993-ce6cd857-e9ba-4af0-b7fc-7d77cf547d84.jpeg" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pvqe8m/1616165220004-51f8038e-598e-427a-9b04-8f1987475f04.jpeg" alt="">&lt;/p>
&lt;ul>
&lt;li>/etc/vsftpd.chroot_list # 此文件包含对服务器上所有 FTP 内容有权限的用户名，该文件中每个用户名写一行&lt;/li>
&lt;/ul>
&lt;h2 id="ftp-命令行工具说明">FTP 命令行工具说明&lt;/h2>
&lt;p>ftp [[UserName@]目标 IP] # 可以使用 UserName 用户来登录目标 IP 的 ftp 服务器或直接进入 ftp 客户端&lt;/p>
&lt;ol>
&lt;li>open [IP] # 连接某个 FTP 服务器。&lt;/li>
&lt;li>close # 关闭连接&lt;/li>
&lt;li>user # 对已经连接的 FTP 再输入一次用户名和密码，类似于更改登陆账户&lt;/li>
&lt;li>？ # 查看可以使用的命令&lt;/li>
&lt;li>get [需要下载的文件] [下载路径以及定义文件名] # 下载文件&lt;/li>
&lt;li>put [需要上传的文件] [上传路径以及定义文件名] # 上传文件&lt;/li>
&lt;/ol>
&lt;p>注意：这些命令可以直接输入然后根据提示再输入想要执行的内容；也可直接在命令后接需要完成的动作，直接执行&lt;/p>
&lt;p>注意文件夹权限以免无法上传下载&lt;/p>
&lt;h1 id="ftp-被动模式注意事项">ftp 被动模式注意事项&lt;/h1>
&lt;p>如果 iptables 仅仅放了 21 端口，在启动被动模式后，需要给 iptables 添加模块以便被动模式正常运行&lt;/p>
&lt;p>在/etc/sysconfig/iptalbes-conf 文件中，修改成 IPTABLES_MODULES=&amp;ldquo;nf_conntrack_ftp nf_nat_ftp&amp;rdquo; 这个内容&lt;/p></description></item><item><title>Docs: iptables 源码解析</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/iptablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/iptables-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/iptablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/iptables-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</guid><description>
&lt;p>原文链接：&lt;a href="https://mp.weixin.qq.com/s/O084fYzUFk7jAzJ2DDeADg">https://mp.weixin.qq.com/s/O084fYzUFk7jAzJ2DDeADg&lt;/a>&lt;/p>
&lt;p>大家好，我是飞哥！&lt;/p>
&lt;p>现在 iptables 这个工具的应用似乎是越来越广了。不仅仅是在传统的防火墙、NAT 等功能出现，在今天流行的的 Docker、Kubernets、Istio 项目中也经常能见着对它的身影。正因为如此，所以深入理解 iptables 工作原理是非常有价值的事情。&lt;/p>
&lt;p>Linux 内核网络栈是一个纯内核态的东西，和用户层功能是天然隔离。但为了迎合各种各样用户层不同的需求，内核开放了一些口子出来供用户干预。使得用户层可以通过一些配置，改变内核的工作方式，从而实现特殊的需求。&lt;/p>
&lt;p>Linux 在内核网络组件中很多关键位置布置了  netfilter  过滤器。Iptables 就是基于 netfilter 来实现的。所以本文中 iptables 和 netfilter 这两个名词有时候就混着用了。&lt;/p>
&lt;p>飞哥也在网上看过很多关于 netfilter 技术文章，但是我觉得都写的不够清晰。所以咱们撸起袖子，自己写一篇。Netfilter 的实现可以简单地归纳为四表五链。我们来详细看看四表、五链究竟是啥意思。&lt;/p>
&lt;h2 id="一iptables-中的五链">一、Iptables 中的五链&lt;/h2>
&lt;p>Linux 下的 netfilter 在内核协议栈的各个重要关卡埋下了五个钩子。每一个钩子都对应是一系列规则，以链表的形式存在，所以俗称&lt;strong>五链&lt;/strong>。当网络包在协议栈中流转到这些关卡的时候，就会依次执行在这些钩子上注册的各种规则，进而实现对网络包的各种处理。&lt;/p>
&lt;p>要想把五链理解好，飞哥认为&lt;strong>最关键是要把内核接收、发送、转发三个过程分开来看&lt;/strong>。&lt;/p>
&lt;h3 id="11-接收过程">1.1 接收过程&lt;/h3>
&lt;p>Linux 在网络包接收在 IP 层的入口函数是 ip_rcv。网络在这里包碰到的第一个 HOOK 就是 PREROUTING。当该钩子上的规则都处理完后，会进行路由选择。如果发现是本设备的网络包，进入 ip_local_deliver 中，在这里又会遇到 INPUT 钩子。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>我们来看下详细的代码，先看 ip_rcv。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_input.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_rcv&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb, ......){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">NF_HOOK&lt;/span>(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip_rcv_finish);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>NF_HOOK 这个函数会执行到 iptables 中 pre_routing 里的各种表注册的各种规则。当处理完后，进入 ip_rcv_finish。在这里函数里将进行路由选择。这也就是 PREROUTING 这一链名字得来的原因，因为是在路由前执行的。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_input.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_rcv_finish&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">skb_dst&lt;/span>(skb)) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> err &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ip_route_input_noref&lt;/span>(skb, iph&lt;span style="color:#f92672">-&amp;gt;&lt;/span>daddr, iph&lt;span style="color:#f92672">-&amp;gt;&lt;/span>saddr,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> iph&lt;span style="color:#f92672">-&amp;gt;&lt;/span>tos, skb&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dev);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">dst_input&lt;/span>(skb);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果发现是本地设备上的接收，会进入 ip_local_deliver 函数。接着是又会执行到 LOCAL_IN 钩子，这也就是我们说的 INPUT 链。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_input.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_local_deliver&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">NF_HOOK&lt;/span>(NFPROTO_IPV4, NF_INET_LOCAL_IN, skb, skb&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dev, NULL,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip_local_deliver_finish);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>简单总结接收数据的处理流程是：&lt;strong>PREROUTING 链  -&amp;gt; 路由判断（是本机）-&amp;gt; INPUT 链&lt;/strong> -&amp;gt; &amp;hellip;&lt;/p>
&lt;h3 id="12-发送过程">1.2 发送过程&lt;/h3>
&lt;p>Linux 在网络包发送的过程中，首先是发送的路由选择，然后碰到的第一个 HOOK 就是 OUTPUT，然后接着进入 POSTROUTING 链。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>来大致过一下源码，网络层发送的入口函数是 ip_queue_xmit。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_output.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_queue_xmit&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb, &lt;span style="color:#66d9ef">struct&lt;/span> flowi &lt;span style="color:#f92672">*&lt;/span>fl)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 路由选择过程
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 选择完后记录路由信息到 skb 上
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> rt &lt;span style="color:#f92672">=&lt;/span> (&lt;span style="color:#66d9ef">struct&lt;/span> rtable &lt;span style="color:#f92672">*&lt;/span>)&lt;span style="color:#a6e22e">__sk_dst_check&lt;/span>(sk, &lt;span style="color:#ae81ff">0&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (rt &lt;span style="color:#f92672">==&lt;/span> NULL) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 没有缓存则查找路由项
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> rt &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ip_route_output_ports&lt;/span>(...);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">sk_setup_caps&lt;/span>(sk, &lt;span style="color:#f92672">&amp;amp;&lt;/span>rt&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dst);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">skb_dst_set_noref&lt;/span>(skb, &lt;span style="color:#f92672">&amp;amp;&lt;/span>rt&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dst);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//发送
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">ip_local_out&lt;/span>(skb);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在这里先进行了发送时的路由选择，然后进入发送时的 IP 层函数 __ip_local_out。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_output.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">__ip_local_out&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> iphdr &lt;span style="color:#f92672">*&lt;/span>iph &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ip_hdr&lt;/span>(skb);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>iph&lt;span style="color:#f92672">-&amp;gt;&lt;/span>tot_len &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">htons&lt;/span>(skb&lt;span style="color:#f92672">-&amp;gt;&lt;/span>len);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ip_send_check&lt;/span>(iph);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">nf_hook&lt;/span>(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">skb_dst&lt;/span>(skb)&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dev, dst_output);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的 NF_HOOK 将发送数据包送入到 NF_INET_LOCAL_OUT (OUTPUT) 链。执行完后，进入 dst_output。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: include/net/dst.h
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">inline&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">dst_output&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">skb_dst&lt;/span>(skb)&lt;span style="color:#f92672">-&amp;gt;&lt;/span>&lt;span style="color:#a6e22e">output&lt;/span>(skb);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在这里获取到之前的选路，并调用选到的 output 发送。将进入 ip_output。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_output.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_output&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//再次交给 netfilter，完毕后回调 ip_finish_output
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">NF_HOOK_COND&lt;/span>(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip_finish_output,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">!&lt;/span>(&lt;span style="color:#a6e22e">IPCB&lt;/span>(skb)&lt;span style="color:#f92672">-&amp;gt;&lt;/span>flags &lt;span style="color:#f92672">&amp;amp;&lt;/span> IPSKB_REROUTED));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>总结下发送数据包流程是：&lt;strong>路由选择&lt;/strong> -&amp;gt; &lt;strong>OUTPUT 链&lt;/strong>-&amp;gt; &lt;strong>POSTROUTING 链&lt;/strong> -&amp;gt; &amp;hellip;&lt;/p>
&lt;h3 id="13-转发过程">1.3 转发过程&lt;/h3>
&lt;p>其实除了接收和发送过程以外，Linux 内核还可以像路由器一样来工作。它将接收到网络包（不属于自己的），然后根据路由表选到合适的网卡设备将其转发出去。&lt;/p>
&lt;p>这个过程中，先是经历接收数据的前半段。在 ip_rcv 中经过 PREROUTING 链，然后路由后发现不是本设备的包，那就进入 ip_forward 函数进行转发，在这里又会遇到 FORWARD 链。最后还会进入 ip_output 进行真正的发送，遇到 POSTROUTING 链。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>我们来过一下源码，先是进入 IP 层入口 ip_rcv，在这里遇到 PREROUTING 链。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_input.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_rcv&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb, ......){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">NF_HOOK&lt;/span>(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip_rcv_finish);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>PREROUTING 链条上的规则都处理完后，进入 ip_rcv_finish，在这里路由选择，然后进入 dst_input。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: include/net/dst.h
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">inline&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">dst_input&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">skb_dst&lt;/span>(skb)&lt;span style="color:#f92672">-&amp;gt;&lt;/span>&lt;span style="color:#a6e22e">input&lt;/span>(skb);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>转发过程的这几步和接收过程一模一样的。不过内核路径就要从上面的 input 方法调用开始分道扬镳了。非本设备的不会进入 ip_local_deliver，而是会进入到 ip_forward。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_forward.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_forward&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">NF_HOOK&lt;/span>(NFPROTO_IPV4, NF_INET_FORWARD, skb, skb&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dev,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rt&lt;span style="color:#f92672">-&amp;gt;&lt;/span>dst.dev, ip_forward_finish);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 ip_forward_finish 里会送到 IP 层的发送函数 ip_output。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/ip_output.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">ip_output&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sk_buff &lt;span style="color:#f92672">*&lt;/span>skb)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//再次交给 netfilter，完毕后回调 ip_finish_output
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">NF_HOOK_COND&lt;/span>(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip_finish_output,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">!&lt;/span>(&lt;span style="color:#a6e22e">IPCB&lt;/span>(skb)&lt;span style="color:#f92672">-&amp;gt;&lt;/span>flags &lt;span style="color:#f92672">&amp;amp;&lt;/span> IPSKB_REROUTED));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 ip_output 里会遇到 POSTROUTING 链。再后面的流程就和发送过程的下半段一样了。&lt;/p>
&lt;p>总结下转发数据过程：&lt;strong>PREROUTING 链&lt;/strong> -&amp;gt; 路由判断（不是本设备，找到下一跳） -&amp;gt; &lt;strong>FORWARD 链&lt;/strong> -&amp;gt; &lt;strong>POSTROUTING 链&lt;/strong> -&amp;gt; &amp;hellip;&lt;/p>
&lt;h3 id="14-iptables-汇总">1.4 iptables 汇总&lt;/h3>
&lt;p>理解了接收、发送和转发三个过程以后，让我们把上面三个流程汇总起来。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>数据&lt;strong>接收过程&lt;/strong>走的是 1 和 2，&lt;strong>发送过程&lt;/strong>走的是 4 、5，&lt;strong>转发过程&lt;/strong>是 1、3、5。有了这张图，我们能更清楚地理解 iptables 和内核的关系。&lt;/p>
&lt;h2 id="二iptables-的四表">二、Iptables 的四表&lt;/h2>
&lt;p>在上一节中，我们介绍了 iptables 中的五个链。在每一个链上都可能是由许多个规则组成的。在 NF_HOOK 执行到这个链的时候，就会把规则按照优先级挨个过一遍。如果有符合条件的规则，则执行规则对应的动作。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>而这些规则根据用途的不同，又可以 raw、mangle、nat 和 filter。&lt;/p>
&lt;ul>
&lt;li>row 表的作用是将命中规则的包，跳过其它表的处理，它的优先级最高。&lt;/li>
&lt;li>mangle 表的作用是根据规则修改数据包的一些标志位，比如 TTL&lt;/li>
&lt;li>nat 表的作用是实现网络地址转换&lt;/li>
&lt;li>filter 表的作用是过滤某些包，这是防火墙工作的基础&lt;/li>
&lt;/ul>
&lt;p>例如在 PREROUTING 链中的规则中，分别可以执行 row、mangle 和 nat 三种功能。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>我们再来聊聊，为什么不是全部四个表呢。这是由于功能的不同，不是所有功能都会完全使用到五个链。&lt;/p>
&lt;p>Raw 表目的是跳过其它表，所以只需要在接收和发送两大过程的最开头处把关，所以只需要用到 PREROUTING 和 OUTPUT 两个钩子。&lt;/p>
&lt;p>Mangle 表有可能会在任意位置都有可能会修改网络包，所以它是用到了全部的钩子位置。&lt;/p>
&lt;p>NAT 分为 SNAT（Source NAT）和 DNAT（Destination NAT）两种，可能会工作在 PREROUTING、INPUT、OUTPUT、POSTROUTING 四个位置。&lt;/p>
&lt;p>Filter 只在 INPUT、OUTPUT 和 FORWARD 这三步中工作就够了。&lt;/p>
&lt;p>从整体上看，四链五表的关系如下图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>这里再多说一点，每个命名空间都是有自己独立的 iptables 规则的。我们拿 NAT 来举例，内核在遍历 NAT 规则的时候，是从 net（命名空间变量）的 ipv4.nat_table 上取下来的。NF_HOOK 最终会执行到 nf_nat_rule_find 函数。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//file: net/ipv4/netfilter/iptable_nat.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">nf_nat_rule_find&lt;/span>(...)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> net &lt;span style="color:#f92672">*&lt;/span>net &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">nf_ct_net&lt;/span>(ct);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> ret;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//重要!!!!!! nat_table 是在 namespace 中存储着的
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> ret &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">ipt_do_table&lt;/span>(skb, hooknum, in, out, net&lt;span style="color:#f92672">-&amp;gt;&lt;/span>ipv4.nat_table);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (ret &lt;span style="color:#f92672">==&lt;/span> NF_ACCEPT) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (&lt;span style="color:#f92672">!&lt;/span>&lt;span style="color:#a6e22e">nf_nat_initialized&lt;/span>(ct, &lt;span style="color:#a6e22e">HOOK2MANIP&lt;/span>(hooknum)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ret &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">alloc_null_binding&lt;/span>(ct, hooknum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> ret;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Docker 容器就是基于命名空间来工作的，所以每个 Docker 容器中都可以配置自己独立的 iptables 规则。&lt;/p>
&lt;h2 id="三iptables-使用举例">三、Iptables 使用举例&lt;/h2>
&lt;p>看完前面两小节，大家已经理解了四表五链是如何实现的了。那我们接下来通过几个实际的功能来看下实践中是如何使用 iptables 的。&lt;/p>
&lt;h3 id="31-nat">3.1 nat&lt;/h3>
&lt;p>假如说我们有一台 Linux，它的 eth0 的 IP 是 10.162.0.100，通过这个 IP 可以访问另外其它服务器。现在我们在这台机器上创建了个 Docker 虚拟网络环境 net1 出来，它的网卡 veth1 的 IP 是 192.168.0.2。&lt;/p>
&lt;p>如果想让 192.168.0.2 能访问外部网络，则需要宿主网络命名空间下的设备工作帮其进行网络包转发。由于这是个私有的地址，只有这台 Linux 认识，所以它是无法访问外部的服务器的。这个时候如果想要让 net1 正常访问 10.162.0.101，就必须在转发时执行 SNAT - 源地址替换。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>SNAT 工作在路由之后，网络包发送之前，也就是 POSTROUTING 链。我们在宿主机的命名空间里增加如下这条 iptables 规则。这条规则判断如果源是 192.168.0 网段，且目的不是 br0 的，统统执行源 IP 替换判断。&lt;/p>
&lt;p>&lt;code># iptables -t nat -A POSTROUTING -s 192.168.0.0/24 ! -o br0 -j MASQUERADE&lt;/code>&lt;/p>
&lt;p>有了这条规则，我们来看下整个发包过程。&lt;/p>
&lt;p>当数据包发出来的时候，先从 veth 发送到 br0。由于 br0 在宿主机的命名空间中，这样会执行到 POSTROUTING 链。在这个链有我们刚配置的 snat 规则。根据这条规则，内核将网络包中 192.168.0.2（外界不认识） 替换成母机的 IP 10.162.0.100（外界都认识）。同时还要跟踪记录链接状态。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>然后宿主机根据自己的路由表进行判断，选择默认发送设备将包从 eth0 网卡发送出去，直到送到 10.162.0.101。&lt;/p>
&lt;p>接下来在 10.162.0.100 上会收到来自 10.162.0.101 的响应包。由于上一步记录过链接跟踪，所以宿主机能知道这个回包是给 192.168.0.2 的。再反替换并通过 br0 将返回送达正确的 veth 上。&lt;/p>
&lt;p>这样 net1 环境中的 veth1  就可以访问外部网络服务了。&lt;/p>
&lt;h3 id="32-dnat-目的地址替换">3.2 DNAT 目的地址替换&lt;/h3>
&lt;p>接着上面小节里的例子，假设我们想在 192.168.0.2 上提供 80 端口的服务。同样，外面的服务器是无法访问这个地址的。这个时候要用到 DNAT 目的地址替换。需要在数据包进来的时候，将其目的地址替换成 192.168.0.2:80 才行。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>DNAT 工作在内核接收到网络包的第一个链中，也就是 PREROUTING。我们增加一条 DNAT 规则，具体的配置如下。&lt;/p>
&lt;pre>&lt;code># iptables -t nat -A PREROUTING  ! -i br0 -p tcp -m tcp --dport 8088 -j DNAT --to-destination 192.168.0.2:80
&lt;/code>&lt;/pre>
&lt;p>当有外界来的网络包到达 eth0 的时候。由于 eth0 在母机的命名空间中，所以会执行到 PREROUTING 链。&lt;/p>
&lt;p>该规则判断如果端口是 8088 的 TCP 请求，则将目的地址替换为 192.168.0.2:80。再通过 br0（192.168.0.1）转发数据包，数据包将到达真正提供服务的 192.168.0.2:80 上。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>同样在 DNAT 中也会有链接跟踪记录，所以  192.168.0.2 给 10.162.0.101 的返回包中的源地址会被替换成 10.162.0.100:8088。之后  10.162.0.101 收到包，它一直都以为自己是真的和  10.162.0.100:8088 通信。&lt;/p>
&lt;p>这样 net1 环境中的 veth1 也可以提供服务给外网使用了。事实上，单机的 Docker 就是通过这两小节介绍的 SNAT 和 DNAT 配置来进行网络通信的。&lt;/p>
&lt;h3 id="33filter">3.3 filter&lt;/h3>
&lt;p>Filter 表主要实现网络包的过滤。假如我们发现了一个恶意 IP 疯狂请求我们的服务器，对服务造成了影响。那么我们就可以用 filter 把它禁掉。其工作原理就是在接收包的 INPUT 链位置处进行判断，发现是恶意请求就尽早干掉不处理。避免进入到更上层继续浪费 CPU 开销。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>具体的配置方法细节如下：&lt;/p>
&lt;p>&lt;code># iptables -I INPUT -s 1.2.3.4 -j DROP //封禁 # iptables -D INPUT -s 1.2.3.4 -j DROP //解封&lt;/code>&lt;/p>
&lt;p>当然也可以封禁某个 IP 段。&lt;/p>
&lt;p>&lt;code># iptables -I INPUT -s 121.0.0.0/8 -j DROP //封禁 # iptables -I INPUT -s 121.0.0.0/8 -j DROP //解封&lt;/code>&lt;/p>
&lt;p>再比如说假设你不想让别人任意 ssh 登录你的服务器，只允许你的 IP 访问。那就只放开你自己的 IP，其它的都禁用掉就好了。&lt;/p>
&lt;p>&lt;code># iptables -t filter -I INPUT -s 1.2.3.4 -p tcp --dport 22 -j ACCEPT&lt;/code>
&lt;code># iptables -t filter -I INPUT -p tcp --dport 22 -j DROP&lt;/code>&lt;/p>
&lt;h3 id="34raw">3.4 raw&lt;/h3>
&lt;p>Raw 表中的规则可以绕开其它表的处理。在 nat 表中，为了保证双向的流量都能正常完成地址替换，会跟踪并且记录链接状态。每一条连接都会有对应的记录生成。使用以下两个命令可以查看。&lt;/p>
&lt;p>&lt;code># conntrack -L  # cat /proc/net/ip_conntrack&lt;/code>&lt;/p>
&lt;p>但在高流量的情况下，可能会有连接跟踪记录满的问题发生。我就遇到过一次在测试单机百万并发连接的时候，发生因连接数超过了 nf_conntrack_max 而导致新连接无法建立的问题。&lt;/p>
&lt;p>&lt;code># ip_conntrack: table full, dropping packet&lt;/code>&lt;/p>
&lt;p>但其实如果不使用 NAT 功能的话，链接跟踪功能是可以关闭的，例如。&lt;/p>
&lt;p>&lt;code># iptables -t raw -A PREROUTING -d 1.2.3.4 -p tcp --dport 80 -j NOTRACK&lt;/code>
&lt;code># iptables -A FORWARD -m state --state UNTRACKED -j ACCEPT&lt;/code>&lt;/p>
&lt;h3 id="35mangle">3.5 mangle&lt;/h3>
&lt;p>路由器在转发网络包的时候，ttl 值会减 1 ，该值为 0 时，最后一个路由就会停止再转发这个数据包。如若不想让本次路由影响 ttl，便可以在 mangel 表中加个 1，把它给补回来。&lt;/p>
&lt;p>&lt;code># ptables -t mangle -A PREROUTING -i eth0 -j TTL --ttl-inc 1&lt;/code>&lt;/p>
&lt;p>所有从 eth0 接口进来的数据包的 ttl 值加 1，以抵消路由转发默认减的 1。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Iptables 是一个非常常用，也非常重要的工具。Linux 上的防火墙、nat 等基础功能都是基于它实现的。还有现如今流行的的 Docker、Kubernets、Istio 项目中也经常能见着对它的身影。正因为如此，所以深入理解 iptables 工作原理是非常有价值的事情。&lt;/p>
&lt;p>今天我们先是在第一节里从内核接收、发送、转发三个不同的过程理解了五链的位置。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>接着又根据描述了 iptables 从功能上看的另外一个维度，表。每个表都是在多个钩子位置处注册自己的规则。当处理包的时候触发规则，并执行。从整体上看，四链五表的关系如下图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/b62be505-720a-465f-9eb2-322fd17c3f13/640" alt="">&lt;/p>
&lt;p>最后我们又分别在 raw、mangle、nat、filter 几个表上举了简单的应用例子。希望通过今天的学习，你能将 iptables 彻底融会贯通。相信这一定会对你的工作有很大的帮助的！&lt;/p></description></item><item><title>Docs: iptables(Netfilter 的实现)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/iptablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/iptablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man8/iptables.8.html">Manual(手册),iptables(8)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.frozentux.net/iptables-tutorial/iptables-tutorial.html">Netfilter 官方文档，iptables 教程&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>iptables 是一组工具合集的统称，其中包括 iptables、ip6tables、arptables、ebtables 等，用于与 netfilter 模块进行交互的 CLI 工具&lt;/p>
&lt;p>iptables 和 ip6tables 用于建立、维护和检查 Linux 内核中的 IPv4 和 IPv6 包过滤规则表。可以定义几个不同的表中的各种规则，也可以定义用户定义的链。并把已经定义的规则发送给 netfilter 模块。&lt;/p>
&lt;h2 id="四表table-note四表是-iptables-框架中的概念不是-netfilter-中的">四表(Table) Note:四表是 iptables 框架中的概念，不是 netfilter 中的&lt;/h2>
&lt;p>iptables 框架将流量抽象分为 4 类：过滤类、网络地址转换类、拆解报文类、原始类。每种类型的链作用在 Netfilter 系统中的 Hook 各不不相同，每种类型具有不同的功能。每一类都称为一张表。比如 fileter 表用来在指定链上检查流量是否可以通过，nat 表用来在指定链上检查流量是否可以进行地址转换，等等。Note：不是所有表都可以在所有链上具有规则，下表是 4 个表在 5 个 Hook 上的可用关系。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>表名\链名&lt;/th>
&lt;th>PREROUTING&lt;/th>
&lt;th>INPUT&lt;/th>
&lt;th>FORWARD&lt;/th>
&lt;th>OUTPUT&lt;/th>
&lt;th>POSTROUTING&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>filter&lt;/td>
&lt;td>&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>nat&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mangle&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>可用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>raw&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>可用&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>iptables 中有默认的内置 4 个表，每个表的名称就是其 chain 类型的名称&lt;/p>
&lt;h3 id="filter过滤器--过滤防火墙">filter(过滤器) # 过滤，防火墙&lt;/h3>
&lt;ul>
&lt;li>该类型的链可作用在以下几个 Hook 点上：INPUT、FORWARD、OUTPUT&lt;/li>
&lt;/ul>
&lt;h3 id="nat网络地址转换--网络地址转换">nat(网络地址转换) # 网络地址转换&lt;/h3>
&lt;ul>
&lt;li>该类型的链可作用在以下几个 Hook 点上：PREROUTING(DNAT)、INPUT、OUTPUT、POSTROUTING(SNAT)
&lt;ul>
&lt;li>其中 PREROUTING 与 POSTROUTING 是流量经过物理网卡设备时做 nat 的地方&lt;/li>
&lt;li>其中 INPUT 与 OUTPUT 则是主机内部从网络栈直接下来的流量做 nat 的地方。e.g.从主机一个服务发送数据到同一个主机另一个服务的流量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="mangle--拆解报文做出修改封装报文">mangle # 拆解报文，做出修改，封装报文&lt;/h3>
&lt;ul>
&lt;li>该类型的链可作用在以下几个 Hook 点上：PREROUTING、INPUT、FORWARD、OUTPUT、POSTROUTING&lt;/li>
&lt;/ul>
&lt;h3 id="raw原始--用于跳过-nat-表以及连接追踪机制ip_conntrack的处理详见-连接跟踪系统it-学习笔记1操作系统2kernel内核8network20-管理linux20-网络流量控制connnection20tracking连接跟踪md-tracking连接跟踪md">raw(原始) # 用于跳过 nat 表以及连接追踪机制(ip_conntrack)的处理，详见 [连接跟踪系统](✏IT 学习笔记/📄1.操作系统/2.Kernel(内核)/8.Network%20 管理/Linux%20 网络流量控制/Connnection%20Tracking(连接跟踪).md Tracking(连接跟踪).md)。&lt;/h3>
&lt;ul>
&lt;li>该类型的链可作用在以下几个 Hook 点上：PREROUTING、OUTPUT&lt;/li>
&lt;/ul>
&lt;p>整个表只有这一个作用，且仅有一个 target 就是 NOTRACK。具有最高优先级，所有流量先在两个 Hook 的 raw 功能上进行检查。一旦在 raw 上配置了规则，则 raw 表处理完成后，跳过 nat 表和 ip_conntrack 处理，i.e.不再做地址转换和数据包的链接跟踪处理，不把匹配到的数据包保存在“链接跟踪表”中。常用于那些不需要做 nat 的情况下以提高性能。e.g.大量访问的 web 服务器，可以让 80 端口不再让 iptables 做数据包的链接跟踪处理 ，以提高用户的访问速度。不过该功能不影响其余表的连接追踪追踪功能的正常使用，依然会有记录写到连接追踪文件中去&lt;/p>
&lt;ol>
&lt;li>该功能的起源：iptables 表有大小的上限，如果每个数据包进来都要进行检查，会非常影响性能，可以对那些不用进行 nat 功能的数据进行放弃后面的检查，i.e.可以在 raw 配置然后直接让这些数据包跳过后面的表对该数据包的处理&lt;/li>
&lt;/ol>
&lt;p>Note：四表的优先级从高到低依次为：raw-mangle-nat-filter，i.e.数据到达某个 Hook 上，则会优先使用优先级最高类型的链来处理数据包。其实，iptables 表的作用更像是用来划分优先级的。&lt;/p>
&lt;h2 id="iptables-处理链上规则的顺序以及规范">iptables 处理链上规则的顺序以及规范&lt;/h2>
&lt;p>注意：每个数据包在 CHAIN 中匹配到适用于自己的规则之后，则直接进入下一个 CHAIN，而不会遍历 CHAIN 中每条规则去挨个匹配适用于自己的规则。比如下面两种情况&lt;/p>
&lt;p>INPUT 链默认 DROP，匹配第一条：目的端口是 9090 的数据 DROP，然后不再检查下一项，那么 9090 无法访问&lt;/p>
&lt;pre>&lt;code>-P INPUT DROP
-A INPUT -p tcp -m tcp --dport 9090 -j DROP
-A INPUT -p tcp -m tcp --dport 9090 -j ACCEPT
&lt;/code>&lt;/pre>
&lt;p>INPUT 链默认 DROP，匹配第一条目的端口是 9090 的数据 ACCEPT，然后不再检查下一条规则，则 9090 可以访问&lt;/p>
&lt;pre>&lt;code>-P INPUT DROP
-A INPUT -p tcp -m tcp --dport 9090 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 9090 -j DROP
&lt;/code>&lt;/pre>
&lt;h1 id="iptables-关联文件与配置">iptables 关联文件与配置&lt;/h1>
&lt;p>/etc/sysconfig/iptables # 该文件存放用户定义的规则信息，每次重启 iptabels 后，都会读取该配置文件信息并应用到系统中
/etc/sysconfig/iptables-conf # 该文件存放 iptables 工具的具体配置信息
/run/xtables.lock # 该文件在 iptables 程序启动时被使用，以获取排他锁&lt;/p>
&lt;ul>
&lt;li>可以通过 &lt;code>XTABLES_LOCKFILE&lt;/code> 环境变量修改 iptables 需要使用 xtalbes.lock 文件的路径&lt;/li>
&lt;/ul>
&lt;h1 id="iptables-命令行工具详解">iptables 命令行工具详解&lt;/h1>
&lt;h2 id="syntax语法">Syntax(语法)&lt;/h2>
&lt;p>&lt;strong>iptables [-t TABLE] [OPTIONS] SubCOMMAND CHAIN [RuleSpecifitcation]&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>subCommand # 指对命令进行什么操作，CHAIN 指定要执行操作的链&lt;/li>
&lt;li>RuleSpecifitcation=MATCHES TARGET # 由两部分组成 [MATCHES&amp;hellip;] 和 [TARGET]
&lt;ul>
&lt;li>matches 由一个多个参数组成。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="options">OPTIONS&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>-t TALBLE&lt;/strong> # 指定 iptables 命令要对 TABLE 这个表进行操作，默认 filter 表&lt;/li>
&lt;li>&lt;strong>-n&lt;/strong> # 不显示域名，直接显示 IP&lt;/li>
&lt;li>&lt;strong>&amp;ndash;line-numbers&lt;/strong> # 显示每个 chain 中的行号&lt;/li>
&lt;li>&lt;strong>-v&lt;/strong> # 显示更详细的信息，vv 更详细，vvv 再详细一些
&lt;ul>
&lt;li>pkts #报文数&lt;/li>
&lt;li>bytes #字节数&lt;/li>
&lt;li>target #&lt;/li>
&lt;li>prot #&lt;/li>
&lt;li>in/out #显示要限制的具体网卡，*为所有&lt;/li>
&lt;li>source/destination #&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>-S&lt;/strong> # 以人类方便阅读的方式打印出来 iptables 规则&lt;/li>
&lt;/ul>
&lt;h3 id="subcommand">SubCOMMAND&lt;/h3>
&lt;ul>
&lt;li>增
&lt;ul>
&lt;li>&lt;strong>-I &lt;!-- raw HTML omitted --> [RuleNum] &lt;!-- raw HTML omitted -->&lt;/strong> # 在规则链开头加入规则详情，也可以指定添加到指定的规则号&lt;/li>
&lt;li>&lt;strong>-A &lt;!-- raw HTML omitted --> &lt;!-- raw HTML omitted -->&lt;/strong> # 在规则连末尾加入规则详情&lt;/li>
&lt;li>&lt;strong>-N ChainName&lt;/strong> # 创建名为 ChainName 的自定义规则链&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>删
&lt;ul>
&lt;li>&lt;strong>-F [CHAIN [RuleNum]]&lt;/strong> # 删除所有 chain 下的所有规则，也可删除指定 chain 下的指定的规则&lt;/li>
&lt;li>&lt;strong>-D &lt;!-- raw HTML omitted --> &lt;!-- raw HTML omitted -->&lt;/strong> # 删除一个 chain 中规则，RULE 可以是该 chain 中的行号，也可以是规则具体配置&lt;/li>
&lt;li>&lt;strong>-X [CHAIN]&lt;/strong> # 删除用户自定义的空的 chain&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>改
&lt;ul>
&lt;li>&lt;strong>-P &lt;!-- raw HTML omitted --> &lt;!-- raw HTML omitted -->&lt;/strong> # 设置指定的规则链(CHAIN)的默认策略为指定目标(Targe)&lt;/li>
&lt;li>&lt;strong>-E &lt;!-- raw HTML omitted --> &amp;lt;NewChainName&lt;/strong>&amp;gt;# 重命名自定义 chain，引用计数不为 0 的自定义 chain，无法改名也无法删除&lt;/li>
&lt;li>&lt;strong>-R&lt;/strong> # 替换指定链上的指定规则&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>查
&lt;ul>
&lt;li>&lt;strong>-L [CHAIN [RuleNum]]&lt;/strong> # 列出防火墙所有 CHAIN 的配置，可以列出指定的 CHAIN 的配置&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="matches">MATCHES&lt;/h3>
&lt;p>一个或多个 parameters(参数) 构成 MATCHES # i.e.Match(匹配)相关的 OPTIONS。在每个 parameters 之前添加!，即可将该匹配取反(比如：不加叹号是匹配某个 IP 地址，加了叹号表示除了这个地址外都匹配)&lt;/p>
&lt;p>MATCHES=[-m] MatchName [Per-Match-Options]&lt;/p>
&lt;p>基本匹配规则&lt;/p>
&lt;ul>
&lt;li>&lt;strong>-s IP/MASK&lt;/strong> # 指定规则中要匹配的来源地址的 IP/MASK&lt;/li>
&lt;li>&lt;strong>-d IP/MASK&lt;/strong> # 指定规则中要匹配的目标地址的 IP/MASK&lt;/li>
&lt;li>&lt;strong>-i 网卡名称&lt;/strong> # 指定数据流入规则中要匹配的网卡，仅用于 PREROUTING、INPUT、FORWARD 链&lt;/li>
&lt;li>&lt;strong>-o 网卡名称&lt;/strong> # 指定数据流出规则中要匹配的网卡，仅用于 FORWARD、OUTPUT、POSTROUTING 链&lt;/li>
&lt;li>&lt;strong>-p tcp|udp|icmp&lt;/strong> # 指定规则中要匹配的协议，即 ip 首部中的 protocols 所标识的协议&lt;/li>
&lt;/ul>
&lt;p>扩展匹配规则(ExtendedMatch)&lt;/p>
&lt;p>使用格式：-m ExtendedMatchName &amp;ndash;MatchRule&lt;/p>
&lt;p>通用的扩展匹配，指定具体的扩展匹配名以及该扩展匹配的匹配规则&lt;/p>
&lt;ul>
&lt;li>&lt;strong>-m conntrack &amp;ndash;ctstate CTState1[,CTState2&amp;hellip;]&lt;/strong> # 匹配指定的名为 CTState 的[连接追踪](✏IT 学习笔记/📄1.操作系统/2.Kernel(内核)/8.Network%20 管理/Linux%20 网络流量控制/Netfilter%20 流量控制系统/Connection%20Tracking(连接跟踪)机制.md Tracking(连接跟踪)机制.md)状态。CTState 为 conntrack State，可用的状态有{INVALID|ESTABLISHED|NEW|RELATED|UNTRACKED|SNAT|DNAT}
&lt;ul>
&lt;li>-m state &amp;ndash;state STATE1[,STATE2,&amp;hellip;.] # conntrack 的老式用法，慢慢会被淘汰&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>-m set &amp;ndash;match-set SetName {src|dst}..&lt;/strong>. #匹配指定的{源|目标}IP 是名为 SetName 的 ipset 集合
&lt;ul>
&lt;li>其中 FLAG 是逗号分隔的 src 和 dst 规范列表，其中不能超过六个。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>-&lt;strong>m iprange {&amp;ndash;src-range|&amp;ndash;dst-range} IP1-IP2&lt;/strong> # 匹配的指定的{源|目标}IP 范围&lt;/li>
&lt;li>&lt;strong>-m sting &amp;ndash;MatchRule&lt;/strong> # 指明要匹配的字符串，用于检查报文中出现的字符串(e.g.某个网页出现某个字符串则拒绝)
&lt;ul>
&lt;li>OPTIONS
&lt;ul>
&lt;li>&lt;strong>&amp;ndash;algo {bm|kmp}&lt;/strong> # 指明使用哪个搜索算法&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>基于基本匹配的扩展匹配&lt;/p>
&lt;ul>
&lt;li>-p tcp 的扩展匹配
&lt;ul>
&lt;li>&lt;strong>-m [tcp] &amp;ndash;dport NUM&lt;/strong> # 指定规则中要匹配的目标端口号&lt;/li>
&lt;li>&lt;strong>-m [tcp] &amp;ndash;sport NUM&lt;/strong> # 指定规则中要匹配的来源端口号&lt;/li>
&lt;li>&lt;strong>-m multiport {&amp;ndash;dport|&amp;ndash;sport} NUM&lt;/strong> # 让 tcp 匹配多个端口，可以是目标端口(dport)或者源端口(sport)&lt;/li>
&lt;li>&lt;strong>-m [tcp] &amp;ndash;tcp-flags LIST1 LIST2&lt;/strong> # 检查 LIST1 所指明的所有标志位，且这其中 LIST2 所表示出的所有标志位必须为 1，而余下的必须为 0,；没有 LIST1 中指明的，不做检查(e.g.&amp;ndash;tcp-flags SYN,ACK,FIN,RST SYN)。LIST 包括“SYN ACK FIN RST URG PSH ALL NONE”&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>-p udp 的扩展匹配：&amp;ndash;dport、&amp;ndash;sport 与 tcp 的扩展匹配用法一样&lt;/li>
&lt;li>-p icmp 的扩展匹配
&lt;ul>
&lt;li>&lt;strong>-m [icmp] &amp;ndash;icmp-type TYPE&lt;/strong> # 指定 icmp 的类型，具体类型可以搜 icmp type 获得，可以是数字&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="target">TARGET&lt;/h3>
&lt;p>&lt;strong>TARGET =-j|g TARGET [Per-Target-Options]&lt;/strong> # 指定规则中的目标(target)是什么。即“如果数据包匹配上规则之后应该做什么”。如果目标是自定义链，则指明具体的自定义 Chain 的名称。TARGET 都有哪些详见 Netfilter 流量控制系统。下面是各种 TARGET 的类型：&lt;/p>
&lt;ul>
&lt;li>ACCEPT # 允许流量通过&lt;/li>
&lt;li>REJECT # 拒绝流量通过
&lt;ul>
&lt;li>OPTIONS
&lt;ul>
&lt;li>--reject-with icmp-host-prohibited # 通过 icmp 协议显示给客户机一条消息:主机拒绝(icmp-host-prohibited)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DROP # 丢弃，不响应，发送方无法判断是被拒绝&lt;/li>
&lt;li>RETURN # 返回调用链&lt;/li>
&lt;li>MARK # 做防火墙标记&lt;/li>
&lt;li>用于 nat 表的 target
&lt;ul>
&lt;li>DNAT|SNAT # {目的|源}地址转换&lt;/li>
&lt;li>REDIRECT # 端口重定向&lt;/li>
&lt;li>MASQUERADE #地址伪装类似于 SNAT，但是不用指明要转换的地址，而是自动选择要转换的地址，用于外部地址不固定的情况&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>用于 raw 表的 target
&lt;ul>
&lt;li>NOTRACK # raw 表专用的 target，用于对匹配规则进行 notrack(不跟踪)处理&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>LOG #记录日志信息&lt;/li>
&lt;li>引用自定义链 # 直接使用“-j|-g 自定义链的名称”即可，让基本 5 个 Chain 上匹配成功的数据包继续执行自定义链上的规则。&lt;/li>
&lt;/ul>
&lt;h2 id="example">EXAMPLE&lt;/h2>
&lt;p>注意：在使用 iptables 命令的时候，为了防止配置问题导致网络不通，可以先设置一个定时任务来修改 iptables 规则或者 iptables XX &amp;amp;&amp;amp; sleep 5 &amp;amp;&amp;amp; iptables -P INPUT ACCEPT &amp;amp;&amp;amp; iptables -P OUTPUT ACCEPT&lt;/p>
&lt;h3 id="fileter-表的配置">Fileter 表的配置&lt;/h3>
&lt;h4 id="input-默认为-drop-情况下">INPUT 默认为 DROP 情况下&lt;/h4>
&lt;ul>
&lt;li>iptables -P INPUT DROP&lt;/li>
&lt;/ul>
&lt;p>给 INPUT 链添加一条接受 192.168.19.64/27 这个网段数据包的规则&lt;/p>
&lt;ul>
&lt;li>iptables -I INPUT -s 192.168.19.64/27 -j ACCEPT&lt;/li>
&lt;/ul>
&lt;p>允许指定的网段访问本机的 22 号端口&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -s 192.168.1.0/24 -p tcp -m tcp &amp;ndash;dport 22 -j ACCEPT&lt;/li>
&lt;/ul>
&lt;p>允许所有机器进入本机的 1000 到 1100 号端口&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -p tcp -m tcp &amp;ndash;dport 1000:1100 -j ACCEPT&lt;/li>
&lt;/ul>
&lt;p>允许源地址是 10.10.100.4 到 10.10.100.10 这 7 个 ip 的流量进入本机&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -m iprange &amp;ndash;src-range 10.10.100.4-10.10.100.10 -j ACCEPT&lt;/li>
&lt;/ul>
&lt;p>允许源地址是 10.10.100.4 和 10.10.100.8 且目的端口是 1935 和 4000 的流量进入本机&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -m iprange &amp;ndash;src-range 10.10.100.4,10.10.100.8 -p tcp -m multiport &amp;ndash;dports 1935,4000 -j ACCEPT&lt;/li>
&lt;/ul>
&lt;p>允许源地址是 ipset(名为 cdn2) 中的所有 IP，且目标端口为 80 的所有数据包通过&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -p tcp -m set &amp;ndash;match-set cdn2 src &amp;ndash;dports 80 -j ACCEPT&lt;/li>
&lt;/ul>
&lt;p>允许源地址是 ipset(名为 cdn1) 中的所有 IP 通过&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -m set &amp;ndash;match-set cdn1 src -j ACCEPT&lt;/li>
&lt;/ul>
&lt;h4 id="input-默认为-accept-情况下">INPUT 默认为 ACCEPT 情况下&lt;/h4>
&lt;ul>
&lt;li>iptables -P INPUT DROP&lt;/li>
&lt;/ul>
&lt;p>只放开部分 IP 的 22 端口&lt;/p>
&lt;blockquote>
&lt;p>注意：下面的顺序不可变，由于是顺序执行，指定源地址的先允许之后就不会再拒绝了，凡是没有允许过的源 IP，都会被 DROP&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>-P INPUT ACCEPT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-P FORWARD ACCEPT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-P OUTPUT ACCEPT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-A INPUT -s 116.63.160.74/32 -p tcp -m tcp --dport &lt;span style="color:#ae81ff">22&lt;/span> -j ACCEPT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-A INPUT -s 122.9.154.106/32 -p tcp -m tcp --dport &lt;span style="color:#ae81ff">22&lt;/span> -j ACCEPT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-A INPUT -p tcp -m tcp --dport &lt;span style="color:#ae81ff">22&lt;/span> -j DROP
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>所有 ping 本机的数据包全部丢弃，禁 ping。给 INPUT 链添加一条禁止 icmp 协议的规则&lt;/p>
&lt;ul>
&lt;li>iptables -I INPUT -p icmp -j DROP&lt;/li>
&lt;/ul>
&lt;h4 id="其他">其他&lt;/h4>
&lt;p>显示 INPUT 链中的所有规则，并显示规则行号&lt;/p>
&lt;ul>
&lt;li>iptables -L INPUT &amp;ndash;line-numbers&lt;/li>
&lt;/ul>
&lt;p>删除 INPUT 链中的第 11 条规则&lt;/p>
&lt;ul>
&lt;li>iptables -D INPUT 11&lt;/li>
&lt;/ul>
&lt;p>实现我可以 ping 别人，别人不能 ping 我的方法：&lt;/p>
&lt;ul>
&lt;li>iptables -A INPUT -p icmp &amp;ndash;icmp-type 8 -s 0/0 -j DROP #默认 INPUT 链的策略为 ACCEPT 的时候用&lt;/li>
&lt;li>iptables -A INPUT -p icmp &amp;ndash;icmp-type 0 -s 0/0 -j ACCEPT #默认 INPUT 链的策略为 DROP 的时候用&lt;/li>
&lt;li>iptables -A OUTPUT -p icmp &amp;ndash;icmp-type 0 -s LOCALIP -j DROP #默认 OUTPUT 链的策略为 ACCEPT 的时候用，注意把 Localip 改为本机 IP&lt;/li>
&lt;li>iptables -A OUTPUT -p icmp &amp;ndash;icmp-type 8 -s LOCALIP -j ACCEPT #默认 OUTPUT 链的策略为 DROP 的时候用，注意把 Localip 改为本机 IP&lt;/li>
&lt;/ul>
&lt;h3 id="nat-表的配置">NAT 表的配置&lt;/h3>
&lt;ul>
&lt;li>凡是基于 tcp 协议访问本机 80 端口，且目的地址是 110.119.120.1 的数据包。全部把目的地址转变为 192.168.20.2，且目的端口转换为 8080。
&lt;ul>
&lt;li>iptables -t nat -A PREROUTING -d 110.119.120.1 -p tcp &amp;ndash;dport 80 -j DNAT &amp;ndash;to-destination 192.168.20.2:8080&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将源地址网段是 192.168.122.0/24 并且目的地址网段不是 192.168.122.0/24 的全部转换成 123.213.1.5 这个地址。常用于公司内使用私网 IP 的设备访问互联网使用
&lt;ul>
&lt;li>iptables -t nat -A POSTROUTING -s 192.168.122.0/24 ! -d 192.168.122.0/24 -j SNAT &amp;ndash;to-source 123.213.1.5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将所有到达 eth0 网卡 25 端口的流量转发到 2525 端口。也叫端口转发
&lt;ul>
&lt;li>iptables -t nat -A PREROUTING -i eth0 -p tcp &amp;ndash;dport 25 -j REDIRECT &amp;ndash;to-port 2525&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="raw-表配置">RAW 表配置&lt;/h3>
&lt;ul>
&lt;li>所有来自 10.0.9.0/24 网段的数据包，都不跟踪。
&lt;ul>
&lt;li>iptables -t raw -A PREROUTING -s 10.0.9.0/24 -j NOTRACK&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="iptables-save--将-iptables-规则转储到标准输出">iptables-save # 将 iptables 规则转储到标准输出&lt;/h2>
&lt;p>该命令输出的内容更容易被人类阅读，可以用重定向把内容保存到文件中&lt;/p>
&lt;p>该命令显示出的信息说明
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/fadrg5/1616165483878-a0be9032-9c98-4971-922e-592c61529d86.jpeg" alt="">
&lt;strong>Syntax(语法)&lt;/strong>
iptables-save [-M,&amp;ndash;modprobe modprobe] [-c] [-t table]&lt;/p>
&lt;p>&lt;strong>EXAMPLE&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>iptables-save &amp;gt; /etc/sysconfig/iptables.rules&lt;/li>
&lt;/ol>
&lt;h2 id="iptables-restore-从标准输入恢复-iptables-规则可以视同重定向通过文件来读取到标准输入">iptables-restore #从标准输入恢复 iptables 规则，可以视同重定向通过文件来读取到标准输入&lt;/h2>
&lt;p>&lt;strong>EXAMPLE&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>iptables-restore &amp;lt; /etc/sysconfig/iptables.rules&lt;/li>
&lt;/ol>
&lt;h1 id="ipset--ip-集合的管理工具">ipset # IP 集合的管理工具&lt;/h1>
&lt;p>ipset 是 iptables 的一个协助工具。可以通过 ipset 设置一组 IP 并对这一组 IP 统一命名，然后在 iptables 里的匹配规则里通过名字直接引用该组 IP，并对这组 IP 进行流量控制。注意：由于是 iptables 规则引用，所以我直接修改 ipset 集合里的 IP，并不用重启 iptables 服务，就可以直接生效。这类似于域名解析，我的机器指定访问 baidu.com，至于百度公司他们怎么更改 IP 与域名绑定的关系，作为用户都不用更更改 baidu.com 这个域名。&lt;/p>
&lt;h2 id="ipset-options-command-command-options">ipset [OPTIONS] COMMAND [COMMAND-OPTIONS]&lt;/h2>
&lt;p>COMMANDS：Note：ENTRY 指的就是 ip 地址&lt;/p>
&lt;ol>
&lt;li>create SETNAME TYPENAME [type-specific-options] #创建一个新的集合。Create a new set&lt;/li>
&lt;li>add SETNAME ENTRY # 向指定集合中添加条目。i.e.添加 ip。Add entry to the named set&lt;/li>
&lt;li>del SETNAME ENTRY # 从指定集合中删除条目 Delete entry from the named set&lt;/li>
&lt;li>test SETNAME ENTRY # 测试指定集合中是否包含该条目 Test entry in the named set&lt;/li>
&lt;li>destroy [SETNAME] # 摧毁全部或者指定的集合 Destroy a named set or all sets&lt;/li>
&lt;li>list [SETNAME] # 列出全部或者指定集合中的条目 List the entries of a named set or all sets&lt;/li>
&lt;li>save [SETNAME] # 将指定的集合或者所有集合保存到标准输出&lt;/li>
&lt;li>restore # 还原保存的 ipset 信息&lt;/li>
&lt;li>flush [SETNAME] # 删除全部或者指定集合中的所有条目 Flush a named set or all sets&lt;/li>
&lt;li>rename FROM-SETNAME TO-SETNAME # Rename two sets&lt;/li>
&lt;li>swap FROM-SETNAME TO-SETNAME # 交换两个集合中的内容 Swap the contect of two existing sets&lt;/li>
&lt;/ol>
&lt;p>OPTIONS&lt;/p>
&lt;ol>
&lt;li>&lt;strong>-exist&lt;/strong> # 在 create 已经存在的 ipset、add 已经存在的 entry、del 不存在的 entry 时忽略错误。&lt;/li>
&lt;li>&lt;strong>-f&lt;/strong> # 在使用 save 或者 restore 命令时，可以指定文件，而不是从标准输出来保存或者还原 ipset 信息&lt;/li>
&lt;/ol>
&lt;p>{ -exist | -output { plain | save | xml } | -quiet | -resolve | -sorted | -name | -terse | -file filename }&lt;/p>
&lt;p>EXAMPLE&lt;/p>
&lt;ol>
&lt;li>ipset list #列出 ipset 所设置的所有 IP 集合&lt;/li>
&lt;li>ipset create lichenhao hash:net #创建一个 hash:net 类型的名为 lichenhao 的 ipset&lt;/li>
&lt;li>ipset add lichenhao 1.1.1.0/24 #将 1.1.1.0/24 网段添加到名为 lichenhao 的 ipset 中&lt;/li>
&lt;li>ipset flush #清空所有 ipset 下的 ip&lt;/li>
&lt;li>ipset restore -f /etc/sysconfig/ipset #从/etc/sysconfig/ipset 还原 ipset 的集合和条目信息&lt;/li>
&lt;/ol>
&lt;p>9、屏蔽 HTTP 服务 Flood×××&lt;/p>
&lt;p>有时会有用户在某个服务，例如 HTTP 80 上发起大量连接请求，此时我们可以启用如下规则：&lt;/p>
&lt;p>iptables -A INPUT -p tcp &amp;ndash;dport 80 -m limit &amp;ndash;limit 100/minute &amp;ndash;limit-burst 200 -j ACCEPT&lt;/p>
&lt;p>上述命令会将连接限制到每分钟 100 个，上限设定为 200。&lt;/p>
&lt;p>11、允许访问回环网卡&lt;/p>
&lt;p>环回访问（127.0.0.1）是比较重要的，建议大家都开放：&lt;/p>
&lt;p>iptables -A INPUT -i lo -j ACCEPT&lt;/p>
&lt;p>iptables -A OUTPUT -o lo -j ACCEPT&lt;/p>
&lt;p>12、屏蔽指定 MAC 地址&lt;/p>
&lt;p>使用如下规则可以屏蔽指定的 MAC 地址：&lt;/p>
&lt;p>iptables -A INPUT -m mac &amp;ndash;mac-source 00:00:00:00:00:00 -j DROP&lt;/p>
&lt;p>13、限制并发连接数&lt;/p>
&lt;p>如果你不希望来自特定端口的过多并发连接，可以使用如下规则：&lt;/p>
&lt;p>iptables -A INPUT -p tcp &amp;ndash;syn &amp;ndash;dport 22 -m connlimit &amp;ndash;connlimit-above 3 -j REJECT&lt;/p>
&lt;p>以上规则限制每客户端不超过 3 个连接。&lt;/p>
&lt;p>17、允许建立相关连接&lt;/p>
&lt;p>随着网络流量的进出分离，要允许建立传入相关连接，可以使用如下规则：&lt;/p>
&lt;p>iptables -A INPUT -m conntrack &amp;ndash;ctstate ESTABLISHED,RELATED -j ACCEPT&lt;/p>
&lt;p>允许建立传出相关连接的规则：&lt;/p>
&lt;p>iptables -A OUTPUT -m conntrack &amp;ndash;ctstate ESTABLISHED -j ACCEPT&lt;/p>
&lt;p>18、丢弃无效数据包&lt;/p>
&lt;p>很多网络 ××× 都会尝试用 ××× 自定义的非法数据包进行尝试，我们可以使用如下命令来丢弃无效数据包：&lt;/p>
&lt;p>iptables -A INPUT -m conntrack &amp;ndash;ctstate INVALID -j DROP&lt;/p>
&lt;p>19、IPtables 屏蔽邮件发送规则&lt;/p>
&lt;p>如果你的系统不会用于邮件发送，我们可以在规则中屏蔽 SMTP 传出端口：&lt;/p>
&lt;p>iptables -A OUTPUT -p tcp &amp;ndash;dports 25,465,587 -j REJECT&lt;/p></description></item><item><title>Docs: Linux DNS 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/L9TpAFqT-5V7ppEGdT0cnw">公众号,重新夺回 /etc/resolv.conf 的控制权&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 Linux 中，进行域名解析工作的是 reslover(解析器)。&lt;/p>
&lt;p>&lt;strong>reslover(解析器)&lt;/strong> 是 C 语言库中用于提供 DNS 接口的程序集，当某个进程调用这些程序时将同时读入 reslover 的配置文件，这个文件具有可读性并且包含大量可用的解析参数&lt;/p>
&lt;h1 id="linux-dns-配置">Linux DNS 配置&lt;/h1>
&lt;p>&lt;strong>/etc/resolv.conf&lt;/strong> # reslover 配置文件
**/etc/hosts **# 更改本地主机名和 IP 的对应关系，用于解析指定域名
例：当 ping TEST-1 时，则 ping 192.168.2.3&lt;/p>
&lt;pre>&lt;code>127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.2.3 TEST-1
&lt;/code>&lt;/pre>
&lt;p>***/nsswitch.conf **# 名称服务切换配置。GUN C 库(glibc) 和 某些其他应用程序使用该配置文件来确定从哪些地方获取解析信息。比如是否要读取 /etc/hosts 文件&lt;/p>
&lt;ul>
&lt;li>该文件属于 glibc 包中的一部分。但是由于 CentOS 与 Ubuntu 中 glibc 的巨大差异，该文件所在路径也不同：
&lt;ul>
&lt;li>CentOS 在 &lt;strong>/etc/nsswitch.conf&lt;/strong>&lt;/li>
&lt;li>Ubuntu 在 &lt;strong>/usr/share/libc-bin/nsswitch.conf&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Linux 网络包发送过程</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B/</guid><description>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247485146&amp;amp;idx=1&amp;amp;sn=e5bfc79ba915df1f6a8b32b87ef0ef78&amp;amp;scene=21#wechat_redirect">25 张图，一万字，拆解 Linux 网络包发送过程&lt;/a>&lt;/p>
&lt;p>大家好，我是飞哥!&lt;/p>
&lt;p>半年前我以源码的方式描述了网络包的接收过程。之后不断有粉丝提醒我还没聊发送过程呢。好，安排！&lt;/p>
&lt;p>在开始今天的文章之前，我先来请大家思考几个小问题。&lt;/p>
&lt;ul>
&lt;li>问 1：我们在查看内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？&lt;/li>
&lt;li>问 2：为什么你服务器上的 /proc/softirqs 里 NET_RX 要比 NET_TX 大的多的多？&lt;/li>
&lt;li>问 3：发送网络数据的时候都涉及到哪些内存拷贝操作？&lt;/li>
&lt;/ul>
&lt;p>这些问题虽然在线上经常看到，但我们似乎很少去深究。如果真的能透彻地把这些问题理解到位，我们对性能的掌控能力将会变得更强。&lt;/p>
&lt;p>带着这三个问题，我们开始今天对 Linux 内核网络发送过程的深度剖析。还是按照我们之前的传统，先从一段简单的代码作为切入。如下代码是一个典型服务器程序的典型的缩微代码：&lt;/p>
&lt;p>`int main(){
 fd = socket(AF_INET, SOCK_STREAM, 0);
 bind(fd, &amp;hellip;);
 listen(fd, &amp;hellip;);&lt;/p>
&lt;p>cfd = accept(fd, &amp;hellip;);&lt;/p>
&lt;p>//  接收用户请求
 read(cfd, &amp;hellip;);&lt;/p>
&lt;p>//  用户请求处理
 dosometing();&lt;/p>
&lt;p>//  给用户返回结果
 send(cfd, buf, sizeof(buf), 0);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>今天我们来讨论上述代码中，调用 send 之后内核是怎么样把数据包发送出去的。本文基于 Linux 3.10，网卡驱动采用 Intel 的 igb 网卡举例。&lt;/p>
&lt;p>&lt;strong>预警：本文共有一万多字，25 张图，长文慎入！&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/0" alt="">&lt;/p>
&lt;p>&lt;strong>开发内功修炼&lt;/strong>&lt;/p>
&lt;p>飞哥有鹅厂、搜狗 10 年多的开发工作经验。通过本号，我把多年中对于性能的一些深度思考分享给大家。&lt;/p>
&lt;p>73 篇原创内容&lt;/p>
&lt;p>公众号&lt;/p>
&lt;h2 id="一linux-网络发送过程总览">一、Linux 网络发送过程总览&lt;/h2>
&lt;p>我觉得看 Linux 源码最重要的是得有整体上的把握，而不是一开始就陷入各种细节。&lt;/p>
&lt;p>我这里先给大家准备了一个总的流程图，简单阐述下 send 发送了的数据是如何一步一步被发送到网卡的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>在这幅图中，我们看到用户数据被拷贝到内核态，然后经过协议栈处理后进入到了 RingBuffer 中。随后网卡驱动真正将数据发送了出去。当发送完成的时候，是通过硬中断来通知 CPU，然后清理 RingBuffer。&lt;/p>
&lt;p>因为文章后面要进入源码，所以我们再从源码的角度给出一个流程图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>虽然数据这时已经发送完毕，但是其实还有一件重要的事情没有做，那就是释放缓存队列等内存。&lt;/p>
&lt;p>那内核是如何知道什么时候才能释放内存的呢，当然是等网络发送完毕之后。网卡在发送完毕的时候，会给 CPU 发送一个硬中断来通知 CPU。更完整的流程看图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>注意，我们今天的主题虽然是发送数据，但是硬中断最终触发的软中断却是 NET_RX_SOFTIRQ，而并不是 NET_TX_SOFTIRQ ！！！（T 是 transmit 的缩写，R 表示 receive）&lt;/p>
&lt;p>&lt;strong>意不意外，惊不惊喜？？？&lt;/strong>&lt;/p>
&lt;p>所以这就是开篇问题 1 的一部分的原因（注意，这只是一部分原因）。&lt;/p>
&lt;blockquote>
&lt;p>问 1：在服务器上查看 /proc/softirqs，为什么 NET_RX 要比 NET_TX 大的多的多？&lt;/p>
&lt;/blockquote>
&lt;p>传输完成最终会触发 NET_RX，而不是 NET_TX。所以自然你观测 /proc/softirqs 也就能看到 NET_RX 更多了。&lt;/p>
&lt;p>好，现在你已经对内核是怎么发送网络包的有一个全局上的把握了。不要得意，我们需要了解的细节才是更有价值的地方，让我们继续！！&lt;/p>
&lt;h2 id="二网卡启动准备">二、网卡启动准备&lt;/h2>
&lt;p>现在的服务器上的网卡一般都是支持多队列的。每一个队列上都是由一个 RingBuffer 表示的，开启了多队列以后的的网卡就会对应有多个 RingBuffer。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>网卡在启动时最重要的任务之一就是分配和初始化 RingBuffer，理解了 RingBuffer 将会非常有助于后面我们掌握发送。因为今天的主题是发送，所以就以传输队列为例，我们来看下网卡启动时分配 RingBuffer 的实际过程。&lt;/p>
&lt;p>在网卡启动的时候，会调用到 __igb_open 函数，RingBuffer 就是在这里分配的。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static int __igb_open(struct net_device _netdev, bool resuming)
{
 struct igb_adapter _adapter = netdev_priv(netdev);&lt;/p>
&lt;p>// 分配传输描述符数组
 err = igb_setup_all_tx_resources(adapter);&lt;/p>
&lt;p>// 分配接收描述符数组
 err = igb_setup_all_rx_resources(adapter);&lt;/p>
&lt;p>// 开启全部队列
 netif_tx_start_all_queues(netdev);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在上面 __igb_open 函数调用 igb_setup_all_tx_resources 分配所有的传输 RingBuffer, 调用 igb_setup_all_rx_resources 创建所有的接收 RingBuffer。&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static int igb_setup_all_tx_resources(struct igb_adapter *adapter) {  // 有几个队列就构造几个 RingBuffer  for (i = 0; i &amp;lt; adapter-&amp;gt;num_tx_queues; i++) {   igb_setup_tx_resources(adapter-&amp;gt;tx_ring[i]);  } }&lt;/code>&lt;/p>
&lt;p>真正的 RingBuffer 构造过程是在 igb_setup_tx_resources 中完成的。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb*main.c
int igb_setup_tx_resources(struct igb_ring _tx_ring)
{
 //1. 申请  igb_tx_buffer  数组内存
 size = sizeof(struct igb_tx_buffer) * tx_ring-&amp;gt;count;
 tx_ring-&amp;gt;tx_buffer_info = vzalloc(size);&lt;/p>
&lt;p>//2. 申请  e1000_adv_tx_desc DMA  数组内存
 tx_ring-&amp;gt;size = tx_ring-&amp;gt;count * sizeof(union e1000_adv_tx_desc);
 tx_ring-&amp;gt;size = ALIGN(tx_ring-&amp;gt;size, 4096);
 tx_ring-&amp;gt;desc = dma_alloc_coherent(dev, tx_ring-&amp;gt;size,
        &amp;amp;tx_ring-&amp;gt;dma, GFP_KERNEL);&lt;/p>
&lt;p>//3. 初始化队列成员
 tx_ring-&amp;gt;next_to_use = 0;
 tx_ring-&amp;gt;next_to_clean = 0;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>从上述源码可以看到，实际上一个 RingBuffer 的内部不仅仅是一个环形队列数组，而是有两个。&lt;/p>
&lt;p>1）igb_tx_buffer 数组：这个数组是内核使用的，通过 vzalloc 申请的。
2）e1000_adv_tx_desc 数组：这个数组是网卡硬件使用的，硬件是可以通过 DMA 直接访问这块内存，通过 dma_alloc_coherent 分配。&lt;/p>
&lt;p>这个时候它们之间还没有啥联系。将来在发送的时候，这两个环形数组中相同位置的指针将都将指向同一个 skb。这样，内核和硬件就能共同访问同样的数据了，内核往 skb 里写数据，网卡硬件负责发送。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>最后调用 netif_tx_start_all_queues 开启队列。另外，对于硬中断的处理函数 igb_msix_ring 其实也是在 __igb_open 中注册的。&lt;/p>
&lt;h2 id="三accept-创建新-socket">三、accept 创建新 socket&lt;/h2>
&lt;p>在发送数据之前，我们往往还需要一个已经建立好连接的 socket。&lt;/p>
&lt;p>我们就以开篇服务器缩微源代码中提到的 accept 为例，当 accept 之后，进程会创建一个新的 socket 出来，然后把它放到当前进程的打开文件列表中，专门用于和对应的客户端通信。&lt;/p>
&lt;p>假设服务器进程通过 accept 和客户端建立了两条连接，我们来简单看一下这两条连接和进程的关联关系。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>其中代表一条连接的 socket 内核对象更为具体一点的结构图如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>为了避免喧宾夺主，accept 详细的源码过程这里就不介绍了，感兴趣请参考 &lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484905&amp;amp;idx=1&amp;amp;sn=a74ed5d7551c4fb80a8abe057405ea5e&amp;amp;scene=21#wechat_redirect">《图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！》&lt;/a>。一文中的第一部分。&lt;/p>
&lt;p>今天我们还是把重点放到数据发送过程上。&lt;/p>
&lt;h2 id="四发送数据真正开始">四、发送数据真正开始&lt;/h2>
&lt;h3 id="41-send-系统调用实现">4.1 send 系统调用实现&lt;/h3>
&lt;p>send 系统调用的源码位于文件 net/socket.c 中。在这个系统调用里，内部其实真正使用的是 sendto 系统调用。整个调用链条虽然不短，但其实主要只干了两件简单的事情，&lt;/p>
&lt;ul>
&lt;li>第一是在内核中把真正的 socket 找出来，在这个对象里记录着各种协议栈的函数地址。&lt;/li>
&lt;li>第二是构造一个 struct msghdr 对象，把用户传入的数据，比如 buffer 地址、数据长度啥的，统统都装进去.&lt;/li>
&lt;/ul>
&lt;p>剩下的事情就交给下一层，协议栈里的函数 inet_sendmsg 了，其中 inet_sendmsg 函数的地址是通过 socket 内核对象里的 ops 成员找到的。大致流程如图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>有了上面的了解，我们再看起源码就要容易许多了。源码如下：&lt;/p>
&lt;p>`//file: net/socket.c
SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len,
  unsigned int, flags)
{
 return sys_sendto(fd, buff, len, flags, NULL, 0);
}&lt;/p>
&lt;p>SYSCALL_DEFINE6(&amp;hellip;&amp;hellip;)
{
 //1. 根据  fd  查找到  socket
 sock = sockfd_lookup_light(fd, &amp;amp;err, &amp;amp;fput_needed);&lt;/p>
&lt;p>//2. 构造  msghdr
 struct msghdr msg;
 struct iovec iov;&lt;/p>
&lt;p>iov.iov_base = buff;
 iov.iov_len = len;
 msg.msg_iovlen = 1;&lt;/p>
&lt;p>msg.msg_iov = &amp;amp;iov;
 msg.msg_flags = flags;
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>//3. 发送数据
 sock_sendmsg(sock, &amp;amp;msg, len);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>从源码可以看到，我们在用户态使用的 send 函数和 sendto 函数其实都是 sendto 系统调用实现的。send 只是为了方便，封装出来的一个更易于调用的方式而已。&lt;/p>
&lt;p>在 sendto 系统调用里，首先根据用户传进来的 socket 句柄号来查找真正的 socket 内核对象。接着把用户请求的 buff、len、flag 等参数都统统打包到一个 struct msghdr 对象中。&lt;/p>
&lt;p>接着调用了 sock_sendmsg =&amp;gt; __sock_sendmsg ==&amp;gt;  __sock_sendmsg_nosec。在__sock_sendmsg_nosec 中，调用将会由系统调用进入到协议栈，我们来看它的源码。&lt;/p>
&lt;p>&lt;code>//file: net/socket.c static inline int __sock_sendmsg_nosec(...) {  ......  return sock-&amp;gt;ops-&amp;gt;sendmsg(iocb, sock, msg, size); }&lt;/code>&lt;/p>
&lt;p>通过第三节里的 socket 内核对象结构图，我们可以看到，这里调用的是 sock-&amp;gt;ops-&amp;gt;sendmsg 实际执行的是 inet_sendmsg。这个函数是 AF_INET 协议族提供的通用发送函数。&lt;/p>
&lt;h3 id="42-传输层处理">4.2 传输层处理&lt;/h3>
&lt;h4 id="1传输层拷贝">1）传输层拷贝&lt;/h4>
&lt;p>在进入到协议栈 inet_sendmsg 以后，内核接着会找到 socket 上的具体协议发送函数。对于 TCP 协议来说，那就是 tcp_sendmsg（同样也是通过 socket 内核对象找到的）。&lt;/p>
&lt;p>在这个函数中，内核会申请一个内核态的 skb 内存，将用户待发送的数据拷贝进去。注意这个时候不一定会真正开始发送，如果没有达到发送条件的话很可能这次调用直接就返回了。大概过程如图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看 inet_sendmsg 函数的源码。&lt;/p>
&lt;p>&lt;code>//file: net/ipv4/af_inet.c int inet_sendmsg(......) {  ......  return sk-&amp;gt;sk_prot-&amp;gt;sendmsg(iocb, sk, msg, size); }&lt;/code>&lt;/p>
&lt;p>在这个函数中会调用到具体协议的发送函数。同样参考第三节里的 socket 内核对象结构图，我们看到对于 TCP 协议下的 socket 来说，来说 sk-&amp;gt;sk_prot-&amp;gt;sendmsg 指向的是 tcp_sendmsg（对于 UPD 来说是 udp_sendmsg）。&lt;/p>
&lt;p>tcp_sendmsg 这个函数比较长，我们分多次来看它。先看这一段&lt;/p>
&lt;p>`//file: net/ipv4/tcp.c
int tcp_sendmsg(&amp;hellip;)
{
 while(&amp;hellip;){
  while(&amp;hellip;){
   // 获取发送队列
   skb = tcp_write_queue_tail(sk);&lt;/p>
&lt;p>// 申请 skb  并拷贝
   &amp;hellip;&amp;hellip;
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>&lt;code>//file: include/net/tcp.h static inline struct sk_buff *tcp_write_queue_tail(const struct sock *sk) {  return skb_peek_tail(&amp;amp;sk-&amp;gt;sk_write_queue); }&lt;/code>&lt;/p>
&lt;p>理解对 socket 调用 tcp_write_queue_tail 是理解发送的前提。如上所示，这个函数是在获取 socket 发送队列中的最后一个 skb。skb 是 struct sk_buff 对象的简称，用户的发送队列就是该对象组成的一个链表。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们再接着看 tcp_sendmsg 的其它部分。&lt;/p>
&lt;p>`//file: net/ipv4/tcp.c
int tcp_sendmsg(struct kiocb _iocb, struct sock _sk, struct msghdr *msg,
  size_t size)
{
 // 获取用户传递过来的数据和标志
 iov = msg-&amp;gt;msg_iov; // 用户数据地址
 iovlen = msg-&amp;gt;msg_iovlen; // 数据块数为 1
 flags = msg-&amp;gt;msg_flags; // 各种标志&lt;/p>
&lt;p>// 遍历用户层的数据块
 while (&amp;ndash;iovlen&amp;gt;= 0) {&lt;/p>
&lt;p>// 待发送数据块的地址
  unsigned char __user *from = iov-&amp;gt;iov_base;&lt;/p>
&lt;p>while (seglen&amp;gt; 0) {&lt;/p>
&lt;p>// 需要申请新的  skb
   if (copy &amp;lt;= 0) {&lt;/p>
&lt;p>// 申请  skb，并添加到发送队列的尾部
    skb = sk_stream_alloc_skb(sk,
         select_size(sk, sg),
         sk-&amp;gt;sk_allocation);&lt;/p>
&lt;p>// 把  skb  挂到 socket 的发送队列上
    skb_entail(sk, skb);
   }&lt;/p>
&lt;p>// skb  中有足够的空间
   if (skb_availroom(skb) &amp;gt; 0) {
    // 拷贝用户空间的数据到内核空间，同时计算校验和
    //from 是用户空间的数据地址  
    skb_add_data_nocache(sk, skb, from, copy);
   } 
   &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>`&lt;/p>
&lt;p>这个函数比较长，不过其实逻辑并不复杂。其中 msg-&amp;gt;msg_iov 存储的是用户态内存的要发送的数据的 buffer。接下来在内核态申请内核内存，比如 skb，并把用户内存里的数据拷贝到内核态内存中。&lt;strong>这就会涉及到一次或者几次内存拷贝的开销&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>至于内核什么时候真正把 skb 发送出去。在 tcp_sendmsg 中会进行一些判断。&lt;/p>
&lt;p>`//file: net/ipv4/tcp.c
int tcp_sendmsg(&amp;hellip;)
{
 while(&amp;hellip;){
  while(&amp;hellip;){
   // 申请内核内存并进行拷贝&lt;/p>
&lt;p>// 发送判断
   if (forced_push(tp)) {
    tcp_mark_push(tp, skb);
    __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);
   } else if (skb == tcp_send_head(sk))
    tcp_push_one(sk, mss_now);  
   }
   continue;
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>只有满足 forced_push(tp) 或者 skb == tcp_send_head(sk) 成立的时候，内核才会真正启动发送数据包。其中 forced_push(tp) 判断的是未发送的数据数据是否已经超过最大窗口的一半了。&lt;/p>
&lt;p>条件都不满足的话，&lt;strong>这次的用户要发送的数据只是拷贝到内核就算完事了！&lt;/strong>&lt;/p>
&lt;h4 id="2传输层发送">2）传输层发送&lt;/h4>
&lt;p>假设现在内核发送条件已经满足了，我们再来跟踪一下实际的发送过程。对于上小节函数中，当满足真正发送条件的时候，无论调用的是 __tcp_push_pending_frames 还是 tcp_push_one 最终都实际会执行到 tcp_write_xmit。&lt;/p>
&lt;p>所以我们直接从 tcp_write_xmit 看起，这个函数处理了传输层的拥塞控制、滑动窗口相关的工作。满足窗口要求的时候，设置一下 TCP 头然后将 skb 传到更低的网络层进行处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看下 tcp_write_xmit 的源码。&lt;/p>
&lt;p>`//file: net/ipv4/tcp_output.c
static bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,
      int push_one, gfp_t gfp)
{
 // 循环获取待发送  skb
 while ((skb = tcp_send_head(sk))) 
 {
  // 滑动窗口相关
  cwnd_quota = tcp_cwnd_test(tp, skb);
  tcp_snd_wnd_test(tp, skb, mss_now);
  tcp_mss_split_point(&amp;hellip;);
  tso_fragment(sk, skb, &amp;hellip;);
  &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>// 真正开启发送
  tcp_transmit_skb(sk, skb, 1, gfp);
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>可以看到我们之前在网络协议里学的滑动窗口、拥塞控制就是在这个函数中完成的，这部分就不过多展开了，感兴趣同学自己找这段源码来读。我们今天只看发送主过程，那就走到了 tcp_transmit_skb。&lt;/p>
&lt;p>`//file: net/ipv4/tcp_output.c
static int tcp_transmit_skb(struct sock _sk, struct sk_buff _skb, int clone_it,
    gfp_t gfp_mask)
{
 //1. 克隆新  skb  出来
 if (likely(clone_it)) {
  skb = skb_clone(skb, gfp_mask);
  &amp;hellip;&amp;hellip;
 }&lt;/p>
&lt;p>//2. 封装  TCP  头
 th = tcp_hdr(skb);
 th-&amp;gt;source  = inet-&amp;gt;inet_sport;
 th-&amp;gt;dest  = inet-&amp;gt;inet_dport;
 th-&amp;gt;window  = &amp;hellip;;
 th-&amp;gt;urg   = &amp;hellip;;
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>//3. 调用网络层发送接口
 err = icsk-&amp;gt;icsk_af_ops-&amp;gt;queue_xmit(skb, &amp;amp;inet-&amp;gt;cork.fl);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>第一件事是先克隆一个新的 skb，这里重点说下为什么要复制一个 skb 出来呢？&lt;/p>
&lt;p>是因为 skb 后续在调用网络层，最后到达网卡发送完成的时候，这个 skb 会被释放掉。而我们知道 TCP 协议是支持丢失重传的，在收到对方的 ACK 之前，这个 skb 不能被删除。所以内核的做法就是每次调用网卡发送的时候，实际上传递出去的是 skb 的一个拷贝。等收到 ACK 再真正删除。&lt;/p>
&lt;p>第二件事是修改 skb 中的 TCP header，根据实际情况把 TCP 头设置好。这里要介绍一个小技巧，skb 内部其实包含了网络协议中所有的 header。在设置 TCP 头的时候，只是把指针指向 skb 的合适位置。后面再设置 IP 头的时候，在把指针挪一挪就行，避免频繁的内存申请和拷贝，效率很高。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>tcp_transmit_skb 是发送数据位于传输层的最后一步，接下来就可以进入到网络层进行下一层的操作了。调用了网络层提供的发送接口 icsk-&amp;gt;icsk_af_ops-&amp;gt;queue_xmit()。&lt;/p>
&lt;p>在下面的这个源码中，我们的知道了 queue_xmit 其实指向的是 ip_queue_xmit 函数。&lt;/p>
&lt;p>&lt;code>//file: net/ipv4/tcp_ipv4.c const struct inet_connection_sock_af_ops ipv4_specific = {  .queue_xmit    = ip_queue_xmit,  .send_check    = tcp_v4_send_check,  ... }&lt;/code>&lt;/p>
&lt;p>自此，传输层的工作也就都完成了。数据离开了传输层，接下来将会进入到内核在网络层的实现里。&lt;/p>
&lt;h3 id="43-网络层发送处理">4.3 网络层发送处理&lt;/h3>
&lt;p>Linux 内核网络层的发送的实现位于 net/ipv4/ip_output.c 这个文件。传输层调用到的 ip_queue_xmit 也在这里。（从文件名上也能看出来进入到 IP 层了，源文件名已经从 tcp_xxx 变成了 ip_xxx。）&lt;/p>
&lt;p>在网络层里主要处理路由项查找、IP 头设置、netfilter 过滤、skb 切分（大于 MTU 的话）等几项工作，处理完这些工作后会交给更下层的邻居子系统来处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看网络层入口函数 ip_queue_xmit 的源码：&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c
int ip_queue_xmit(struct sk_buff _skb, struct flowi _fl)
{
 // 检查  socket  中是否有缓存的路由表
 rt = (struct rtable *)__sk_dst_check(sk, 0);
 if (rt == NULL) {
  // 没有缓存则展开查找
  // 则查找路由项，  并缓存到  socket  中
  rt = ip_route_output_ports(&amp;hellip;);
  sk_setup_caps(sk, &amp;amp;rt-&amp;gt;dst);
 }&lt;/p>
&lt;p>// 为  skb  设置路由表
 skb_dst_set_noref(skb, &amp;amp;rt-&amp;gt;dst);&lt;/p>
&lt;p>// 设置  IP header
 iph = ip_hdr(skb);
 iph-&amp;gt;protocol = sk-&amp;gt;sk_protocol;
 iph-&amp;gt;ttl      = ip_select_ttl(inet, &amp;amp;rt-&amp;gt;dst);
 iph-&amp;gt;frag_off = &amp;hellip;;&lt;/p>
&lt;p>// 发送
 ip_local_out(skb);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>ip_queue_xmit 已经到了网络层，在这个函数里我们看到了网络层相关的功能路由项查找，如果找到了则设置到 skb 上（没有路由的话就直接报错返回了）。&lt;/p>
&lt;p>在 Linux 上通过 route 命令可以看到你本机的路由配置。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>在路由表中，可以查到某个目的网络应该通过哪个 Iface（网卡），哪个 Gateway（网卡）发送出去。查找出来以后缓存到 socket 上，下次再发送数据就不用查了。&lt;/p>
&lt;p>接着把路由表地址也放到 skb 里去。&lt;/p>
&lt;p>&lt;code>//file: include/linux/skbuff.h struct sk_buff {  // 保存了一些路由相关信息  unsigned long  _skb_refdst; }&lt;/code>&lt;/p>
&lt;p>接下来就是定位到 skb 里的 IP 头的位置上，然后开始按照协议规范设置 IP header。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>再通过 ip_local_out 进入到下一步的处理。&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c  
int ip_local_out(struct sk_buff *skb)
{
 // 执行  netfilter  过滤
 err = __ip_local_out(skb);&lt;/p>
&lt;p>// 开始发送数据
 if (likely(err == 1))
  err = dst_output(skb);
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 ip_local_out =&amp;gt; __ip_local_out =&amp;gt; nf_hook 会执行 netfilter 过滤。如果你使用 iptables 配置了一些规则，那么这里将检测是否命中规则。&lt;strong>如果你设置了非常复杂的 netfilter 规则，在这个函数这里将会导致你的进程 CPU 开销会极大增加&lt;/strong>。&lt;/p>
&lt;p>还是不多展开说，继续只聊和发送有关的过程 dst_output。&lt;/p>
&lt;p>&lt;code>//file: include/net/dst.h static inline int dst_output(struct sk_buff *skb) {  return skb_dst(skb)-&amp;gt;output(skb); }&lt;/code>&lt;/p>
&lt;p>此函数找到到这个 skb 的路由表（dst 条目） ，然后调用路由表的 output 方法。这又是一个函数指针，指向的是 ip_output 方法。&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c
int ip_output(struct sk_buff *skb)
{
 // 统计
 &amp;hellip;..&lt;/p>
&lt;p>// 再次交给  netfilter，完毕后回调  ip_finish_output
 return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,
    ip_finish_output,
    !(IPCB(skb)-&amp;gt;flags &amp;amp; IPSKB_REROUTED));
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 ip_output 中进行一些简单的，统计工作，再次执行 netfilter 过滤。过滤通过之后回调 ip_finish_output。&lt;/p>
&lt;p>&lt;code>//file: net/ipv4/ip_output.c static int ip_finish_output(struct sk_buff *skb) {  // 大于 mtu 的话就要进行分片了  if (skb-&amp;gt;len &amp;gt; ip_skb_dst_mtu(skb) &amp;amp;&amp;amp; !skb_is_gso(skb))   return ip_fragment(skb, ip_finish_output2);  else   return ip_finish_output2(skb); }&lt;/code>&lt;/p>
&lt;p>在 ip_finish_output 中我们看到，&lt;strong>如果数据大于 MTU 的话，是会执行分片的。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>实际 MTU 大小确定依赖 MTU 发现，以太网帧为 1500 字节。之前 QQ 团队在早期的时候，会尽量控制自己数据包尺寸小于 MTU，通过这种方式来优化网络性能。因为分片会带来两个问题：1、需要进行额外的切分处理，有额外性能开销。2、只要一个分片丢失，整个包都得重传。所以避免分片既杜绝了分片开销，也大大降低了重传率。&lt;/p>
&lt;/blockquote>
&lt;p>在 ip_finish_output2 中，终于发送过程会进入到下一层，邻居子系统中。&lt;/p>
&lt;p>`//file: net/ipv4/ip_output.c
static inline int ip_finish_output2(struct sk_buff *skb)
{
 // 根据下一跳 IP 地址查找邻居项，找不到就创建一个
 nexthop = (**force u32) rt_nexthop(rt, ip_hdr(skb)-&amp;gt;daddr);  
 neigh = **ipv4_neigh_lookup_noref(dev, nexthop);
 if (unlikely(!neigh))
  neigh = __neigh_create(&amp;amp;arp_tbl, &amp;amp;nexthop, dev, false);&lt;/p>
&lt;p>// 继续向下层传递
 int res = dst_neigh_output(dst, neigh, skb);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;h3 id="44-邻居子系统">4.4 邻居子系统&lt;/h3>
&lt;p>邻居子系统是位于网络层和数据链路层中间的一个系统，其作用是对网络层提供一个封装，让网络层不必关心下层的地址信息，让下层来决定发送到哪个 MAC 地址。&lt;/p>
&lt;p>而且这个邻居子系统并不位于协议栈 net/ipv4/ 目录内，而是位于 net/core/neighbour.c。因为无论是对于 IPv4 还是 IPv6 ，都需要使用该模块。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>在邻居子系统里主要是查找或者创建邻居项，在创造邻居项的时候，有可能会发出实际的 arp 请求。然后封装一下 MAC 头，将发送过程再传递到更下层的网络设备子系统。大致流程如图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>理解了大致流程，我们再回头看源码。在上面小节 ip_finish_output2 源码中调用了 __ipv4_neigh_lookup_noref。它是在 arp 缓存中进行查找，其第二个参数传入的是路由下一跳 IP 信息。&lt;/p>
&lt;p>`//file: include/net/arp.h
extern struct neigh_table arp_tbl;
static inline struct neighbour ___ipv4_neigh_lookup_noref(
 struct net_device _dev, u32 key)
{
 struct neigh_hash_table *nht = rcu_dereference_bh(arp_tbl.nht);&lt;/p>
&lt;p>// 计算  hash  值，加速查找
 hash_val = arp_hashfn(&amp;hellip;&amp;hellip;);
 for (n = rcu_dereference_bh(nht-&amp;gt;hash_buckets[hash_val]);
   n != NULL;
   n = rcu_dereference_bh(n-&amp;gt;next)) {
  if (n-&amp;gt;dev == dev &amp;amp;&amp;amp; _(u32 _)n-&amp;gt;primary_key == key)
   return n;
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>如果查找不到，则调用 __neigh_create 创建一个邻居。&lt;/p>
&lt;p>`//file: net/core/neighbour.c
struct neighbour ___neigh_create(&amp;hellip;&amp;hellip;)
{
 // 申请邻居表项
 struct neighbour _n1, _rc, _n = neigh_alloc(tbl, dev);&lt;/p>
&lt;p>// 构造赋值
 memcpy(n-&amp;gt;primary_key, pkey, key_len);
 n-&amp;gt;dev = dev;
 n-&amp;gt;parms-&amp;gt;neigh_setup(n);&lt;/p>
&lt;p>// 最后添加到邻居  hashtable  中
 rcu_assign_pointer(nht-&amp;gt;hash_buckets[hash_val], n);
 &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>`&lt;/p>
&lt;p>有了邻居项以后，此时仍然还不具备发送 IP 报文的能力，因为目的 MAC 地址还未获取。调用 dst_neigh_output 继续传递 skb。&lt;/p>
&lt;p>&lt;code>//file: include/net/dst.h static inline int dst_neigh_output(struct dst_entry *dst,       struct neighbour *n, struct sk_buff *skb) {  ......  return n-&amp;gt;output(n, skb); }&lt;/code>&lt;/p>
&lt;p>调用 output，实际指向的是 neigh_resolve_output。在这个函数内部有可能会发出 arp 网络请求。&lt;/p>
&lt;p>`//file: net/core/neighbour.c
int neigh_resolve_output(){&lt;/p>
&lt;p>// 注意：这里可能会触发 arp 请求
 if (!neigh_event_send(neigh, skb)) {&lt;/p>
&lt;p>//neigh-&amp;gt;ha  是  MAC  地址
  dev_hard_header(skb, dev, ntohs(skb-&amp;gt;protocol),
           neigh-&amp;gt;ha, NULL, skb-&amp;gt;len);
  // 发送
  dev_queue_xmit(skb);
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>当获取到硬件 MAC 地址以后，就可以封装 skb 的 MAC 头了。最后调用 dev_queue_xmit 将 skb 传递给 Linux 网络设备子系统。&lt;/p>
&lt;h3 id="45-网络设备子系统">4.5 网络设备子系统&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>邻居子系统通过 dev_queue_xmit 进入到网络设备子系统中来。&lt;/p>
&lt;p>`//file: net/core/dev.c 
int dev_queue_xmit(struct sk_buff *skb)
{
 // 选择发送队列
 txq = netdev_pick_tx(dev, skb);&lt;/p>
&lt;p>// 获取与此队列关联的排队规则
 q = rcu_dereference_bh(txq-&amp;gt;qdisc);&lt;/p>
&lt;p>// 如果有队列，则调用**dev_xmit_skb  继续处理数据
 if (q-&amp;gt;enqueue) {
  rc = **dev_xmit_skb(skb, q, dev, txq);
  goto out;
 }&lt;/p>
&lt;p>// 没有队列的是回环设备和隧道设备
 &amp;hellip;&amp;hellip;
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>开篇第二节网卡启动准备里我们说过，网卡是有多个发送队列的（尤其是现在的网卡）。上面对 netdev_pick_tx 函数的调用就是选择一个队列进行发送。&lt;/p>
&lt;p>netdev_pick_tx 发送队列的选择受 XPS 等配置的影响，而且还有缓存，也是一套小复杂的逻辑。这里我们只关注两个逻辑，首先会获取用户的 XPS 配置，否则就自动计算了。代码见 netdev_pick_tx =&amp;gt; __netdev_pick_tx。&lt;/p>
&lt;p>`//file: net/core/flow_dissector.c
u16 __netdev_pick_tx(struct net_device _dev, struct sk_buff _skb)
{
 // 获取  XPS  配置
 int new_index = get_xps_queue(dev, skb);&lt;/p>
&lt;p>// 自动计算队列
 if (new_index &amp;lt; 0)
  new_index = skb_tx_hash(dev, skb);}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>然后获取与此队列关联的 qdisc。在 linux 上通过 tc 命令可以看到 qdisc 类型，例如对于我的某台多队列网卡机器上是 mq disc。&lt;/p>
&lt;p>&lt;code>#tc qdisc qdisc mq 0: dev eth0 root&lt;/code>&lt;/p>
&lt;p>大部分的设备都有队列（回环设备和隧道设备除外），所以现在我们进入到 __dev_xmit_skb。&lt;/p>
&lt;p>`//file: net/core/dev.c
static inline int __dev_xmit_skb(struct sk_buff _skb, struct Qdisc _q,
     struct net_device _dev,
     struct netdev_queue _txq)
{
 //1. 如果可以绕开排队系统
 if ((q-&amp;gt;flags &amp;amp; TCQ_F_CAN_BYPASS) &amp;amp;&amp;amp; !qdisc_qlen(q) &amp;amp;&amp;amp;
     qdisc_run_begin(q)) {
  &amp;hellip;&amp;hellip;
 }&lt;/p>
&lt;p>//2. 正常排队
 else {&lt;/p>
&lt;p>// 入队
  q-&amp;gt;enqueue(skb, q)&lt;/p>
&lt;p>// 开始发送
  __qdisc_run(q);
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>上述代码中分两种情况，1 是可以 bypass（绕过）排队系统的，另外一种是正常排队。我们只看第二种情况。&lt;/p>
&lt;p>先调用 q-&amp;gt;enqueue 把 skb 添加到队列里。然后调用 __qdisc_run 开始发送。&lt;/p>
&lt;p>`//file: net/sched/sch_generic.c
void __qdisc_run(struct Qdisc *q)
{
 int quota = weight_p;&lt;/p>
&lt;p>// 循环从队列取出一个  skb  并发送
 while (qdisc_restart(q)) {&lt;/p>
&lt;p>//  如果发生下面情况之一，则延后处理：
  // 1. quota  用尽
  // 2.  其他进程需要  CPU
  if (&amp;ndash;quota &amp;lt;= 0 || need_resched()) {
   // 将触发一次  NET_TX_SOFTIRQ  类型  softirq
   __netif_schedule(q);
   break;
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在上述代码中，我们看到 while 循环不断地从队列中取出 skb 并进行发送。注意，这个时候其实都占用的是用户进程的系统态时间 (sy)。只有当 quota 用尽或者其它进程需要 CPU 的时候才触发软中断进行发送。&lt;/p>
&lt;p>&lt;strong>所以这就是为什么一般服务器上查看 /proc/softirqs，一般 NET_RX 都要比 NET_TX 大的多的第二个原因&lt;/strong>。对于读来说，都是要经过 NET_RX 软中断，而对于发送来说，只有系统态配额用尽才让软中断上。&lt;/p>
&lt;p>我们来把精力在放到 qdisc_restart 上，继续看发送过程。&lt;/p>
&lt;p>`static inline int qdisc_restart(struct Qdisc *q)
{
 // 从  qdisc  中取出要发送的  skb
 skb = dequeue_skb(q);
 &amp;hellip;&lt;/p>
&lt;p>return sch_direct_xmit(skb, q, dev, txq, root_lock);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>qdisc_restart 从队列中取出一个 skb，并调用 sch_direct_xmit 继续发送。&lt;/p>
&lt;p>&lt;code>//file: net/sched/sch_generic.c int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,    struct net_device *dev, struct netdev_queue *txq,    spinlock_t *root_lock) {  // 调用驱动程序来发送数据  ret = dev_hard_start_xmit(skb, dev, txq); }&lt;/code>&lt;/p>
&lt;h3 id="46-软中断调度">4.6 软中断调度&lt;/h3>
&lt;p>在 4.5 咱们看到了如果系统态 CPU 发送网络包不够用的时候，会调用 __netif_schedule 触发一个软中断。该函数会进入到 __netif_reschedule，由它来实际发出 NET_TX_SOFTIRQ 类型软中断。&lt;/p>
&lt;p>软中断是由内核线程来运行的，该线程会进入到 net_tx_action 函数，在该函数中能获取到发送队列，并也最终调用到驱动程序里的入口函数 dev_hard_start_xmit。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>`//file: net/core/dev.c
static inline void **netif_reschedule(struct Qdisc *q)
{
 sd = &amp;amp;**get_cpu_var(softnet_data);
 q-&amp;gt;next_sched = NULL;
 *sd-&amp;gt;output_queue_tailp = q;
 sd-&amp;gt;output_queue_tailp = &amp;amp;q-&amp;gt;next_sched;&lt;/p>
&lt;p>&amp;hellip;&amp;hellip;
 raise_softirq_irqoff(NET_TX_SOFTIRQ);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在该函数里在软中断能访问到的 softnet_data 里设置了要发送的数据队列，添加到了 output_queue 里了。紧接着触发了 NET_TX_SOFTIRQ 类型的软中断。（T 代表 transmit 传输）&lt;/p>
&lt;p>软中断的入口代码我这里也不详细扒了，感兴趣的同学参考&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">《图解 Linux 网络包接收过程》&lt;/a>一文中的 3.2 小节 - ksoftirqd 内核线程处理软中断。&lt;/p>
&lt;p>我们直接从 NET_TX_SOFTIRQ softirq 注册的回调函数 net_tx_action 讲起。用户态进程触发完软中断之后，会有一个软中断内核线程会执行到 net_tx_action。&lt;/p>
&lt;p>&lt;strong>牢记，这以后发送数据消耗的 CPU 就都显示在 si 这里了，不会消耗用户进程的系统时间了&lt;/strong>。&lt;/p>
&lt;p>`//file: net/core/dev.c
static void net_tx_action(struct softirq_action _h)
{
 // 通过  softnet_data  获取发送队列
 struct softnet_data _sd = &amp;amp;__get_cpu_var(softnet_data);&lt;/p>
&lt;p>//  如果  output queue  上有  qdisc
 if (sd-&amp;gt;output_queue) {&lt;/p>
&lt;p>//  将  head  指向第一个  qdisc
  head = sd-&amp;gt;output_queue;&lt;/p>
&lt;p>// 遍历  qdsics  列表
  while (head) {
   struct Qdisc *q = head;
   head = head-&amp;gt;next_sched;&lt;/p>
&lt;p>// 发送数据
   qdisc_run(q);
  }
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>软中断这里会获取 softnet_data。前面我们看到进程内核态在调用 __netif_reschedule 的时候把发送队列写到 softnet_data 的 output_queue 里了。软中断循环遍历 sd-&amp;gt;output_queue 发送数据帧。&lt;/p>
&lt;p>来看 qdisc_run，它和进程用户态一样，也会调用到 __qdisc_run。&lt;/p>
&lt;p>&lt;code>//file: include/net/pkt_sched.h static inline void qdisc_run(struct Qdisc *q) {  if (qdisc_run_begin(q))   __qdisc_run(q); }&lt;/code>&lt;/p>
&lt;p>然后一样就是进入 qdisc_restart =&amp;gt; sch_direct_xmit，直到驱动程序函数 dev_hard_start_xmit。&lt;/p>
&lt;h3 id="47-igb-网卡驱动发送">4.7 igb 网卡驱动发送&lt;/h3>
&lt;p>我们前面看到，无论是对于用户进程的内核态，还是对于软中断上下文，都会调用到网络设备子系统中的 dev_hard_start_xmit 函数。在这个函数中，会调用到驱动里的发送函数 igb_xmit_frame。&lt;/p>
&lt;p>在驱动函数里，将 skb 会挂到 RingBuffer 上，驱动调用完毕后，数据包将真正从网卡发送出去。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>我们来看看实际的源码：&lt;/p>
&lt;p>`//file: net/core/dev.c
int dev_hard_start_xmit(struct sk_buff _skb, struct net_device _dev,
   struct netdev_queue _txq)
{
 // 获取设备的回调函数集合  ops
 const struct net_device_ops _ops = dev-&amp;gt;netdev_ops;&lt;/p>
&lt;p>// 获取设备支持的功能列表
 features = netif_skb_features(skb);&lt;/p>
&lt;p>// 调用驱动的  ops  里面的发送回调函数  ndo_start_xmit  将数据包传给网卡设备
 skb_len = skb-&amp;gt;len;
 rc = ops-&amp;gt;ndo_start_xmit(skb, dev);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>其中 ndo_start_xmit 是网卡驱动要实现的一个函数，是在 net_device_ops 中定义的。&lt;/p>
&lt;p>`//file: include/linux/netdevice.h
struct net_device_ops {
 netdev_tx_t  (_ndo_start_xmit) (struct sk_buff _skb,
         struct net_device *dev);&lt;/p>
&lt;p>}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在 igb 网卡驱动源码中，我们找到了。&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static const struct net_device_ops igb_netdev_ops = {  .ndo_open  = igb_open,  .ndo_stop  = igb_close,  .ndo_start_xmit  = igb_xmit_frame,   ... };&lt;/code>&lt;/p>
&lt;p>也就是说，对于网络设备层定义的 ndo_start_xmit， igb 的实现函数是 igb_xmit_frame。这个函数是在网卡驱动初始化的时候被赋值的。具体初始化过程参见&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">《图解 Linux 网络包接收过程》&lt;/a>一文中的 2.4 节，网卡驱动初始化。&lt;/p>
&lt;p>所以在上面网络设备层调用 ops-&amp;gt;ndo_start_xmit 的时候，会实际上进入 igb_xmit_frame 这个函数中。我们进入这个函数来看看驱动程序是如何工作的。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static netdev_tx_t igb_xmit_frame(struct sk_buff _skb,
      struct net_device _netdev)
{
 &amp;hellip;&amp;hellip;
 return igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));
}&lt;/p>
&lt;p>netdev_tx_t igb_xmit_frame_ring(struct sk_buff _skb,
    struct igb_ring _tx_ring)
{
 // 获取 TX Queue  中下一个可用缓冲区信息
 first = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[tx_ring-&amp;gt;next_to_use];
 first-&amp;gt;skb = skb;
 first-&amp;gt;bytecount = skb-&amp;gt;len;
 first-&amp;gt;gso_segs = 1;&lt;/p>
&lt;p>//igb_tx_map 函数准备给设备发送的数据。
 igb_tx_map(tx_ring, first, hdr_len);
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>在这里从网卡的发送队列的 RingBuffer 中取下来一个元素，并将 skb 挂到元素上。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>igb_tx_map 函数处理将 skb 数据映射到网卡可访问的内存 DMA 区域。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static void igb_tx_map(struct igb_ring _tx_ring,
      struct igb_tx_buffer _first,
      const u8 hdr_len)
{
 // 获取下一个可用描述符指针
 tx_desc = IGB_TX_DESC(tx_ring, i);&lt;/p>
&lt;p>// 为  skb-&amp;gt;data  构造内存映射，以允许设备通过  DMA  从  RAM  中读取数据
 dma = dma_map_single(tx_ring-&amp;gt;dev, skb-&amp;gt;data, size, DMA_TO_DEVICE);&lt;/p>
&lt;p>// 遍历该数据包的所有分片, 为  skb  的每个分片生成有效映射
 for (frag = &amp;amp;skb_shinfo(skb)-&amp;gt;frags[0];; frag++) {&lt;/p>
&lt;p>tx_desc-&amp;gt;read.buffer_addr = cpu_to_le64(dma);
  tx_desc-&amp;gt;read.cmd_type_len = &amp;hellip;;
  tx_desc-&amp;gt;read.olinfo_status = 0;
 }&lt;/p>
&lt;p>// 设置最后一个 descriptor
 cmd_type |= size | IGB_TXD_DCMD;
 tx_desc-&amp;gt;read.cmd_type_len = cpu_to_le32(cmd_type);&lt;/p>
&lt;p>/_ Force memory writes to complete before letting h/w know there
  _ are new descriptors to fetch
  */
 wmb();
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>当所有需要的描述符都已建好，且 skb 的所有数据都映射到 DMA 地址后，驱动就会进入到它的最后一步，触发真实的发送。&lt;/p>
&lt;h3 id="48-发送完成硬中断">4.8 发送完成硬中断&lt;/h3>
&lt;p>当数据发送完成以后，其实工作并没有结束。因为内存还没有清理。当发送完成的时候，网卡设备会触发一个硬中断来释放内存。&lt;/p>
&lt;p>在&lt;a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;amp;mid=2247484058&amp;amp;idx=1&amp;amp;sn=a2621bc27c74b313528eefbc81ee8c0f&amp;amp;scene=21#wechat_redirect">《图解 Linux 网络包接收过程》&lt;/a> 一文中的 3.1 和 3.2 小节，我们详细讲述过硬中断和软中断的处理过程。&lt;/p>
&lt;p>在发送完成硬中断里，会执行 RingBuffer 内存的清理工作，如图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>再回头看一下硬中断触发软中断的源码。&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static inline void ____napi_schedule(...){  list_add_tail(&amp;amp;napi-&amp;gt;poll_list, &amp;amp;sd-&amp;gt;poll_list);  __raise_softirq_irqoff(NET_RX_SOFTIRQ); }&lt;/code>&lt;/p>
&lt;p>这里有个很有意思的细节，无论硬中断是因为是有数据要接收，还是说发送完成通知，&lt;strong>从硬中断触发的软中断都是 NET_RX_SOFTIRQ&lt;/strong>。这个我们在第一节说过了，这是软中断统计中 RX 要高于 TX 的一个原因。&lt;/p>
&lt;p>好我们接着进入软中断的回调函数 igb_poll。在这个函数里，我们注意到有一行 igb_clean_tx_irq，参见源码：&lt;/p>
&lt;p>&lt;code>//file: drivers/net/ethernet/intel/igb/igb_main.c static int igb_poll(struct napi_struct *napi, int budget) {  //performs the transmit completion operations  if (q_vector-&amp;gt;tx.ring)   clean_complete = igb_clean_tx_irq(q_vector);  ... }&lt;/code>&lt;/p>
&lt;p>我们来看看当传输完成的时候，igb_clean_tx_irq 都干啥了。&lt;/p>
&lt;p>`//file: drivers/net/ethernet/intel/igb/igb_main.c
static bool igb_clean_tx_irq(struct igb_q_vector *q_vector)
{
 //free the skb
 dev_kfree_skb_any(tx_buffer-&amp;gt;skb);&lt;/p>
&lt;p>//clear tx_buffer data
 tx_buffer-&amp;gt;skb = NULL;
 dma_unmap_len_set(tx_buffer, len, 0);&lt;/p>
&lt;p>// clear last DMA location and unmap remaining buffers */
 while (tx_desc != eop_desc) {
 }
}&lt;/p>
&lt;p>`&lt;/p>
&lt;p>无非就是清理了 skb，解除了 DMA 映射等等。到了这一步，传输才算是基本完成了。&lt;/p>
&lt;p>为啥我说是基本完成，而不是全部完成了呢？因为传输层需要保证可靠性，所以 skb 其实还没有删除。它得等收到对方的 ACK 之后才会真正删除，那个时候才算是彻底的发送完毕。&lt;/p>
&lt;h2 id="最后">最后&lt;/h2>
&lt;p>用一张图总结一下整个发送过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>了解了整个发送过程以后，我们回头再来回顾开篇提到的几个问题。&lt;/p>
&lt;p>&lt;strong>1. 我们在监控内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？&lt;/strong>&lt;/p>
&lt;p>在网络包的发送过程中，用户进程（在内核态）完成了绝大部分的工作，甚至连调用驱动的事情都干了。只有当内核态进程被切走前才会发起软中断。发送过程中，绝大部分（90%）以上的开销都是在用户进程内核态消耗掉的。&lt;/p>
&lt;p>只有一少部分情况下才会触发软中断（NET_TX 类型），由软中断 ksoftirqd 内核进程来发送。&lt;/p>
&lt;p>所以，在监控网络 IO 对服务器造成的 CPU 开销的时候，不能仅仅只看 si，而是应该把 si、sy 都考虑进来。&lt;/p>
&lt;p>&lt;strong>2. 在服务器上查看 /proc/softirqs，为什么 NET_RX 要比 NET_TX 大的多的多？&lt;/strong>&lt;/p>
&lt;p>之前我认为 NET_RX 是读取，NET_TX 是传输。对于一个既收取用户请求，又给用户返回的 Server 来说。这两块的数字应该差不多才对，至少不会有数量级的差异。但事实上，飞哥手头的一台服务器是这样的：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>经过今天的源码分析，发现这个问题的原因有两个。&lt;/p>
&lt;p>第一个原因是当数据发送完成以后，通过硬中断的方式来通知驱动发送完毕。但是硬中断无论是有数据接收，还是对于发送完毕，触发的软中断都是 NET_RX_SOFTIRQ，而并不是 NET_TX_SOFTIRQ。&lt;/p>
&lt;p>第二个原因是对于读来说，都是要经过 NET_RX 软中断的，都走 ksoftirqd 内核进程。而对于发送来说，绝大部分工作都是在用户进程内核态处理了，只有系统态配额用尽才会发出 NET_TX，让软中断上。&lt;/p>
&lt;p>综上两个原因，那么在机器上查看 NET_RX 比 NET_TX 大的多就不难理解了。&lt;/p>
&lt;p>&lt;strong>3. 发送网络数据的时候都涉及到哪些内存拷贝操作？&lt;/strong>&lt;/p>
&lt;p>这里的内存拷贝，我们只特指待发送数据的内存拷贝。&lt;/p>
&lt;p>第一次拷贝操作是内核申请完 skb 之后，这时候会将用户传递进来的 buffer 里的数据内容都拷贝到 skb 中。如果要发送的数据量比较大的话，这个拷贝操作开销还是不小的。&lt;/p>
&lt;p>第二次拷贝操作是从传输层进入网络层的时候，每一个 skb 都会被克隆一个新的副本出来。网络层以及下面的驱动、软中断等组件在发送完成的时候会将这个副本删除。传输层保存着原始的 skb，在当网络对方没有 ack 的时候，还可以重新发送，以实现 TCP 中要求的可靠传输。&lt;/p>
&lt;p>第三次拷贝不是必须的，只有当 IP 层发现 skb 大于 MTU 时才需要进行。会再申请额外的 skb，并将原来的 skb 拷贝为多个小的 skb。&lt;/p>
&lt;blockquote>
&lt;p>这里插入个题外话，大家在网络性能优化中经常听到的零拷贝，我觉得这有点点夸张的成分。TCP 为了保证可靠性，第二次的拷贝根本就没法省。如果包再大于 MTU 的话，分片时的拷贝同样也避免不了。&lt;/p>
&lt;/blockquote>
&lt;p>看到这里，相信内核发送数据包对于你来说，已经不再是一个完全不懂的黑盒了。本文哪怕你只看懂十分之一，你也已经掌握了这个黑盒的打开方式。这在你将来优化网络性能时你就会知道从哪儿下手了。&lt;/p>
&lt;p>还愣着干啥，赶紧帮飞哥&lt;strong>赞、再看、转发&lt;/strong>三连走起！&lt;/p>
&lt;p>&lt;strong>Github:&lt;/strong>&lt;a href="https://github.com/yanfeizhang/coder-kung-fu">https://github.com/yanfeizhang/coder-kung-fu&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;hr>
&lt;p>由于本文比较长，在公众号上看起来确实是有点费劲。所以飞哥搞了个 pdf，带目录结构，可快速跳转，看起来更方便。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p>
&lt;p>不过获取方式仍然有点小门槛：&lt;strong>带任意推荐语转发本文到朋友圈，加飞哥微信知会&lt;/strong>即可。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/77c123f6-fc35-4ed5-8a1a-aa37c6f474d2/640" alt="">&lt;/p></description></item><item><title>Docs: Linux 网络流量控制</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Network_traffic_control">Wiki-Network Traffic Control&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>：
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/lartc-qdisc-zh/#91-%E9%98%9F%E5%88%97queues%E5%92%8C%E6%8E%92%E9%98%9F%E8%A7%84%E5%88%99queueing-disciplines">[译] 《Linux 高级路由与流量控制手册（2012）》第九章：用 tc qdisc 管理 Linux 网络带宽&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://lartc.org/LARTC-zh_CN.GB2312.pdf">《Linux 高级路由与流量控制手册（2003）》 中文翻译&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在计算机网络中，&lt;strong>Traffic Control(流量控制，简称 TC)&lt;/strong> 系统可以让服务器，像路由器一样工作，这也是 &lt;a href="https://www.yuque.com/go/doc/33218298">SDN&lt;/a> 中重要的组成部分。通过精准的流量控制，可以让服务器减少拥塞、延迟、数据包丢失；实现 NAT 功能、控制带宽、阻止入侵；等等等等。&lt;/p>
&lt;p>Traffic Control(流量控制) 在不同的语境中有不同的含义，可以表示一整套完整功能的系统、也可以表示为一种处理网络数据包的行为&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>众所周知，在互联网诞生之初都是各个高校和科研机构相互通讯，并没有网络流量控制方面的考虑和设计，TCP/IP 协议的原则是尽可能好地为所有数据流服务，不同的数据流之间是平等的。然而多年的实践表明，这种原则并不是最理想的，有些数据流应该得到特别的照顾， 比如，远程登录的交互数据流应该比数据下载有更高的优先级。&lt;/p>
&lt;p>针对不同的数据流采取不同的策略，这种可能性是存在的。并且，随着研究的发展和深入， 人们已经提出了各种不同的管理模式。&lt;a href="https://www.yuque.com/go/doc/34208492">IETF&lt;/a> 已经发布了几个标准， 如综合服务(Integrated Services)、区分服务(Diferentiated Services)等。其实，Linux 内核从 2 2 开始，就已经实现了相关的 &lt;strong>Traffic Control(流量控制)&lt;/strong> 功能。&lt;/p>
&lt;p>实际上，流量控制系统可以想象成 &lt;strong>Message Queue(消息队列)&lt;/strong> 的功能。都是为了解决数据量瞬间太大导致处理不过来的问题。&lt;/p>
&lt;h1 id="traffic-control-的实现">Traffic Control 的实现&lt;/h1>
&lt;p>想要实现 Traffic Control(流量控制) 系统，通常需要以下功能中的一个或多个：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Queuing(队列)&lt;/strong> # 每个进出服务器的数据包，都排好队逐一处理。&lt;/li>
&lt;li>&lt;strong>Hook(钩子)&lt;/strong> # 也可以称为** DataPath**。用于拦截进出服务器的每个数据包，并对数据包进行处理。
&lt;ul>
&lt;li>每种实现流量控制的程序，在内核中添加的 Hook 的功能各不相同，Hook 的先后顺序也各不相同，甚至可以多个 Traffic Control 共存，然后在各自的 Hook 上处理数据包&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Connection Tracking(连接跟踪)&lt;/strong> # 每个被拦截到的数据包，都需要记录其信息以跟踪他们。&lt;/li>
&lt;/ul>
&lt;p>通过对数据包进行 &lt;strong>Queuing(排队)&lt;/strong>，我们可以决定数据的发送方式。我们只能对发送的数据进行整形。&lt;/p>
&lt;p>&lt;strong>互联网的工作机制&lt;/strong>决定了&lt;strong>接收端无法直接控制发送端的行为&lt;/strong>。这就像你家的邮箱一样：除非能联系到所有人（告诉他们未经同意不要寄信给你），否则 你无法控制别人寄多少东西过来。&lt;/p>
&lt;p>但与实际生活不同的是，互联网基于 TCP/IP 协议栈，这多少会带来一些帮助。TCP/IP 无法提前知道两台主机之间的网络带宽，因此开始时它会以越来越快的速度发送数据，直到开始出现丢包，这时它知道已经没有可用空间来存储这些待发送的包了，因此就会 降低发送速度。TCP/IP 的实际工作过程比这个更智能一点，后面会再讨论。&lt;/p>
&lt;p>这就好比你留下一半的信件在实体邮箱里不取，期望别人知道这个状况后会停止给你寄新的信件。 但不幸的是，&lt;strong>这种方式只对互联网管用，对你的实体邮箱无效&lt;/strong> :-)&lt;/p>
&lt;p>如果内网有一台路由器，你希望&lt;strong>限制某几台主机的下载速度&lt;/strong>，那你应该找到发送数据到这些主机的路由器内部的接口，然后在这些 &lt;strong>路由器内部接口&lt;/strong>上做 &lt;strong>整流&lt;/strong>（traffic shaping，流量整形）。&lt;/p>
&lt;p>此外，还要确保链路瓶颈（bottleneck of the link）也在你的控制范围内。例如，如果网卡是 100Mbps，但路由器的链路带宽是 256Kbps，那首先应该确保不要发送过多数据给路由 器，因为它扛不住。否则，&lt;strong>链路控制和带宽整形的决定权就不在主机侧而到路由器侧了&lt;/strong>。要达到限速目的，需要对**“发送队列”&lt;strong>有完全的把控（”own the queue”），这里的“发送队列”也就是&lt;/strong>整条链路上最慢的一段**（slowest link in the chain）。 幸运的是，大多数情况下这个条件都是能满足的。&lt;/p>
&lt;p>再用白话一点的描述：其实所谓的控制发送端行为，这种描述中的 发送端 是一个相对概念，在 Linux 每个 Hook 发给下一个 Hook 的时候，前一个 Hook 就是下一个 Hook 的发送端，所以，控制发送端行为，就是在第一个 Hook 收到数据包时，控制他发給下一个 Hook 或应用程序的数据包的行为。&lt;/p>
&lt;h2 id="实现流量控制系统的具体方式">实现流量控制系统的具体方式&lt;/h2>
&lt;p>流量控制系统的行为通常都是在内核中完成的，所有一般都是将将官代码直接写进内核，或者使用模块加载进内核，还有新时代的以 BPF 模式加在进内核。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/34346353">&lt;strong>Netfilter 框架&lt;/strong>&lt;/a>
&lt;ul>
&lt;li>通过 iptables、nftables 控制 Netfilter 框架中的 Hook 行为&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/34380573">&lt;strong>tc 模块&lt;/strong>&lt;/a>
&lt;ul>
&lt;li>通过 tc 二进制程序控制 Hook 行为&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/33144610">&lt;strong>BPF 接口&lt;/strong>&lt;/a>
&lt;ul>
&lt;li>待整理，暂时不知道 Linux 中有什么会基于 BPF 的应用程序。
&lt;ul>
&lt;li>但是有一个 Cilium 程序，是基于 BPF 做的，只不过只能部署在 Kubernetes 集群中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>如下图所示，每种实现方式，都具有不同的 Hook：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1616164826770-1d929135-1194-44e1-91a9-3dd4e99c34ca.png" alt="">&lt;/p>
&lt;ul>
&lt;li>其中 Netfilter 框架具有最庞大的 Hook 以及 DataPath，上图中间带颜色的部分，基本都是 Netfilter 框架可以处理流量的地方
&lt;ul>
&lt;li>包括 prerouting、input、forward、output、postrouting 这几个默认的 Hook&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ingress(qdisc)、egress(qdisc) 属于 TC 模块的 Hook&lt;/li>
&lt;li>其他的则是 eBPF 添加的新 Hook&lt;/li>
&lt;/ul>
&lt;p>当然，随着 eBPF 的兴起，Netfilter 这冗长的流量处理过程，被历史淘汰也是必然的趋势~~~~&lt;/p>
&lt;h1 id="各种流量控制方法的区别">各种流量控制方法的区别&lt;/h1>
&lt;h2 id="kube-proxy-包转发路径">kube-proxy 包转发路径&lt;/h2>
&lt;p>从网络角度看，使用传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241861-a7af19e7-ea7f-49ec-ac8d-3ed6979e1f9e.png" alt="">
步骤：&lt;/p>
&lt;ol>
&lt;li>网卡收到一个包（通过 DMA 放到 ring-buffer）。&lt;/li>
&lt;li>包经过 XDP hook 点。&lt;/li>
&lt;li>内核&lt;strong>给包分配内存&lt;/strong>，此时才有了大家熟悉的 &lt;code>skb&lt;/code>（包的内核结构体表示），然后 送到内核协议栈。&lt;/li>
&lt;li>包经过 GRO 处理，对分片包进行重组。&lt;/li>
&lt;li>包进入 tc（traffic control）的 ingress hook。接下来，&lt;strong>所有橙色的框都是 Netfilter 处理点&lt;/strong>。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>raw&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>包经过内核的&lt;strong>连接跟踪&lt;/strong>（conntrack）模块。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>PREROUTING&lt;/code> hook 点处理 &lt;code>nat&lt;/code> table 的 iptables 规则。&lt;/li>
&lt;li>进行&lt;strong>路由判断&lt;/strong>（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。&lt;/li>
&lt;li>Netfilter：在 &lt;code>FORWARD&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>FORWARD&lt;/code> hook 点处理 &lt;code>filter&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>POSTROUTING&lt;/code> hook 点处理 &lt;code>mangle&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>Netfilter：在 &lt;code>POSTROUTING&lt;/code> hook 点处理 &lt;code>nat&lt;/code> table 里的 iptables 规则。&lt;/li>
&lt;li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。&lt;/li>
&lt;li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：&lt;/li>
&lt;li>发送到一个本机 veth 设备，或者一个本机 service endpoint，&lt;/li>
&lt;li>或者，如果目的 IP 是主机外，就通过网卡发出去。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>相关阅读，有助于理解以上过程：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2019-11-30-cracking-k8s-node-proxy.md%20%25%7D">Cracking Kubernetes Node Proxy (aka kube-proxy)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2019-02-18-deep-dive-into-iptables-and-netfilter-arch-zh.md%20%25%7D">(译) 深入理解 iptables 和 netfilter 架构&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/%7B%20%25%20link%20_posts/2020-08-05-conntrack-design-and-implementation-zh.md%20%25%7D">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">(译) 深入理解 Cilium 的 eBPF 收发包路径（datapath）&lt;/a>&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>译者注。&lt;/p>
&lt;h2 id="cilium-ebpf-包转发路径">Cilium eBPF 包转发路径&lt;/h2>
&lt;p>作为对比，再来看下 Cilium eBPF 中的包转发路径：&lt;/p>
&lt;blockquote>
&lt;p>建议和 &lt;a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">(译) 深入理解 Cilium 的 eBPF 收发包路径（datapath）&lt;/a> 对照看。
译者注。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241799-94b516d7-7bf7-4b37-adf5-1c2defbac27c.png" alt="">
对比可以看出，&lt;strong>Cilium eBPF datapath 做了短路处理&lt;/strong>：从 tc ingress 直接 shortcut 到 tc egress，节省了 9 个中间步骤（总共 17 个）。更重要的是：这个 datapath &lt;strong>绕过了 整个 Netfilter 框架&lt;/strong>（橘黄色的框们），Netfilter 在大流量情况下性能是很差的。
去掉那些不用的框之后，Cilium eBPF datapath 长这样：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936242480-85cfc77b-217e-44d9-936a-b4d982cf1e7f.png" alt="">
&lt;strong>Cilium/eBPF 还能走的更远&lt;/strong>。例如，如果包的目的端是另一台主机上的 service endpoint，那你可以直接在 XDP 框中完成包的重定向（收包 &lt;code>1-&amp;gt;2&lt;/code>，在步骤 &lt;code>2&lt;/code> 中对包 进行修改，再通过 &lt;code>2-&amp;gt;1&lt;/code> 发送出去），将其发送出去，如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pryclo/1617936241746-6f9a6415-8747-49ca-9a74-cbf06c7a7be8.png" alt="">
可以看到，这种情况下包都**没有进入内核协议栈（准确地说，都没有创建 skb）**就被转 发出去了，性能可想而知。&lt;/p>
&lt;blockquote>
&lt;p>XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码，而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Linux 网络设备详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;/blockquote>
&lt;h1 id="虚拟网络设备">虚拟网络设备&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#">https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#&lt;/a>&lt;/li>
&lt;li>[本知识库虚拟化网络章节](✏IT 学习笔记/☁️10.云原生/1.1.虚拟化/Network%20Virtual(网络虚拟化).md Virtual(网络虚拟化).md)&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 具有丰富的虚拟网络功能，可用作托管 VM 和容器以及云环境的基础。在这篇文章中，我将简要介绍所有常用的虚拟网络接口类型。没有代码分析，只简单介绍了接口及其在 Linux 上的使用。任何有网络背景的人都可能对这篇博文感兴趣。可以使用命令 ip link help 获取接口列表。&lt;/p>
&lt;p>这篇文章涵盖了以下常用网络设备和一些容易相互混淆的网络设备：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#bridge">Bridge&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#bonded">Bond&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#team">Team device&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vlan">VLAN (Virtual LAN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vxlan">VXLAN (Virtual eXtensible Local Area Network)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#macvlan">MACVLAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ipvlan">IPVLAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#macvtap">MACVTAP/IPVTAP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#macsec">MACsec (Media Access Control Security)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#veth">VETH (Virtual Ethernet)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vcan">VCAN (Virtual CAN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#vxcan">VXCAN (Virtual CAN tunnel)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ipoib">IPOIB (IP-over-InfiniBand)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#nlmon">NLMON (NetLink MONitor)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#dummy">Dummy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#ifb">IFB (Intermediate Functional Block)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#netdevsim">netdevsim&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bridge">Bridge&lt;/h2>
&lt;p>Bridge 网络设备的行为类似于网络交换机。它在连接到它的网络设备之间转发数据包。通常用于在 路由器、网关、虚拟机、网络名称空间之间转发数据包。同时 Bridge 设备还支持 STP、VLAN 过滤和组播侦听。&lt;/p>
&lt;p>当我们想要在 虚拟机、容器、宿主机 之间建立通信时，Bridge 设备是必不可少的。
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/bridge.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495381-a7c7e048-4783-45a6-a1d1-c44526401132.png" alt="image.png">&lt;/a>
下面是一个是创建 Brdige 并连接其他网络设备的示例：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ip link add br0 type bridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link set eth0 master br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link set tap1 master br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link set tap2 master br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link set veth1 master br0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的例子将会创建一个名为 &lt;code>br0&lt;/code> 的 Bridge 设备，并将两个 TAP 设备和一个 VETH 设备作为其从属设备。&lt;/p>
&lt;h2 id="bond">Bond&lt;/h2>
&lt;p>The Linux bonding driver provides a method for aggregating multiple network interfaces into a single logical &amp;ldquo;bonded&amp;rdquo; interface. The behavior of the bonded interface depends on the mode; generally speaking, modes provide either hot standby or load balancing services.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/bond.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495394-4aaea4bd-34d9-4987-9873-38af16247d1b.png" alt="image.png">&lt;/a>
Use a bonded interface when you want to increase your link speed or do a failover on your server.
Here&amp;rsquo;s how to create a bonded interface:
ip link add bond1 type bond miimon 100 mode active-backup ip link set eth0 master bond1 ip link set eth1 master bond1
This creates a bonded interface named bond1 with mode active-backup. For other modes, please see the &lt;a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt">kernel documentation&lt;/a>.&lt;/p>
&lt;h2 id="team">Team&lt;/h2>
&lt;p>Similar a bonded interface, the purpose of a team device is to provide a mechanism to group multiple NICs (ports) into one logical one (teamdev) at the L2 layer.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/team.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495378-cdc041c4-6321-49b7-b532-63f2cded7392.png" alt="image.png">&lt;/a>
The main thing to realize is that a team device is not trying to replicate or mimic a bonded interface. What it does is to solve the same problem using a different approach, using, for example, a lockless (RCU) TX/RX path and modular design.
But there are also some functional differences between a bonded interface and a team. For example, a team supports LACP load-balancing, NS/NA (IPV6) link monitoring, D-Bus interface, etc., which are absent in bonding. For further details about the differences between bonding and team, see &lt;a href="https://github.com/jpirko/libteam/wiki/Bonding-vs.-Team-features">Bonding vs. Team features&lt;/a>.
Use a team when you want to use some features that bonding doesn&amp;rsquo;t provide.
Here&amp;rsquo;s how to create a team:
# teamd -o -n -U -d -t team0 -c &amp;lsquo;{&amp;ldquo;runner&amp;rdquo;: {&amp;ldquo;name&amp;rdquo;: &amp;ldquo;activebackup&amp;rdquo;},&amp;ldquo;link_watch&amp;rdquo;: {&amp;ldquo;name&amp;rdquo;: &amp;ldquo;ethtool&amp;rdquo;}}&amp;rsquo; # ip link set eth0 down # ip link set eth1 down # teamdctl team0 port add eth0 # teamdctl team0 port add eth1
This creates a team interface named team0 with mode active-backup, and it adds eth0 and eth1 as team0&amp;rsquo;s sub-interfaces.
A new driver called &lt;a href="https://www.kernel.org/doc/html/latest/networking/net_failover.html">net_failover&lt;/a> has been added to Linux recently. It&amp;rsquo;s another failover master net device for virtualization and manages a primary (&lt;a href="https://wiki.libvirt.org/page/Networking#PCI_Passthrough_of_host_network_devices">passthru/VF [Virtual Function]&lt;/a> device) slave net device and a standby (the original paravirtual interface) slave net device.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/net_failover.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495395-cb9efff7-7595-4750-b0a0-18f0ef15f765.png" alt="image.png">&lt;/a>&lt;/p>
&lt;h2 id="vlan">VLAN&lt;/h2>
&lt;p>A VLAN, aka virtual LAN, separates broadcast domains by adding tags to network packets. VLANs allow network administrators to group hosts under the same switch or between different switches.
The VLAN header looks like:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493495406-ba20c202-7aec-4f2d-8e4a-718c8db481ad.png" alt="image.png">&lt;/a>
Use a VLAN when you want to separate subnet in VMs, namespaces, or hosts.
Here&amp;rsquo;s how to create a VLAN:
# ip link add link eth0 name eth0.2 type vlan id 2 # ip link add link eth0 name eth0.3 type vlan id 3
This adds VLAN 2 with name eth0.2 and VLAN 3 with name eth0.3. The topology looks like this:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496104-953046d4-0fac-4ca7-908d-45785c3a097d.png" alt="image.png">&lt;/a>
&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: When configuring a VLAN, you need to make sure the switch connected to the host is able to handle VLAN tags, for example, by setting the switch port to trunk mode.&lt;/p>
&lt;h2 id="vxlan">VXLAN&lt;/h2>
&lt;p>VXLAN (Virtual eXtensible Local Area Network) is a tunneling protocol designed to solve the problem of limited VLAN IDs (4,096) in IEEE 802.1q. It is described by &lt;a href="https://tools.ietf.org/html/rfc7348">IETF RFC 7348&lt;/a>.
With a 24-bit segment ID, aka VXLAN Network Identifier (VNI), VXLAN allows up to 2^24 (16,777,216) virtual LANs, which is 4,096 times the VLAN capacity.
VXLAN encapsulates Layer 2 frames with a VXLAN header into a UDP-IP packet, which looks like this:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vxlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496161-ef228755-9102-427a-b3fa-f9d8f5cae248.png" alt="image.png">&lt;/a>
VXLAN is typically deployed in data centers on virtualized hosts, which may be spread across multiple racks.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/vxlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496231-8fa6bbd6-045f-43f3-a2f9-58039a973086.png" alt="image.png">&lt;/a>
Here&amp;rsquo;s how to use VXLAN:
# ip link add vx0 type vxlan id 100 local 1.1.1.1 remote 2.2.2.2 dev eth0 dstport 4789
For reference, you can read the &lt;a href="https://www.kernel.org/doc/Documentation/networking/vxlan.txt">VXLAN kernel documentation&lt;/a> or &lt;a href="https://vincent.bernat.ch/en/blog/2017-vxlan-linux">this VXLAN introduction&lt;/a>.&lt;/p>
&lt;h2 id="macvlan">MACVLAN&lt;/h2>
&lt;p>With VLAN, you can create multiple interfaces on top of a single one and filter packages based on a VLAN tag. With MACVLAN, you can create multiple interfaces with different Layer 2 (that is, Ethernet MAC) addresses on top of a single one.
Before MACVLAN, if you wanted to connect to physical network from a VM or namespace, you would have needed to create TAP/VETH devices and attach one side to a bridge and attach a physical interface to the bridge on the host at the same time, as shown below.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/br_ns.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496373-bba2ae90-3c29-497b-9cf2-9707146ab063.png" alt="image.png">&lt;/a>
Now, with MACVLAN, you can bind a physical interface that is associated with a MACVLAN directly to namespaces, without the need for a bridge.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496402-8b2910b5-780c-4a7b-885d-e1ae96f70a20.png" alt="image.png">&lt;/a>
There are five MACVLAN types:
1. Private: doesn&amp;rsquo;t allow communication between MACVLAN instances on the same physical interface, even if the external switch supports hairpin mode.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493496672-ece58d6f-86b8-4bb6-bfdc-4877583cf2a8.png" alt="image.png">&lt;/a>
2. VEPA: data from one MACVLAN instance to the other on the same physical interface is transmitted over the physical interface. Either the attached switch needs to support hairpin mode or there must be a TCP/IP router forwarding the packets in order to allow communication.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_02.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497664-fcf059dc-8d97-4d85-83dc-79e35d8a0502.png" alt="image.png">&lt;/a>
3. Bridge: all endpoints are directly connected to each other with a simple bridge via the physical interface.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_03.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497742-be699130-38ef-45a2-a657-a7d90bcbbe81.png" alt="image.png">&lt;/a>
4. Passthru: allows a single VM to be connected directly to the physical interface.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvlan_04.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497676-73e85ca2-4f2a-47b9-b14b-1d5fb34c877b.png" alt="image.png">&lt;/a>
5. Source: the source mode is used to filter traffic based on a list of allowed source MAC addresses to create MAC-based VLAN associations. Please see the &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?id=79cf79abce71">commit message&lt;/a>.
The type is chosen according to different needs. Bridge mode is the most commonly used.
Use a MACVLAN when you want to connect directly to a physical network from containers.
Here&amp;rsquo;s how to set up a MACVLAN:
# ip link add macvlan1 link eth0 type macvlan mode bridge # ip link add macvlan2 link eth0 type macvlan mode bridge # ip netns add net1 # ip netns add net2 # ip link set macvlan1 netns net1 # ip link set macvlan2 netns net2
This creates two new MACVLAN devices in bridge mode and assigns these two devices to two different namespaces.&lt;/p>
&lt;h2 id="ipvlan">IPVLAN&lt;/h2>
&lt;p>IPVLAN is similar to MACVLAN with the difference being that the endpoints have the same MAC address.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/ipvlan.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497707-c70596b7-5689-41be-a5b3-94b69d978090.png" alt="image.png">&lt;/a>
IPVLAN supports L2 and L3 mode. IPVLAN L2 mode acts like a MACVLAN in bridge mode. The parent interface looks like a bridge or switch.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/ipvlan_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493497736-65194dad-9ecb-4e91-af55-b16fae98721f.png" alt="image.png">&lt;/a>
In IPVLAN L3 mode, the parent interface acts like a router and packets are routed between endpoints, which gives better scalability.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/ipvlan_02.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498348-d23366af-ed6b-46f0-8bcc-4997dacca461.png" alt="image.png">&lt;/a>
Regarding when to use an IPVLAN, the &lt;a href="https://www.kernel.org/doc/Documentation/networking/ipvlan.txt">IPVLAN kernel documentation&lt;/a> says that MACVLAN and IPVLAN &amp;ldquo;are very similar in many regards and the specific use case could very well define which device to choose. if one of the following situations defines your use case then you can choose to use ipvlan -
(a) The Linux host that is connected to the external switch / router has policy configured that allows only one mac per port.
(b) No of virtual devices created on a master exceed the mac capacity and puts the NIC in promiscuous mode and degraded performance is a concern.
(c) If the slave device is to be put into the hostile / untrusted network namespace where L2 on the slave could be changed / misused.&amp;rdquo;
Here&amp;rsquo;s how to set up an IPVLAN instance:
# ip netns add ns0 # ip link add name ipvl0 link eth0 type ipvlan mode l2 # ip link set dev ipvl0 netns ns0
This creates an IPVLAN device named ipvl0 with mode L2, assigned to namespace ns0.&lt;/p>
&lt;h2 id="macvtapipvtap">MACVTAP/IPVTAP&lt;/h2>
&lt;p>MACVTAP/IPVTAP is a new device driver meant to simplify virtualized bridged networking. When a MACVTAP/IPVTAP instance is created on top of a physical interface, the kernel also creates a character device/dev/tapX to be used just like a &lt;a href="https://en.wikipedia.org/wiki/TUN/TAP">TUN/TAP&lt;/a> device, which can be directly used by KVM/QEMU.
With MACVTAP/IPVTAP, you can replace the combination of TUN/TAP and bridge drivers with a single module:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macvtap.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498407-337b9bb8-cbee-4990-8283-55a3f5c36e66.png" alt="image.png">&lt;/a>
Typically, MACVLAN/IPVLAN is used to make both the guest and the host show up directly on the switch to which the host is connected. The difference between MACVTAP and IPVTAP is same as with MACVLAN/IPVLAN.
Here&amp;rsquo;s how to create a MACVTAP instance:
# ip link add link eth0 name macvtap0 type macvtap&lt;/p>
&lt;h2 id="macsec">MACsec&lt;/h2>
&lt;p>MACsec (Media Access Control Security) is an IEEE standard for security in wired Ethernet LANs. Similar to IPsec, as a layer 2 specification, MACsec can protect not only IP traffic but also ARP, neighbor discovery, and DHCP. The MACsec headers look like this:
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macsec_01.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498628-bcdf3cf6-08f5-4bd3-bac8-ef3412bea0d3.png" alt="image.png">&lt;/a>
The main use case for MACsec is to secure all messages on a standard LAN including ARP, NS, and DHCP messages.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/macsec.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498641-f72e7987-0e5b-438c-959d-6d0397b337d6.png" alt="image.png">&lt;/a>
Here&amp;rsquo;s how to set up a MACsec configuration:
# ip link add macsec0 link eth1 type macsec
&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: This only adds a MACsec device called macsec0 on interface eth1. For more detailed configurations, please see the &amp;ldquo;Configuration example&amp;rdquo; section in this &lt;a href="https://developers.redhat.com/blog/2016/10/14/macsec-a-different-solution-to-encrypt-network-traffic/">MACsec introduction by Sabrina Dubroca&lt;/a>.&lt;/p>
&lt;h2 id="veth">VETH&lt;/h2>
&lt;p>The VETH (virtual Ethernet) device is a local Ethernet tunnel. Devices are created in pairs, as shown in the diagram below.
Packets transmitted on one device in the pair are immediately received on the other device. When either device is down, the link state of the pair is down.
&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/10/veth.png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493498671-cf9ff7ff-0ee0-469e-adff-620edf2a237a.png" alt="image.png">&lt;/a>
Use a VETH configuration when namespaces need to communicate to the main host namespace or between each other.
Here&amp;rsquo;s how to set up a VETH configuration:
# ip netns add net1 # ip netns add net2 # ip link add veth1 netns net1 type veth peer name veth2 netns net2
This creates two namespaces, net1 and net2, and a pair of VETH devices, and it assigns veth1 to namespace net1 and veth2 to namespace net2. These two namespaces are connected with this VETH pair. Assign a pair of IP addresses, and you can ping and communicate between the two namespaces.&lt;/p>
&lt;h2 id="vcan">VCAN&lt;/h2>
&lt;p>Similar to the network loopback devices, the VCAN (virtual CAN) driver offers a virtual local CAN (Controller Area Network) interface, so users can send/receive CAN messages via a VCAN interface. CAN is mostly used in the automotive field nowadays.
For more CAN protocol information, please refer to the &lt;a href="https://www.kernel.org/doc/Documentation/networking/can.txt">kernel CAN documentation&lt;/a>.
Use a VCAN when you want to test a CAN protocol implementation on the local host.
Here&amp;rsquo;s how to create a VCAN:
# ip link add dev vcan1 type vcan&lt;/p>
&lt;h2 id="vxcan">VXCAN&lt;/h2>
&lt;p>Similar to the VETH driver, a VXCAN (Virtual CAN tunnel) implements a local CAN traffic tunnel between two VCAN network devices. When you create a VXCAN instance, two VXCAN devices are created as a pair. When one end receives the packet, the packet appears on the device&amp;rsquo;s pair and vice versa. VXCAN can be used for cross-namespace communication.
Use a VXCAN configuration when you want to send CAN message across namespaces.
Here&amp;rsquo;s how to set up a VXCAN instance:
# ip netns add net1 # ip netns add net2 # ip link add vxcan1 netns net1 type vxcan peer name vxcan2 netns net2
&lt;strong>&lt;em>Note&lt;/em>&lt;/strong>: VXCAN is not yet supported in Red Hat Enterprise Linux.&lt;/p>
&lt;h2 id="ipoib">IPOIB&lt;/h2>
&lt;p>An IPOIB device supports the IP-over-InfiniBand protocol. This transports IP packets over InfiniBand (IB) so you can use your IB device as a fast NIC.
The IPoIB driver supports two modes of operation: datagram and connected. In datagram mode, the IB UD (Unreliable Datagram) transport is used. In connected mode, the IB RC (Reliable Connected) transport is used. The connected mode takes advantage of the connected nature of the IB transport and allows an MTU up to the maximal IP packet size of 64K.
For more details, please see the &lt;a href="https://www.kernel.org/doc/Documentation/infiniband/ipoib.txt">IPOIB kernel documentation&lt;/a>.
Use an IPOIB device when you have an IB device and want to communicate with a remote host via IP.
Here&amp;rsquo;s how to create an IPOIB device:
# ip link add ib0 name ipoib0 type ipoib pkey IB_PKEY mode connected&lt;/p>
&lt;h2 id="nlmon">NLMON&lt;/h2>
&lt;p>NLMON is a Netlink monitor device.
Use an NLMON device when you want to monitor system Netlink messages.
Here&amp;rsquo;s how to create an NLMON device:
# ip link add nlmon0 type nlmon # ip link set nlmon0 up # tcpdump -i nlmon0 -w nlmsg.pcap
This creates an NLMON device named nlmon0 and sets it up. Use a packet sniffer (for example, tcpdump) to capture Netlink messages. Recent versions of Wireshark feature decoding of Netlink messages.&lt;/p>
&lt;h2 id="dummy">Dummy&lt;/h2>
&lt;p>dummy 是一个网络接口，是完全虚拟的，就像 loopback 设备。dummy 设备的目的是提供一种网络设备，用以路由数据包，而无需实际传输他们&lt;/p>
&lt;p>使用 dummy 设备使不活动的 SLIP(串行线路 Internet 协议) 地址看起来像本地程序的真实地址。 现在，dummy 设备主要用于测试和调试，同时，在 Kubernetes 中，Flannel 在 ipvs 模式下，也会创建一个名为 kube-ipvs0 的网络设备来路由数据包。&lt;/p>
&lt;p>dummy 设备的创建方式如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ip link add dummy1 type dummy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip addr add 1.1.1.1/24 dev dummy1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip link set dummy1 up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ifb">IFB&lt;/h2>
&lt;p>The IFB (Intermediate Functional Block) driver supplies a device that allows the concentration of traffic from several sources and the shaping incoming traffic instead of dropping it.
Use an IFB interface when you want to queue and shape incoming traffic.
Here&amp;rsquo;s how to create an IFB interface:
# ip link add ifb0 type ifb # ip link set ifb0 up # tc qdisc add dev ifb0 root sfq # tc qdisc add dev eth0 handle ffff: ingress # tc filter add dev eth0 parent ffff: u32 match u32 0 0 action mirred egress redirect dev ifb0
This creates an IFB device named ifb0 and replaces the root qdisc scheduler with SFQ (Stochastic Fairness Queueing), which is a classless queueing scheduler. Then it adds an ingress qdisc scheduler on eth0 and redirects all ingress traffic to ifb0.
For more IFB qdisc use cases, please refer to this &lt;a href="https://wiki.linuxfoundation.org/networking/ifb">Linux Foundation wiki on IFB&lt;/a>.&lt;/p>
&lt;h2 id="additional-resources">Additional resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/tag/virtual-networking/">Virtual networking articles&lt;/a> on the Red Hat Developer blog&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/">Dynamic IP Address Management in Open Virtual Network (OVN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/03/23/non-root-open-vswitch-rhel/">Non-root Open vSwitch in Red Hat Enterprise Linux&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/tag/open-vswitch/">Open vSwitch articles&lt;/a> on the Red hat Developer Blog&lt;/li>
&lt;/ul>
&lt;h2 id="netdevsim-interface">netdevsim interface&lt;/h2>
&lt;p>netdevsim is a simulated networking device which is used for testing various networking APIs. At this time it is particularly focused on testing hardware
offloading, tc/XDP BPF and SR-IOV.
A netdevsim device can be created as follows
# ip link add dev sim0 type netdevsim # ip link set dev sim0 up
To enable tc offload:
# ethtool -K sim0 hw-tc-offload on
To load XDP BPF or tc BPF programs:
# ip link set dev sim0 xdpoffload obj prog.o
To add VFs for SR-IOV testing:
# echo 3 &amp;gt; /sys/class/net/sim0/device/sriov&lt;em>numvfs # ip link set sim0 vf 0 mac
To change the vf numbers, you need to disable them completely first:
# echo 0 &amp;gt; /sys/class/net/sim0/device/sriov_numvfs # echo 5 &amp;gt; /sys/class/net/sim0/device/sriov_numvfs
Note: netdevsim is not compiled in RHEL by default
_Last updated: September 11, 2019&lt;/em>&lt;/p>
&lt;h1 id="隧道网络设备">隧道网络设备&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#">https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 支持多种隧道，但新用户可能会对它们的差异感到困惑，并不确定哪一种最适合给定的用例。在本文中，我将简要介绍 Linux 内核中常用的隧道接口。没有代码分析，只简单介绍了接口及其在 Linux 上的使用。任何有网络背景的人都可能对这些信息感兴趣。可以通过发出 iproute2 命令 ip link help 获得隧道接口列表以及特定隧道配置的帮助。&lt;/p>
&lt;p>这篇文章涵盖了以下常用接口：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#ipip">IPIP Tunnel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#sit">SIT Tunnel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#ip6tnl">ip6tnl Tunnel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#vti">VTI and VTI6&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#gre">GRE and GRETAP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#ip6gre">IP6GRE and IP6GRETAP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#fou">FOU&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#gue">GUE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#geneve">GENEVE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels#erspan">ERSPAN and IP6ERSPAN&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="ipip-tunnel">IPIP Tunnel&lt;/h2>
&lt;p>IPIP tunnel, just as the name suggests, is an IP over IP tunnel, defined in &lt;a href="https://tools.ietf.org/html/rfc2003">RFC 2003&lt;/a>. The IPIP tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541929-3c498a49-7406-4a02-ae97-da31bd386c6b.png" alt="image.png">
It&amp;rsquo;s typically used to connect two internal IPv4 subnets through public IPv4 internet. It has the lowest overhead but can only transmit IPv4 unicast traffic. That means you &lt;strong>cannot&lt;/strong> send multicast via IPIP tunnel.
IPIP tunnel supports both IP over IP and MPLS over IP.
&lt;strong>Note&lt;/strong>: When the ipip module is loaded, or an IPIP device is created for the first time, the Linux kernel will create a tunl0 default device in each namespace, with attributes local=any and remote=any. When receiving IPIP protocol packets, the kernel will forward them to tunl0 as a fallback device if it can&amp;rsquo;t find another device whose local/remote attributes match their source or destination address more closely.
Here is how to create an IPIP tunnel:
On Server A: # ip link add name ipip0 type ipip local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR # ip link set ipip0 up # ip addr add INTERNAL_IPV4_ADDR/24 dev ipip0 Add a remote internal subnet route if the endpoints don&amp;rsquo;t belong to the same subnet # ip route add REMOTE_INTERNAL_SUBNET/24 dev ipip0 On Server B: # ip link add name ipip0 type ipip local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR # ip link set ipip0 up # ip addr add INTERNAL_IPV4_ADDR/24 dev ipip0 # ip route add REMOTE_INTERNAL_SUBNET/24 dev ipip0
Note: Please replace LOCAL_IPv4_ADDR, REMOTE_IPv4_ADDR, INTERNAL_IPV4_ADDR, REMOTE_INTERNAL_SUBNET to the addresses based on your testing environment. The same with following example configs.&lt;/p>
&lt;h2 id="sit-tunnel">SIT Tunnel&lt;/h2>
&lt;p>SIT stands for Simple Internet Transition. The main purpose is to interconnect isolated IPv6 networks, located in global IPv4 internet.
Initially, it only had an IPv6 over IPv4 tunneling mode. After years of development, however, it acquired support for several different modes, such as ipip (the same with IPIP tunnel), ip6ip, mplsip, and any. Mode any is used to accept both IP and IPv6 traffic, which may prove useful in some deployments. SIT tunnel also supports &lt;a href="https://www.ietf.org/rfc/rfc4214.txt">ISATA&lt;/a>, and here is a &lt;a href="http://www.litech.org/isatap">usage example&lt;/a>.
The SIT tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541941-c151c630-2925-4a09-aa89-845588e3e5b3.png" alt="image.png">
When the sit module is loaded, the Linux kernel will create a default device, named sit0.
Here is how to create a SIT tunnel:
On Server A: # ip link add name sit1 type sit local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR mode any # ip link set sit1 up # ip addr add INTERNAL_IPV4_ADDR/24 dev sit1
Then, perform the same steps on the remote side.&lt;/p>
&lt;h2 id="ip6tnl-tunnel">ip6tnl Tunnel&lt;/h2>
&lt;p>ip6tnl is an IPv4/IPv6 over IPv6 tunnel interface, which looks like an IPv6 version of the SIT tunnel. The tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541939-55d26347-12f2-4dab-b439-a20720e2714e.png" alt="image.png">
ip6tnl supports modes ip6ip6, ipip6, any. Mode ipip6 is IPv4 over IPv6, and mode ip6ip6 is IPv6 over IPv6, and mode any supports both IPv4/IPv6 over IPv6.
When the ip6tnl module is loaded, the Linux kernel will create a default device, named ip6tnl0.
Here is how to create an ip6tnl tunnel:
# ip link add name ipip6 type ip6tnl local LOCAL_IPv6_ADDR remote REMOTE_IPv6_ADDR mode any&lt;/p>
&lt;h2 id="vti-and-vti6">VTI and VTI6&lt;/h2>
&lt;p>Virtual Tunnel Interface (VTI) on Linux is similar to Cisco&amp;rsquo;s VTI and Juniper&amp;rsquo;s implementation of secure tunnel (st.xx).
This particular tunneling driver implements IP encapsulations, which can be used with xfrm to give the notion of a secure tunnel and then use kernel routing on top.
In general, VTI tunnels operate in almost the same way as ipip or sit tunnels, except that they add a fwmark and IPsec encapsulation/decapsulation.
VTI6 is the IPv6 equivalent of VTI.
Here is how to create a VTI tunnel:
# ip link add name vti1 type vti key VTI_KEY local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR # ip link set vti1 up # ip addr add LOCAL_VIRTUAL_ADDR/24 dev vti1 # ip xfrm state add src LOCAL_IPv4_ADDR dst REMOTE_IPv4_ADDR spi SPI PROTO ALGR mode tunnel # ip xfrm state add src REMOTE_IPv4_ADDR dst LOCAL_IPv4_ADDR spi SPI PROTO ALGR mode tunnel # ip xfrm policy add dir in tmpl src REMOTE_IPv4_ADDR dst LOCAL_IPv4_ADDR PROTO mode tunnel mark VTI_KEY # ip xfrm policy add dir out tmpl src LOCAL_IPv4_ADDR dst REMOTE_IPv4_ADDR PROTO mode tunnel mark VTI_KEY
You can also configure IPsec via &lt;a href="https://libreswan.org/wiki/Route-based_VPN_using_VTI">libreswan&lt;/a> or &lt;a href="https://wiki.strongswan.org/projects/strongswan/wiki/RouteBasedVPN">strongSwan&lt;/a>.&lt;/p>
&lt;h2 id="gre-and-gretap">GRE and GRETAP&lt;/h2>
&lt;p>Generic Routing Encapsulation, also known as GRE, is defined in &lt;a href="https://tools.ietf.org/html/rfc2784">RFC 2784&lt;/a>
GRE tunneling adds an additional GRE header between the inside and outside IP headers. In theory, GRE could encapsulate any Layer 3 protocol with a valid Ethernet type, unlike IPIP, which can only encapsulate IP. The GRE header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541950-7306c06b-3438-4a0c-98f1-1dfac809d7a8.png" alt="image.png">
Note that you can transport multicast traffic and IPv6 through a GRE tunnel.
When the gre module is loaded, the Linux kernel will create a default device, named gre0.
Here is how to create a GRE tunnel:
# ip link add name gre1 type gre local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR [seq] key KEY
While GRE tunnels operate at OSI Layer 3, GRETAP works at OSI Layer 2, which means there is an Ethernet header in the inner header.
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493541978-5b18c4ce-a484-4882-90a4-1341aab450fd.png" alt="image.png">
Here is how to create a GRETAP tunnel:
# ip link add name gretap1 type gretap local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR&lt;/p>
&lt;h2 id="ip6gre-and-ip6gretap">IP6GRE and IP6GRETAP&lt;/h2>
&lt;p>IP6GRE is the IPv6 equivalent of GRE, which allows us to encapsulate any Layer 3 protocol over IPv6. The tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543192-8bbd04d7-88b1-407e-b183-27e70ac3142e.png" alt="image.png">
IP6GRETAP, just like GRETAP, has an Ethernet header in the inner header:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543437-bc1c8e95-4dee-449b-a31d-61154a0bbd85.png" alt="image.png">
Here is how to create a GRE tunnel:
# ip link add name gre1 type ip6gre local LOCAL_IPv6_ADDR remote REMOTE_IPv6_ADDR # ip link add name gretap1 type ip6gretap local LOCAL_IPv6_ADDR remote REMOTE_IPv6_ADDR&lt;/p>
&lt;h2 id="fou">FOU&lt;/h2>
&lt;p>Tunneling can happen at multiple levels in the networking stack. IPIP, SIT, GRE tunnels are at the IP level, while FOU (foo over UDP) is UDP-level tunneling.
There are some advantages of using UDP tunneling as UDP works with existing HW infrastructure, like &lt;a href="https://en.wikipedia.org/wiki/Network_interface_controller#RSS">RSS&lt;/a> in NICs, &lt;a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP&lt;/a> in switches, and checksum offload. The developer&amp;rsquo;s &lt;a href="https://lwn.net/Articles/614433/">patch set&lt;/a> shows significant performance increases for the SIT and IPIP protocols.
Currently, the FOU tunnel supports encapsulation protocol based on IPIP, SIT, GRE. An example FOU header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543253-f5dddca7-c7be-42cd-8f26-7f17a6b45499.png" alt="image.png">
Here is how to create a FOU tunnel:
# ip fou add port 5555 ipproto 4 # ip link add name tun1 type ipip remote 192.168.1.1 local 192.168.1.2 ttl 225 encap fou encap-sport auto encap-dport 5555
The first command configured a FOU receive port for IPIP bound to 5555; for GRE, you need to set ipproto 47. The second command set up a new IPIP virtual interface (tun1) configured for FOU encapsulation, with dest port 5555.
&lt;strong>NOTE&lt;/strong>: FOU is not supported in Red Hat Enterprise Linux.&lt;/p>
&lt;h2 id="gue">GUE&lt;/h2>
&lt;p>&lt;a href="https://tools.ietf.org/html/draft-ietf-intarea-gue">Generic UDP Encapsulation&lt;/a> (GUE) is another kind of UDP tunneling. The difference between FOU and GUE is that GUE has its own encapsulation header, which contains the protocol info and other data.
Currently, GUE tunnel supports inner IPIP, SIT, GRE encapsulation. An example GUE header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543283-20ce12c4-edf1-4a2b-8888-42d3bdb4d08e.png" alt="image.png">
Here is how to create a GUE tunnel:
# ip fou add port 5555 gue # ip link add name tun1 type ipip remote 192.168.1.1 local 192.168.1.2 ttl 225 encap gue encap-sport auto encap-dport 5555
This will set up a GUE receive port for IPIP bound to 5555, and an IPIP tunnel configured for GUE encapsulation.
&lt;strong>NOTE&lt;/strong>: GUE is not supported in Red Hat Enterprise Linux.&lt;/p>
&lt;h2 id="geneve">GENEVE&lt;/h2>
&lt;p>Generic Network Virtualization Encapsulation (GENEVE) supports all of the capabilities of VXLAN, NVGRE, and STT and was designed to overcome their perceived limitations. Many believe GENEVE could eventually replace these earlier formats entirely. The tunnel header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493543437-55251107-feaf-4448-9a65-9b726a1fa1bb.png" alt="image.png">
which looks very similar to &lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/#vxlan">VXLAN&lt;/a>. The main difference is that the GENEVE header is flexible. It&amp;rsquo;s very easy to add new features by extending the header with a new Type-Length-Value (TLV) field. For more details, you can see the latest &lt;a href="https://tools.ietf.org/html/draft-ietf-nvo3-geneve-08">geneve ietf draft&lt;/a> or refer to this &lt;a href="https://www.redhat.com/en/blog/what-geneve">What is GENEVE?&lt;/a> article.
&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/networking_with_open_virtual_network/open_virtual_network_ovn">Open Virtual Network (OVN)&lt;/a> uses GENEVE as default encapsulation. Here is how to create a GENEVE tunnel:
# ip link add name geneve0 type geneve id VNI remote REMOTE_IPv4_ADDR&lt;/p>
&lt;h2 id="erspan-and-ip6erspan">ERSPAN and IP6ERSPAN&lt;/h2>
&lt;p>Encapsulated Remote Switched Port Analyzer (ERSPAN) uses GRE encapsulation to extend the basic port mirroring capability from Layer 2 to Layer 3, which allows the mirrored traffic to be sent through a routable IP network. The ERSPAN header looks like:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/figk4l/1630493544476-8decf814-c9a0-4c61-9005-e7626ca9ae4a.png" alt="image.png">
The ERSPAN tunnel allows a Linux host to act as an ERSPAN traffic source and send the ERSPAN mirrored traffic to either a remote host or to an ERSPAN destination, which receives and parses the ERSPAN packets generated from Cisco or other ERSPAN-capable switches. This setup could be used to analyze, diagnose, and detect malicious traffic.
Linux currently supports most features of two ERSPAN versions: v1 (type II) and v2 (type III).
Here is how to create an ERSPAN tunnel:
# ip link add dev erspan1 type erspan local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR seq key KEY erspan_ver 1 erspan IDX or # ip link add dev erspan1 type erspan local LOCAL_IPv4_ADDR remote REMOTE_IPv4_ADDR seq key KEY erspan_ver 2 erspan_dir DIRECTION erspan_hwid HWID Add tc filter to monitor traffic # tc qdisc add dev MONITOR_DEV handle ffff: ingress # tc filter add dev MONITOR_DEV parent ffff: matchall skip_hw action mirred egress mirror dev erspan1&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Here is a summary of all the tunnels we introduced.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Tunnel/Link Type&lt;/th>
&lt;th>Outer Header&lt;/th>
&lt;th>Encapsulate Header&lt;/th>
&lt;th>Inner Header&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ipip&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>None&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sit&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>None&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6tnl&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>None&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vti&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>IPsec&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vti6&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>IPsec&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gre&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gretap&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>Ether + IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6gre&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6gretap&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>GRE&lt;/td>
&lt;td>Ether + IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>fou&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;td>UDP&lt;/td>
&lt;td>IPv4/IPv6/GRE&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gue&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;td>UDP + GUE&lt;/td>
&lt;td>IPv4/IPv6/GRE&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>geneve&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;td>UDP + Geneve&lt;/td>
&lt;td>Ether + IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>erspan&lt;/td>
&lt;td>IPv4&lt;/td>
&lt;td>GRE + ERSPAN&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ip6erspan&lt;/td>
&lt;td>IPv6&lt;/td>
&lt;td>GRE + ERSPAN&lt;/td>
&lt;td>IPv4/IPv6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Note&lt;/strong>: All configurations in this tutorial are volatile and won’t survive to a server reboot. If you want to make the configuration persistent across reboots, please consider using a networking configuration daemon, such as &lt;a href="https://developer.gnome.org/NetworkManager/stable/">NetworkManager&lt;/a>, or distribution-specific mechanisms.
_Also read: _&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/">Introduction to Linux interfaces for virtual networking&lt;/a>
&lt;em>Last updated: October 18, 2019&lt;/em>&lt;/p></description></item><item><title>Docs: Linux 网络栈管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/networking/index.html">Kernel 文档-Linux Networkiing Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/networking/kapi.html">Kernel 文档-Linux Networking and Network Devices APIs&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">[译] Linux 网络栈监控和调优：接收数据（2016）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">[译] Linux 网络栈监控和调优：发送数据（2017）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>和磁盘设备类似，Linux 用户想要使用网络功能，不能通过直接操作硬件完成，而需要直接或间接的操作一个 Linux 为我们抽象出来的设备，即通用的 &lt;strong>Linux 网络设备&lt;/strong>来完成。一个常见的情况是，系统里装有一个硬件网卡，Linux 会在系统里为其生成一个网络设备实例，如 eth0，用户需要对 eth0 发出命令以配置或使用它了。更多的硬件会带来更多的设备实例，虚拟的硬件也会带来更多的设备实例。&lt;/p>
&lt;p>网卡本身并不会连接连接任何网络，网卡需要相应的配置文件来告诉他们如何实现网络连接。而让网卡与配置文件关联的过程，就是 network.service 这类服务来实现的&lt;/p>
&lt;p>在 Linux 系统中，一般使用“网络设备”这种称呼，来描述硬件物理网卡设备在系统中的实例。在不同的语境中，有时也简称为 “设备”、“DEV” 等等。网络设备可以是一块真实机器上的网卡，也可以是创建的虚拟的网卡。&lt;/p>
&lt;p>而网络设备与网卡之间如何建立关系，就是网卡驱动程序的工作了，不同的网卡，驱动不一样，可以实现的功能也各有千秋。所以，想要系统出现 eth0 这种网络设备，网卡驱动程序是必须存在的，否则，没有驱动，也就无法识别硬件，无法识别硬件，在系统中也就不知道如何操作这个硬件。&lt;/p>
&lt;h2 id="常见术语">常见术语&lt;/h2>
&lt;h3 id="datapath数据路径">DataPath(数据路径)&lt;/h3>
&lt;p>网络数据在内核中进行网络传输时，所经过的所有点组合起来，称为数据路径。&lt;/p>
&lt;h3 id="socket-buffer简称-sk_buff-或-skb">Socket Buffer(简称 sk_buff 或 skb)&lt;/h3>
&lt;p>在内核代码中是一个名为 &lt;a href="https://www.kernel.org/doc/html/latest/networking/kapi.html#c.sk_buff">&lt;strong>sk_buff&lt;/strong>&lt;/a>** **的结构体。内核显然需要一个数据结构来储存报文的信息。这就是 skb 的作用。
sk_buff 结构自身并不存储报文内容，它通过多个指针指向真正的报文内存空间:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/efrsi8/1617849698535-471768e0-dcf8-4471-8dd2-605a1bc4e020.png" alt="image.png">
sk_buff 是一个贯穿整个协议栈层次的结构，在各层间传递时，内核只需要调整 sk_buff 中的指针位置就行。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/efrsi8/1617849692989-54095177-b85c-449e-8c66-3b026e4925da.png" alt="image.png">&lt;/p>
&lt;h3 id="device设备">DEVICE(设备)&lt;/h3>
&lt;p>在内核代码中，是一个名为 &lt;a href="https://www.kernel.org/doc/html/latest/networking/kapi.html#c.net_device">&lt;strong>net_device&lt;/strong>&lt;/a> 的结构体。一个巨大的数据结构，描述一个网络设备的所有 属性、数据 等信息。&lt;/p>
&lt;h1 id="linux-网络功能的实现">Linux 网络功能的实现&lt;/h1>
&lt;h1 id="数据包的-transmit发送-与-receive接收-过程概览">数据包的 Transmit(发送) 与 Receive(接收) 过程概览&lt;/h1>
&lt;h2 id="receive接收-过程">Receive(接收) 过程&lt;/h2>
&lt;p>本文将拿 &lt;strong>Intel I350&lt;/strong> 网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下 载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从被网卡接收到进入 socket 接收队列的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>加载网卡驱动，初始化&lt;/li>
&lt;li>包从外部网络进入网卡&lt;/li>
&lt;li>网卡（通过 DMA）将包 copy 到内核内存中的 ring buffer&lt;/li>
&lt;li>产生硬件中断，通知系统收到了一个包&lt;/li>
&lt;li>驱动调用 NAPI，如果轮询（poll）还没开始，就开始轮询&lt;/li>
&lt;li>&lt;code>ksoftirqd&lt;/code> 进程调用 NAPI 的 &lt;code>poll&lt;/code> 函数从 ring buffer 收包（&lt;code>poll&lt;/code> 函数是网卡 驱动在初始化阶段注册的；每个 CPU 上都运行着一个 &lt;code>ksoftirqd&lt;/code> 进程，在系统启动期 间就注册了）&lt;/li>
&lt;li>ring buffer 里包对应的内存区域解除映射（unmapped）&lt;/li>
&lt;li>（通过 DMA 进入）内存的数据包以 &lt;code>skb&lt;/code> 的形式被送至更上层处理&lt;/li>
&lt;li>如果 packet steering 功能打开，或者网卡有多队列，网卡收到的包会被分发到多个 CPU&lt;/li>
&lt;li>包从队列进入协议层&lt;/li>
&lt;li>协议层处理包&lt;/li>
&lt;li>包从协议层进入相应 socket 的接收队列&lt;/li>
&lt;/ol>
&lt;p>接下来会详细介绍这个过程。&lt;/p>
&lt;h2 id="transmit发送-过程">Transmit(发送) 过程&lt;/h2>
&lt;p>本文将拿&lt;strong>Intel I350&lt;/strong>网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从用户程序到达硬件网卡的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>使用&lt;strong>系统调用&lt;/strong>（如 &lt;code>sendto&lt;/code>，&lt;code>sendmsg&lt;/code> 等）写数据&lt;/li>
&lt;li>数据穿过&lt;strong>socket 子系统&lt;/strong>，进入&lt;strong>socket 协议族&lt;/strong>（protocol family）系统（在我们的例子中为 &lt;code>AF_INET&lt;/code>）&lt;/li>
&lt;li>协议族处理：数据穿过&lt;strong>协议层&lt;/strong>，这一过程（在许多情况下）会将&lt;strong>数据&lt;/strong>（data）转换成&lt;strong>数据包&lt;/strong>（packet）&lt;/li>
&lt;li>数据穿过&lt;strong>路由层&lt;/strong>，这会涉及路由缓存和 ARP 缓存的更新；如果目的 MAC 不在 ARP 缓存表中，将触发一次 ARP 广播来查找 MAC 地址&lt;/li>
&lt;li>穿过协议层，packet 到达&lt;strong>设备无关层&lt;/strong>（device agnostic layer）&lt;/li>
&lt;li>使用 XPS（如果启用）或散列函数&lt;strong>选择发送队列&lt;/strong>&lt;/li>
&lt;li>调用网卡驱动的&lt;strong>发送函数&lt;/strong>&lt;/li>
&lt;li>数据传送到网卡的 &lt;code>qdisc&lt;/code>（queue discipline，排队规则）&lt;/li>
&lt;li>qdisc 会直接&lt;strong>发送数据&lt;/strong>（如果可以），或者将其放到队列，下次触发&lt;code>**NET_TX**&lt;/code>** 类型软中断**（softirq）的时候再发送&lt;/li>
&lt;li>数据从 qdisc 传送给驱动程序&lt;/li>
&lt;li>驱动程序创建所需的&lt;strong>DMA 映射&lt;/strong>，以便网卡从 RAM 读取数据&lt;/li>
&lt;li>驱动向网卡发送信号，通知&lt;strong>数据可以发送了&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网卡从 RAM 中获取数据并发送&lt;/strong>&lt;/li>
&lt;li>发送完成后，设备触发一个&lt;strong>硬中断&lt;/strong>（IRQ），表示发送完成&lt;/li>
&lt;li>&lt;strong>硬中断处理函数&lt;/strong>被唤醒执行。对许多设备来说，这会&lt;strong>触发 &lt;strong>&lt;code>**NET_RX**&lt;/code>&lt;/strong> 类型的软中断&lt;/strong>，然后 NAPI poll 循环开始收包&lt;/li>
&lt;li>poll 函数会调用驱动程序的相应函数，&lt;strong>解除 DMA 映射&lt;/strong>，释放数据&lt;/li>
&lt;/ol>
&lt;h1 id="网络栈关联文件">网络栈关联文件&lt;/h1>
&lt;p>不同的 Linux 发行版，所使用的上层网络配置程序各不相同，各种程序所读取的配置文件也各不相同。&lt;/p>
&lt;ul>
&lt;li>对于 RedHat 相关的发行版，网络配置在 /etc/sysconfig/network-scripts/ 目录中&lt;/li>
&lt;li>对于 Debian 相关的发行版，网络配置在 /etc/network/ 目录中&lt;/li>
&lt;/ul>
&lt;p>在这些目录中，其实都是通过脚本来实现的&lt;/p>
&lt;p>后来随着时代的发展，涌现出很多通用的网络管理程序，比如 Netplan、NetworkManager、systemd-networkd 等等，这样就可以让各个发行版使用相同的程序来管理网络了，减少切换发行版而需要学习对应配置的成本，并且也更利于发展。&lt;/p></description></item><item><title>Docs: Netfilter 流量控制系统</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.netfilter.org/index.html">Netfilter 官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.netfilter.org/documentation/index.html">Netfilter 官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Netfilter">Wiki-Netfilter&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>：
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/">[译] 深入理解 iptables 和 netfilter 架构&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/conntrack-design-and-implementation-zh/">连接跟踪（conntrack）：原理、应用及 Linux 内核实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/nat-zh/">[译] NAT - 网络地址转换（2016）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="netfilterhttpsnotes-learningoss-cn-beijingaliyuncscomgral7u1616165512374-db897dd5-0704-42f2-a1d8-441af05f247cjpeg">Netfilter&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512374-db897dd5-0704-42f2-a1d8-441af05f247c.jpeg" alt="">&lt;/h2>
&lt;p>Netfilter 是 Linux 操作系统核心层内部的一个数据包处理模块集合的统称。一种网络筛选系统，对数据包进入以及出去本机进行的一些控制与管理。该功能的所有模块可以通过下图所示的目录进行查找，其中还包括 ipvs 等。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512330-720231f3-a4f2-4a51-96cf-137a36724b74.jpeg" alt="">
Netfilter 项目支持如下功能&lt;/p>
&lt;ul>
&lt;li>网络地址转换(Network Address Translate)&lt;/li>
&lt;li>数据包过滤&lt;/li>
&lt;li>数据包日志记录&lt;/li>
&lt;li>用户空间数据包队列&lt;/li>
&lt;li>其他数据包处理&lt;/li>
&lt;li>等等&lt;/li>
&lt;/ul>
&lt;p>Netfilter Hooks 是 Linux 内核中的一个框架，它会让 Netfilter 的模块在 Linux 网络堆栈的不同位置注册回调函数。然后，为遍历 Linux 网络堆栈中相应 Hook 的每个数据包调用已注册的回调函数。&lt;/p>
&lt;ul>
&lt;li>用白话说：内核加入了 Netfilter 模块后，每个数据包进来之后，都会经过五个 Hooks 点来处理，以便决定每个数据包的走向。&lt;/li>
&lt;/ul>
&lt;h2 id="hooks">Hooks&lt;/h2>
&lt;p>hooks function(钩子函数) 是 Linux 网络栈中的流量检查点。所有流量通过网卡进入内核或从内核出去都会调用 Hook 函数来进行检查，并根据其规则进行过滤。Netfilter 框架中一共有 5 个 Hook，就是下文定义的“五链”。&lt;/p>
&lt;ul>
&lt;li>当一个数据包在其中一个 Hooks 中匹配到自己的规则后，则会进入下一个 Hook 寻找匹配自身的规则，直到将 5 个 Hook 挨个匹配一遍。&lt;/li>
&lt;li>可以把 Hook 想象成地铁站的闸机，通过闸机的人，就是数据流量，这个能不能从闸机过去，则看闸机对这个人身份验证的结果，是放行还是阻止&lt;/li>
&lt;/ul>
&lt;h2 id="iptabelesnftables">iptabeles/nftables&lt;/h2>
&lt;p>工作于用户空间的管理工具，对 5 个 hook 进行规则管理，iptabels 或 nftables 进程，开机后，只是把设定好的规则写进 hook 中&lt;/p>
&lt;p>Netfilter 所设置的规则是存放在内核内存中的，Iptables 是一个应用层(Ring3)的应用程序，它通过 Netfilter 放出的接口来对存放在内核内存中的 Xtables(Netfilter 的配置表)进行修改(这是一个典型的 Ring3 和 Ring0 配合的架构)&lt;/p>
&lt;h1 id="五链chain">五链(Chain)&lt;/h1>
&lt;p>把每个 Hook 上的规则都串起来类似于一条链子，所以称为链，一共 5 个 Hook，所以有 5 个 Chain。每个规则都是由“源 IP、目标 IP、端口、目标”等信息组合起来的。(i.e 对从哪来的或者到哪去的 IP 的哪个端口，要执行什么动作或‘引用什么 Chain 来对这个数据包执行什么动作’)&lt;/p>
&lt;ol>
&lt;li>&lt;strong>PREROUTING 链&lt;/strong> # 路由前，处理刚到达本机并在路由转发前的数据包。它会转换数据包中的目标 IP 地址（destination ip address），通常用于 DNAT(destination NAT)。处理完成之后分成两种情况，目的 IP 为本机网口则 INPUT，目的 IP 非本机网口则 FORWARD&lt;/li>
&lt;li>&lt;strong>INPUT 链&lt;/strong> # 进入，处理来自外部的数据。&lt;/li>
&lt;li>&lt;strong>FORWARD 链&lt;/strong> # 转发，将数据转发到本机的其他网络设备上。(需要开启 linux 的 IP 转发功能 net.ipv4.ip_forward=1 才会进入该流程；就算 ping 的是本机的其余网络设备上的 IP，也是由接收该数据包的网络设备进行回应)，FORWARD 的行为类似于路由器，系统中每个网络设备就是路由器上的每个端口，只有打开转发功能，才可以把数据包路由到其余端口上。
&lt;ol>
&lt;li>虚拟化或容器技术中，如果一台设备中有多个网段，一般都会打开转发功能，以实现不同网段路由互通的效果。&lt;/li>
&lt;li>或者服务器作为 VPN 使用时，由于不同网络设备所属网段不同，也需要打开转发功能。&lt;/li>
&lt;li>等等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>**OUTPUT 链 **# 出去，处理向外发送的数据。&lt;/li>
&lt;li>**POSTROUTING 链 **# 路由后，处理即将离开本机的数据包。它会转换数据包中的源 IP 地址（source ip address），通常用于 SNAT（source NAT）。(该路由是通过 Linux 中定义的 route 规则发送的，与内核的 ip_forward 无关)&lt;/li>
&lt;li>**自定义链 **# 用户自己定义的链，不会调用系统 Hook，而是由系统默认的 5 个链在 target 中定义引用&lt;/li>
&lt;/ol>
&lt;h2 id="规则rule匹配match规则的匹配条件匹配的用法详见iptables-框架工具介绍">规则(Rule)匹配(Match)：(规则的匹配条件)匹配的用法详见：iptables 框架工具介绍&lt;/h2>
&lt;p>规则，需要有具体的内容才能称为规则，所以 Match 就是规则中的具体内容。&lt;/p>
&lt;p>每条链上的规则，需要对流量进行匹配后才能对该流量进行相应的处理，匹配内容包括“数据包的源地址、目标地址、协议、目标等”，(e.g.这个数据使用哪个协议从哪来的到哪去的目标是什么)
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512349-f2e6f4c5-d617-4b04-a432-f9a7389120df.jpeg" alt="">
Match 功能的实现依赖于模块(类似于内核的模块)，比如右图，可以使用命令 rpm -ql iptables | grep &amp;ldquo;.so&amp;quot;查看都有哪些模块，其中的 XXX.so 就是各个功能的模块，大写字母是 target 所用的模块，小写字母是基本匹配与扩展匹配所用的模块&lt;/p>
&lt;ol>
&lt;li>基本匹配：源地址、目标地址、协议、入流网卡、出流网卡&lt;/li>
&lt;li>扩展匹配：用于对基本匹配的内容扩充，包括两类，普通的扩展匹配和基于
&lt;ol>
&lt;li>通用扩展匹配，可以直接使用。&lt;/li>
&lt;li>基于基本匹配的扩展匹配。需要有基本匹配规则才可以使用。
&lt;ol>
&lt;li>e.g.需要匹配某些端口，这类匹配必须基于 tcp 匹配规则上使用，否则无效(e.g.-p tcp -m tcp -m multiport &amp;ndash;dport22,23,24)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>目标(target)：每个规则中的目标。即在每条链上对每个进出流量匹配上之后应该执行什么动作，Target 包括以下几种
&lt;ol>
&lt;li>ACCEPT #允许流量通过&lt;/li>
&lt;li>REJECT #拒绝流量通过&lt;/li>
&lt;li>DROP #丢弃，不响应，发送方无法判断是被拒绝&lt;/li>
&lt;li>RETURN #返回调用链&lt;/li>
&lt;li>MARK #做防火墙标记&lt;/li>
&lt;li>用于 nat 表的 target
&lt;ol>
&lt;li>DNAT|SNAT #{目的|源}地址转换&lt;/li>
&lt;li>REDIRECT #端口重定向&lt;/li>
&lt;li>MASQUERADE #地址伪装类似于 SNAT，但是不用指明要转换的地址，而是自动选择要转换的地址，用于外部地址不固定的情况&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>用于 raw 表的 target
&lt;ol>
&lt;li>NOTRACK #raw 表专用的 target，用于对匹配规则进行 notrack(不跟踪)处理&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>LOG #将数据包的相关信息记录日志，执行完该目标后，会继续匹配后面的规则&lt;/li>
&lt;li>引用自定义链 #直接使用“-j 自定义链的名称”即可，让基本 5 个 Chain 上匹配成功的数据包继续执行自定义链上的规则。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>注意，这里面的路由指数据包在 Linux 本机内部路由&lt;/p>
&lt;h2 id="linux-数据包路由原理iptablesnetfilter-入门学习">Linux 数据包路由原理、Iptables/netfilter 入门学习&lt;/h2>
&lt;p>数据流处理流程简介&lt;/p>
&lt;p>注意：每个数据包在 CHAIN 中匹配到适用于自己的规则之后，则直接进入下一个 CHAIN，而不会遍历 CHAIN 中每条规则去挨个匹配适用于自己的规则。比如下面两种情况&lt;/p>
&lt;p>INPUT 链默认 DROP，匹配第一条：目的端口是 9090 的数据 DROP，然后不再检查下一项，那么 9090 无法访问&lt;/p>
&lt;pre>&lt;code>-P INPUT DROP
-A INPUT -p tcp -m tcp --dport 9090 -j DROP
-A INPUT -p tcp -m tcp --dport 9090 -j ACCEPT
&lt;/code>&lt;/pre>
&lt;p>INPUT 链默认 DROP，匹配第一条目的端口是 9090 的数据 ACCEPT，然后不再检查下一条规则，则 9090 可以访问&lt;/p>
&lt;pre>&lt;code>-P INPUT DROP
-A INPUT -p tcp -m tcp --dport 9090 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 9090 -j DROP
&lt;/code>&lt;/pre>
&lt;p>匹配条件：根据协议报文特征指定&lt;/p>
&lt;ol>
&lt;li>基本匹配条件&lt;/li>
&lt;li>扩展匹配条件&lt;/li>
&lt;/ol>
&lt;p>处理动作：&lt;/p>
&lt;ol>
&lt;li>内建处理机制&lt;/li>
&lt;li>自定义处理机制&lt;/li>
&lt;li>注意：自定义的链不会有流量经过，而是在主要的 5 链中引用自定义链上的规则，来实现对流量的处理&lt;/li>
&lt;/ol>
&lt;p>下图是从服务器外部进入网卡，再进入网络栈的数据流走向，如果直接是服务器内部服务生成的数据包进入网络栈，则不适用于该图
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gral7u/1616165512341-aeeeff06-b602-4340-bc4f-cd582144f85f.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>当一个数据包进入网卡时，数据包首先进入 PREROUTING 链，在 PREROUTING 链中我们有机会修改数据包的 DestIP(目的 IP)，然后内核的&amp;quot;路由模块&amp;quot;根据&amp;quot;数据包目的 IP&amp;quot;以及&amp;quot;内核中的路由表&amp;quot;判断是否需要转送出去(注意，这个时候数据包的 DestIP 有可能已经被我们修改过了)&lt;/li>
&lt;li>如果数据包就是进入本机的(即数据包的目的 IP 是本机的网口 IP)，数据包就会沿着图向下移动，到达 INPUT 链。数据包到达 INPUT 链后，任何进程都会收到它&lt;/li>
&lt;li>本机上运行的程序也可以发送数据包，这些数据包经过 OUTPUT 链，然后到达 POSTROTING 链输出(注意，这个时候数据包的 SrcIP 有可能已经被我们修改过了)&lt;/li>
&lt;li>如果数据包是要转发出去的(即目的 IP 地址不再当前子网中)，且内核允许转发，数据包就会向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出(选择对应子网的网口发送出去)&lt;/li>
&lt;/ol>
&lt;p>出于安全考虑，Linux 系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的 ip 地址将包发往本机另一网卡，该网卡根据路由表继续发送数据包。这通常就是路由器所要实现的功能。&lt;/p>
&lt;p>配置 Linux 系统的 ip 转发功能，首先保证硬件连通，然后打开系统的转发功能，less /proc/sys/net/ipv4/ip_forward，该文件内容为 0，表示禁止数据包转发，1 表示允许，将其修改为 1。可使用命令 echo &amp;ldquo;1&amp;rdquo; &amp;gt; /proc/sys/net/ipv4/ip_forward 修改文件内容，重启网络服务或主机后效果不再。若要其自动执行，可将命令 echo &amp;ldquo;1&amp;rdquo; &amp;gt; /proc/sys/net/ipv4/ip_forward 写入脚本/etc/rc.d/rc.local 或者 在/etc/sysconfig/network 脚本中添加 FORWARD_IPV4=&amp;ldquo;YES&amp;rdquo;&lt;/p>
&lt;h1 id="natnetwork-address-translation网络地址转换">NAT(Network Address Translation)网络地址转换&lt;/h1>
&lt;p>NAT 为了安全性而产生的，主要用来隐藏本地主机的 IP 地址&lt;/p>
&lt;h2 id="snatsource-源地址转换针对请求报文的源地址而言">SNAT：Source 源地址转换，针对请求报文的源地址而言&lt;/h2>
&lt;p>当想访问外网的时候，把源地址转换，作用于 POSTROUTING 链&lt;/p>
&lt;p>常用于内网私网地址转换成公网地址，比如家用路由器&lt;/p>
&lt;h2 id="dnatdestination-目的地址转换针对请求报文的目标地址而言">DNAT：Destination 目的地址转换，针对请求报文的目标地址而言&lt;/h2>
&lt;p>当从外部访问某 IP 时，把目的 IP 转换，作用于 PREROUTING、FORWARD 链&lt;/p>
&lt;p>把内网中的服务器发布到外网中去，&lt;/p>
&lt;p>常用于公网访问一个公司的公网 IP，但是由私网 IP 来提供服务，比如 LVS 的 nat 模型&lt;/p>
&lt;p>比如在公司内网中提供一个 web 服务，但是由于是私网地址，来自互联网的任何请求无法送达这台 web 服务器，这时候我们可以对外宣称公司的 web 服务在一个公网的 IP 地址上，但是公网的 IP 地址所在服务器上又没有提供 web 服务，这时候，来自外网访问的请求，全部 DNAT 成私网 IP，即可对外提供请求。&lt;/p>
&lt;h2 id="注意">注意：&lt;/h2>
&lt;p>由于 SNAT 与 DNAT 在描述的时候主要是都是针对请求报文而言的，那么当地址转换以后，响应报文响应的是转换后的地址，这时候就无法把响应请求送还给发起请求的设备了，这怎么办呢？这时候，同样需要一个地址转换，只不过通过 NAT 机制自行完成的，如何自动完成呢？这里面会有一个连接追踪机制，跟踪每一个数据连接（详见：&lt;a href="https://www.yuque.com/go/doc/33221811">ConnTrack 连接跟踪机制&lt;/a>），当响应报文到来的时候，根据连接追踪表中的信息记录的请求报文是怎么转换的相关信息，来对响应报文进行 NAT 转换。&lt;/p></description></item><item><title>Docs: Netlink</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netlink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netlink/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man7/netlink.7.html">Manual(手册),netlink&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man7/rtnetlink.7.html">Manual(手册),rtnetlink&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Netlink">Wiki,Netlink&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/networking/generic_netlink.html">内核官方文档,Linux 网络文档-通用 Netlink&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Netlink&lt;/strong> 是一个 Linux 内核接口，用于在 内核 与 用户空间进程 之间传输信息。还可以用作两个用户空间进程之间、甚至内核子系统之间的数据通信。说白了，就是一个通过 Socket 实现 IPC 的方式。&lt;/p>
&lt;p>[Iproute2](✏IT 学习笔记/📄1.操作系统/X.Linux%20 管理/Linux%20 网络管理工具/Iproute%20 工具包.md 管理/Linux 网络管理工具/Iproute 工具包.md)、keepalived、ethtool 等等 应用程序，很多功能都是基于 Netlink 开发的。&lt;/p>
&lt;p>Netlink 由两部分组成：&lt;/p>
&lt;h2 id="rtnetlink-概述">Rtnetlink 概述&lt;/h2>
&lt;p>rtnetlink 是 Linux 路由套接字&lt;/p>
&lt;p>RTNETLINK 允许读取和更改内核的路由表。它在内核中使用以在各种子系统之间进行通信，尽管此处未记录此使用，并且与用户空间程序通信。可以通过 NetLink_Route 套接字来控制网络路由，IP 地址，链接参数，邻居设置，排队学科，流量类和数据包分类器。它基于&lt;a href="https://man7.org/linux/man-pages/man7/netlink.7.html">NetLink&lt;/a> 消息;有关更多信息。&lt;/p></description></item><item><title>Docs: Netplan</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netplan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netplan/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/canonical/netplan">GitHub 项目，canonical/netplan&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://netplan.io/">官网&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Netplan&lt;/strong> 是一个网络配置抽象渲染器。属于 netplan.io 包，通过 yaml 文件来管理 Linux 的网络配置。&lt;/p>
&lt;p>&lt;strong>Netplan&lt;/strong> 是用于在 Linux 系统上轻松配置网络的实用程序。只需为每个网络设备应该具有的配置，创建一个 YAML 格式的描述文件。 Netplan 将根据此描述为指定的 &lt;strong>Renderer(渲染器)&lt;/strong> 生成所有必要的配置。剩下的工作，就是由这些 Renderer 来处理配置，并配置网络了。&lt;/p>
&lt;h1 id="netplan-的工作方式">Netplan 的工作方式&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vv96im/1616165548496-6a738611-4db5-4f06-9cfe-ce0c82d9cf01.png" alt="">&lt;/p>
&lt;p>Netplan 从 /etc/netplan/*.yaml 文件中读取配置信息。Netplan 启动初期，在 /run 目录中生成特定于后端的配置文件，以便让这些后端的网络守护程序根据这些配置文件管理网络设备。在 Netplan 中，这些特定的 **后端 **被称为 &lt;strong>Renderers(渲染器)&lt;/strong>。&lt;/p>
&lt;p>Netplan 当前支持如下 Renderers(渲染器)：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>networkd&lt;/strong> # 默认 Renderer。该 Renderers 是 systemd 管理的网络管理程序 systemd-networkd，它属于 systemd 包&lt;/li>
&lt;li>&lt;strong>Network Manager&lt;/strong> # 详见：&lt;a href="https://www.yuque.com/go/doc/33221845">Network Manager 章节&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>注意：殊途同归，就算是 systemd-networkd，同样是会在 d-bus 中保存信息的&lt;/p>
&lt;pre>&lt;code>root@lichenhao:/etc/networkd-dispatcher# busctl get-property org.freedesktop.network1 /org/freedesktop/network1/network/_310_2dnetplan_2dens3 org.freedesktop.network1.Network MatchName
as 1 &amp;quot;ens3&amp;quot;
root@lichenhao:/etc/networkd-dispatcher# busctl get-property org.freedesktop.network1 /org/freedesktop/network1/network/_310_2dnetplan_2dens3 org.freedesktop.network1.Network MatchDriver
as 0
root@lichenhao:/etc/networkd-dispatcher# busctl get-property org.freedesktop.network1 /org/freedesktop/network1/network/_310_2dnetplan_2dens3 org.freedesktop.network1.Network SourcePath
s &amp;quot;/run/systemd/network/10-netplan-ens3.network&amp;quot;
&lt;/code>&lt;/pre>
&lt;h1 id="netplan-关联文件与配置">Netplan 关联文件与配置&lt;/h1>
&lt;p>&lt;strong>/etc/netplan/*&lt;/strong> # netplan 读取 yaml 格式配置文件的路径&lt;/p></description></item><item><title>Docs: Netplan 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netplan/netplan-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netplan/netplan-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://netplan.io/reference">官网，参考&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Netplan 的配置文件使用 YAML 格式。&lt;code>/{lib,etc,run}/netplan/*.yaml&lt;/code> 都是 Netplan 程序读取配置文件的路径。&lt;/p>
&lt;h1 id="配置文件详解">配置文件详解&lt;/h1>
&lt;p>该 YAML 文件只有一个顶级节点：&lt;code>network: &amp;lt;Object&amp;gt;&lt;/code>，其中包括 version、设备类型(例如 ethernets、modems、wifis、birdge 等)、renderer。&lt;/p>
&lt;h2 id="version-int">version: &lt;!-- raw HTML omitted -->&lt;/h2>
&lt;h2 id="renderere-string">renderere: &lt;!-- raw HTML omitted -->&lt;/h2>
&lt;h2 id="ethernetes-object">ethernetes: &amp;lt;&lt;strong>OBJECT&lt;/strong>&amp;gt;&lt;/h2>
&lt;h2 id="所有设备的通用属性">所有设备的通用属性&lt;/h2>
&lt;p>&lt;strong>addresses: &amp;lt;[]OBJECT&amp;gt;&lt;/strong> #
&lt;strong>dtcp4: &lt;!-- raw HTML omitted -->&lt;/strong> # 为 IPv4 启用 DHCP。&lt;code>默认值：false&lt;/code>
&lt;strong>dhcp6: &lt;!-- raw HTML omitted -->&lt;/strong> # 为 IPv6 启用 DHCP。&lt;code>默认值：false&lt;/code>
&lt;strong>gateway4 | gateway6: &lt;!-- raw HTML omitted -->&lt;/strong> # &lt;strong>已弃用，使用 &lt;strong>&lt;code>**routes**&lt;/code>&lt;/strong> 字段。&lt;/strong>
**nameservers: &lt;!-- raw HTML omitted --> **# 设置 DNS 服务器和搜索域，用于手动地址配置
&lt;strong>routes: &amp;lt;[]OBJECT&amp;gt;&lt;/strong> # 为设备配置静态路由；请参阅下面的路由部分。&lt;/p>
&lt;h1 id="配置示例">配置示例&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://netplan.io/examples">官网，示例&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># This is the network config written by &amp;#39;subiquity&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">network&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ethernets&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ens3&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">addresses&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">172.19.42.248&lt;/span>&lt;span style="color:#ae81ff">/24&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dhcp4&lt;/span>: &lt;span style="color:#66d9ef">no&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dhcp6&lt;/span>: &lt;span style="color:#66d9ef">no&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">optional&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">routes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">to&lt;/span>: &lt;span style="color:#ae81ff">default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">via&lt;/span>: &lt;span style="color:#ae81ff">172.19.42.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nameservers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">addresses&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">8.8.8.8&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">renderer&lt;/span>: &lt;span style="color:#ae81ff">networkd&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>生成如下配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@lichenhao:/etc/netplan# cat /run/systemd/network/10-netplan-ens3.network
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Match&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Name&lt;span style="color:#f92672">=&lt;/span>ens3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Link&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RequiredForOnline&lt;span style="color:#f92672">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Network&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LinkLocalAddressing&lt;span style="color:#f92672">=&lt;/span>ipv6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Address&lt;span style="color:#f92672">=&lt;/span>172.19.42.248/24
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Gateway&lt;span style="color:#f92672">=&lt;/span>172.19.42.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS&lt;span style="color:#f92672">=&lt;/span>8.8.8.8
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="bridge-配置示例">Bridge 配置示例&lt;/h2>
&lt;p>应用配置
&lt;strong>netplan apply&lt;/strong>&lt;/p></description></item><item><title>Docs: network-scripts</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/network-scripts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/network-scripts/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s1-networkscripts-interfaces">RedHat 官方文档，生产文档-RedHatEnterpriseLinux-6-部署指南-11.2.接口配置文件&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/OpenMandrivaSoftware/initscripts/blob/master/sysconfig.txt">/usr/share/doc/initscripts-XX/sysconfig.txt &lt;/a>中的 ifcfg-&lt;!-- raw HTML omitted --> 部分&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-ifcfg-rh.html">Manual(手册),nm-settings-ifcfg-rh(5)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>RedHad 相关发行版的网络配置通过一系列脚本实现，随着时代的发展，已经逐渐启用，并由 NetworkManager 取代，NetworkManager 还单独出了一个适用于 RedHad 发行版的插件，名为 nm-setting-ifcfg-rh。这样，NetworkManager 可以将原本的配置目录中文件的格式，转变为适应 RedHad 的格式，并将配置文件保存到 /etc/sysconfig/network-scripts/ 目录下。&lt;/p>
&lt;h1 id="关联文件">关联文件&lt;/h1>
&lt;p>**/etc/sysconfig/* **# 全局&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./network&lt;/strong> # 全局网络配置&lt;/li>
&lt;li>**./network-scripts/* **# 曾经是网络配置脚本文件所在目录。CentOS 8 以后，移除了所有脚本，只用来为网络配置程序提供网络设备的配置文件
&lt;ul>
&lt;li>&lt;strong>./ifcfg-INTERFACE&lt;/strong> # 名为 INTERFACE 网络设备配置文件。通常情况下，INTERFACE 的值通常与配置文件中 DEVICE 指令的值相同。&lt;/li>
&lt;li>&lt;strong>./route-INTERFACE&lt;/strong> # IPv4 静态路由配置文件。INTERFACE 为网络设备名称，该路由条目仅对名为 INTERFACE 的网络设备起作用&lt;/li>
&lt;li>&lt;strong>./route6-INTERFACE&lt;/strong> # IPv6 静态路由配置文件。INTERFACE 为网络设备名称，该路由条目仅对名为 INTERFACE 的网络设备起作用&lt;/li>
&lt;li>&lt;strong>./rule-INTERFACE&lt;/strong> # 定义内核将流量路由到特定路由表的 IPv4 源网络规则。&lt;/li>
&lt;li>&lt;strong>./rule6-INTERFACE&lt;/strong> # 定义内核将流量路由到特定路由表的 IPv6 源网络规则。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>./networking/*&lt;/strong> #
&lt;ul>
&lt;li>注意：在 &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/ch-network_interfaces#s1-networkscripts-files">RedHat 6 文档&lt;/a>中表示，/etc/sysconfig/networking/ 目录由现在已经弃用的网络管理工具(system-config-network) 管理，这个内容不应该手动编辑。推荐使用 NetworkManager。并且在后续的版本中， NetworkManager 也接管了这些文件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>**/etc/iproute2/rt_tables **# 如果您想要使用名称而不是数字来引用特定的路由表，这个文件会定义映射映射。&lt;/p>
&lt;h2 id="rule-interface-文件">rule-INTERFACE 文件&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>from 192.0.2.0/24 lookup &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>from 203.0.113.0/24 lookup &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>来自 192.0.2.0/24 的流量根据 1 号路由表规则进行路由
来自 203.0.113.0/24 的流量根据 2 号路由表规则进行路由&lt;/p>
&lt;h2 id="route-interface-文件">route-INTERFACE 文件&lt;/h2>
&lt;p>第一种格式：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 默认路由下一条是 192.168.1.1，从 eth0 网络设备发出&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default via 192.168.1.1 dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 目的网段是 10.0.0.1 且掩码是 255.255.255.0，从 eth1 网络设备发出数据包，下一跳为 192.168.0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.0.0.1 192.168.0.1 255.255.255.0 eth1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>第二种格式：每一个路由用 0,1,2&amp;hellip;.等表示&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ADDRESS0&lt;span style="color:#f92672">=&lt;/span>10.10.10.0 &lt;span style="color:#75715e">#目的网段IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NETMASK0&lt;span style="color:#f92672">=&lt;/span>255.255.255.0 &lt;span style="color:#75715e">#目的网段掩码&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>GATEWAY0&lt;span style="color:#f92672">=&lt;/span>192.168.0.10 &lt;span style="color:#75715e">#下一跳IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ADDRESS1&lt;span style="color:#f92672">=&lt;/span>172.16.1.10 &lt;span style="color:#75715e">#目的网段IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NETMASK1&lt;span style="color:#f92672">=&lt;/span>255.255.255.0 &lt;span style="color:#75715e">#目的网段掩码&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>GATEWAY1&lt;span style="color:#f92672">=&lt;/span>192.168.0.10 &lt;span style="color:#75715e">#下一跳IP&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ifcfg-interface-文件">ifcfg-INTERFACE 文件&lt;/h2>
&lt;h3 id="httpsnotes-learningoss-cn-beijingaliyuncscomggdfnf1616165813293-bc7fde9f-4810-4fc8-b6b8-67282aaef73djpeg">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggdfnf/1616165813293-bc7fde9f-4810-4fc8-b6b8-67282aaef73d.jpeg" alt="">&lt;/h3>
&lt;p>ifcfg-INTERFACE 文件由多个 &lt;strong>Items(条目)&lt;/strong> 组成，每个 Item 类似于 键值对，以 &lt;code>=&lt;/code> 分割，Item 分为很多类型：&lt;/p>
&lt;ul>
&lt;li>Base items:&lt;/li>
&lt;li>Mandriva specific items for DHCP clients:&lt;/li>
&lt;li>Mandriva items:&lt;/li>
&lt;li>Base items being deprecated:&lt;/li>
&lt;li>Alias specific items:&lt;/li>
&lt;li>IPv6-only items for real interfaces:&lt;/li>
&lt;li>IPv6-only items for static tunnel interface:&lt;/li>
&lt;li>Ethernet-only items:&lt;/li>
&lt;li>Ethernet 802.1q VLAN items:&lt;/li>
&lt;li>PPP/SLIP items:&lt;/li>
&lt;li>PPP-specific items&lt;/li>
&lt;li>IPPP-specific items (ISDN)&lt;/li>
&lt;li>ippp0 items being deprecated:&lt;/li>
&lt;li>Wireless-specific items:&lt;/li>
&lt;li>IPSEC specific items&lt;/li>
&lt;li>Bonding-specific items&lt;/li>
&lt;li>Tunnel-specific items:&lt;/li>
&lt;li>Bridge-specific items:&lt;/li>
&lt;li>TUN/TAP-specific items:&lt;/li>
&lt;/ul>
&lt;h3 id="base-items基本条目">Base items(基本条目)&lt;/h3>
&lt;h3 id="bonding-specific-items特定于-bonding-的条目">Bonding-specific items(特定于 Bonding 的条目)&lt;/h3>
&lt;p>**SLAVE={yes|no} **# 指定设备是否为 slave 设备。&lt;code>默认值：no&lt;/code>
&lt;strong>MASTER=bondXX&lt;/strong> # 指定要绑定的主设备
&lt;strong>BONDING_OPTS=&lt;!-- raw HTML omitted -->&lt;/strong> # Bonding 驱动运行时选项，多个选项以空格分割&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;mode=active-backup arp_interval=60 arp_ip_target=192.168.1.1,192.168.1.2&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h4 id="配置示例">配置示例&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>DEVICE&lt;span style="color:#f92672">=&lt;/span>bond0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPADDR&lt;span style="color:#f92672">=&lt;/span>192.168.1.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NETMASK&lt;span style="color:#f92672">=&lt;/span>255.255.255.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ONBOOT&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BOOTPROTO&lt;span style="color:#f92672">=&lt;/span>none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>USERCTL&lt;span style="color:#f92672">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NM_CONTROLLED&lt;span style="color:#f92672">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BONDING_OPTS&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;bonding parameters separated by spaces&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: NetworkManager</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/NetworkManager/NetworkManager">GitHub 项目，NetworkManager/NetworkManager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gitlab.freedesktop.org/NetworkManager/NetworkManager">GitLab 项目，freedesktop-NetworkManager/NetworkManager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/NetworkManager.conf.html">Manual(手册),NetworkManager.conf(5)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/">官网&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**NetworkManager daemon **是管理网络的守护进程&lt;/p>
&lt;p>NetworkManager daemon 尝试通过管理主网络连接和其他网络接口（如以太网，WiFi 和移动宽带设备），使网络配置和操作尽可能轻松自动。 除非禁用该行为，否则 NetworkManager 将在该设备的连接可用时连接任何网络设备。 有关网络的信息通过 D-Bus 接口导出到任何感兴趣的应用程序，提供丰富的 API，用于检查和控制网络设置和操作。&lt;/p>
&lt;h1 id="connection">Connection&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-nmcli.html">Manual(手册),nm-settings-nmcli(5)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-dbus.html">Manual(手册),nm-settings-dbus(5)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-keyfile.html">Manual(手册),nm-settings-keyfile(5)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/nm-settings-ifcfg-rh.html">Manual(手册),nm-settings-ifcfg-rh(5)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>NetworkManager 将所有网络配置抽象成 &lt;strong>Connection(连接)&lt;/strong>，这些 Connection 的配置中包含网络配置(比如 IP 地址、网关等)。当 NetworkManager 激活网络设备上的 Connection 时，将为这个网络设备应用配置文件中的内容，并建立活动的网络连接。所以，可以创建多个 Connection 来关联到一个网络设备上；这样，它们就可以灵活地具有用于不同网络需求的各种网络配置。&lt;/p>
&lt;p>&lt;strong>用白话说就是：Connection 就是“网络配置”，网络设备(device)关联并使用“网络配置”来实现联网。而 NetworkManager 就是管理这些 Connection 的&lt;/strong>。&lt;strong>Connection 可以表示一个概念，也可以表示一个配置文件。&lt;/strong>&lt;/p>
&lt;h2 id="connection-插件">Connection 插件&lt;/h2>
&lt;p>NetworkManager 通过 &lt;strong>Plugins(插件)&lt;/strong> 的方式来管理 Connection 配置文件。在不同的 Linux 发行版中，所使用的插件各不相同，但是默认情况下，NetworkManager 使始终启用名为 &lt;strong>keyfile&lt;/strong> 的插件，这是一个通用插件，当其他插件无法支持某些类型的 Connection 配置时，keyfile 插件将会自动提供支持。keyfile 插件会将 Connection 文件保存到 /etc/NetworkManager/system-connections/、/usr/lib/NetworkManager/system-connections/、/run/NetworkManager/system-connections/ 这三个目录中。&lt;/p>
&lt;p>可以在 /etc/NetworkManager/NetworkManager.conf 文件中配置想要使用的插件，插件用于读写系统范围的连接配置文件。当指定多个插件时，将从所有列出的插件中读取 Connections。写入 Connections 时，会要求插件按照此处列出的顺序保存连接；如果第一个插件无法写出该连接类型（或无法写出任何连接），则尝试下一个插件。如果没有插件可以保存连接，则会向用户返回错误。&lt;/p>
&lt;p>可用插件的数量是特定于发行版的。所有可用的插件详见 &lt;a href="https://networkmanager.dev/docs/api/latest/NetworkManager.conf.html#settings-plugins">Manual(手册) 中 Plugins 章节&lt;/a>
&lt;strong>keyfile&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>keyfile 插件是支持 NetworkManager 拥有的所有连接类型和功能的&lt;strong>通用插件&lt;/strong>。它以 .ini 格式在 /etc/NetworkManager/system-connections 文件中写入连接配置。
&lt;ul>
&lt;li>有关文件格式的详细信息，请参阅 nm-settings-keyfile(5)。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>keyfile 插件存储的连接文件可能包含纯文本形式的 passwords、secrets、private keys，因此它将仅对 root 用户可读，并且插件将忽略除 root 用户或组之外的任何用户或组可读或可写的文件。
&lt;ul>
&lt;li>有关如何避免以纯文本形式存储密码，请参阅 nm-settings(5) 中的“秘密标志类型”。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>此插件始终处于活动状态，并将自动用于存储其他插件不支持的连接。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ifcfg-rh&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>此插件用于 Fedora 和 Red Hat Enterprise Linux 发行版，用于从标准 /etc/sysconfig/network-scripts/ifcfg-* 文件读取和写入配置。它目前支持读取 Ethernet, Wi-Fi, InfiniBand, VLAN, Bond, Bridge, Team 这几种类型的连接。启用 ifcfg-rh 隐式启用 ibft 插件(如果可用)。这可以通过添加 no-ibft 来禁用。&lt;/li>
&lt;li>有关 ifcfg 文件格式的更多信息，请参见 /usr/share/doc/initscripts/sysconfig.txt 和 nm-settings-ifcfg-rh(5)。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ifupdown&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>This plugin is used on the Debian and Ubuntu distributions, and reads Ethernet and Wi-Fi connections from /etc/network/interfaces.&lt;/li>
&lt;li>This plugin is read-only; any connections (of any type) added from within NetworkManager when you are using this plugin will be saved using the keyfile plugin instead.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ibft, no-ibft&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>These plugins are deprecated and their selection has no effect. This is now handled by nm-initrd-generator.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ifcfg-suse, ifnet&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>These plugins are deprecated and their selection has no effect. The keyfile plugin should be used instead.&lt;/li>
&lt;/ul>
&lt;h2 id="connection-d-bus">Connection D-Bus&lt;/h2>
&lt;p>NetworkManager 还会将这些 Connection 配置导出到 D-Bus 上，比如，通过** busctl **命令，可也获取 Connection 中的内容：&lt;/p>
&lt;pre>&lt;code>[root@ansible dispatcher.d]# busctl get-property org.freedesktop.NetworkManager /org/freedesktop/NetworkManager/Devices/2 org.freedesktop.NetworkManager.Device Interface
s &amp;quot;ens33&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>所以，真正的底层实现，是通过 D-bus 中的网络设备配置文件来实现的&lt;/p>
&lt;h2 id="connection-关联文件">Connection 关联文件&lt;/h2>
&lt;p>默认情况下，由 &lt;strong>keyfile 插件&lt;/strong>管理 &lt;strong>INI 格式&lt;/strong>的 Connection 配置文件。并默认保存在 /etc/NetworkManager/system-connections/ 目录中。&lt;/p>
&lt;blockquote>
&lt;p>注意：&lt;/p>
&lt;ul>
&lt;li>在 RedHad 相关的发行版中，NetworkManager 会运行名为 ifcfg-rh 的插件，插件会将 /etc/NetworkManager/system-connections/ 目录中的 Connection 配置文件翻译成老式配置文件格式，并保存在 /etc/sysconfig/network-scripts/ 目录中&lt;/li>
&lt;li>&lt;strong>所以，在 RedHad 中，是无法从 /etc/NetworkManager/system-connections/ 目录中找到连接配置文件&lt;/strong>&lt;/li>
&lt;li>若想禁用 ifcfg-rh 插件，只需要在 /etc/NetworkManager/NetworkManager.conf 文件中的 main 部分添加 plugins=keyfile 即可&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 D-Bus API 上的 Connection 配置中，将 INI 中的 &lt;strong>Sections(部分) 称为 Settings(设置)&lt;/strong>，Setting 即是 &lt;strong>Properties(属性)&lt;/strong> 的集合。所以，很多文档，都将 Connection 表示为一组特定的、封装好的、独立的 &lt;strong>Settings(集合)&lt;/strong> 集合。Connection 由一个或多个 Settings 组成。&lt;/p>
&lt;p>**Settings **用于描述一个 Connection。每个 Setting 都具有一个或多个 &lt;code>**Property(属性)**&lt;/code> 。Setting 与 Property 中间以点 &lt;code>.&lt;/code> 连接。每个 Setting.Property 都会有一个值。&lt;/p>
&lt;p>一个 Connection 有哪些 Settings，Setting 又有哪些 Property，以及这些 Property 都有什么作用，详见&lt;a href="https://www.yuque.com/go/doc/33221861"> Connection 配置文件详解&lt;/a>&lt;/p>
&lt;p>下面的命令，可以从 D-Bus API 中获取配置文件所在路径&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># CentOS 中使用 ifcfg-rh 插件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@host-3 system-connections&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># busctl get-property org.freedesktop.NetworkManager /org/freedesktop/NetworkManager/Settings/4 org.freedesktop.NetworkManager.Settings.Connection Filename&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>s &lt;span style="color:#e6db74">&amp;#34;/etc/sysconfig/network-scripts/ifcfg-enp25s0f3&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># CentOS 中不使用 ifcfg-rh 插件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@common-centos-test system-connections&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># busctl get-property org.freedesktop.NetworkManager /org/freedesktop/NetworkManager/Settings/4 org.freedesktop.NetworkManager.Settings.Connection Filename&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>s &lt;span style="color:#e6db74">&amp;#34;/etc/NetworkManager/system-connections/eth1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以看到，使用不同的插件，配置文件所在路径是不同的&lt;/p>
&lt;p>&lt;strong>用白话说：如果说 Connection 是一个配置文件的话，Setting 就是配置文件中的&lt;/strong>&lt;code>**context（配置段，或称为&amp;quot;配置环境&amp;quot;)**&lt;/code>&lt;strong>，Property(属性) 是该配置环境下的&lt;/strong>&lt;code>**keyword(关键字,或称为&amp;quot;键&amp;quot;、&amp;quot;字段&amp;quot;)**&lt;/code>**。**所以，一般情况下，Connection 也可以描述为由一个或多个 Property(属性) 组成。我们都把 &lt;code>Setting.Property&lt;/code> 简称为 &lt;code>属性&lt;/code>。**&lt;/p>
&lt;h3 id="配置文件示例">配置文件示例&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@lichenhao:~# cat /etc/NetworkManager/system-connections/ens3.nmconnection
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>connection&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>id&lt;span style="color:#f92672">=&lt;/span>ens3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uuid&lt;span style="color:#f92672">=&lt;/span>8f8541bc-4893-418b-98d4-fbc7433747cf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type&lt;span style="color:#f92672">=&lt;/span>ethernet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>interface-name&lt;span style="color:#f92672">=&lt;/span>ens3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>permissions&lt;span style="color:#f92672">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>ethernet&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mac-address-blacklist&lt;span style="color:#f92672">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>ipv4&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>address1&lt;span style="color:#f92672">=&lt;/span>172.19.42.248/24,172.19.42.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dns-search&lt;span style="color:#f92672">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>method&lt;span style="color:#f92672">=&lt;/span>manual
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>ipv6&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>addr-gen-mode&lt;span style="color:#f92672">=&lt;/span>stable-privacy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dns-search&lt;span style="color:#f92672">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>method&lt;span style="color:#f92672">=&lt;/span>auto
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>proxy&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果通过 nmcli 命令查看这个 Connection，格式如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nmcli connection show eth0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>connection.id: ens3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>connection.uuid: 8f8541bc-4893-418b-98d4-fbc7433747cf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>connection.type: 802-3-ethernet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>connection.interface-name: eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.........
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4.method: manual
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4.dns: 223.5.5.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4.addresses: 172.19.42.248/24
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4.gateway: 172.19.42.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.......
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>第一列中的 connection 与 ipv4 就是 Setting。其中 id、uuid、type、interface-name 都是 connection 这个 Setting 的 Property，而 method、dns 等等都是 ipv4 这个 Setting 的 Property。第二列就是同一行 Property 对应的值。&lt;/p>
&lt;h2 id="networkmanager-api">NetworkManager API&lt;/h2>
&lt;p>NetworkManager 提供了一个 API，用来管理 Connection、检查网络配置等。&lt;a href="https://www.yuque.com/go/doc/33221856">nmcli 这个命令行工具&lt;/a>是官方提供的用于使用 API 的客户端应用程序。&lt;/p>
&lt;blockquote>
&lt;p>也可以手动管理 Connection 文件，就跟出现 NetworkManager 之前一样，手动配置 /etc/sysconfig/network-scripts 目录下的网络设备配置文件，然后重启 deamon 进程以便加载这些文件即可。&lt;/p>
&lt;/blockquote>
&lt;p>注意：&lt;/p>
&lt;ol>
&lt;li>一个网络设备(device)可以关联多个 connection，但是同一时间只能有一个与该网络设备(device)关联 connection 处于 active 状态。这就可以让一个网卡(device)同时具备多个配置，可以随时切换。&lt;/li>
&lt;li>NetworkManager 默认不会识别到配置文件的更改 并会继续使用旧的配置数据。如果更改 /etc/NetworkManager/system-connections/ 目录下的配置文件，那么需要让 NetworkManager 再次读取已经改动过的配置文件，如果想要确保这件事，需要执行如下几条命令
&lt;ol>
&lt;li>nmcli connection reload # 让 Connection 重新加载以读取配置文件&lt;/li>
&lt;li>nmcli connection up ConnectionName # 再次启动指定的 Connection，这里的 up 也有 restart 的意思&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h1 id="networkmanager-关联文件">NetworkManager 关联文件&lt;/h1>
&lt;p>&lt;strong>/etc/NetworkManager/*&lt;/strong> #&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./conf.d/*&lt;/strong> # 类似 include 功能，是 NetworkManager.conf 文件的内容片段。&lt;/li>
&lt;li>&lt;strong>./NetworkManager.conf&lt;/strong> # NetworkManager 程序的运行时配置文件&lt;/li>
&lt;li>&lt;strong>./system-connections/*&lt;/strong> # 每个 Connection 的配置文件保存路径。
&lt;ul>
&lt;li>在 RedHad 中，该路径被修改到 /etc/sysconfig/network-scripts/ 上去了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/run/NetworkManager/*&lt;/strong> #&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./system-connections/*&lt;/strong> # 自动生成的 Connection 的配置文件保存路径。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/usr/lib/NetworkManager/*&lt;/strong> #&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./system-connections/*&lt;/strong> #&lt;/li>
&lt;/ul>
&lt;h1 id="常见问题">常见问题&lt;/h1>
&lt;h2 id="lacp-在-networkmanager-管理的-bonding-不工作">LACP 在 NetworkManager 管理的 Bonding 不工作&lt;/h2>
&lt;p>&lt;a href="https://github.com/systemd/systemd/issues/15208">https://github.com/systemd/systemd/issues/15208&lt;/a>&lt;/p>
&lt;p>当 systemd 版本在 242、243、245 时，NetworkManager 对于 802.3ad 模式的 Bonding 在发送 LACP 包是可能会产生异常&lt;/p>
&lt;p>如果通过 NetworkManager 创建的 Bond 网络设备失效，有如下几种可用的解决方式：&lt;/p>
&lt;ul>
&lt;li>通过 ip 命令先删除网络设备，再通过 ip 命令添加即可。
&lt;ul>
&lt;li>ip link set bond1 down&lt;/li>
&lt;li>ip link del bond1&lt;/li>
&lt;li>ip link add bond1 type bond mod 802.3ad&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: NetworkManager 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/networkmanager-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/networkmanager-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://networkmanager.dev/docs/api/latest/NetworkManager.conf.html">Manual(手册),NetworkManager.conf(5)&lt;/a>&lt;/li>
&lt;li>在 &lt;a href="https://developer-old.gnome.org/NetworkManager/">GNOME 开发者中心官网&lt;/a>中，也可以查到 Manual&lt;/li>
&lt;li>&lt;a href="https://wiki.gnome.org/Projects/NetworkManager/DNS">https://wiki.gnome.org/Projects/NetworkManager/DNS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cloud.tencent.com/developer/article/1710514">https://cloud.tencent.com/developer/article/1710514&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>NetworkManager 的配置文件是 INI 格式的，由 **Sections(部分) **和 &lt;strong>Key/Value Pairs(键/值对)&lt;/strong> 组成。&lt;/p>
&lt;p>可用的 Sections 有如下几种：&lt;/p>
&lt;ul>
&lt;li>main #&lt;/li>
&lt;li>keyfile # 用于配置 keyfile 插件。通常只在不使用任何特定 Linux 发行版的插件时才进行配置。&lt;/li>
&lt;li>ifupdown #&lt;/li>
&lt;li>logging # 控制 NetworkManager 的日志记录。此处的任何设置都被 &amp;ndash;log-level 和 &amp;ndash;log-domains 命令行选项覆盖。&lt;/li>
&lt;li>connection #&lt;/li>
&lt;li>device #&lt;/li>
&lt;li>connectivity #&lt;/li>
&lt;li>global-dns #&lt;/li>
&lt;li>global-dns-domain #&lt;/li>
&lt;li>.config #&lt;/li>
&lt;/ul>
&lt;h1 id="main-部分">main 部分&lt;/h1>
&lt;p>&lt;strong>dns=&lt;!-- raw HTML omitted --> # 设置 DNS 处理模式。&lt;/strong>
可用的模式有如下几种：&lt;/p>
&lt;ul>
&lt;li>default #&lt;/li>
&lt;li>dnsmasq #&lt;/li>
&lt;li>systemd-resolved #&lt;/li>
&lt;li>unbound #&lt;/li>
&lt;li>&lt;strong>none&lt;/strong> # NetworkManager 程序不会修改 resovl.conf 文件。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>plugins=&lt;!-- raw HTML omitted --> # 设置控制 Connections 配置文件的插件，多个插件以 &lt;strong>&lt;code>**,**&lt;/code>&lt;/strong> 分隔。&lt;/strong>
注意，NetworkManager 原生的 keyfile 插件始终附加到此列表的末尾。也就意味着，NetworkManager 始终都会加载 keyfile 插件。&lt;/p>
&lt;h1 id="keyfile-部分">keyfile 部分&lt;/h1>
&lt;p>&lt;strong>path=&lt;!-- raw HTML omitted --> # 读取和存储 Connection 配置文件的目录。&lt;/strong>&lt;code>默认值：/etc/NetworkManager/system-connections&lt;/code>
&lt;strong>unmanaged-devices=&lt;!-- raw HTML omitted --> # 指定 keyfile 不管理的网络设备&lt;/strong>&lt;/p>
&lt;h1 id="logging-部分">logging 部分&lt;/h1></description></item><item><title>Docs: nftables(Netfilter 的实现)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/nftablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/nftablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方 wiki：&lt;a href="https://wiki.nftables.org/wiki-nftables/index.php/Main_Page">https://wiki.nftables.org/wiki-nftables/index.php/Main_Page&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>nftables 是一个 netfilter 项目，旨在替换现有的 {ip,ip6,arp,eb}tables 框架，为 {ip,ip6}tables 提供一个新的包过滤框架、一个新的用户空间实用程序（nft）和一个兼容层。它使用现有的钩子、链接跟踪系统、用户空间排队组件和 netfilter 日志子系统。&lt;/p>
&lt;p>nftables 主要由三个组件组成：内核实现、libnl netlink 通信、 nftables 用户空间。 其中内核提供了一个 netlink 配置接口以及运行时规则集评估，libnl 包含了与内核通信的基本函数，用户空间可以通过 nft 和用户进行交互。&lt;/p>
&lt;p>nftables 与 iptables 的区别&lt;/p>
&lt;p>nftables 和 iptables 一样，由 table(表)、chain(链)、rule(规则) 组成。nftables 中，表包含链，链包含规则，规则是真正的 action。与 iptables 相比，nftables 主要有以下几个变化：&lt;/p>
&lt;ul>
&lt;li>iptables 规则的布局是基于连续的大块内存的，即数组式布局；而 nftables 的规则采用链式布局。其实就是数组和链表的区别&lt;/li>
&lt;li>iptables 大部分工作在内核态完成，如果要添加新功能，只能重新编译内核；而 nftables 的大部分工作是在用户态完成的，添加新功能很 easy，不需要改内核。&lt;/li>
&lt;li>iptables 有内置的链，即使你只需要一条链，其他的链也会跟着注册；而 nftables 不存在内置的链，你可以按需注册。由于 iptables 内置了一个数据包计数器，所以即使这些内置的链是空的，也会带来性能损耗。&lt;/li>
&lt;li>简化了 IPv4/IPv6 双栈管理&lt;/li>
&lt;li>原生支持集合、字典和映射&lt;/li>
&lt;/ul>
&lt;p>nftables 没有任何默认规则，如果关闭了 firewalld 服务，则命令 nft list ruleset 输出结果为空。意思就是没有任何内置链或者表 2.&lt;/p>
&lt;h2 id="nftables-table-表-与-nftables-family-簇">nftables table 表 与 nftables family 簇&lt;/h2>
&lt;p>nftables 没有内置表，表的数量与名称由用户决定。&lt;/p>
&lt;p>family(簇) 是 nftables 技术引用的新概念。一共有 6 种簇。不同的 family 可以处理不同 Hook 上的数据包。&lt;/p>
&lt;p>Note：&lt;/p>
&lt;ol>
&lt;li>&lt;code>簇&lt;/code> 可以当做 &lt;code>类型&lt;/code> 来理解，比如建立一个名为 test 的表，该表的簇为 inet(i.e.表的类型是 inet)。&lt;/li>
&lt;li>所以每个表应且只应指定一个簇，且当表中的链被指定类型时，只能指定该簇下可以处理的链类型，详情见本文《nftables chain 链》章节&lt;/li>
&lt;/ol>
&lt;p>nftables 中一同以下几种 family：&lt;/p>
&lt;ol>
&lt;li>ip #IPv4 地址簇。对应 iptables 中 iptables 命令行工具所实现的效果。默认簇，nft 命令的所有操作如果不指定具体的 family，则默认对 ip 簇进行操作
&lt;ol>
&lt;li>可处理流量的 Hook：与 inet 簇相同&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>ip6 #IPv6 地址簇。对应 iptables 中 ip6tables 命令行工具所实现的效果
&lt;ol>
&lt;li>可处理流量的 Hook：与 inet 簇相同&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>inet #Internet (IPv4/IPv6)地址簇。对应 iptables 中 iptables 和 ip6tables 命令行工具所实现的效果
&lt;ol>
&lt;li>可处理流量的的 Hook：prerouting、input、forward、output、postrouting。ip 与 ip6 簇与 inet 簇所包含的 Hook 相同&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>arp #ARP 地址簇，处理 IPv4 ARP 包。对应 iptables 中 arptables 命令行工具所实现的效果
&lt;ol>
&lt;li>可处理流量的 Hook：input、output。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>bridge #桥地址簇。处理通过桥设备的数据包对应 iptables 中 ebtables 命令行工具所实现的效果
&lt;ol>
&lt;li>可处理流量的 Hook：与 inet 簇相同&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>netdev #Netdev address family, handling packets from ingress.
&lt;ol>
&lt;li>可处理流量的 Hook：ingress&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>基本效果示例如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@centos8 ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nft add table test # 创建名为test的表，簇为默认的ip簇&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@centos8 ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nft list ruleset # 列出所有规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>table ip test &lt;span style="color:#f92672">{&lt;/span> &lt;span style="color:#75715e"># 仅有一个名为test的表，簇为ip，没有任何规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@centos8 ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nft add table inet test # 创建名为test的表，使用inet簇&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@centos8 ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># nft list ruleset&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>table ip test &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>table inet test &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="nftables-chain链">nftables chain(链)&lt;/h2>
&lt;p>在 nftables 中，链是用来保存规则的。链在逻辑上被分为下述三种类型：&lt;/p>
&lt;ol>
&lt;li>filter 类型的链 #用于过滤数据包所用
&lt;ol>
&lt;li>允许定义在哪些 family 下：all family&lt;/li>
&lt;li>链中的规则会处理这些 Hook 点的数据包：all Hook&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>nat 类型的链 #用于进行地址转换
&lt;ol>
&lt;li>允许定义在哪些 family 下：ip、ip6&lt;/li>
&lt;li>链中的规则会处理这些 Hook 点的数据包：prerouting、input、output、postrouting&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>route 类型的链
&lt;ol>
&lt;li>允许定义在哪些 family 下：ip、ip6&lt;/li>
&lt;li>链中的规则会处理这些 Hook 点的数据包：output&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>在创建 nftables 中的链时，通常有两种叫法，没有类型的叫常规链，含有类型的叫基本链：&lt;/p>
&lt;p>常规链(也叫自定义链) : 不需要指定钩子类型和优先级，可以用来做链与链之间的跳转，从逻辑上对规则进行分类。&lt;/p>
&lt;p>基本链 : 数据包的入口点，需要指定该链的基本信息(类型、作用的 Hook 点、优先级、默认策略等)才可以让链中的规则生效(在链管理命令的 {} 中添加链信息)。因为链中包含一条一条的规则，所以一个可以正常处理流量的链，需要指定其类型来区分该链上的规则干什么用的，还需要指定 Hook 来指明数据包到哪个 Hook 了来使用这个规则，还需要配置优先级来处理相同类型的规则，该规则应该先执行还是后执行。&lt;/p>
&lt;h2 id="nftables-rule规则">nftables rule(规则)&lt;/h2>
&lt;p>nftables 中的规则标识符有两种，一种 index，一种 handle&lt;/p>
&lt;p>&lt;strong>index #规则的索引。每条规则在其链中，从 0 开始计数(每条链中的规则，第一条规则的 index 为 0，第二条规则的 indext 为 2，依次类推)。&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> chain DOCKER &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport tcpmux accept &lt;span style="color:#75715e"># 规则index为0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">5&lt;/span> accept &lt;span style="color:#75715e">#规则index为1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">6&lt;/span> accept &lt;span style="color:#75715e">#后续依次类推&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">2&lt;/span> accept
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">3&lt;/span> accept
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport afs3-fileserver accept
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>handle #规则的句柄。句柄对于整个 nftalbes 而言，不管添加在哪个链中，第一条规则的句柄为 1，第二条规则句柄为 2。如果规则句柄为 33 号被删除，则新添加的规则的句柄为 34&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> chain DOCKER &lt;span style="color:#f92672">{&lt;/span> &lt;span style="color:#75715e"># handle 4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport tcpmux accept &lt;span style="color:#75715e"># handle 28&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">5&lt;/span> accept &lt;span style="color:#75715e"># handle 32&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">6&lt;/span> accept &lt;span style="color:#75715e"># handle 33&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">2&lt;/span> accept &lt;span style="color:#75715e"># handle 29&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport &lt;span style="color:#ae81ff">3&lt;/span> accept &lt;span style="color:#75715e"># handle 30&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tcp dport afs3-fileserver accept &lt;span style="color:#75715e"># handle 31&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note：对于每条规则而言，其 index 可以随时改变，当在多个规则中间插入新规则时，新插入规则下面的规则 index 则会改变。而 handle 则不会改变，除非删除后重新添加&lt;/p>
&lt;h2 id="总结">总结：&lt;/h2>
&lt;p>nftables 的结构为：表包含链，链包含规则，这个逻辑是非常清晰明了的。而 iptable 呢，则需要先指定什么类型的表，再添加规则，规则与链则互相存在，让人摸不清关系；其实也可以说，iptables 的表类型，就是 nftables 中的链的类型。&lt;/p>
&lt;h1 id="nftable-配置">Nftable 配置&lt;/h1>
&lt;p>/etc/sysconfig/nftables.conf #CentOS 8 中，nftables.service 的规则被存储在此目录中，其中 include 一些其他的示例规则
/etc/sysconfig/nftables/* #nftables.conf 文件中 include 的文件，都在该目录下&lt;/p>
&lt;p>备份规则：
$ nft list ruleset &amp;gt; /root/nftables.conf&lt;/p>
&lt;h1 id="nftable-的-set集合与-map字典-特性介绍">nftable 的 set(集合)与 map(字典) 特性介绍&lt;/h1>
&lt;p>nftables 的语法原生支持集合，集合可以用来匹配多个 IP 地址、端口号、网卡或其他任何条件。类似于 ipset 的功能。&lt;/p>
&lt;p>集合分为匿名集合与命名集合。&lt;/p>
&lt;h2 id="匿名集合">匿名集合&lt;/h2>
&lt;p>匿名集合比较适合用于未来不需要更改的规则&lt;/p>
&lt;p>例如下面的两个示例，&lt;/p>
&lt;ol>
&lt;li>该规则允许来自源 IP 处于 10.10.10.123 ~ 10.10.10.231 这个区间内的主机的流量通过。
&lt;ol>
&lt;li>nft add rule inet my_table my_filter_chain ip saddr { 10.10.10.123, 10.10.10.231 } accept&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>该规则允许来自目的端口是 http、nfs、ssh 的流量通过。
&lt;ol>
&lt;li>nft add rule inet my_table my_filter_chain tcp dport { http, nfs, ssh } accept&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>匿名集合的缺点是，如果需要修改集合中的内容，比如像 ipset 中修改 ip 似的，就得替换规则。如果后面需要频繁修改集合，推荐使用命名集合。&lt;/p>
&lt;h2 id="命令集合">命令集合&lt;/h2>
&lt;p>iptables 可以借助 ipset 来使用集合，而 nftables 中的命名集合就相当于 ipset 的功能。&lt;/p>
&lt;p>命名集合需要使用 nft add set XXXX 命令进行创建，创建时需要指定簇名、表名、以及 set 的属性&lt;/p>
&lt;p>命名集合中包括以下几种属性，其中 type 为必须指定的属性，其余属性可选。&lt;/p>
&lt;ol>
&lt;li>type #集合中所有元素的类型，包括 ipv4_addr(ipv4 地址), ipv6_addr(ipv6 地址), ether_addr(以太网地址), inet_proto(网络协议), inet_service(网络服务), mark(标记类型) 这几类&lt;/li>
&lt;li>flags #集合的标志。包括 constant、interval、timeout 。
&lt;ol>
&lt;li>interval #让集合支持区间模式。默认集合中无法使用这种方式 nft add element inet my_table my_set { 10.20.20.0-10.20.20.255 } 来添加集合 。当给集合添加类型 flag 时，就可以在给集合添加元素时，使用‘区间’的表示方法。因为内核必须提前确认该集合存储的数据类型，以便采用适当的数据结构。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>timeout #&lt;/li>
&lt;li>gc-interval #&lt;/li>
&lt;li>elements #&lt;/li>
&lt;li>size #&lt;/li>
&lt;li>policy #&lt;/li>
&lt;li>auto-merge #&lt;/li>
&lt;/ol>
&lt;p>像 ipset 一样，光创建完还没法使用，需要在 iptables 中添加规则引用 ipset 才可以。nftables 的 set 一样，创建完成后，需要在规则中引用，引用集合规则时使用 @ 并跟上集合的名字，即可引用指定的集合(e.g.nft insert rule inet my_table my_filter_chain ip saddr @my_set drop)这条命令即时引用了 my_set 集合中的内容&lt;/p>
&lt;p>级联不同类型&lt;/p>
&lt;p>命名集合也支持对不同类型的元素进行级联，通过级联操作符 . 来分隔。例如，下面的规则可以一次性匹配 IP 地址、协议和端口号。&lt;/p>
&lt;p>$ nft add set inet my_table my_concat_set { type ipv4_addr . inet_proto . inet_service ; }&lt;/p>
&lt;p>$ nft list set inet my_table my_concat_set&lt;/p>
&lt;p>table inet my_table {&lt;/p>
&lt;p>set my_concat_set {&lt;/p>
&lt;p>type ipv4_addr . inet_proto . inet_service&lt;/p>
&lt;pre>&lt;code> }
&lt;/code>&lt;/pre>
&lt;p>}&lt;/p>
&lt;p>向集合中添加元素：&lt;/p>
&lt;p>$ nft add element inet my_table my_concat_set { 10.30.30.30 . tcp . telnet }&lt;/p>
&lt;p>在规则中引用级联类型的集合和之前一样，但需要标明集合中每个元素对应到规则中的哪个位置。&lt;/p>
&lt;p>$ nft add rule inet my_table my_filter_chain ip saddr . meta l4proto . tcp dport @my_concat_set accept&lt;/p>
&lt;p>这就表示如果数据包的源 IP、协议类型、目标端口匹配 10.30.30.30、tcp、telnet 时，nftables 就会允许该数据包通过。&lt;/p>
&lt;p>匿名集合也可以使用级联元素，例如：&lt;/p>
&lt;p>$ nft add rule inet my_table my_filter_chain ip saddr . meta l4proto . udp dport { 10.30.30.30 . udp . bootps } accept&lt;/p>
&lt;p>现在你应该能体会到 nftables 集合的强大之处了吧。&lt;/p>
&lt;p>nftables 级联类型的集合类似于 ipset 的聚合类型，例如 hash:ip,port。&lt;/p>
&lt;h1 id="nft-命令行工具介绍">nft 命令行工具介绍&lt;/h1>
&lt;p>&lt;strong>nft [OPTIONS] [COMMANDS]&lt;/strong>&lt;/p>
&lt;p>COMMANDS 包括：&lt;/p>
&lt;ol>
&lt;li>ruleset #规则集管理命令&lt;/li>
&lt;li>table #表管理命令&lt;/li>
&lt;li>chain #链管理命令&lt;/li>
&lt;li>rule #规则管理命令&lt;/li>
&lt;li>set #集合管理命令&lt;/li>
&lt;li>map #字典管理命令&lt;/li>
&lt;li>NOTE：
&lt;ol>
&lt;li>该 COMMANDS 与后面子命令中的 COMMAND 不同，前者是 nft 命令下的子命令，后者是 nft 命令下子命令的子命令&lt;/li>
&lt;li>nft 子命令默认对 ip 簇进行操作，当指定具体的 FAMILY 时，则对指定的簇进行操作&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>OPTIONS&lt;/p>
&lt;ol>
&lt;li>-a,&amp;ndash;handle #在使用命令获得输出时，显示每个对象的句柄
&lt;ol>
&lt;li>Note：handle(句柄)在 nftables 中，相当于标识符，nftables 中的每一行内容都有一个 handle。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>-e,&amp;ndash;echo #回显已添加、插入或替换的内容&lt;/li>
&lt;li>-f,&amp;ndash;file FILE #从指定的文件 FILE 中读取 netfilter 配置加载到内核中&lt;/li>
&lt;/ol>
&lt;p>EXAMPLE：&lt;/p>
&lt;ol>
&lt;li>nft -f /root/nftables.conf #从 nftables.conf 文件中，将配置规则加载到系统中&lt;/li>
&lt;/ol>
&lt;p>Note：下面子命令中的 FAMILY 如果不指定，则所有命令默认都是对 ip 簇进行操作。&lt;/p>
&lt;h2 id="表管理命令">表管理命令&lt;/h2>
&lt;p>nft COMMAND table [FAMILY] TABLE #FAMILY 指定簇名，TABLE 为表的名称&lt;/p>
&lt;p>nft list tables #列出所有的表，不包含表中的链和规则&lt;/p>
&lt;p>COMMAND&lt;/p>
&lt;ol>
&lt;li>add #添加指定簇下的表。&lt;/li>
&lt;li>create #与 add 命令类似，但是如果表已经存在，则返回错误信息。&lt;/li>
&lt;li>delete #删除指定的表。不管表中是否有内容都一并删除&lt;/li>
&lt;li>flush #清空指定的表下的所有规则，保留链&lt;/li>
&lt;li>list #列出指定的表的所有链，及其链中的规则&lt;/li>
&lt;/ol>
&lt;p>EXAMPLE&lt;/p>
&lt;ol>
&lt;li>nft add table my_table #创建一个 ip 簇的，名为 my_table 的表&lt;/li>
&lt;li>nft add table inet my_table #创建一个 inet 簇的，名为 my_table 的表&lt;/li>
&lt;li>nft list table inet my_table #列出 inet 簇的名为 my_table 的表及其链和规则&lt;/li>
&lt;/ol>
&lt;h2 id="链管理命令">链管理命令&lt;/h2>
&lt;p>nft COMMAND chain [FAMILY] TABLE CHAIN [{ type TYPE hook HOOK [device DEVICE] priority PRIORITY; [policy POLICY;] }] #FAMILY 指定簇名，TABLE 指定表名，CHAIN 指定链名，TYPE 指定该链的类型，HOOK 指定该链作用在哪个 hook 上，DEVICE 指定该链作用在哪个网络设备上，PRIORITY 指定该链的优先级，POLICY 指定该链的策略(i.e.该链的默认策略，accept、drop 等等。)&lt;/p>
&lt;p>nft list chains #列出所有的链&lt;/p>
&lt;p>Note:&lt;/p>
&lt;ul>
&lt;li>在输入命令时，使用反斜线 \ 用来转义分号 ; ，这样 shell 就不会将分号解释为命令的结尾。如果是直接编辑 nftables 的配置文件则不用进行转义&lt;/li>
&lt;li>PRIORITY 采用整数值，可以是负数，值较小的链优先处理。&lt;/li>
&lt;/ul>
&lt;p>COMMAND&lt;/p>
&lt;ol>
&lt;li>add #在指定的表中添加一条链&lt;/li>
&lt;li>create #与 add 命令类似，但是如果链已经存在，则返回错误信息。&lt;/li>
&lt;li>delete #删除指定的链。该链不能包含任何规则，或者被其它规则作为跳转目标，否则删除失败。&lt;/li>
&lt;li>flush #&lt;/li>
&lt;li>list #列出指定表下指定的链，及其链中的规则&lt;/li>
&lt;li>rename #&lt;/li>
&lt;/ol>
&lt;p>EXAMPLE&lt;/p>
&lt;ol>
&lt;li>nft add chain inet my_table my_utility_chain #在 inet 簇的 my_table 表上创建一个名为 my_utility_chain 的常规链，没有任何参数&lt;/li>
&lt;li>nft add chain inet my_table my_filter_chain{type filter hook input priority 0;} #在 inet 簇的 my_table 表上创建一个名为 my_filter_chain 的链，链的类型为 filter，作用在 input 这个 hook 上，优先级为 0&lt;/li>
&lt;li>nft list chain inet my_table my_filter_chain #列出 inet 簇的 my_table 表下的 my_filter_chain 链的信息，包括其所属的表和其包含的规则&lt;/li>
&lt;/ol>
&lt;h2 id="规则管理命令">规则管理命令&lt;/h2>
&lt;p>nft COMMAND rule [FAMILY] TABLE CHAIN [handle HANDLE|index INDEX] STATEMENT&amp;hellip; #FAMILY 指定簇名，HANDLE 和 INDEX 指定规则的句柄值或索引值，STATEMENT 指明该规则的语句&lt;/p>
&lt;p>nft list ruleset [FAMILY] #列出所有规则，包括规则所在的链，链所在的表。i.e.列出 nftables 中的所有信息。可以指定 FAMILY 来列出指定簇的规则信息&lt;/p>
&lt;p>[FAMILY] #清除所有规则，包括表。i.e.清空 nftables 中所有信息。可以指定 FAMILY 来清空指定簇的规则信息&lt;/p>
&lt;p>COMMAND&lt;/p>
&lt;ol>
&lt;li>add #将规则添加到链的末尾，或者指定规则的 handle 或 index 之后&lt;/li>
&lt;li>insert #将规则添加到链的开头，或者指定规则的 handle 或 index 之前&lt;/li>
&lt;li>delete #删除指定的规则。Note:只能通过 handle 删除&lt;/li>
&lt;li>replace #替换指定规则为新规则&lt;/li>
&lt;/ol>
&lt;p>EXAMPLE&lt;/p>
&lt;ol>
&lt;li>nft add rule inet my_table my_filter_chain tcp dport ssh accept #在 inet 簇的 my_table 表中的 my_filter_chain 链中添加一条规则，目标端口是 ssh 服务的数据都接受&lt;/li>
&lt;li>nft add rule inet my_table my_filter_chain ip saddr @my_set drop #创建规则时引用 my_set 集合&lt;/li>
&lt;/ol>
&lt;h2 id="集合管理命令">集合管理命令&lt;/h2>
&lt;p>COMMAND set [FAMILY] table set { type TYPE; [flags FLAGS;] [timeout TIMEOUT ;] [gc-interval GC-INTERVAL ;] [elements = { ELEMENT[,&amp;hellip;] } ;] [size SIZE;] [policy POLICY;] [auto-merge AUTO-MERGE ;] } #各字段解释详见上文 nftables 的 set 与 map 特性介绍&lt;/p>
&lt;p>list sets #列出所有结合&lt;/p>
&lt;p>{add | delete} element [family] table set { element[,&amp;hellip;] } #在指定集合中添加或删除元素&lt;/p>
&lt;p>Note:&lt;/p>
&lt;ul>
&lt;li>在输入命令时，使用反斜线 \ 用来转义分号 ; ，这样 shell 就不会将分号解释为命令的结尾。如果是直接编辑 nftables 的配置文件则不用进行转义&lt;/li>
&lt;/ul>
&lt;p>COMMAND&lt;/p>
&lt;ol>
&lt;li>add&lt;/li>
&lt;li>delete #通过 handle 删除指定的集合&lt;/li>
&lt;li>flush #&lt;/li>
&lt;li>list #&lt;/li>
&lt;/ol>
&lt;p>EXAMPLE&lt;/p>
&lt;ol>
&lt;li>nft add set inet my_table my_set {type ipv4_addr; } #在 inet 簇的 my_table 表中创建一个名为 my_set 的集合，集合的类型为 ipv4_addr&lt;/li>
&lt;li>nft add set my_table my_set {type ipv4_addr; flags interval;} #在默认 ip 簇的 my_table 表中创建一个名为 my_set 的集合，集合类型为 ipv4_addr ，标签为 interval。让该集合支持区间&lt;/li>
&lt;li>nft add element inet my_table my_set { 10.10.10.22, 10.10.10.33 } #向 my_set 集合中添加元素，一共添加了两个元素，是两个 ipv4 的地址&lt;/li>
&lt;/ol>
&lt;h2 id="字典管理命令">字典管理命令&lt;/h2>
&lt;p>字典&lt;/p>
&lt;p>字典是 nftables 的一个高级特性，它可以使用不同类型的数据并将匹配条件映射到某一个规则上面，并且由于是哈希映射的方式，可以完美的避免链式规则跳转的性能开销。&lt;/p>
&lt;p>例如，为了从逻辑上将对 TCP 和 UDP 数据包的处理规则拆分开来，可以使用字典来实现，这样就可以通过一条规则实现上述需求。&lt;/p>
&lt;p>$ nft add chain inet my_table my_tcp_chain&lt;/p>
&lt;p>$ nft add chain inet my_table my_udp_chain&lt;/p>
&lt;p>$ nft add rule inet my_table my_filter_chain meta l4proto vmap { tcp : jump my_tcp_chain, udp : jump my_udp_chain }&lt;/p>
&lt;p>$ nft list chain inet my_table my_filter_chain&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>table inet my_table &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> chain my_filter_chain &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> meta nfproto ipv4 ip saddr . meta l4proto . udp dport &lt;span style="color:#f92672">{&lt;/span> 10.30.30.30 . udp . bootps &lt;span style="color:#f92672">}&lt;/span> accept
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> meta l4proto vmap &lt;span style="color:#f92672">{&lt;/span> tcp : jump my_tcp_chain, udp : jump my_udp_chain &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>和集合一样，除了匿名字典之外，还可以创建命名字典：&lt;/p>
&lt;p>$ nft add map inet my_table my_vmap { type inet_proto : verdict ; }&lt;/p>
&lt;p>向字典中添加元素：&lt;/p>
&lt;p>$ nft add element inet my_table my_vmap { 192.168.0.10 : drop, 192.168.0.11 : accept }&lt;/p>
&lt;p>后面就可以在规则中引用字典中的元素了：&lt;/p>
&lt;p>$ nft add rule inet my_table my_filter_chain ip saddr vmap @my_vmap&lt;/p>
&lt;ol start="9">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>表与命名空间&lt;/p>
&lt;p>在 nftables 中，每个表都是一个独立的命名空间，这就意味着不同的表中的链、集合、字典等都可以有相同的名字。例如：&lt;/p>
&lt;p>$ nft add table inet table_one&lt;/p>
&lt;p>$ nft add chain inet table_one my_chain&lt;/p>
&lt;p>$ nft add table inet table_two&lt;/p>
&lt;p>$ nft add chain inet table_two my_chain&lt;/p>
&lt;p>$ nft list ruleset&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>table inet table_one &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> chain my_chain &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>table inet table_two &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> chain my_chain &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>有了这个特性，不同的应用就可以在相互不影响的情况下管理自己的表中的规则，而使用 iptables 就无法做到这一点。&lt;/p>
&lt;p>当然，这个特性也有缺陷，由于每个表都被视为独立的防火墙，那么某个数据包必须被所有表中的规则放行，才算真正的放行，即使 table_one 允许该数据包通过，该数据包仍然有可能被 table_two 拒绝。为了解决这个问题，nftables 引入了优先级，priority 值越高的链优先级越低，所以 priority 值低的链比 priority 值高的链先执行。如果两条链的优先级相同，就会进入竞争状态。&lt;/p>
&lt;p>总结&lt;/p>
&lt;p>希望通过本文的讲解，你能对 nftables 的功能和用法有所了解，当然本文只涉及了一些浅显的用法，更高级的用法可以查看 nftables 的官方 wiki，或者坐等我接下来的文章。相信有了本文的知识储备，你应该可以愉快地使用 nftables 实现 Linux 的智能分流了，具体扫一扫下方的二维码参考这篇文章：Linux 全局智能分流方案。&lt;/p></description></item><item><title>Docs: nmcli connection 子命令</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/nmcli-connection-%E5%AD%90%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/nmcli-connection-%E5%AD%90%E5%91%BD%E4%BB%A4/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-vlan_on_bond_and_bridge_using_the_networkmanager_command_line_tool_nmcli">红帽官方文档,RedHat7-网络指南-使用 nmcli 创建带有 VLAN 的 bond 并作为 Bridge 的从设备&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>nmcli connection {show | up | down | modify | add | edit | clone | delete | monitor | reload | load | import | export} [ARGUMENTS&amp;hellip;]&lt;/strong>&lt;/p>
&lt;h1 id="up--down--启动--停止连接">up | down # 启动 | 停止连接&lt;/h1>
&lt;p>nmcli connection up [[id | uuid | path] &lt;!-- raw HTML omitted -->] [ifname &lt;!-- raw HTML omitted -->] [ap &lt;!-- raw HTML omitted -->] [passwd-file &lt;!-- raw HTML omitted -->] # 启动连接&lt;/p>
&lt;p>nmcli connection down [id | uuid | path | apath] &lt;!-- raw HTML omitted --> &amp;hellip; # 停止连接&lt;/p>
&lt;p>EXAMPLE&lt;/p>
&lt;ul>
&lt;li>启动名为 eth0 的 connection。i.e.把配置应用到指定的网络设备上，并且会自动重启网络设备
&lt;ul>
&lt;li>&lt;strong>nmcli con up eth0&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="add--增加连接">add # 增加连接&lt;/h1>
&lt;p>使用指定的 PROPERTY(属性) 创建一个新的连接&lt;/p>
&lt;p>Note：&lt;/p>
&lt;ol>
&lt;li>add 与 modify 命令的参数用法基本一致。delete 连接没有多少复杂的参数，直接指定连接的标识符，即可将连接删除&lt;/li>
&lt;li>如果想对该连接进行更详细的配置，比如配置 ip、网关、bond 参数等等。就需要指定具体的 PROPETY 和对应的 VALUE。&lt;/li>
&lt;/ol>
&lt;h2 id="syntax语法">Syntax(语法)&lt;/h2>
&lt;p>&lt;strong>nmcli connection add [save BOOLEAN] SETTING.PROPERTY VALUE &amp;hellip;&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>save&lt;/strong> # 指定该连接创建完成后，是否以文件形式保存到本地磁盘。默认为 true。&lt;/li>
&lt;li>&lt;strong>SETTING.PROPERTY(设置.属性)&lt;/strong> # 该连接包含的属性。SETTING.PROPERTY 简称 property(属性) 用来指定要增加的连接的配置信息。
&lt;ul>
&lt;li>如果 SETTING 和 PROPERTY 是 唯一的，则可以使用缩写(比如 connection.type 缩写为 type)。不同的 SETTING 中有不同的 PROPERTY。并非所有属性都适用于所有类型的连接(type 是创建连接时必须指定的一个属性)。也就是说 property(属性) 分为两种，一种适用于全局的通用属性，另一种是只对特定类型的连接生效。比如我当前创建一个 ethernet 类型的连接，那么就不能使用 bond 属性。&lt;/li>
&lt;li>注意：如果要在脚本中使用 nmcli 命令，最好不要使用别名&lt;/li>
&lt;li>可用的 SETTING.PROPERTY 详见 &lt;a href="https://www.yuque.com/go/doc/33221861">Connection 配置详解&lt;/a>，在命令行中的写法与配置文件中是一致的，SETTING 就是 配置文件中的 SETTING，PROPERTY 就是配置文件中所属 SETTING 的 PROPERTY。&lt;/li>
&lt;li>特殊连接类型的属性
&lt;ul>
&lt;li>在命令使用中，特殊类型的属性可能并不在文档中，比如 bond 类型中，可以使用 mode 属性指定 bond 类型。&lt;/li>
&lt;li>bond 类型的连接可以使用 mode 属性，该属性的值会添加到 bond.options 属性的值中，作为 mode=MODE 这种键值对存在。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>VALUE&lt;/strong> # SETTING.PROPERTY 的值&lt;/li>
&lt;/ul>
&lt;h2 id="example">EXAMPLE&lt;/h2>
&lt;ul>
&lt;li>创建名为 eth0 的 connection 并关联到 eth0 上，手动指定 IP，并设置开机自动启动网络,关闭 ipv6 网络。
&lt;ul>
&lt;li>nmcli con add type ethernet con-name eth0 ifname eth0 ipv4.method manual ip4 10.10.10.10/24 gw4 10.10.10.1 autoconnect yes ipv6.method disabled&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="delete-删除连接">delete 删除连接&lt;/h1>
&lt;h2 id="syntax语法-1">Syntax(语法)&lt;/h2>
&lt;p>&lt;strong>nmcli connection delete [id | uuid | path] &lt;!-- raw HTML omitted --> # 删除连接&lt;/strong>&lt;/p>
&lt;h1 id="modify-修改连接">modify 修改连接&lt;/h1>
&lt;h2 id="syntax语法-2">Syntax(语法)&lt;/h2>
&lt;p>&lt;strong>nmcli connection modify [+|-]PROPETY VALUE # 修改连接。&lt;/strong>&lt;/p>
&lt;p>Note：&lt;/p>
&lt;ul>
&lt;li>在使用 modify 变更网络配置时，可以使用+或者-来实现&amp;quot;增加&amp;quot;或者&amp;quot;删除&amp;quot;某项配置的功能&lt;/li>
&lt;/ul>
&lt;h2 id="example-1">EXAMPLE&lt;/h2>
&lt;ul>
&lt;li>给名为 bond0 的 Connection 添加两个参数，使用加号可以不负载之前配置的参数而添加新的参数
&lt;ul>
&lt;li>nmcli con modify bond0 +bond.options miimon=200 +bond.options xmit_hash_policy=layer3+4&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="show-查看连接">show 查看连接&lt;/h1>
&lt;h2 id="syntax语法-3">Syntax(语法)&lt;/h2>
&lt;p>&lt;strong>nmcli connection show [&amp;ndash;active] [&amp;ndash;order &lt;!-- raw HTML omitted -->] [id | uuid | path | apath] &lt;!-- raw HTML omitted --> &amp;hellip;&lt;/strong>&lt;/p>
&lt;h2 id="example-2">EXAMPLE&lt;/h2>
&lt;ul>
&lt;li>nmcli con show eth0 # 查看 eth1 这个 connectin 的所有状态，该命令会列出该 connection 的全部属性&lt;/li>
&lt;li>nmcli -f bond con show bridge-slave-bond0 # 查看 bridge-slave-bond0 这个连接配置中，bond 这个 SETTING 的所有属性及其值。效果如下&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>[root@lichenhao ~]# nmcli -f bond con show bridge-slave-bond0
bond.options: mode=active-backup
&lt;/code>&lt;/pre>
&lt;h1 id="应用示例">应用示例&lt;/h1>
&lt;h2 id="静态路由配置">静态路由配置&lt;/h2>
&lt;ul>
&lt;li>在 eth0 网卡上添加静态路由，目的网段是 192.168.122.0/24 的流量下一跳是 10.10.10.1
&lt;ul>
&lt;li>nmcli connection modify eth0 +ipv4.routes &amp;ldquo;192.168.122.0/24 10.10.10.1&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>删除 eth0 网卡上的静态路由
&lt;ul>
&lt;li>nmcli connection modify eth0 -ipv4.routes &amp;ldquo;192.168.122.0/24 10.10.10.1&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="路由策略配置">路由策略配置&lt;/h2>
&lt;ul>
&lt;li>添加一条路由策略：优先级为 5，从 10.0.0.0/24 网段来的数据包，都通过 5000 路由表处理
&lt;ul>
&lt;li>nmcli connection add type ethernet con-name eth0 ifname eth0 ipv4.routing-rules &amp;ldquo;priority 5 from 10.0.0.0/24 table 5000&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="多路由表双网卡双网关">多路由表，双网卡，双网关&lt;/h2>
&lt;ul>
&lt;li>添加 ens9 连接，配置 IP 地址，不分配默认网关，而是在 3 号路由表中配置一条路由条目：任意目的地址的下一跳是 192.168.122.1
&lt;ul>
&lt;li>nmcli con add type ethernet con-name ens9 ifname ens9 ipv4.method manual ipv4.addresses 192.168.122.2 ipv4.routes &amp;ldquo;0.0.0.0/0 192.168.122.1 table=3&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>为 ens9 连接添加一个路由策略，将源地址是 192.168.122.0/24 网段的数据包，都交给 3 号路由表处理
&lt;ul>
&lt;li>nmcli con mod ens9 ipv4.routing-rules &amp;ldquo;priority 5 from 192.168.122.0/24 table 3&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bond-配置">Bond 配置&lt;/h2>
&lt;ul>
&lt;li>添加一个 Bond 类型的连接
&lt;ul>
&lt;li>使用 bond0 网络设备，bond 模式为主备
&lt;ul>
&lt;li>nmcli con add type bond con-name bond0 ifname bond0 mode active-backup&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>使用 bond1 网络设备，bond 模式为 802.3ad
&lt;ul>
&lt;li>nmcli con add type bond con-name bond1 ifname bond1 mode 802.3ad&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>创建一个 bond 类型的 connection，名字叫 bond0 且与 bond0 网络设备绑定(若没有 bond0 网络设备则自动创建)；手动设定 ip 并指定 ip、prefix；指定该 bond 的 3 个参数（bond 模式、检测时间、hash 算法）
&lt;ul>
&lt;li>nmcli con add type bond con-name bond0 ifname bond0 ipv4.method manual ipv4.addr 192.168.20.22/24 bond.options &amp;ldquo;mode=802.3ad,miimon=100,xmit_hash_policy=layer3+4&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>添加 eth0 网络设备到 bond0 中
&lt;ul>
&lt;li>nmcli con add type ethernet master bond0 ifname eth0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bridge-配置">Bridge 配置&lt;/h2>
&lt;ul>
&lt;li>创建一个 Bridge 类型的 connection，名字叫 br0 且与 br0 网络设备绑定(若没有 br0 网络设备则自动创建)，手动获取 ip 并设定 ip、prefix、gateway。
&lt;ul>
&lt;li>nmcli con add type bridge con-name br0 ifname br0 ipv4.method manual ip4 192.168.10.10/24 gw4 192.168.10.1&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>添加 eth0 网络设备到 br0 中
&lt;ul>
&lt;li>nmcli con add type ethernet ifname eth0 master br0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>注意：若不为 bridge 类型的网络设备配置 IP，比如在虚拟化环境中，需要关闭 STP 功能&lt;/p>
&lt;h2 id="vlan-配置">Vlan 配置&lt;/h2>
&lt;p>创建一个 Blan 类型的连接，意思就是为指定的网络设备 DEV 划分 vlan。创建一个名为 DEV.VLANID 的新网络设备，凡是通过该设备发送的数据包都会添加上 VLANID。&lt;/p>
&lt;p>为 Bond 配置 VLAN 标签&lt;/p>
&lt;ul>
&lt;li>创建 bond 类型的连接，名为 bond1，关闭 IPv4 和 IPv6
&lt;ul>
&lt;li>nmcli con add type bond con-name bond1 ifname bond1 ipv4.method disabled ipv6.method ignore bond.options &amp;ldquo;mode=802.3ad,miimon=100,xmit_hash_policy=layer3+4&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>创建 vlan 类型的连接，绑定到 bond1 设备上，vlan 号为 2409，配置 IP
&lt;ul>
&lt;li>nmcli con add type vlan con-name vlan2409-bond1 dev bond1 id 2409 ipv4.method manual ipv4.addresses 100.75.9.17/24 ipv4.gateway 100.75.9.254&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>为 Ethernet 配置 VLAN 标签&lt;/p>
&lt;ul>
&lt;li>创建 ethernet 类型的连接，名为 eth0，关闭 IPv4 和 IPv6
&lt;ul>
&lt;li>nmcli con add ethernet con-name eth0 ifname eth0 ipv4.method disabled ipv6.method ignore&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>创建一个 vlan 类型的连接，连接名为 vlan2-bond0，为 eth0 划分 vlan，vlan 号为 2
&lt;ul>
&lt;li>nmcli con add type vlan con-name vlan2-eth0 dev eth0 id 2 ipv4.method manual ipv4.addresses 100.75.9.17/24&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="具有-vlan-tag-的-ethernet-绑定到-bridge-上">具有 VLAN TAG 的 Ethernet 绑定到 Bridge 上&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nmcli connection add type ethernet con-name ens1f0 ifname ens1f0 ipv4.method disabled ipv6.method ignore
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type bridge con-name br0 ifname br0 ipv4.method manual ipv4.address 10.253.26.242/24 ipv4.gateway 10.253.26.254
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type vlan con-name ens1f0.1251 ifname ens1f0.1251 dev ens1f0 id &lt;span style="color:#ae81ff">1251&lt;/span> master br0 slave-type bridge
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这种配置方法待验证，使用红帽官方的方法配完了起不来，bridge 设备是 down 的状态，ens1f0.1251 设备是 lowerlayerdown 状态。主要问题出在 bridge 设备的配置上，通过简单的传统配置，只要不由 NetworkManager 管理 ifcfg-br0，即可正常使用。&lt;/p>
&lt;blockquote>
&lt;p>在寻找该问题的解决方法时，发现了一个相关 BUG，详见：&lt;a href="https://serverfault.com/questions/682183/bridge-on-vlan-on-teaming-for-kvm/861450">https://serverfault.com/questions/682183/bridge-on-vlan-on-teaming-for-kvm/861450&lt;/a>。不过这个连接的解决方案并不适合我&lt;/p>
&lt;/blockquote>
&lt;p>最主要的还是 Bridge 设备的配置，&lt;strong>在创建 Bridge 设备的时候，关闭 STP 即可解决&lt;/strong>。可能与交换机那边的设置有关。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nmcli connection add type ethernet con-name ens1f0 ifname ens1f0 ipv4.method disabled ipv6.method ignore
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type bridge con-name br0 ifname br0 ipv4.method manual ipv4.address 10.253.26.242/24 ipv4.gateway 10.253.26.254 bridge.stp no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type vlan con-name ens1f0.1251 ifname ens1f0.1251 dev ens1f0 id &lt;span style="color:#ae81ff">1251&lt;/span> master br0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="具有-vlan-tag-的-bond-并绑定到-bridge-上">具有 VLAN TAG 的 Bond 并绑定到 Bridge 上&lt;/h2>
&lt;p>若出现问题无法启动，解决方法与上面的《配置 VLAN TAG 到 Ethernet 绑定到 Bridge 上》的例子解决方法一样。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nmcli connection add type bond con-name Bond0 ifname bond0 bond.options &lt;span style="color:#e6db74">&amp;#34;mode=active-backup,miimon=100&amp;#34;&lt;/span> ipv4.method disabled ipv6.method ignore
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type ethernet con-name Slave1 ifname ens1f0 master bond0 slave-type bond
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type ethernet con-name Slave2 ifname ens1f1 master bond0 slave-type bond
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type bridge con-name Bridge0 ifname br0 ip4 10.253.26.242/24
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection add type vlan con-name Vlan1251 ifname bond0.1251 dev bond0 id &lt;span style="color:#ae81ff">1251&lt;/span> master br0 slave-type bridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nmcli connection show
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> NAME UUID TYPE DEVICE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Bond0 f05806fa-72c3-4803-8743-2377f0c10bed bond bond0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Bridge0 22d3c0de-d79a-4779-80eb-10718c2bed61 bridge br0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Slave1 e59e13cb-d749-4df2-aee6-de3bfaec698c 802-3-ethernet em1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Slave2 25361a76-6b3c-4ae5-9073-005be5ab8b1c 802-3-ethernet em2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Vlan2 e2333426-eea4-4f5d-a589-336f032ec822 vlan bond0.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: nmcli 命令行工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/nmcli-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/networkmanager/nmcli-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>红帽官方文档：&lt;a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/networking_guide/sec-using_the_networkmanager_command_line_tool_nmcli">https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/networking_guide/sec-using_the_networkmanager_command_line_tool_nmcli&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://man.cx/nmcli(1)">Manual(手册),nmcli(1)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>nmcli 用于 NetworkManager 的命令行工具&lt;/p>
&lt;h1 id="syntax语法">Syntax(语法)&lt;/h1>
&lt;p>&lt;strong>nmcli [OPTIONS] OBJECT { COMMAND | help }&lt;/strong>&lt;/p>
&lt;p>OBJECT 和 COMMAND 可以用全称也可以用简称，最少可以只用一个字母&lt;/p>
&lt;p>OPTIONS&lt;/p>
&lt;ul>
&lt;li>-a, &amp;ndash;ask ask for missing parameters&lt;/li>
&lt;li>-c, &amp;ndash;colors auto|yes|no whether to use colors in output&lt;/li>
&lt;li>-e, &amp;ndash;escape yes|no escape columns separators in values&lt;/li>
&lt;li>&lt;strong>-f, &amp;ndash;fields &amp;lt;FIELD,&amp;hellip;&amp;gt;|all|common&lt;/strong> # 指定要输出的字段，FIELD 可以是 任意 setting&lt;/li>
&lt;li>-g, &amp;ndash;get-values &amp;lt;field,&amp;hellip;&amp;gt;|all|common shortcut for -m tabular -t -f&lt;/li>
&lt;li>-h, &amp;ndash;help print this help&lt;/li>
&lt;li>&lt;strong>-m, &amp;ndash;mode &amp;lt;tabular|multiline&amp;gt;&lt;/strong> # 指定输出模式,tabular 输出为表格样式，multiline 是多行样式。
&lt;ul>
&lt;li>nmcli con show # 默认为表格样式&lt;/li>
&lt;li>nmcli con show DEV # 默认为多行样式&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>-o, &amp;ndash;overview overview mode&lt;/li>
&lt;li>&lt;strong>-p, &amp;ndash;pretty&lt;/strong> # 美化输出，以连接中的 setting 分段落展示&lt;/li>
&lt;li>-s, &amp;ndash;show-secrets allow displaying passwords&lt;/li>
&lt;li>&lt;strong>-t, &amp;ndash;terse&lt;/strong> # 简洁的输出&lt;/li>
&lt;li>-v, &amp;ndash;version show program version&lt;/li>
&lt;li>-w, &amp;ndash;wait &lt;!-- raw HTML omitted --> set timeout waiting for finishing operations&lt;/li>
&lt;/ul>
&lt;p>OBJECT&lt;/p>
&lt;ul>
&lt;li>&lt;strong>g[eneral]&lt;/strong> # NetworkManager&amp;rsquo;s general status and operations&lt;/li>
&lt;li>&lt;strong>n[etworking]&lt;/strong> # overall networking control&lt;/li>
&lt;li>&lt;strong>r[adio]&lt;/strong> # NetworkManager radio switches&lt;/li>
&lt;li>&lt;strong>c[onnection]&lt;/strong> # NetworkManager&amp;rsquo;s connections&lt;/li>
&lt;li>&lt;strong>d[evice]&lt;/strong> # devices managed by NetworkManager&lt;/li>
&lt;li>&lt;strong>a[gent]&lt;/strong> # NetworkManager secret agent or polkit agent&lt;/li>
&lt;li>&lt;strong>m[onitor]&lt;/strong> #monitor NetworkManager changes&lt;/li>
&lt;/ul>
&lt;h1 id="general--networkmanager-的一般状态和操作">g[eneral] # NetworkManager 的一般状态和操作&lt;/h1>
&lt;h1 id="networking--overall-networking-control">n[etworking] # overall networking control&lt;/h1>
&lt;h1 id="radio--networkmanager-radio-switches">r[adio] # NetworkManager radio switches&lt;/h1>
&lt;h1 id="connection--connections-的管理命令常用命令">c[onnection] # Connections 的管理命令，常用命令&lt;/h1>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33221854">connection 子命令章节&lt;/a>&lt;/p>
&lt;h2 id="clone-克隆连接">clone 克隆连接&lt;/h2>
&lt;p>clone [&amp;ndash;temporary] [id | uuid | path ] &lt;!-- raw HTML omitted --> &lt;!-- raw HTML omitted --> # 克隆连接&lt;/p>
&lt;h2 id="edit-在交互模式的编辑器中修改连接">edit 在交互模式的编辑器中修改连接&lt;/h2>
&lt;p>edit [id | uuid | path] &lt;!-- raw HTML omitted --> # 进入交互编辑器修改连接&lt;/p>
&lt;p>edit [type &amp;lt;new_con_type&amp;gt;] [con-name &amp;lt;new_con_name&amp;gt;]&lt;/p>
&lt;h2 id="monitor-监控连接">monitor 监控连接&lt;/h2>
&lt;p>monitor [id | uuid | path] &lt;!-- raw HTML omitted --> &amp;hellip; #监控连接&lt;/p>
&lt;h2 id="reloadload-加载连接信息">reload、load 加载连接信息&lt;/h2>
&lt;p>从磁盘重新加载所有连接文件。 默认情况下，NetworkManager 不会监视对连接文件的更改。 因此，您需要使用此命令来告诉 NetworkManager 在对它们进行更改时从磁盘重新读取连接配置文件。 但是，可以启用自动加载功能，然后 NetworkManager 会在每次更改连接文件时重新加载它们（NetworkManager.conf（5）中的 monitor-connection-files = true）。&lt;/p>
&lt;p>从磁盘加载/重新加载一个或多个连接文件。 手动编辑连接文件后使用此命令可确保 NetworkManager 知道其最新状态。&lt;/p>
&lt;p>reload #从 /etc/sysconfig/network-scripts/ 目录重新加载连接文件&lt;/p>
&lt;p>load &lt;!-- raw HTML omitted --> [ &lt;!-- raw HTML omitted -->&amp;hellip; ] #&lt;/p>
&lt;h2 id="其他">其他&lt;/h2>
&lt;p>import [&amp;ndash;temporary] type &lt;!-- raw HTML omitted --> file &lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>export [id | uuid | path] &lt;!-- raw HTML omitted --> [&lt;!-- raw HTML omitted -->]&lt;/p>
&lt;h1 id="device--通过-networkmanager-来管理网络设备">d[evice] # 通过 NetworkManager 来管理网络设备&lt;/h1>
&lt;h1 id="agent--networkmanager-secret-agent-or-polkit-agent">a[gent] # NetworkManager secret agent or polkit agent&lt;/h1>
&lt;h1 id="monitor--monitor-networkmanager-changes">m[onitor] # monitor NetworkManager changes&lt;/h1></description></item><item><title>Docs: systemd-networkd</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/systemd-networkd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/systemd-networkd/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;/blockquote>
&lt;h1 id="配置">配置&lt;/h1>
&lt;p>&lt;strong>/run/systemd/network/*&lt;/strong> # 读取网络设备配置的目录&lt;/p></description></item><item><title>Docs: systemd-resolved.service</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/systemd-resolved.service/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/systemd-resolved.service/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;a href="http://www.jinbuguo.com/systemd/systemd-resolved.service.html">金步国-system 中文手册&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>systemd-resolved.service 是一个类似于 DNSmasq 的域名解析服务，只不过这个服务只适用于 Linux 中，且被 systemd 所管理。&lt;/p>
&lt;h1 id="配置">配置&lt;/h1>
&lt;p>**/run/systemd/resolve/resolv.conf **# 具体的解析配置
&lt;strong>/usr/lib/systemd/resolv.conf&lt;/strong> # 顶替 /etc/resolv.conf 文件&lt;/p></description></item><item><title>Docs: TC 模块</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/</guid><description/></item><item><title>Docs: TC 模块</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/tc-%E6%A8%A1%E5%9D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/tc-%E6%A8%A1%E5%9D%97/tc-%E6%A8%A1%E5%9D%97/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://github.com/liucimin/Learning/blob/master/linux%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/Linux-TC%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3.md">原文链接&lt;/a>、&lt;a href="http://www.lartc.org/LARTC-zh_CN.GB2312.pdf">Linux 的高级路由和流量控制&lt;/a>、&lt;a href="http://www.sdnlab.com/19208.html">Open vSwitch 之 QoS 的实现&lt;/a> &amp;gt; &lt;a href="http://arthurchiao.art/index.html">arthurchiao.art 的文章&lt;/a>：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/lartc-qdisc-zh/#91-%E9%98%9F%E5%88%97queues%E5%92%8C%E6%8E%92%E9%98%9F%E8%A7%84%E5%88%99queueing-disciplines">[译] 《Linux 高级路由与流量控制手册（2012）》第九章：用 tc qdisc 管理 Linux 网络带宽&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/understanding-tc-da-mode-zh/#5-%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8Bebpf-%E7%A8%8B%E5%BA%8F--tc-%E5%91%BD%E4%BB%A4">[译] 深入理解 tc ebpf 的 direct-action (da) 模式（2020）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux 中的 TC 模块已经在内核中存在很多年了，但是直到 eBPF 流行起来之前，文档以及使用者都非常之少，并仍处于活跃开发状态中。Kernel 4.1 版本中添加了一些新的 Hook，并支持将 eBPF 程序作为 tc classifier(也称为 filter) 或 tc action 加载到这些 Hook 点。大概六个月后，Kernel 4.4 版本发布时，iproute2 引入了一个 direct-action 模式，但是关于这个模式的&lt;a href="https://qmonnet.github.io/whirl-offload/2016/09/01/dive-into-bpf/#about-tc">文档依然少得可怜&lt;/a>。。。。。&lt;/p>
&lt;h2 id="glossary术语">Glossary(术语)&lt;/h2>
&lt;p>TC 是一个强大但复杂的框架。 它的&lt;strong>几个核心概念&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>queueing discipline(排队规则，简称 qdisc)&lt;/strong> # 根据某种算法完成限速、整形等功能&lt;/li>
&lt;li>&lt;strong>class&lt;/strong> # 用户定义的流量类别&lt;/li>
&lt;li>&lt;strong>classifier(分类器，也称为 filter)&lt;/strong> # 分类规则&lt;/li>
&lt;li>&lt;strong>action&lt;/strong> # 要对包执行什么动作&lt;/li>
&lt;/ul>
&lt;p>组合以上概念，下面是对某个网络设备上的流量进行分类和限速时，所需完成的大致步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>为网络设备&lt;strong>创建一个 qdisc&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>qdisc 是一个整流器/整形器（shaper），&lt;strong>可以包含多个 class&lt;/strong>，不同 class 可以应用不同的策略。&lt;/li>
&lt;li>qdisc 需要附着（attach）到某个网络接口（network interface），及流量方向（ingress or egress）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>创建流量类别（class）&lt;/strong>，并 attach 到 qdisc。&lt;/p>
&lt;ul>
&lt;li>例如，根据带宽分类，创建高、中、低三个类别。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>创建 filter（classifier）&lt;/strong>，并 attach 到 qdisc。filters 用于&lt;strong>对网络设备上的流量进行分类&lt;/strong>，并&lt;strong>将包分发（dispatch）到前面定义的不同 class&lt;/strong>。filter 会对每个包进行过滤，返回下列值之一：&lt;/p>
&lt;ul>
&lt;li>&lt;code>0&lt;/code>：表示 mismatch。如果后面还有其他 filters，则&lt;strong>继续对这个包应用下一个 filter&lt;/strong>。&lt;/li>
&lt;li>&lt;code>-1&lt;/code>：表示这个 filter 上配置的&lt;strong>默认 classid&lt;/strong>。&lt;/li>
&lt;li>其他值：&lt;strong>表示一个 classid&lt;/strong>。系统接下来应该将包送往这个指定的 class。可以看到，通过这种方式可以实现非线性分类（non-linear classification）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>另外，&lt;strong>可以给 filter 添加 action&lt;/strong>。例如，将选中的包丢弃（drop），或者将流量镜像到另一个网络设备等等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>除此之外，qdisc 和 class 还可以循环嵌套，即： &lt;strong>class 里加入新 qdisc，然后新 qdisc 里又可以继续添加新 class&lt;/strong>， 最终形成的是一个以 root qdisc 为根的树。但对于本文接下来的内容，我们不需要了解这么多。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="tc-原理介绍">TC 原理介绍&lt;/h1>
&lt;p>Linux 中的 QoS 分为 入口(Ingress) 部分和 出口(Egress) 部分，入口部分主要用于进行入口流量限速(policing)，出口部分主要用于队列调度(queuingscheduling)。大多数排队规则(qdisc)都是用于输出方向的，输入方向只有一个排队规则，即 ingressqdisc。ingressqdisc 本身的功能很有限，输入方向只有一个排队规则，即 ingressqdisc（因为没有缓存只能实现流量的 drop）但可用于重定向 incomingpackets。通过 Ingressqdisc 把输入方向的数据包重定向到虚拟设备 ifb，而 ifb 的输出方向可以配置多种 qdisc，就可以达到对输入方向的流量做队列调度的目的。&lt;/p>
&lt;p>说白了，TC 模块就是进入 Netfilter 经典图中 5 个 Hook 之前和之后，添加 ingress 与 egress，这就是 TC 的 Hook，用来处理数据包的流入和流出。&lt;/p>
&lt;p>Ingress 限速只能对整个网卡入流量限速，无队列之分：&lt;/p>
&lt;p>Ingress 流量的限速&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>tc qdisc add dev eth0 ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tc filter add dev eth0 parent ffff: protocol ip prio &lt;span style="color:#ae81ff">10&lt;/span> u32 match ipsrc 0.0.0.0/0 police rate 2048kbps burst 1m drop flowid :1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Egress 限速：&lt;/p>
&lt;p>Linux 操作系统中的流量控制器 TC(Traffic Control) 用于 Linux 内核的流量控制，它利用队列规定建立处理数据包的队列，并定义队列中的数据包被发送的方式，从而实现对流量的控制。TC 模块实现流量控制功能使用的队列规定分为两类，一类是无类队列规定，另一类是分类队列规定。无类队列规定相对简单，而分类队列规定则引出了分类和过滤器等概念，使其流量控制功能增强。&lt;/p>
&lt;p>&lt;strong>无类队列&lt;/strong>规定是对进入网络设备（网卡）的数据流不加区分统一对待的队列规定。使用无类队列规定形成的队列能够接收数据包以及重新编排、延迟或丢弃数据包。这类队列规定形成的队列可以对整个网络设备（网卡）的流量进行整形，但不能细分各种情况。常用的无类队列规定主要有 pfifo_fast（先进先出）、TBF（令牌桶过滤器）、SFQ（随机公平队列）、ID（前向随机丢包）等等。这类队列规定使用的流量整形手段主要是排序、限速和丢包。&lt;/p>
&lt;p>&lt;strong>分类队列&lt;/strong>规定是对进入网络设备的数据包根据不同的需求以分类的方式区分对待的队列规定。数据包进入一个分类的队列后，它就需要被送到某一个类中，也就是说需要对数据包做分类处理。对数据包进行分类的工具是过滤器，过滤器会返回一个决定，队列规定就根据这个决定把数据包送入相应的类进行排队。每个子类都可以再次使用它们的过滤器进行进一步的分类。直到不需要进一步分类时，数据包才进入该类包含的队列排队。除了能够包含其他队列规定之外，绝大多数分类的队列规定还能够对流量进行整形。这对于需要同时进行调度（如使用 SFQ）和流量控制的场合非常有用。&lt;/p>
&lt;p>Linux 流量控制的基本原理如下图所示。&lt;/p>
&lt;p>&lt;img src="..%5CImages%5C20150511093020861.png#alt=" alt="">&lt;/p>
&lt;p>接收包从输入接口（Input Interface）进来后，经过流量限制（Ingress Policing）丢弃不符合规定的数据包，由输入多路分配器（Input  De-Multiplexing）进行判断选择。如果接收包的目的地是本主机，那么将该包送给上层处理，否则需要进行转发，将接收包交到转发块（ForwardingBlock）处理。转发块同时也接收本主机上层（TCP、UDP 等）产生的包。转发块通过查看路由表，决定所处理包的下一跳。然后，对包进行排列以便将它们传送到输出接口（Output Interface）。&lt;/p>
&lt;p>&lt;strong>一般我们只能限制网卡发送的数据包，不能限制网卡接收的数据包&lt;/strong>，所以我们可以通过改变发送次序来控制传输速率。Linux 流量控制主要是在&lt;strong>输出接口排列&lt;/strong>时进行处理和实现的。&lt;/p>
&lt;h1 id="tc-规则">TC 规则&lt;/h1>
&lt;h2 id="1流量控制方式">1.流量控制方式&lt;/h2>
&lt;p>流量控制包括一下几种方式：&lt;/p>
&lt;h3 id="shaping限制">SHAPING（限制）&lt;/h3>
&lt;p>当流量被限制时，它的传输速率就被控制在某个值以下。限制值可以大大小于有效带宽，这样可以平滑突发数据流量，使网络更为稳定。SHAPING（限制）只适用于向外的流量。&lt;/p>
&lt;h3 id="scheduling调度">SCHEDULING（调度）&lt;/h3>
&lt;p>通过调度数据包的传输，可以在带宽范围内，按照优先级分配带宽。SCHEDULING（调度）也只适用于向外的流量。&lt;/p>
&lt;h3 id="policing策略">POLICING（策略）&lt;/h3>
&lt;p>SHAPING（限制）用于处理向外的流量，而 POLICING（策略）用于处理接收到的数据。&lt;/p>
&lt;h3 id="dropping丢弃">DROPPING（丢弃）&lt;/h3>
&lt;p>如果流量超过某个设定的带宽，就丢弃数据包，不管是向内还是向外。&lt;/p>
&lt;h2 id="2流量控制处理对象">2.流量控制处理对象&lt;/h2>
&lt;p>流量的处理由三种对象控制，它们是**：qdisc（排队规则）、class（类别）和 filter（过滤器**）。&lt;/p>
&lt;p>qdisc（排队规则）是 queueing discipline 的简写，它是理解流量控制（traffic control）的基础。**无论何时，内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。**然后，内核会尽可能多的从 qdisc 里面取出数据包，把它们交给网络适配器驱动模块。最简单的 qdisc 是 pfifo 他不对进入的数据包做任何的处理，数据包采用先进先出的方式通过队列。不过，它会保存网络接口一时无法处 理的数据包。&lt;/p>
&lt;p>qddis（排队规则）分为 CLASSLESS QDISC 和 CLASSFUL QDISC   类别如下：&lt;/p>
&lt;h3 id="classless-qdisc-无类别-qdisc">CLASSLESS QDISC （无类别 QDISC）&lt;/h3>
&lt;h4 id="1无类别-qdisc-包括">1.无类别 QDISC 包括：&lt;/h4>
&lt;p>**              [ p | b ]fifo**,使用最简单的 qdisc（排队规则），纯粹的先进先出。只有一个参数：limit ，用来设置队列的长度，pfifo 是以数据包的个数为单位；bfifo 是以字节数为单位。&lt;/p>
&lt;hr>
&lt;p>**               pfifo_fast**，在编译内核时，如果打开了高级路由器（Advanced Router）编译选项，pfifo_fast 就是系统的标准 qdisc(排队规则)。它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。而三个波段（band）的优先级也不相同，band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样的。数据包是按照服务类型（Type Of Service，TOS ）被分配到三个波段（band）里面的。&lt;/p>
&lt;p>**               red**，red 是 Random Early Detection（随机早期探测）的简写。如果使用这种 qdsic，当带宽的占用接近与规定的带宽时，系统会随机的丢弃一些数据包。他非常适合高带宽的应用。&lt;/p>
&lt;p>**              sfq**，sfq 是 Stochastic Fairness Queueing 的简写。它会按照会话（session &amp;ndash;对应与每个 TCP 连接或者 UDP 流）为流量进行排序，然后循环发送每个会话的数据包。&lt;/p>
&lt;p>**             tbf**，tbf 是 Token Bucket Filter 的简写，适用于把流速降低到某个值。&lt;/p>
&lt;h4 id="2无类别-qdisc-的配置">2.无类别 qdisc 的配置&lt;/h4>
&lt;p>如果没有可分类 qdisc，不可分类 qdisc 只能附属于设备的根。它们的用法如下:&lt;/p>
&lt;pre>&lt;code>tc qdisc add dev DEV root QDISC QDISC_PARAMETERS
&lt;/code>&lt;/pre>
&lt;p>要删除一个不可分类 qdisc，需要使用如下命令&lt;/p>
&lt;pre>&lt;code>tc qdisc del dev DEV root
&lt;/code>&lt;/pre>
&lt;p>一个网络接口上如果没有设置 qdisc，pfifo_fast 就作为缺省的 qdisc。&lt;/p>
&lt;h3 id="classful-qdisc分类-qdisc">CLASSFUL QDISC(分类 QDISC)&lt;/h3>
&lt;h4 id="可分类-qdisc-包括">可分类 QDISC 包括：&lt;/h4>
&lt;p>**   CBQ**，CBQ 是 Class Based Queueing（基于类别排队）的缩写。它实现了一个丰富的连接共享类别结构，既有限制（shaping）带宽的能力，也具有带宽优先级别管理的能力。带宽限制是通过计算连接的空闲时间完成的。空闲时间的计算标准是数据包离队事件的频率和下层连接（数据链路层）的带宽。&lt;/p>
&lt;p>** HTB**，HTB 是 Hierarchy Token Bucket 的缩写。通过在实践基础上的改进，它实现一个丰富的连接共享类别体系。使用 HTB 可以很容易地保证每个类别的带宽，虽然它也允许特定的类可以突破带宽上限，占用别的类的带宽。HTB 可以通过 TBF（Token Bucket Filter）实现带宽限制，也能够划分类别的优先级。&lt;/p>
&lt;p>&lt;strong>PRIO&lt;/strong>，PRIO qdisc 不能限制带宽，因为属于不同类别的数据包是顺序离队的。使用 PRIO qdisc 可以很容易对流量进行优先级管理，只有属于高优先级类别的数据包全部发送完毕，参会发送属于低优先级类别的数据包。为了方便管理，需要使用 iptables 或者 ipchains 处理数据包的服务类型（Type Of Service，TOS）。&lt;/p>
&lt;p>HTB 型的一些注意：
1. HTB 型 class 具有优先级，prio。可以指定优先级，数字低的优先级高，优先级范围从 0~7，0 最高。
它的效果是：存在空闲带宽时，优先满足高优先级 class 的需求，使得其可以占用全部空闲带宽，上限为 ceil 所指定的值。若此时还有剩余空闲带宽，则优先级稍低的 class 可以借用之。依优先级高低，逐级分配。&lt;/p>
&lt;pre>&lt;code>2. 相同优先级的class分配空闲带宽时，按照自身class所指定的rate（即保证带宽）之间的比例瓜分空闲带宽。
例如：clsss A和B优先级相同，他们的rate比为3：5，则class A占用空闲的3/8，B占5/8。
3. tc filter的prio使用提示，prio表示该filter的测试顺序，小的会优先进行测试，如果匹配到，则不会继续测试。故此filter 的prio跟class的prio并不一样，class的prio表明了此class的优先级：当有空闲带宽时prio 数值低的（优先级高）class会优先占用空闲。
若不同优先级的filter都可以匹配到包，则优先级高的filter起作用。相同优先级的filter（必须为同种类型classifier）严格按照命令的输入顺序进行匹配，先执行的filter起作用。
&lt;/code>&lt;/pre>
&lt;h4 id="------操作原理">**           操作原理**&lt;/h4>
&lt;p>**             **类（class）组成一个树，每个类都只有一个父类，而一个类可以有多个子类。某些 qdisc （例如：CBQ 和 HTB）允许在运行时动态添加类，而其它的 qdisc（例如：PRIO）不允许动态建立类。允许动态添加类的 qdisc 可以有零个或者多个子类，由它们为数据包排队。此外，每个类都有一个叶子 qdisc，默认情况下，这个也在 qdisc 有可分类，不过每个子类只能有一个叶子 qdisc。 当一个数据包进入一个分类 qdisc，它会被归入某个子类。我们可以使用一下三种方式为数据包归类，不过不是所有的 qdisc 都能够使用这三种方式。&lt;/p>
&lt;p>如果过滤器附属于一个类，相关的指令就会对它们进行查询。过滤器能够匹配数据包头所有的域，也可以匹配由 ipchains 或者 iptables 做的标记。&lt;/p>
&lt;p>树的每个节点都可以有自己的过滤器，但是高层的过滤器也可以一直接用于其子类。如果数据包没有被成功归类，就会被排到这个类的叶子 qdisc 的队中。相关细节在各个 qdisc 的手册页中。&lt;/p>
&lt;h4 id="命名规则">&lt;strong>命名规则&lt;/strong>&lt;/h4>
&lt;p>所有的 qdisc、类、和过滤器都有 ID。ID 可以手工设置，也可以由内核自动分配。ID 由一个主序列号和一个从序列号组成，两个数字用一个冒号分开。&lt;/p>
&lt;p>qdisc，一个 qdisc 会被分配一个主序列号，叫做句柄（handle），然后把从序列号作为类的命名空间。句柄才有像 1:0 一样的表达方式。习惯上，需要为有子类的 qdisc 显式的分配一个句柄。&lt;/p>
&lt;p>类（Class），在同一个 qdisc 里面的类共享这个 qdisc 的主序列号，但是每个类都有自己的从序列号，叫做类识别符（classid）。类识别符只与父 qdisc 有关，与父类无关。类的命名习惯和 qdisc 相同。&lt;/p>
&lt;p>过滤器（Filter），过滤器的 ID 有三部分，只有在对过滤器进行散列组织才会用到。详情请参考 tc-filtes 手册页。&lt;/p>
&lt;h4 id="-----单位">**         单位**&lt;/h4>
&lt;p>**            **tc 命令所有的参数都可以使用浮点数，可能会涉及到以下计数单位。&lt;/p>
&lt;h5 id="带宽或者流速单位">带宽或者流速单位：&lt;/h5>
&lt;p>&lt;img src=".%5CImages%5C20150511134819738.png#alt=" alt="">&lt;/p>
&lt;h5 id="数据的数量单位">数据的数量单位&lt;/h5>
&lt;p>&lt;img src=".%5CImages%5C20150511135207564.png#alt=" alt="">&lt;/p>
&lt;h5 id="时间的计量单位">时间的计量单位：&lt;/h5>
&lt;p>&lt;img src=".%5CImages%5C20150511135126939.png#alt=" alt="">&lt;/p>
&lt;h1 id="tc-的安装">TC 的安装&lt;/h1>
&lt;p>TC 是 Linux 自带的模块，一般情况下不需要另行安装，可以用 man tc 查看 tc 相关命令细节，tc 要求内核 2.4.18 以上&lt;/p>
&lt;pre>&lt;code>TC的安装
TC是Linux自带的模块，一般情况下不需要另行安装，可以用 man tc 查看tc 相关命令细节，tc 要求内核 2.4.18 以上
&lt;/code>&lt;/pre>
&lt;h1 id="tc-命令">TC 命令&lt;/h1>
&lt;p>tc 可以使用以下命令对 qdisc、类和过滤器进行操作：&lt;/p>
&lt;p>add， 在一个节点里加入一个 qdisc、类、或者过滤器。添加时，需要传递一个祖先作为参数，传递参数时既可以使用 ID 也跨越式直接传递设备的根。如果要建立一个 qdisc 或者过滤器，可以使用句柄（handle）来命名。如果要建立一个类，可以使用类识别符（classid）来命名。&lt;/p>
&lt;p>remove， 删除由某个句柄（handle）指定的 qdisc，根 qdisc（root）也可以删除。被删除 qdisc 上所有的子类以及附属于各个类的过滤器都会被自动删除。&lt;/p>
&lt;p>change， 以替代的方式修改某些条目。除了句柄（handle）和祖先不能修改以外，change 命令的语法和 add 命令相同。换句话说，change 命令不能指定节点的位置。&lt;/p>
&lt;p>replace， 对一个现有节点进行近于原子操作的删除/添加。如果节点不存在，这个命令就会建立节点。&lt;/p>
&lt;p>link， 只适用于 qdisc，替代一个现有的节点。&lt;/p>
&lt;p>tc 命令的格式：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> tc qdisc &lt;span style="color:#f92672">[&lt;/span> add | change | replace | link &lt;span style="color:#f92672">]&lt;/span> dev DEV &lt;span style="color:#f92672">[&lt;/span> parent qdisc-id | root &lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#f92672">[&lt;/span> handle qdisc-id &lt;span style="color:#f92672">]&lt;/span> qdisc &lt;span style="color:#f92672">[&lt;/span> qdisc specific parameters &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc class &lt;span style="color:#f92672">[&lt;/span> add | change | replace &lt;span style="color:#f92672">]&lt;/span> dev DEV parent qdisc-id &lt;span style="color:#f92672">[&lt;/span> classid class-id &lt;span style="color:#f92672">]&lt;/span> qdisc &lt;span style="color:#f92672">[&lt;/span> qdisc specific parameters &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc filter &lt;span style="color:#f92672">[&lt;/span> add | change | replace &lt;span style="color:#f92672">]&lt;/span> dev DEV &lt;span style="color:#f92672">[&lt;/span> parent qdisc-id | root &lt;span style="color:#f92672">]&lt;/span> protocol protocol prio priority filtertype &lt;span style="color:#f92672">[&lt;/span> filtertype specific param‐eters &lt;span style="color:#f92672">]&lt;/span> flowid flow-id
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc &lt;span style="color:#f92672">[&lt;/span> FORMAT &lt;span style="color:#f92672">]&lt;/span> qdisc show &lt;span style="color:#f92672">[&lt;/span> dev DEV &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc &lt;span style="color:#f92672">[&lt;/span> FORMAT &lt;span style="color:#f92672">]&lt;/span> class show dev DEV
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc filter show dev DEV
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">###### FORMAT := { -s[tatistics] | -d[etails] | -r[aw] | -p[retty] | i[ec] }&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="具体操作">具体操作&lt;/h1>
&lt;p>Linux 流量控制主要分为建立队列、建立分类和建立过滤器三个方面。&lt;/p>
&lt;h2 id="--基本实现步骤">**   基本实现步骤**。&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>针对网络物理设备（如以太网卡 eth0）绑定一个队列 qdisc；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在该队列上建立分类 class；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为每一分类建立一个基于路由的过滤器 filter；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后与过滤器相配合，建立特定的路由表。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="-环境模拟实例">** 环境模拟实例**。&lt;/h2>
&lt;p>流量控制器上的以太网卡（eth0）的 IP 地址为 192.168.1.66， 在其上建立一个 CBQ 队列。假设包的平均大小为 1000 字节，包间隔发送单元的大小             为 8 字节，可接收冲突的发送最长包的数目为 20 字节。&lt;/p>
&lt;p>加入有三种类型的流量需要控制：&lt;/p>
&lt;p>1）是发往主机 1 的，其 IP 地址为 192.168.1.24。其流量带宽控制在 8Mbit，优先级为 2；&lt;/p>
&lt;p>2）是发往主机 2 的，其 IP 地址为 192.168.1.30。其流量带宽控制在 1Mbit，优先级为 1；&lt;/p>
&lt;p>3）是发往子网 1 的，其子网号为 192.168.1.0。子网掩码为 255.255.255.0。流量带宽控制在 1Mbit，优先级为 6。&lt;/p>
&lt;h2 id="建立队列">建立队列&lt;/h2>
&lt;p>一般情况下，针对一个网卡只需建立一个队列。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">###将一个cbq队列绑定到网络物理设备eth0上，其编号为1:0；网络物理设备eth0的实际带宽为10Mbit，包的平均大小为1000字节；包间隔发送单元的大小为8字节，最小传输包大小为64字节。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc qdisc add dev eth0 root handle 1: cbq bandwidth 10Mbit avpkt &lt;span style="color:#ae81ff">1000&lt;/span> cell &lt;span style="color:#ae81ff">8&lt;/span> mpu &lt;span style="color:#ae81ff">64&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="建立分类">建立分类&lt;/h2>
&lt;p>一般情况下，针对一个队列需建立一个根分类，然后再在其上建立子分类。对于分类，按其分类的编号顺序起作用，编号小的优先；一但符合某个分类匹配规则，通过该分类发送数据包，则其后的分类不在起作用。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">##创建根分类1:1；分配带宽为10Mbit，优先级别为8.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc class add dev eth0 parent 1:0 classid 1:1 cbq bandidth 10Mbit rate 10Mbit maxburst &lt;span style="color:#ae81ff">20&lt;/span> allot &lt;span style="color:#ae81ff">1514&lt;/span> prio &lt;span style="color:#ae81ff">8&lt;/span> avpkt &lt;span style="color:#ae81ff">1000&lt;/span> cell &lt;span style="color:#ae81ff">8&lt;/span> weight 1Mbit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">###该队列的最大可用带宽为10Mbit，实际分配的带宽为10Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为8，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相当于实际带宽的加权速率为1Mbit。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 创建分类1:2，其父类为1:1，分配带宽为 8Mbit， 优先级别为2.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc class add dev eth0 parent 1:1 cbq bandwidth 10Mbit rate 8Mbit maxburst &lt;span style="color:#ae81ff">20&lt;/span> allot &lt;span style="color:#ae81ff">1514&lt;/span> prio &lt;span style="color:#ae81ff">2&lt;/span> avpkt &lt;span style="color:#ae81ff">1000&lt;/span> cell &lt;span style="color:#ae81ff">8&lt;/span> weight 800Kbit split 1:0 bounded
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 该队列的最大可用带宽为10Mbit，实际分配的带宽为8Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为2，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相当于实际带宽的加权速率为800Kbit，分类的分离点为1:0，且不可借用未使用带宽。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 创建分类 1:4，其父分类为1:1，分配带宽为1Mbit，优先级别为6。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc class add dev eth0 parent 1:1 classid 1:4 cbq bandwidth &lt;span style="color:#ae81ff">10&lt;/span> Mbit rate 1Mbit maxburst &lt;span style="color:#ae81ff">20&lt;/span> allot &lt;span style="color:#ae81ff">1514&lt;/span> prio &lt;span style="color:#ae81ff">6&lt;/span> avpkt &lt;span style="color:#ae81ff">1000&lt;/span> cell &lt;span style="color:#ae81ff">8&lt;/span> weight 100Kbit split 1:0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 该队列的最大可用带宽为10Mbit，实际分配的带宽为1Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为6，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相当于实际带宽的加权速率为100Kbit，分类的分离点为1:0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="创建过滤器">创建过滤器&lt;/h2>
&lt;p>过滤器主要服务于分类。&lt;/p>
&lt;p>一般只需针对根分类提供一个过滤器，然后为每个子分类提供路由映射。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 1)应用路由分类器到cbq队列的根，父分类编号为1:0；过滤协议为ip，优先级别为100，过滤器为基于路由表。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc filter add dev eth0 parent 1:0 protocol ip prio &lt;span style="color:#ae81ff">100&lt;/span> route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 2)建立路由映射分类 1:2 , 1:3 , 1:4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc filter add dev eth0 parent 1:0 protocol ip prio &lt;span style="color:#ae81ff">100&lt;/span> route to &lt;span style="color:#ae81ff">2&lt;/span> flowid 1:2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc filter add dev eth0 parent 1:0 protocol ip prio &lt;span style="color:#ae81ff">100&lt;/span> route to &lt;span style="color:#ae81ff">3&lt;/span> flowid 1:3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc filter add dev eth0 parent 1:0 protocol ip prio &lt;span style="color:#ae81ff">100&lt;/span> route to &lt;span style="color:#ae81ff">4&lt;/span> flowid 1:4
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="建立路由">建立路由&lt;/h2>
&lt;p>该路由是与前面所建立的路由映射一一对应。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 1)发往主机192.168.1.24的数据包通过分类2转发（分类2的速率8Mbit）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip route add 192.168.1.24 dev eth0 via 192.168.1.66 realm &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 2)发往主机192.168.1.30的数据包通过分类3转发（分类3的速率1Mbit）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip route add 192.168.1.30 dev eth0 via 192.168.1.66 realm &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 3)发往子网192.168.1.0/24 的数据包通过分类4转发（分类4的速率1Mbit）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip route add 192.168.1.0/24 dev eth0 via 192.168.1.66 realm &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>注：一般对于流泪控制器所直接连接的网段建议使用IP主机地址流量控制限制，不要使用子网流量控制限制。如一定需要对直连子网使用子网流量控制限制，则在建立该子网的路由映射前，需将原先由系统建立的路由删除，才可以完成相应步骤
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="监视">监视&lt;/h2>
&lt;p>主要包括对现有队列、分类、过滤器和路由状况进行监视。&lt;/p>
&lt;p>1）显示队列的状况&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 简单显示指定设备（这里为eth0）的队列状况&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc qdisc ls dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> qdisc cbq 1: rate 10Mbit &lt;span style="color:#f92672">(&lt;/span>bounded,isolated&lt;span style="color:#f92672">)&lt;/span> prio no-transmit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 详细显示指定设备（这里为eth0）的队列状况&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc -s qdisc ls dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> qdisc cbq 1: rate 10Mbit &lt;span style="color:#f92672">(&lt;/span>bounded,isolated&lt;span style="color:#f92672">)&lt;/span> prio no-transmit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Sent &lt;span style="color:#ae81ff">7646731&lt;/span> bytes &lt;span style="color:#ae81ff">13232&lt;/span> pkts &lt;span style="color:#f92672">(&lt;/span>dropped 0, overlimits 0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> borrowed &lt;span style="color:#ae81ff">0&lt;/span> overactions &lt;span style="color:#ae81ff">0&lt;/span> avgidle &lt;span style="color:#ae81ff">31&lt;/span> undertime &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 这里主要显示了通过该队列发送了13232个数据包，数据流量为7646731个字节，丢弃的包数目为0，超过速率限制的包数目为0。&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>2）显示分类的状况&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 简单显示指定设备（这里为eth0）的分类状况&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc class ls dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1: root rate 10Mbit &lt;span style="color:#f92672">(&lt;/span>bounded,isolated&lt;span style="color:#f92672">)&lt;/span> prio no-transmit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:1 parent 1: rate 10Mbit prio no-transmit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:2 parent 1:1 rate 8Mbit prio &lt;span style="color:#f92672">(&lt;/span>bounded&lt;span style="color:#f92672">)&lt;/span> prio &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:3 parent 1:1 rate 1Mbit prio &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:4 parent 1:1 rate 1Mbit prio &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 详细显示指定设备（这里为eth0）的分类状况&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tc -s class ls dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1: root rate 10000Kbit &lt;span style="color:#f92672">(&lt;/span>bounded,isolated&lt;span style="color:#f92672">)&lt;/span> prio no-transmit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Sent &lt;span style="color:#ae81ff">17725304&lt;/span> bytes &lt;span style="color:#ae81ff">32088&lt;/span> pkt &lt;span style="color:#f92672">(&lt;/span>dropped 0, overlimits &lt;span style="color:#ae81ff">0&lt;/span> requeues 0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backlog 0b 0p requeues &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> borrowed &lt;span style="color:#ae81ff">0&lt;/span> overactions &lt;span style="color:#ae81ff">0&lt;/span> avgidle &lt;span style="color:#ae81ff">31&lt;/span> undertime &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:1 parent 1: rate 10000Kbit prio no-transmit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Sent &lt;span style="color:#ae81ff">16627774&lt;/span> bytes &lt;span style="color:#ae81ff">28884&lt;/span> pkts &lt;span style="color:#f92672">(&lt;/span>dropped 0, overlimits &lt;span style="color:#ae81ff">0&lt;/span> requeues 0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backlog 0b 0p requeues &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> borrowed &lt;span style="color:#ae81ff">16163&lt;/span> overactions &lt;span style="color:#ae81ff">0&lt;/span> avgidle &lt;span style="color:#ae81ff">587&lt;/span> undertime &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:2 parent 1:1 rate 8000Kbit &lt;span style="color:#f92672">(&lt;/span>bounded&lt;span style="color:#f92672">)&lt;/span> prio &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Sent &lt;span style="color:#ae81ff">628829&lt;/span> bytes &lt;span style="color:#ae81ff">3130&lt;/span> pkts &lt;span style="color:#f92672">(&lt;/span>dropped 0, overlimits &lt;span style="color:#ae81ff">0&lt;/span> requeues 0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backlog 0b 0p requeues &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> borrowed &lt;span style="color:#ae81ff">0&lt;/span> overactions &lt;span style="color:#ae81ff">0&lt;/span> avgidle &lt;span style="color:#ae81ff">4137&lt;/span> undertime &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:3 parent 1:1 rate 1000Kbit prio &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Sent &lt;span style="color:#ae81ff">0&lt;/span> bytes &lt;span style="color:#ae81ff">0&lt;/span> pkts &lt;span style="color:#f92672">(&lt;/span>dropped 0, overlimits &lt;span style="color:#ae81ff">0&lt;/span> requeues 0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backlog 0b 0p requeues &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> borrowed &lt;span style="color:#ae81ff">0&lt;/span> overactions &lt;span style="color:#ae81ff">0&lt;/span> avgidle 3.19309e+06 undertime &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class cbq 1:4 parent 1:1 rate 1000Kbit prio &lt;span style="color:#ae81ff">6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Sent &lt;span style="color:#ae81ff">552879&lt;/span> bytes &lt;span style="color:#ae81ff">8076&lt;/span> pkts &lt;span style="color:#f92672">(&lt;/span>dropped 0, overlimits &lt;span style="color:#ae81ff">0&lt;/span> requeues 0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backlog 0b 0p requeues &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> borrowed &lt;span style="color:#ae81ff">3797&lt;/span> overactions &lt;span style="color:#ae81ff">0&lt;/span> avgidle &lt;span style="color:#ae81ff">159557&lt;/span> undertime &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">##这里主要显示了通过不同分类发送的数据包，数据流量，丢弃的包数目，超过速率限制的包数目等等。其中根分类(class cbq 1:0)的状况与队列的状况类似&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">##例如，分类class cbq 1:4 发送了8076个包，数据流量为 552879 个字节，丢弃的包数目为0，超过速率限制的包数目为0。&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>3）显示过滤器的状况&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> tc -s filter ls dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filter parent 1: protocol ip pref &lt;span style="color:#ae81ff">100&lt;/span> route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filter parent 1: protocol ip pref &lt;span style="color:#ae81ff">100&lt;/span> route fh 0xffff0002 flowid 1:2 to &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filter parent 1: protocol ip pref &lt;span style="color:#ae81ff">100&lt;/span> route fh 0xffff0003 flowid 1:3 to &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filter parent 1: protocol ip pref &lt;span style="color:#ae81ff">100&lt;/span> route fh 0xffff0004 flowid 1:4 to &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">## 这里flowid 1:2 代表分类 class cbq 1:2, to 2 代表通过路由 2 发送&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>4）显示现有路由的状况&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> ip route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> default via 192.168.1.1 dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.66
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 192.168.1.24 via 192.168.1.66 dev eth0 realm &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 192.168.1.30 via 192.168.1.66 dev eth0 realm &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">#如上所示，结尾包含有realm 的显示行是起作用的路由过滤器。&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="维护">维护&lt;/h2>
&lt;p>主要包括对队列、分类、过滤器和路由的增添、修改和删除。&lt;/p>
&lt;p>增添动作一般依照   队列 -&amp;gt; 分类 -&amp;gt; 过滤器 -&amp;gt; 路由   的顺序进行；修改动作则没有什么要求；删除则依照   路由 -&amp;gt; 过滤器 -&amp;gt; 分类 -&amp;gt; 队列   的顺序进行。&lt;/p>
&lt;p>1）队列的维护&lt;/p>
&lt;p>一般对于一台流量控制器来说，出厂时针对每个以太网卡均已配置好一个队列了，通常情况下对队列无需进行增添、修改和删除动作。&lt;/p>
&lt;p>2）分类的维护&lt;/p>
&lt;p>增添，增添动作通过 tc class add 命令实现，如前面所示。&lt;/p>
&lt;p>修改，修改动作通过 tc class change 命令实现，如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>tc class change dev eth0 parent 1:1 classid 1:2 cbq bandwidth 10Mbit rate 7Mbit maxburst &lt;span style="color:#ae81ff">20&lt;/span> allot &lt;span style="color:#ae81ff">1514&lt;/span> prio &lt;span style="color:#ae81ff">2&lt;/span> avpkt &lt;span style="color:#ae81ff">1000&lt;/span> cell &lt;span style="color:#ae81ff">8&lt;/span> weigth 700Kbit split 1:0 bounded
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>对于 bounded 命令应慎用，一旦添加后就进行修改，只可通过删除后再添加来实现。&lt;/p>
&lt;p>删除，删除动作只在该分类没有工作前才可以进行，一旦通过该分类发送过数据，则无法删除它。因此，需要通过 shell 文件方式来修改，通过重新启动来完成删除动作。&lt;/p>
&lt;p>3）过滤器的维护&lt;/p>
&lt;p>增添，增添动作通过 tc filter add 命令实现，如前面所示。&lt;/p>
&lt;p>修改，修改动作通过 tc filter change 命令实现，如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>tc filter change dev eth0 parent 1:0 protocol ip prio &lt;span style="color:#ae81ff">100&lt;/span> route to &lt;span style="color:#ae81ff">10&lt;/span> flowid 1:8
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> 删除，删除动作通过 tc filter del 命令实现，如下所示：
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>tc filter del dev eth0 parent 1:0 protocol ip prio &lt;span style="color:#ae81ff">100&lt;/span> route to &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>4)与过滤器————映射路由的维护&lt;/p>
&lt;p>增添，增添动作通过 ip route add 命令实现，如前面所示。&lt;/p>
&lt;p>修改，修改动作通过 ip route change 命令实现，如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> ip route change 192.168.1.30 dev eth0 via 192.168.1.66 realm &lt;span style="color:#ae81ff">8&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code> 删除，删除动作通过 ip route del 命令实现，如下所示：
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span> ip route del 192.168.1.30 dev eth0 via 192.168.1.66 realm &lt;span style="color:#ae81ff">8&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ip route del 192.168.1.0/24 dev eth0 via 192.168.1.66 realm &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-dns-%E7%AE%A1%E7%90%86/%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://man7.org/linux/man-pages/man5/resolv.conf.5.html">man 手册,resolv.conf&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man5/nsswitch.conf.5.html">man 手册,nsswitch.conf&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="etcresolvconf-详解">/etc/resolv.conf 详解&lt;/h1>
&lt;p>该文件可以手动管理，或者由某个应用程序(e.g.NetworkManager)来管理。如果由应用程序管理，则会在文件开头以#进行备注&lt;/p>
&lt;p>配置文件格式：每行以一个关键字开头，后面接一个或多个由空格隔开的参数。&lt;/p>
&lt;p>下面是一个最简单的配置文件示例&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nameserver 114.114.114.114
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nameserver 202.96.128.166
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="关键字详解">关键字详解：&lt;/h2>
&lt;h3 id="nameserver-ip--指定用来进行域名解析的服务器地址">nameserver IP # 指定用来进行域名解析的服务器地址&lt;/h3>
&lt;p>IP 可以是 ipv4 或者 ipv6 的地址，该关键字最多配置 3 个。当配置大于 1 个的 nameserver 时，则 reslover 按列出的顺序使用。&lt;/p>
&lt;h3 id="search-string--指定域名的搜索列表">search STRING # 指定域名的搜索列表&lt;/h3>
&lt;p>解析域名时，会搜索 search 关键字指定的 STRING ，并将 STRING 附加到要解析的域名后方。效果如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /etc/resolv.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nameserver 114.114.114.114
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ping www&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ping: www: Name or service not known
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /etc/resolv.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nameserver 114.114.114.114
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>search qq.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ping www&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING https.qq.com &lt;span style="color:#f92672">(&lt;/span>123.151.137.18&lt;span style="color:#f92672">)&lt;/span> 56&lt;span style="color:#f92672">(&lt;/span>84&lt;span style="color:#f92672">)&lt;/span> bytes of data.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">64&lt;/span> bytes from 123.151.137.18 &lt;span style="color:#f92672">(&lt;/span>123.151.137.18&lt;span style="color:#f92672">)&lt;/span>: icmp_seq&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> ttl&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">55&lt;/span> time&lt;span style="color:#f92672">=&lt;/span>2.17 ms
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>如果使用 FQDN (即在域名最后加个 . )，那么解析不会通过 search ，而是直接解析指定的域名&lt;/p>
&lt;/li>
&lt;li>
&lt;p>可以指定多个可供搜索的 STRING，在搜索时，从左至右依次尝试。比如我定义了 search qq.com qq1.com qq2.com 这个搜索域，那么在我 ping www 时，解析过程如下：&lt;/p>
&lt;ol>
&lt;li>先解析 &lt;a href="https://www.qq.com">www.qq.com&lt;/a>，如果解析无结果；则解析 &lt;a href="https://www.qq1.com">www.qq1.com&lt;/a>，如果无结果；则解析 &lt;a href="https://www.qq2.com">www.qq2.com&lt;/a>，如果无结果则返回解析失败的信息。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="sortlist-string">sortlist STRING&lt;/h3>
&lt;h3 id="options-string--通过-options-指定的参数来配置-resolver-的运行方式">options STRING # 通过 options 指定的参数来配置 resolver 的运行方式&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>**attempts:NUM **# 当一个 nameserver 上域名查询失败时进行重试的次数，默认是 2 次。当重试次数达到上限则返回错误信息:无法进行域名解析&lt;/p>
&lt;/li>
&lt;li>
&lt;p>**ndots:NUM **# 指定域名中必须出现的 . 的个数。默认值为 1，上限为 15。&lt;/p>
&lt;ul>
&lt;li>如果 &lt;code>.&lt;/code> 的个数小于 NUM，则会在 search 关键字指定的搜索列表中进行查询；否则直接查询指定域名，当查询无结果时，再去 search 列表查询。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>**timeout:NUM **#在一个 nameserver 上进行域名查询的超时时间，单位是秒&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Note：如果想要测试 resolv.conf 文件，不要使用 dig, host, nslook 这类工具，因为他们并没有调用 resolver 的库(i.e.resolv.conf 文件中的 option 内的设置不会生效)可以使用 getent 来测试。一般情况下正常的应用程序，都会调用 resolver，并使用 resolv.conf 文件(比如 ping 程序)。&lt;/p>
&lt;p>应用示例&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>search test.example.com example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>options timeout:1 attempts:1 rotate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nameserver 192.168.0.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nameserver 192.168.0.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>假设 192.168.0.1 不返回结果(可能根本就不是台 DNS)，我们假设需要解析 &amp;ldquo;www.devel&amp;rdquo;，而这个仅仅在 &lt;a href="https://www.devel.example.com">www.devel.example.com&lt;/a> 里面有记录，下面是整个执行的步骤:&lt;/p>
&lt;ol>
&lt;li>&amp;ldquo;www.devel&amp;rdquo; on 192.168.0.1, wait until timeout (default 5 secs)&lt;/li>
&lt;li>&amp;ldquo;www.devel&amp;rdquo; on 192.168.0.2, get reply: no such hostname&lt;/li>
&lt;li>&amp;ldquo;www.devel&amp;rdquo; on 192.168.0.1, wait until timeout (default 5 secs)&lt;/li>
&lt;li>&amp;ldquo;www.devel&amp;rdquo; on 192.168.0.2, get reply: no such hostname&lt;/li>
&lt;li>&amp;ldquo;&lt;a href="https://www.devel.test.example.com">www.devel.test.example.com&lt;/a>&amp;rdquo; on 192.168.0.1, wait until timeout (default 5 secs)&lt;/li>
&lt;li>&amp;ldquo;&lt;a href="https://www.devel.test.example.com">www.devel.test.example.com&lt;/a>&amp;rdquo; on 192.168.0.2, reply no such hostname&lt;/li>
&lt;li>&amp;ldquo;&lt;a href="https://www.devel.test.example.com">www.devel.test.example.com&lt;/a>&amp;rdquo; on 192.168.0.1, wait until timeout (default 5 secs)&lt;/li>
&lt;li>&amp;ldquo;&lt;a href="https://www.devel.test.example.com">www.devel.test.example.com&lt;/a>&amp;rdquo; on 192.168.0.2, reply no such hostname&lt;/li>
&lt;li>&amp;ldquo;&lt;a href="https://www.devel.example.com">www.devel.example.com&lt;/a>&amp;rdquo; on 192.168.0.1, wait until timeout (default 5 secs)&lt;/li>
&lt;li>&amp;ldquo;&lt;a href="https://www.devel.example.com">www.devel.example.com&lt;/a>&amp;rdquo; on 192.168.0.2, reply with IP address&lt;/li>
&lt;/ol>
&lt;p>默认情况下是 5s 超时，我做了两个简单的测试，把 resolv.conf 的前三个 nameserver 全部换成不存在的 1.1.1.1, 2.2.2.2, 3.3.3.3，然后可以观察下面 strace 跟踪的结果，对于 ping 以及 getent 来说，已经算是上层的应用结果了。
1. strace -t getent hosts baidu.com
2. strace ping baidu.com&lt;/p>
&lt;p>把 timeout 设置为 1s 的结果可以看下面这个测试结果:
strace -t ping baidu.com(options timeout:1)&lt;/p>
&lt;h1 id="etcnsswitchconf-详解">/etc/nsswitch.conf 详解&lt;/h1>
&lt;p>该文件是纯 ASCII 文本，列由空格或制表符分隔。第一列指定数据库名称。其余列描述了要查询的源的顺序以及可以由查找结果执行的有限的一组操作。&lt;/p>
&lt;p>注意：如果是某个程序使用 nsswitch.conf 或者系统中没有该文件，则 /etc/hosts 文件也就不会被使用。&lt;/p>
&lt;p>示例配置：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>root&lt;span style="color:#960050;background-color:#1e0010">@&lt;/span>lichenhao:&lt;span style="color:#f92672">~/&lt;/span>test_dir&lt;span style="color:#f92672">/&lt;/span>c&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> cat &lt;span style="color:#f92672">/&lt;/span>etc&lt;span style="color:#f92672">/&lt;/span>nsswitch.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /etc/nsswitch.conf
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Example configuration of GNU Name Service Switch functionality.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># If you have the `glibc-doc-reference&amp;#39; and `info&amp;#39; packages installed, try:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># `info libc &amp;#34;Name Service Switch&amp;#34;&amp;#39; for information about this file.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>passwd: files systemd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>group: files systemd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shadow: files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gshadow: files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hosts: files mdns4_minimal [NOTFOUND&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">return&lt;/span>] dns
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>networks: files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>protocols: db files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>services: db files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ethers: db files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rpc: db files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>netgroup: nis
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>以下数据库由 GNU C 库理解:&lt;/p></description></item><item><title>Docs: 使用 iptables 对多租户环境中的 TCP 限速</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/iptablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/%E4%BD%BF%E7%94%A8-iptables-%E5%AF%B9%E5%A4%9A%E7%A7%9F%E6%88%B7%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84-tcp-%E9%99%90%E9%80%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/netfilter-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/iptablesnetfilter-%E7%9A%84%E5%AE%9E%E7%8E%B0/%E4%BD%BF%E7%94%A8-iptables-%E5%AF%B9%E5%A4%9A%E7%A7%9F%E6%88%B7%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84-tcp-%E9%99%90%E9%80%9F/</guid><description>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/n7bRJb-u5bzIj4TMb8JE-A">使用 iptables 对多租户环境中的 TCP 限速&lt;/a>&lt;/p>
&lt;p>我们有个服务以类似 SideCar 的方式和应用一起运行，SideCar 和应用通过 Unix Domain Socket 进行通讯。为了方便用户，在开发的时候不必在自己的开发环境中跑一个 SideCar，我用 socat 在一台开发环境的机器上 map UDS 到一个端口。这样用户在开发的时候就可以直接通过这个 TCP 端口测试服务，而不用自己开一个 SideCar 使用 UDS 了。&lt;/p>
&lt;p>因为所有人都要用这一个地址做开发，所以就有互相影响的问题。虽然性能还可以，几十万 QPS 不成问题，但是总有憨憨拿来搞压测，把资源跑满，影响别人。我在使用说明文档里用红色大字写了这是开发测试用的，不能压测，还是有一些视力不好的同事会强行压测。隔三差五我就得去解释一番，礼貌地请同事不要再这样做了。&lt;/p>
&lt;p>最近实在累了。研究了一下直接给这个端口加上 per IP 的 rate limit，效果还不错。方法是在 Per-IP rate limiting with iptables[1] 学习到的，这个公司是提供一个多租户的 SaaS 服务，也有类似的问题：有一些非正常用户 abuse 他们的服务，由于 abuse 发生在连接建立阶段，还没有进入到业务代码，所以无法从应用的层面进行限速，解决发现就是通过 iptables 实现的。详细的实现方法可以参考这篇文章。&lt;/p>
&lt;p>iptables 本身是无状态的，每一个进入的 packet 都单独判断规则。rate limit 显然是一个有状态的规则，所以要用到 module: &lt;code>hashlimit&lt;/code>。（原文中还用到了 &lt;code>conntrack&lt;/code>，他是想只针对新建连接做限制，已经建立的连接不限制速度了。因为这个应用内部就可以控制了，但是我这里是想对所有的 packet 进行限速，所以就不需要用到这个 module）&lt;/p>
&lt;p>完整的命令如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ iptables --new-chain SOCAT-RATE-LIMIT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ iptables --append SOCAT-RATE-LIMIT &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --match hashlimit &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --hashlimit-mode srcip &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --hashlimit-upto 50/sec &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --hashlimit-burst &lt;span style="color:#ae81ff">100&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --hashlimit-name conn_rate_limit &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --jump ACCEPT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ iptables --append SOCAT-RATE-LIMIT --jump DROP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ iptables -I INPUT -p tcp --dport &lt;span style="color:#ae81ff">1234&lt;/span> --jump SOCAT-RATE-LIMIT
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>第一行是新建一个 iptables Chain，做 rate limit；&lt;/p>
&lt;p>第二行处理如果在 rate limit 限额内，就接受包；否则跳到第三行，直接将包 DROP；&lt;/p>
&lt;p>最后将新的 Chain 加入到 INPUT 中，对此端口的流量进行限制。&lt;/p>
&lt;p>有关 rate limit 的算法，主要是两个参数：&lt;/p>
&lt;ol>
&lt;li>&lt;code>--hashlimit-upto&lt;/code> 其实本质上是 1s 内可以进入多少 packet，&lt;code>50/sec&lt;/code> 就是 &lt;code>20ms&lt;/code> 一个 packet；&lt;/li>
&lt;li>那如何在 &lt;code>10ms&lt;/code> 发来 10 个 packet，后面一直没发送，怎么办？这个在测试情景下也比较常见，不能要求用户一直匀速地发送。所以就要用到 &lt;code>--hashlimit-burst&lt;/code>。字面意思是瞬间可以发送多少 packet，但实际上，可以理解这个参数就是可用的 credit。&lt;/li>
&lt;/ol>
&lt;p>两个指标配合起来理解，就是每个 ip 刚开始都会有 &lt;code>burst&lt;/code> 个 credit，每个 ip 发送来的 packet 都会占用 &lt;code>burst&lt;/code> 里面的 credit，用完了之后再发来的包就会被直接 DROP。这个 credit 会以 &lt;code>upto&lt;/code> 的速度一直增加，但是最多增加到 &lt;code>burst&lt;/code>（初始值），之后就 &lt;em>use it or lost it&lt;/em>.&lt;/p>
&lt;p>举个例子，假如 &lt;code>--hashlimit-upto 50/sec --hashlimit-burst 20&lt;/code> 的话，某个 IP 以匀速每 ms 一个 packet 的速度发送，最终会有多少 packets 被接受？答案是 70. 最初的 20ms，所有的 packet 都会被接受，因为 &lt;code>--hashlimit-burst&lt;/code> 是 20，所以最初的 credit 是 20. 这个用完之后就要依赖 &lt;code>--hashlimit--upto 50/sec&lt;/code> 来每 20ms 获得一个 packet credit 了。所以每 20ms 可以接受一个。&lt;/p>
&lt;p>这是限速之后的效果，非常明显：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/3af7c350-867e-429b-be82-d0a4816715c0/640" alt="">&lt;/p>
&lt;blockquote>
&lt;p>原文链接：&lt;a href="https://www.kawabangga.com/posts/4594">https://www.kawabangga.com/posts/4594&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;p>[1]&lt;/p>
&lt;p>Per-IP rate limiting with iptables: &lt;a href="https://making.pusher.com/per-ip-rate-limiting-with-iptables/index.html">&lt;em>https://making.pusher.com/per-ip-rate-limiting-with-iptables/index.html&lt;/em>&lt;/a>&lt;/p></description></item><item><title>Docs: 数据包发送过程详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="译-linux-网络栈监控和调优发送数据2017">[译] Linux 网络栈监控和调优：发送数据（2017）&lt;/h1>
&lt;p>Published at 2018-12-17 | Last Update 2020-09-29&lt;/p>
&lt;h2 id="译者序">译者序&lt;/h2>
&lt;p>本文翻译自 2017 年的一篇英文博客 &lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data">Monitoring and Tuning the Linux Networking Stack: Sending Data&lt;/a>。&lt;strong>如果能看懂英文，建议阅读原文，或者和本文对照看。&lt;/strong>
这篇文章写的是 &lt;strong>“Linux networking stack”&lt;/strong>，这里的 ”stack“ 不仅仅是内核协议栈， 而是包括内核协议栈在内的，从应用程序通过系统调用&lt;strong>写数据到 socket&lt;/strong>，到数据被组织 成一个或多个数据包最终被物理网卡发出去的整个路径。所以文章有三方面，交织在一起， 看起来非常累（但是很过瘾）：&lt;/p>
&lt;ol>
&lt;li>原理及代码实现：网络各层，包括驱动、硬中断、软中断、内核协议栈、socket 等等。&lt;/li>
&lt;li>监控：对代码中的重要计数进行监控，一般在&lt;code>/proc&lt;/code> 或&lt;code>/sys&lt;/code> 下面有对应输出。&lt;/li>
&lt;li>调优：修改网络配置参数。&lt;/li>
&lt;/ol>
&lt;p>本文的另一个特色是，几乎所有讨论的内核代码，都在相应的地方给出了 github 上的链接， 具体到行。
网络栈非常复杂，原文太长又没有任何章节号，看起来非常累。因此本文翻译时添加了适当 的章节号，以期按图索骥。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>2020 更新&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>基于 Prometheus+Grafana 监控网络栈：&lt;a href="http://arthurchiao.art/blog/monitoring-network-stack/">Monitoring Network Stack&lt;/a>。&lt;/li>
&lt;/ul>
&lt;p>以下是翻译。&lt;/p>
&lt;hr>
&lt;h2 id="太长不读tl-dr">太长不读（TL; DR）&lt;/h2>
&lt;p>本文介绍了运行 Linux 内核的机器是如何&lt;strong>发包&lt;/strong>（send packets）的，包是怎样从用户程 序一步步到达硬件网卡并被发出去的，以及如何&lt;strong>监控&lt;/strong>（monitoring）和&lt;strong>调优&lt;/strong>（ tuning）这一路径上的各个网络栈组件。
本文的姊妹篇是 &lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">Linux 网络栈监控和调优：接收数据&lt;/a>， 对应的原文是 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data&lt;/a> 。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E8%AF%91%E8%80%85%E5%BA%8F">译者序&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E5%A4%AA%E9%95%BF%E4%B8%8D%E8%AF%BBtl-dr">太长不读（TL; DR）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#1-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%9B%91%E6%8E%A7%E5%92%8C%E8%B0%83%E4%BC%98%E5%B8%B8%E8%A7%84%E5%BB%BA%E8%AE%AE">1 网络栈监控和调优：常规建议&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#2-%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E4%BF%AF%E7%9E%B0">2 发包过程俯瞰&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#3-%E5%8D%8F%E8%AE%AE%E5%B1%82%E6%B3%A8%E5%86%8C">3 协议层注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#4-%E9%80%9A%E8%BF%87-socket-%E5%8F%91%E9%80%81%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE">4 通过 socket 发送网络数据&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#41-sock_sendmsg-__sock_sendmsg-__sock_sendmsg_nosec">4.1 &lt;code>sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg_nosec&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#42-inet_sendmsg">4.2 &lt;code>inet_sendmsg&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#5-udp-%E5%8D%8F%E8%AE%AE%E5%B1%82">5 UDP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#51-udp_sendmsg">5.1 &lt;code>udp_sendmsg&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#511-udp-corking%E8%BD%AF%E6%9C%A8%E5%A1%9E">5.1.1 UDP corking（软木塞）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#512-%E8%8E%B7%E5%8F%96%E7%9B%AE%E7%9A%84-ip-%E5%9C%B0%E5%9D%80%E5%92%8C%E7%AB%AF%E5%8F%A3">5.1.2 获取目的 IP 地址和端口&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#513-socket-%E5%8F%91%E9%80%81bookkeeping-%E5%92%8C%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3">5.1.3 Socket 发送：bookkeeping 和打时间戳&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#514-%E8%BE%85%E5%8A%A9%E6%B6%88%E6%81%AFancillary-messages">5.1.4 辅助消息（Ancillary messages）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#515-%E8%AE%BE%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89-ip-%E9%80%89%E9%A1%B9">5.1.5 设置自定义 IP 选项&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#516-%E5%A4%9A%E6%92%AD%E6%88%96%E5%8D%95%E6%92%ADmulticast-or-unicast">5.1.6 多播或单播（Multicast or unicast）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#517-%E8%B7%AF%E7%94%B1">5.1.7 路由&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#518-msg_confirm-%E9%98%BB%E6%AD%A2-arp-%E7%BC%93%E5%AD%98%E8%BF%87%E6%9C%9F">5.1.8 &lt;code>MSG_CONFIRM&lt;/code>: 阻止 ARP 缓存过期&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#519-uncorked-udp-sockets-%E5%BF%AB%E9%80%9F%E8%B7%AF%E5%BE%84%E5%87%86%E5%A4%87%E5%BE%85%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE">5.1.9 uncorked UDP sockets 快速路径：准备待发送数据&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#ip_make_skb">&lt;code>ip_make_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE">发送数据&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#5110-%E6%B2%A1%E6%9C%89%E8%A2%AB-cork-%E7%9A%84%E6%95%B0%E6%8D%AE%E6%97%B6%E7%9A%84%E6%85%A2%E8%B7%AF%E5%BE%84">5.1.10 没有被 cork 的数据时的慢路径&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#ip_append_data">&lt;code>ip_append_data&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#__ip_append_data">&lt;code>__ip_append_data&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#flushing-corked-sockets">Flushing corked sockets&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#5111-error-accounting">5.1.11 Error accounting&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#52-udp_send_skb">5.2 &lt;code>udp_send_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#53-%E7%9B%91%E6%8E%A7udp-%E5%B1%82%E7%BB%9F%E8%AE%A1">5.3 监控：UDP 层统计&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetsnmp">/proc/net/snmp&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetudp">/proc/net/udp&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#54-%E8%B0%83%E4%BC%98socket-%E5%8F%91%E9%80%81%E9%98%9F%E5%88%97%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F">5.4 调优：socket 发送队列内存大小&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#6-ip-%E5%8D%8F%E8%AE%AE%E5%B1%82">6 IP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#61-ip_send_skb">6.1 &lt;code>ip_send_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#62-ip_local_out-and-__ip_local_out">6.2 &lt;code>ip_local_out&lt;/code> and &lt;code>__ip_local_out&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#63-netfilter-and-nf_hook">6.3 netfilter and nf_hook&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#64-%E7%9B%AE%E7%9A%84%E8%B7%AF%E7%94%B1%E7%BC%93%E5%AD%98">6.4 目的（路由）缓存&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#65-ip_output">6.5 &lt;code>ip_output&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#66-ip_finish_output">6.6 &lt;code>ip_finish_output&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#path-mtu-discovery">Path MTU Discovery&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#67-ip_finish_output2">6.7 &lt;code>ip_finish_output2&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#68-dst_neigh_output">6.8 &lt;code>dst_neigh_output&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#69-neigh_hh_output">6.9 &lt;code>neigh_hh_output&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#610-n-output">6.10 &lt;code>n-&amp;gt;output&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#neigh_resolve_output">neigh_resolve_output&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#611-%E7%9B%91%E6%8E%A7-ip-%E5%B1%82">6.11 监控: IP 层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetsnmp-1">/proc/net/snmp&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#procnetnetstat">/proc/net/netstat&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#7-linux-netdevice-%E5%AD%90%E7%B3%BB%E7%BB%9F">7 Linux netdevice 子系统&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#71-linux-traffic-control%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6">7.1 Linux traffic control（流量控制）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#72-dev_queue_xmit-and-__dev_queue_xmit">7.2 &lt;code>dev_queue_xmit&lt;/code> and &lt;code>__dev_queue_xmit&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#721-netdev_pick_tx">7.2.1 &lt;code>netdev_pick_tx&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#722-__netdev_pick_tx">7.2.2 &lt;code>__netdev_pick_tx&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#transmit-packet-steering-xps">Transmit Packet Steering (XPS)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#skb_tx_hash">&lt;code>skb_tx_hash&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#73-%E7%BB%A7%E7%BB%AD__dev_queue_xmit">7.3 继续&lt;code>__dev_queue_xmit&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#74-__dev_xmit_skb">7.4 &lt;code>__dev_xmit_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#75-%E8%B0%83%E4%BC%98-transmit-packet-steering-xps">7.5 调优: Transmit Packet Steering (XPS)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#8-queuing-disciplines%E6%8E%92%E9%98%9F%E8%A7%84%E5%88%99">8 Queuing Disciplines（排队规则）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#81-qdisc_run_begin-and-qdisc_run_end%E4%BB%85%E8%AE%BE%E7%BD%AE-qdisc-%E7%8A%B6%E6%80%81%E4%BD%8D">8.1 &lt;code>qdisc_run_begin()&lt;/code> and &lt;code>qdisc_run_end()&lt;/code>：仅设置 qdisc 状态位&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#82-__qdisc_run%E7%9C%9F%E6%AD%A3%E7%9A%84-qdisc-%E6%89%A7%E8%A1%8C%E5%85%A5%E5%8F%A3">8.2 &lt;code>__qdisc_run()&lt;/code>：真正的 qdisc 执行入口&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#83-qdisc_restart%E4%BB%8E-qdisc-%E9%98%9F%E5%88%97%E4%B8%AD%E5%8F%96%E5%8C%85%E5%8F%91%E9%80%81%E7%BB%99%E7%BD%91%E7%BB%9C%E9%A9%B1%E5%8A%A8">8.3 &lt;code>qdisc_restart&lt;/code>：从 qdisc 队列中取包，发送给网络驱动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#831-dequeue_skb%E4%BB%8E-qdisc-%E9%98%9F%E5%88%97%E5%8F%96%E5%BE%85%E5%8F%91%E9%80%81-skb">8.3.1 &lt;code>dequeue_skb()&lt;/code>：从 qdisc 队列取待发送 skb&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#832-sch_direct_xmit%E5%8F%91%E9%80%81%E7%BB%99%E7%BD%91%E5%8D%A1%E9%A9%B1%E5%8A%A8">8.3.2 &lt;code>sch_direct_xmit()&lt;/code>：发送给网卡驱动&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#833-handle_dev_cpu_collision">8.3.3 &lt;code>handle_dev_cpu_collision()&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#834-dev_requeue_skb%E9%87%8D%E6%96%B0%E5%8E%8B%E5%85%A5-qdisc-%E9%98%9F%E5%88%97%E7%AD%89%E5%BE%85%E4%B8%8B%E6%AC%A1%E5%8F%91%E9%80%81">8.3.4 &lt;code>dev_requeue_skb()&lt;/code>：重新压入 qdisc 队列，等待下次发送&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#84-%E5%A4%8D%E4%B9%A0__qdisc_run-%E4%B8%BB%E9%80%BB%E8%BE%91">8.4 复习：&lt;code>__qdisc_run()&lt;/code> 主逻辑&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#841-__netif_schedule">8.4.1 &lt;code>__netif_schedule&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#842-net_tx_action">8.4.2 &lt;code>net_tx_action()&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#843-net_tx_action-completion-queue%E5%BE%85%E9%87%8A%E6%94%BE-skb-%E9%98%9F%E5%88%97">8.4.3 &lt;code>net_tx_action()&lt;/code> completion queue：待释放 skb 队列&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#844-net_tx_action-output-queue%E5%BE%85%E5%8F%91%E9%80%81-skb-%E9%98%9F%E5%88%97">8.4.4 &lt;code>net_tx_action&lt;/code> output queue：待发送 skb 队列&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#85-%E6%9C%80%E7%BB%88%E6%9D%A5%E5%88%B0-dev_hard_start_xmit">8.5 最终来到 &lt;code>dev_hard_start_xmit&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#86-monitoring-qdiscs">8.6 Monitoring qdiscs&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#using-the-tc-command-line-tool">Using the tc command line tool&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#87-tuning-qdiscs">8.7 Tuning qdiscs&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E8%B0%83%E6%95%B4__qdisc_run-%E5%A4%84%E7%90%86%E6%9D%83%E9%87%8D">调整&lt;code>__qdisc_run&lt;/code> 处理权重&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#%E5%A2%9E%E5%8A%A0%E5%8F%91%E9%80%81%E9%98%9F%E5%88%97%E9%95%BF%E5%BA%A6">增加发送队列长度&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#9-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8">9 网络设备驱动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#91-%E9%A9%B1%E5%8A%A8%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E6%B3%A8%E5%86%8C">9.1 驱动回调函数注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#92-%E9%80%9A%E8%BF%87-ndo_start_xmit-%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE">9.2 通过 &lt;code>ndo_start_xmit&lt;/code> 发送数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#93-igb_tx_map">9.3 &lt;code>igb_tx_map&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#dynamic-queue-limits-dql">Dynamic Queue Limits (DQL)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#94-%E5%8F%91%E9%80%81%E5%AE%8C%E6%88%90transmit-completions">9.4 发送完成（Transmit completions）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#941-transmit-completion-irq">9.4.1 Transmit completion IRQ&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#942-igb_poll">9.4.2 &lt;code>igb_poll&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#943-igb_clean_tx_irq">9.4.3 &lt;code>igb_clean_tx_irq&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#944-igb_poll-%E8%BF%94%E5%9B%9E%E5%80%BC">9.4.4 &lt;code>igb_poll&lt;/code> 返回值&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#95-%E7%9B%91%E6%8E%A7%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87">9.5 监控网络设备&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#951-%E4%BD%BF%E7%94%A8-ethtool--s-%E5%91%BD%E4%BB%A4">9.5.1 使用 &lt;code>ethtool -S&lt;/code> 命令&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#952-%E4%BD%BF%E7%94%A8-sysfs">9.5.2 使用 &lt;code>sysfs&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#953-%E4%BD%BF%E7%94%A8procnetdev">9.5.3 使用&lt;code>/proc/net/dev&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#96-%E7%9B%91%E6%8E%A7-dql">9.6 监控 DQL&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#97-%E8%B0%83%E4%BC%98%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87">9.7 调优网络设备&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#971-%E6%9F%A5%E8%AF%A2-tx-queue-%E6%95%B0%E9%87%8F">9.7.1 查询 TX Queue 数量&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#972-%E8%B0%83%E6%95%B4-tx-queue-%E6%95%B0%E9%87%8F">9.7.2 调整 TX queue 数量&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#973-%E8%B0%83%E6%95%B4-tx-queue-%E5%A4%A7%E5%B0%8F">9.7.3 调整 TX queue 大小&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#10-%E7%BD%91%E7%BB%9C%E6%A0%88%E4%B9%8B%E6%97%85%E7%BB%93%E6%9D%9F">10 网络栈之旅：结束&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#11-extras">11 Extras&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#111-%E5%87%8F%E5%B0%91-arp-%E6%B5%81%E9%87%8F-msg_confirm">11.1 减少 ARP 流量 (MSG_CONFIRM)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#112-udp-corking%E8%BD%AF%E6%9C%A8%E5%A1%9E">11.2 UDP Corking（软木塞）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#113-%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3">11.3 打时间戳&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#12-%E7%BB%93%E8%AE%BA">12 结论&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/#13-%E9%A2%9D%E5%A4%96%E5%B8%AE%E5%8A%A9">13 额外帮助&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>想对 Linux 网络栈进行监控或调优，必须对其正在发生什么有一个深入的理解， 而这离不开读内核源码。希望本文可以给那些正准备投身于此的人提供一份参考。&lt;/p>
&lt;h1 id="1-网络栈监控和调优常规建议">1 网络栈监控和调优：常规建议&lt;/h1>
&lt;p>正如我们前一篇文章提到的，网络栈很复杂，没有一种方式适用于所有场景。如果性能和网络 健康状态对你或你的业务非常重要，那你没有别的选择，只能花大量的时间、精力和金钱去 深入理解系统的各个部分之间是如何交互的。
本文中的一些示例配置仅为了方便理解（效果），并不作为任何特定配置或默认配置的建议 。在做任何配置改动之前，你应该有一个能够对系统进行监控的框架，以查看变更是否带来 预期的效果。
对远程连接上的机器进行网络变更是相当危险的，机器很可能失联。另外，不要在生产环境 直接调整这些配置；如果可能的话，在新机器上改配置，然后将机器灰度上线到生产。&lt;/p>
&lt;h1 id="2-发包过程俯瞰">2 发包过程俯瞰&lt;/h1>
&lt;p>本文将拿&lt;strong>Intel I350&lt;/strong>网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从用户程序到达硬件网卡的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>使用&lt;strong>系统调用&lt;/strong>（如 &lt;code>sendto&lt;/code>，&lt;code>sendmsg&lt;/code> 等）写数据&lt;/li>
&lt;li>数据穿过&lt;strong>socket 子系统&lt;/strong>，进入&lt;strong>socket 协议族&lt;/strong>（protocol family）系统（在我们的例子中为 &lt;code>AF_INET&lt;/code>）&lt;/li>
&lt;li>协议族处理：数据穿过&lt;strong>协议层&lt;/strong>，这一过程（在许多情况下）会将&lt;strong>数据&lt;/strong>（data）转换成&lt;strong>数据包&lt;/strong>（packet）&lt;/li>
&lt;li>数据穿过&lt;strong>路由层&lt;/strong>，这会涉及路由缓存和 ARP 缓存的更新；如果目的 MAC 不在 ARP 缓存表中，将触发一次 ARP 广播来查找 MAC 地址&lt;/li>
&lt;li>穿过协议层，packet 到达&lt;strong>设备无关层&lt;/strong>（device agnostic layer）&lt;/li>
&lt;li>使用 XPS（如果启用）或散列函数&lt;strong>选择发送队列&lt;/strong>&lt;/li>
&lt;li>调用网卡驱动的&lt;strong>发送函数&lt;/strong>&lt;/li>
&lt;li>数据传送到网卡的 &lt;code>qdisc&lt;/code>（queue discipline，排队规则）&lt;/li>
&lt;li>qdisc 会直接&lt;strong>发送数据&lt;/strong>（如果可以），或者将其放到队列，下次触发**&lt;code>NET_TX&lt;/code> 类型软中断**（softirq）的时候再发送&lt;/li>
&lt;li>数据从 qdisc 传送给驱动程序&lt;/li>
&lt;li>驱动程序创建所需的&lt;strong>DMA 映射&lt;/strong>，以便网卡从 RAM 读取数据&lt;/li>
&lt;li>驱动向网卡发送信号，通知&lt;strong>数据可以发送了&lt;/strong>&lt;/li>
&lt;li>&lt;strong>网卡从 RAM 中获取数据并发送&lt;/strong>&lt;/li>
&lt;li>发送完成后，设备触发一个&lt;strong>硬中断&lt;/strong>（IRQ），表示发送完成&lt;/li>
&lt;li>&lt;strong>硬中断处理函数&lt;/strong>被唤醒执行。对许多设备来说，这会&lt;strong>触发 &lt;code>NET_RX&lt;/code> 类型的软中断&lt;/strong>，然后 NAPI poll 循环开始收包&lt;/li>
&lt;li>poll 函数会调用驱动程序的相应函数，&lt;strong>解除 DMA 映射&lt;/strong>，释放数据&lt;/li>
&lt;/ol>
&lt;p>接下来会详细介绍整个过程。&lt;/p>
&lt;h1 id="3-协议层注册">3 协议层注册&lt;/h1>
&lt;p>协议层分析我们将会关注 IP 和 UDP 层，其他协议层可参考这个过程。
我们首先来看协议族是如何注册到内核，并被 socket 子系统使用的。
当用户程序像下面这样创建 UDP socket 时会发生什么？&lt;/p>
&lt;pre>&lt;code>sock = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP)
&lt;/code>&lt;/pre>
&lt;p>简单来说，内核会去查找由 UDP 协议栈导出的一组函数（其中包括用于发送和接收网络数据 的函数），并赋给 socket 的相应字段。准确理解这个过程需要查看 &lt;code>AF_INET&lt;/code> 地址族的 代码。
内核初始化的很早阶段就执行了 &lt;code>inet_init&lt;/code> 函数，这个函数会注册 &lt;code>AF_INET&lt;/code> 协议族 ，以及该协议族内的各协议栈（TCP，UDP，ICMP 和 RAW），并调用初始化函数使协议栈准备 好处理网络数据。&lt;code>inet_init&lt;/code> 定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1678-L1804">net/ipv4/af_inet.c&lt;/a> 。
&lt;code>AF_INET&lt;/code> 协议族导出一个包含 &lt;code>create&lt;/code> 方法的 &lt;code>struct net_proto_family&lt;/code> 类型实例。当从 用户程序创建 socket 时，内核会调用此方法：&lt;/p>
&lt;pre>&lt;code>static const struct net_proto_family inet_family_ops = {
.family = PF_INET,
.create = inet_create,
.owner = THIS_MODULE,
};
&lt;/code>&lt;/pre>
&lt;p>&lt;code>inet_create&lt;/code> 根据传递的 socket 参数，在已注册的协议中查找对应的协议。我们来看一下：&lt;/p>
&lt;pre>&lt;code>/* Look for the requested type/protocol pair. */
lookup_protocol:
err = -ESOCKTNOSUPPORT;
rcu_read_lock();
list_for_each_entry_rcu(answer, &amp;amp;inetsw[sock-&amp;gt;type], list) {
err = 0;
/* Check the non-wild match. */
if (protocol == answer-&amp;gt;protocol) {
if (protocol != IPPROTO_IP)
break;
} else {
/* Check for the two wild cases. */
if (IPPROTO_IP == protocol) {
protocol = answer-&amp;gt;protocol;
break;
}
if (IPPROTO_IP == answer-&amp;gt;protocol)
break;
}
err = -EPROTONOSUPPORT;
}
&lt;/code>&lt;/pre>
&lt;p>然后，将该协议的回调方法（集合）赋给这个新创建的 socket：&lt;/p>
&lt;pre>&lt;code>sock-&amp;gt;ops = answer-&amp;gt;ops;
&lt;/code>&lt;/pre>
&lt;p>可以在 &lt;code>af_inet.c&lt;/code> 中看到所有协议的初始化参数。 下面是&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L998-L1020">TCP 和 UDP&lt;/a>的初始化参数：&lt;/p>
&lt;pre>&lt;code>/* Upon startup we insert all the elements in inetsw_array[] into
* the linked list inetsw.
*/
static struct inet_protosw inetsw_array[] =
{
{
.type = SOCK_STREAM,
.protocol = IPPROTO_TCP,
.prot = &amp;amp;tcp_prot,
.ops = &amp;amp;inet_stream_ops,
.no_check = 0,
.flags = INET_PROTOSW_PERMANENT |
INET_PROTOSW_ICSK,
},
{
.type = SOCK_DGRAM,
.protocol = IPPROTO_UDP,
.prot = &amp;amp;udp_prot,
.ops = &amp;amp;inet_dgram_ops,
.no_check = UDP_CSUM_DEFAULT,
.flags = INET_PROTOSW_PERMANENT,
},
/* .... more protocols ... */
&lt;/code>&lt;/pre>
&lt;p>&lt;code>IPPROTO_UDP&lt;/code> 协议类型有一个 &lt;code>ops&lt;/code> 变量，包含&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L935-L960">很多信息  &lt;/a>，包 括用于发送和接收数据的回调函数：&lt;/p>
&lt;pre>&lt;code>const struct proto_ops inet_dgram_ops = {
.family = PF_INET,
.owner = THIS_MODULE,
/* ... */
.sendmsg = inet_sendmsg,
.recvmsg = inet_recvmsg,
/* ... */
};
EXPORT_SYMBOL(inet_dgram_ops);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>prot&lt;/code> 字段指向一个协议相关的变量（的地址），对于 UDP 协议，其中包含了 UDP 相关的 回调函数。 UDP 协议对应的 &lt;code>prot&lt;/code> 变量为 &lt;code>udp_prot&lt;/code>，定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L2171-L2203">net/ipv4/udp.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>struct proto udp_prot = {
.name = &amp;quot;UDP&amp;quot;,
.owner = THIS_MODULE,
/* ... */
.sendmsg = udp_sendmsg,
.recvmsg = udp_recvmsg,
/* ... */
};
EXPORT_SYMBOL(udp_prot);
&lt;/code>&lt;/pre>
&lt;p>现在，让我们转向发送 UDP 数据的用户程序，看看 &lt;code>udp_sendmsg&lt;/code> 是如何在内核中被调用的。&lt;/p>
&lt;h1 id="4-通过-socket-发送网络数据">4 通过 socket 发送网络数据&lt;/h1>
&lt;p>用户程序想发送 UDP 网络数据，因此它使用 &lt;code>sendto&lt;/code> 系统调用，看起来可能是这样的：&lt;/p>
&lt;pre>&lt;code>ret = sendto(socket, buffer, buflen, 0, &amp;amp;dest, sizeof(dest));
&lt;/code>&lt;/pre>
&lt;p>该系统调用穿过&lt;a href="https://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/">Linux 系统调用（system call）层&lt;/a>，最后到达&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/socket.c#L1756-L1803">net/socket.c&lt;/a>中的这个函数：&lt;/p>
&lt;pre>&lt;code>/*
* Send a datagram to a given address. We move the address into kernel
* space and check the user space data area is readable before invoking
* the protocol.
*/
SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
unsigned int, flags, struct sockaddr __user *, addr,
int, addr_len)
{
/* ... code ... */
err = sock_sendmsg(sock, &amp;amp;msg, len);
/* ... code ... */
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>SYSCALL_DEFINE6&lt;/code> 宏会展开成一堆宏，后者经过一波复杂操作创建出一个带 6 个参数的系统 调用（因此叫 &lt;code>DEFINE6&lt;/code>）。作为结果之一，你会看到内核中的所有系统调用都带 &lt;code>sys_&lt;/code>前 缀。
&lt;code>sendto&lt;/code> 代码会先将数据整理成底层可以处理的格式，然后调用 &lt;code>sock_sendmsg&lt;/code>。特别地， 它将传递给 &lt;code>sendto&lt;/code> 的地址放到另一个变量（&lt;code>msg&lt;/code>）中：&lt;/p>
&lt;pre>&lt;code>iov.iov_base = buff;
iov.iov_len = len;
msg.msg_name = NULL;
msg.msg_iov = &amp;amp;iov;
msg.msg_iovlen = 1;
msg.msg_control = NULL;
msg.msg_controllen = 0;
msg.msg_namelen = 0;
if (addr) {
err = move_addr_to_kernel(addr, addr_len, &amp;amp;address);
if (err &amp;lt; 0)
goto out_put;
msg.msg_name = (struct sockaddr *)&amp;amp;address;
msg.msg_namelen = addr_len;
}
&lt;/code>&lt;/pre>
&lt;p>这段代码将用户程序传入到内核的（存放待发送数据的）地址，作为 &lt;code>msg_name&lt;/code> 字段嵌入到 &lt;code>struct msghdr&lt;/code> 类型变量中。这和用户程序直接调用 &lt;code>sendmsg&lt;/code> 而不是 &lt;code>sendto&lt;/code> 发送 数据差不多，这之所以可行，是因为 &lt;code>sendto&lt;/code> 和 &lt;code>sendmsg&lt;/code> 底层都会调用 &lt;code>sock_sendmsg&lt;/code>。&lt;/p>
&lt;h2 id="41-sock_sendmsg-__sock_sendmsg-__sock_sendmsg_nosec">4.1 &lt;code>sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg&lt;/code>, &lt;code>__sock_sendmsg_nosec&lt;/code>&lt;/h2>
&lt;p>&lt;code>sock_sendmsg&lt;/code> 做一些错误检查，然后调用&lt;code>__sock_sendmsg&lt;/code>；后者做一些自己的错误检查 ，然后调用&lt;code>__sock_sendmsg_nosec&lt;/code>。&lt;code>__sock_sendmsg_nosec&lt;/code> 将数据传递到 socket 子系统 的更深处：&lt;/p>
&lt;pre>&lt;code>static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock,
struct msghdr *msg, size_t size)
{
struct sock_iocb *si = ....
/* other code ... */
return sock-&amp;gt;ops-&amp;gt;sendmsg(iocb, sock, msg, size);
}
&lt;/code>&lt;/pre>
&lt;p>通过我们前面介绍的 socket 创建过程，你应该能看懂，注册到这里的 &lt;code>sendmsg&lt;/code> 方法就是 &lt;code>inet_sendmsg&lt;/code>。&lt;/p>
&lt;h2 id="42-inet_sendmsg">4.2 &lt;code>inet_sendmsg&lt;/code>&lt;/h2>
&lt;p>从名字可以猜到，这是 &lt;code>AF_INET&lt;/code> 协议族提供的通用函数。 此函数首先调用 &lt;code>sock_rps_record_flow&lt;/code> 来记录最后一个处理该（数据所属的）flow 的 CPU; Receive Packet Steering 会用到这个信息。接下来，调用 socket 的协议类型（本例是 UDP）对应的 &lt;code>sendmsg&lt;/code> 方法：&lt;/p>
&lt;pre>&lt;code>int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
size_t size)
{
struct sock *sk = sock-&amp;gt;sk;
sock_rps_record_flow(sk);
/* We may need to bind the socket. */
if (!inet_sk(sk)-&amp;gt;inet_num &amp;amp;&amp;amp; !sk-&amp;gt;sk_prot-&amp;gt;no_autobind &amp;amp;&amp;amp; inet_autobind(sk))
return -EAGAIN;
return sk-&amp;gt;sk_prot-&amp;gt;sendmsg(iocb, sk, msg, size);
}
EXPORT_SYMBOL(inet_sendmsg);
&lt;/code>&lt;/pre>
&lt;p>本例是 UDP 协议，因此上面的 &lt;code>sk-&amp;gt;sk_prot-&amp;gt;sendmsg&lt;/code> 指向的是我们之前看到的（通过 &lt;code>udp_prot&lt;/code> 导出的）&lt;code>udp_sendmsg&lt;/code> 函数。
&lt;strong>sendmsg()函数作为分界点，处理逻辑从 &lt;code>AF_INET&lt;/code> 协议族通用处理转移到具体的 UDP 协议的处理。&lt;/strong>&lt;/p>
&lt;h1 id="5-udp-协议层">5 UDP 协议层&lt;/h1>
&lt;h2 id="51-udp_sendmsg">5.1 &lt;code>udp_sendmsg&lt;/code>&lt;/h2>
&lt;p>这个函数定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L845-L1088">net/ipv4/udp.c&lt;/a> ，函数非常长，我们分段来看。&lt;/p>
&lt;h3 id="511-udp-corking软木塞">5.1.1 UDP corking（软木塞）&lt;/h3>
&lt;p>在变量声明和基本错误检查之后，&lt;code>udp_sendmsg&lt;/code> 所做的第一件事就是检查 socket 是否“ 塞住”了（corked）。 UDP corking 是一项优化技术，允许内核将多次数据累积成单个数据报发 送。在用户程序中有两种方法可以启用此选项：&lt;/p>
&lt;ol>
&lt;li>使用 &lt;code>setsockopt&lt;/code> 系统调用设置 socket 的 &lt;code>UDP_CORK&lt;/code> 选项&lt;/li>
&lt;li>程序调用 &lt;code>send&lt;/code>，&lt;code>sendto&lt;/code> 或 &lt;code>sendmsg&lt;/code> 时，带 &lt;code>MSG_MORE&lt;/code> 参数&lt;/li>
&lt;/ol>
&lt;p>详细信息参考 &lt;a href="http://man7.org/linux/man-pages/man7/udp.7.html">UDP man page&lt;/a>和 &lt;a href="http://man7.org/linux/man-pages/man2/send.2.html">&lt;code>send/sendto/sendmsg&lt;/code> man page&lt;/a>。
&lt;code>udp_sendmsg&lt;/code> 代码检查 &lt;code>up-&amp;gt;pending&lt;/code> 以确定 socket 当前是否已被塞住(corked)，如果是， 则直接跳到 &lt;code>do_append_data&lt;/code> 进行数据追加(append)。 我们将在稍后看到如何追加数据。&lt;/p>
&lt;pre>&lt;code>int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
size_t len)
{
/* variables and error checking ... */
fl4 = &amp;amp;inet-&amp;gt;cork.fl.u.ip4;
if (up-&amp;gt;pending) {
/*
* There are pending frames.
* The socket lock must be held while it's corked.
*/
lock_sock(sk);
if (likely(up-&amp;gt;pending)) {
if (unlikely(up-&amp;gt;pending != AF_INET)) {
release_sock(sk);
return -EINVAL;
}
goto do_append_data;
}
release_sock(sk);
}
&lt;/code>&lt;/pre>
&lt;h3 id="512-获取目的-ip-地址和端口">5.1.2 获取目的 IP 地址和端口&lt;/h3>
&lt;p>接下来获取目标地址和端口，有两个可能的来源：&lt;/p>
&lt;ol>
&lt;li>如果之前 socket 已经建立连接，那 socket 本身就存储了目标地址&lt;/li>
&lt;li>地址通过辅助结构（&lt;code>struct msghdr&lt;/code>）传入，正如我们在 &lt;code>sendto&lt;/code> 的内核代码中看到的那样&lt;/li>
&lt;/ol>
&lt;p>具体逻辑：&lt;/p>
&lt;pre>&lt;code>/*
* Get and verify the address.
*/
if (msg-&amp;gt;msg_name) {
struct sockaddr_in *usin = (struct sockaddr_in *)msg-&amp;gt;msg_name;
if (msg-&amp;gt;msg_namelen &amp;lt; sizeof(*usin))
return -EINVAL;
if (usin-&amp;gt;sin_family != AF_INET) {
if (usin-&amp;gt;sin_family != AF_UNSPEC)
return -EAFNOSUPPORT;
}
daddr = usin-&amp;gt;sin_addr.s_addr;
dport = usin-&amp;gt;sin_port;
if (dport == 0)
return -EINVAL;
} else {
if (sk-&amp;gt;sk_state != TCP_ESTABLISHED)
return -EDESTADDRREQ;
daddr = inet-&amp;gt;inet_daddr;
dport = inet-&amp;gt;inet_dport;
/* Open fast path for connected socket.
Route will not be used, if at least one option is set.
*/
connected = 1;
}
&lt;/code>&lt;/pre>
&lt;p>是的，你没看错，UDP 代码中出现了 &lt;code>TCP_ESTABLISHED&lt;/code>！UDP socket 的状态使用了 TCP 状态 来描述，不知道是好是坏。
回想前面我们看到用户程序调用 &lt;code>sendto&lt;/code> 时，内核如何替用户初始化一个 &lt;code>struct msghdr&lt;/code> 变量。上面的代码显示了内核如何解析该变量以便设置 &lt;code>daddr&lt;/code> 和 &lt;code>dport&lt;/code>。
如果没有 &lt;code>struct msghdr&lt;/code> 变量，内核函数到达 &lt;code>udp_sendmsg&lt;/code> 函数时，会从 socket 本身检索 目标地址和端口，并将 socket 标记为“已连接”。&lt;/p>
&lt;h3 id="513-socket-发送bookkeeping-和打时间戳">5.1.3 Socket 发送：bookkeeping 和打时间戳&lt;/h3>
&lt;p>接下来，获取存储在 socket 上的源地址、设备索引（device index）和时间戳选项（例 如 &lt;code>SOCK_TIMESTAMPING_TX_HARDWARE&lt;/code>, &lt;code>SOCK_TIMESTAMPING_TX_SOFTWARE&lt;/code>, &lt;code>SOCK_WIFI_STATUS&lt;/code>）：&lt;/p>
&lt;pre>&lt;code>ipc.addr = inet-&amp;gt;inet_saddr;
ipc.oif = sk-&amp;gt;sk_bound_dev_if;
sock_tx_timestamp(sk, &amp;amp;ipc.tx_flags);
&lt;/code>&lt;/pre>
&lt;h3 id="514-辅助消息ancillary-messages">5.1.4 辅助消息（Ancillary messages）&lt;/h3>
&lt;p>除了发送或接收数据包之外，&lt;code>sendmsg&lt;/code> 和 &lt;code>recvmsg&lt;/code> 系统调用还允许用户设置或请求辅助数 据。用户程序可以通过将请求信息组织成 &lt;code>struct msghdr&lt;/code> 类型变量来利用此辅助数据。一些辅 助数据类型记录在&lt;a href="http://man7.org/linux/man-pages/man7/ip.7.html">IP man page&lt;/a>中 。
辅助数据的一个常见例子是 &lt;code>IP_PKTINFO&lt;/code>。对于 &lt;code>sendmsg&lt;/code>，&lt;code>IP_PKTINFO&lt;/code> 允许程序在发送 数据时设置一个 &lt;code>in_pktinfo&lt;/code> 变量。程序可以通过填写 &lt;code>struct in_pktinfo&lt;/code> 变量中的字段 来指定要在 packet 上使用的源地址。如果程序是监听多个 IP 地址的服务端程序，那这是一个 很有用的选项。在这种情况下，服务端可能想使用客户端连接服务端的那个 IP 地址来回复客 户端，&lt;code>IP_PKTINFO&lt;/code> 非常适合这种场景。
&lt;code>setsockopt&lt;/code> 可以在&lt;strong>socket 级别&lt;/strong>设置发送包的 &lt;a href="https://en.wikipedia.org/wiki/Time_to_live#IP_packets">IP_TTL&lt;/a>和 &lt;a href="https://en.wikipedia.org/wiki/Type_of_service">IP_TOS&lt;/a>。而辅助消息允 许在每个&lt;strong>数据包级别&lt;/strong>设置 TTL 和 TOS 值。Linux 内核会使用一个&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L179-L197">数组  &lt;/a>将 TOS 转换为优先级，后者会影响数据包如何以及合适从 qdisc 中发送出去。我们稍后会了解到这 意味着什么。
我们可以看到内核如何在 UDP socket 上处理 &lt;code>sendmsg&lt;/code> 的辅助消息：&lt;/p>
&lt;pre>&lt;code>if (msg-&amp;gt;msg_controllen) {
err = ip_cmsg_send(sock_net(sk), msg, &amp;amp;ipc,
sk-&amp;gt;sk_family == AF_INET6);
if (err)
return err;
if (ipc.opt)
free = 1;
connected = 0;
}
&lt;/code>&lt;/pre>
&lt;p>解析辅助消息的工作是由 &lt;code>ip_cmsg_send&lt;/code> 完成的，定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_sockglue.c#L190-L241">net/ipv4/ip_sockglue.c&lt;/a> 。注意，传递一个未初始化的辅助数据，将会把这个 socket 标记为“未建立连接的”（译者注 ：因为从 5.1.2 的代码可以看出，有辅助消息时优先处理辅助消息，没有辅助消息才从 socket 里面拿信息）。&lt;/p>
&lt;h3 id="515-设置自定义-ip-选项">5.1.5 设置自定义 IP 选项&lt;/h3>
&lt;p>接下来，&lt;code>sendmsg&lt;/code> 将检查用户是否通过辅助消息设置了的任何自定义 IP 选项。如果设置了 ，将使用这些自定义值；如果没有，那就使用 socket 中（已经在用）的参数：&lt;/p>
&lt;pre>&lt;code>if (!ipc.opt) {
struct ip_options_rcu *inet_opt;
rcu_read_lock();
inet_opt = rcu_dereference(inet-&amp;gt;inet_opt);
if (inet_opt) {
memcpy(&amp;amp;opt_copy, inet_opt,
sizeof(*inet_opt) + inet_opt-&amp;gt;opt.optlen);
ipc.opt = &amp;amp;opt_copy.opt;
}
rcu_read_unlock();
}
&lt;/code>&lt;/pre>
&lt;p>接下来，该函数检查是否设置了源记录路由（source record route, SRR）IP 选项。 SRR 有两种类型：&lt;a href="https://en.wikipedia.org/wiki/Loose_Source_Routing">宽松源记录路由和严格源记录路由&lt;/a>。 如果设置了此选项，则会记录第一跳地址并将其保存到 &lt;code>faddr&lt;/code>，并将 socket 标记为“未连接”。 这将在后面用到：&lt;/p>
&lt;pre>&lt;code>ipc.addr = faddr = daddr;
if (ipc.opt &amp;amp;&amp;amp; ipc.opt-&amp;gt;opt.srr) {
if (!daddr)
return -EINVAL;
faddr = ipc.opt-&amp;gt;opt.faddr;
connected = 0;
}
&lt;/code>&lt;/pre>
&lt;p>处理完 SRR 选项后，将处理 TOS 选项，这可以从辅助消息中获取，或者从 socket 当前值中获取。 然后检查：&lt;/p>
&lt;ol>
&lt;li>是否（使用 &lt;code>setsockopt&lt;/code>）在 socket 上设置了 &lt;code>SO_DONTROUTE&lt;/code>，或&lt;/li>
&lt;li>是否（调用 &lt;code>sendto&lt;/code> 或 &lt;code>sendmsg&lt;/code> 时）指定了 &lt;code>MSG_DONTROUTE&lt;/code> 标志，或&lt;/li>
&lt;li>是否已设置了 &lt;code>is_strictroute&lt;/code>，表示需要严格的 &lt;a href="http://www.networksorcery.com/enp/protocol/ip/option009.htm">SRR&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>任何一个为真，&lt;code>tos&lt;/code> 字段的 &lt;code>RTO_ONLINK&lt;/code> 位将置 1，并且 socket 被视为“未连接”：&lt;/p>
&lt;pre>&lt;code>tos = get_rttos(&amp;amp;ipc, inet);
if (sock_flag(sk, SOCK_LOCALROUTE) ||
(msg-&amp;gt;msg_flags &amp;amp; MSG_DONTROUTE) ||
(ipc.opt &amp;amp;&amp;amp; ipc.opt-&amp;gt;opt.is_strictroute)) {
tos |= RTO_ONLINK;
connected = 0;
}
&lt;/code>&lt;/pre>
&lt;h3 id="516-多播或单播multicast-or-unicast">5.1.6 多播或单播（Multicast or unicast）&lt;/h3>
&lt;p>接下来代码开始处理 multicast。这有点复杂，因为用户可以通过 &lt;code>IP_PKTINFO&lt;/code> 辅助消息 来指定发送包的源地址或设备号，如前所述。
如果目标地址是多播地址：&lt;/p>
&lt;ol>
&lt;li>将多播设备（device）的索引（index）设置为发送（写）这个 packet 的设备索引，并且&lt;/li>
&lt;li>packet 的源地址将设置为 multicast 源地址&lt;/li>
&lt;/ol>
&lt;p>如果目标地址不是一个组播地址，则发送 packet 的设备制定为 &lt;code>inet-&amp;gt;uc_index&lt;/code>（单播）， 除非用户使用 &lt;code>IP_PKTINFO&lt;/code> 辅助消息覆盖了它。&lt;/p>
&lt;pre>&lt;code>if (ipv4_is_multicast(daddr)) {
if (!ipc.oif)
ipc.oif = inet-&amp;gt;mc_index;
if (!saddr)
saddr = inet-&amp;gt;mc_addr;
connected = 0;
} else if (!ipc.oif)
ipc.oif = inet-&amp;gt;uc_index;
&lt;/code>&lt;/pre>
&lt;h3 id="517-路由">5.1.7 路由&lt;/h3>
&lt;p>现在开始路由！
UDP 层中处理路由的代码以&lt;strong>快速路径&lt;/strong>（fast path）开始。 如果 socket 已连接，则直接尝试获取路由：&lt;/p>
&lt;pre>&lt;code>if (connected)
rt = (struct rtable *)sk_dst_check(sk, 0);
&lt;/code>&lt;/pre>
&lt;p>如果 socket 未连接，或者虽然已连接，但路由辅助函数 &lt;code>sk_dst_check&lt;/code> 认定路由已过期，则代码将进入&lt;strong>慢速路径&lt;/strong>（slow path）以生成一条路由记录。首先调用 &lt;code>flowi4_init_output&lt;/code> 构造一个描述此 UDP 流的变量：&lt;/p>
&lt;pre>&lt;code>if (rt == NULL) {
struct net *net = sock_net(sk);
fl4 = &amp;amp;fl4_stack;
flowi4_init_output(fl4, ipc.oif, sk-&amp;gt;sk_mark, tos,
RT_SCOPE_UNIVERSE, sk-&amp;gt;sk_protocol,
inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP,
faddr, saddr, dport, inet-&amp;gt;inet_sport);
&lt;/code>&lt;/pre>
&lt;p>然后，socket 及其 flow 实例会传递给安全子系统，这样&lt;a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">SELinux&lt;/a>或&lt;a href="https://en.wikipedia.org/wiki/Smack_(software)">SMACK&lt;/a>这样的系统就可以在 flow 实例上设置安全 ID。 接下来，&lt;code>ip_route_output_flow&lt;/code> 将调用 IP 路由代码，创建一个路由实例：&lt;/p>
&lt;pre>&lt;code>security_sk_classify_flow(sk, flowi4_to_flowi(fl4));
rt = ip_route_output_flow(net, fl4, sk);
&lt;/code>&lt;/pre>
&lt;p>如果创建路由实例失败，并且返回码是 &lt;code>ENETUNREACH&lt;/code>, 则 &lt;code>OUTNOROUTES&lt;/code> 计数器将会加 1。&lt;/p>
&lt;pre>&lt;code>if (IS_ERR(rt)) {
err = PTR_ERR(rt);
rt = NULL;
if (err == -ENETUNREACH)
IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);
goto out;
}
&lt;/code>&lt;/pre>
&lt;p>这些统计计数器所在的源文件、其他可用的计数器及其含义，将将在下面的 UDP 监控部分讨 论。
接下来，如果是广播路由，但 socket 的 &lt;code>SOCK_BROADCAST&lt;/code> 选项未设置，则处理过程终止。 如果 socket 被视为“已连接”，则路由实例将缓存到 socket 上：&lt;/p>
&lt;pre>&lt;code>err = -EACCES;
if ((rt-&amp;gt;rt_flags &amp;amp; RTCF_BROADCAST) &amp;amp;&amp;amp;
!sock_flag(sk, SOCK_BROADCAST))
goto out;
if (connected)
sk_dst_set(sk, dst_clone(&amp;amp;rt-&amp;gt;dst));
&lt;/code>&lt;/pre>
&lt;h3 id="518-msg_confirm-阻止-arp-缓存过期">5.1.8 &lt;code>MSG_CONFIRM&lt;/code>: 阻止 ARP 缓存过期&lt;/h3>
&lt;p>如果调用 &lt;code>send&lt;/code>, &lt;code>sendto&lt;/code> 或 &lt;code>sendmsg&lt;/code> 的时候指定了 &lt;code>MSG_CONFIRM&lt;/code> 参数，UDP 协议层将会如下处理：&lt;/p>
&lt;pre>&lt;code>if (msg-&amp;gt;msg_flags&amp;amp;MSG_CONFIRM)
goto do_confirm;
back_from_confirm:
&lt;/code>&lt;/pre>
&lt;p>该标志提示系统去确认一下 ARP 缓存条目是否仍然有效，防止其被垃圾回收。 &lt;code>do_confirm&lt;/code> 标签位于此函数末尾处，很简单：&lt;/p>
&lt;pre>&lt;code>do_confirm:
dst_confirm(&amp;amp;rt-&amp;gt;dst);
if (!(msg-&amp;gt;msg_flags&amp;amp;MSG_PROBE) || len)
goto back_from_confirm;
err = 0;
goto out;
&lt;/code>&lt;/pre>
&lt;p>&lt;code>dst_confirm&lt;/code> 函数只是在相应的缓存条目上设置一个标记位，稍后当查询邻居缓存并找到 条目时将检查该标志，我们后面一些会看到。此功能通常用于 UDP 网络应用程序，以减少 不必要的 ARP 流量。
此代码确认缓存条目然后跳回 &lt;code>back_from_confirm&lt;/code> 标签。
一旦 &lt;code>do_confirm&lt;/code> 代码跳回到 &lt;code>back_from_confirm&lt;/code>（或者之前就没有执行到 &lt;code>do_confirm&lt;/code> ），代码接下来将处理 UDP cork 和 uncorked 情况。&lt;/p>
&lt;h3 id="519-uncorked-udp-sockets-快速路径准备待发送数据">5.1.9 uncorked UDP sockets 快速路径：准备待发送数据&lt;/h3>
&lt;p>如果不需要 corking，数据就可以封装到一个 &lt;code>struct sk_buff&lt;/code> 实例中并传递给 &lt;code>udp_send_skb&lt;/code>，离 IP 协议层更进了一步。这是通过调用 &lt;code>ip_make_skb&lt;/code> 来完成的。
注意，先前通过调用 &lt;code>ip_route_output_flow&lt;/code> 生成的路由条目也会一起传进来， 它将保存到 skb 里，稍后在 IP 协议层中被使用。&lt;/p>
&lt;pre>&lt;code>/* Lockless fast path for the non-corking case. */
if (!corkreq) {
skb = ip_make_skb(sk, fl4, getfrag, msg-&amp;gt;msg_iov, ulen,
sizeof(struct udphdr), &amp;amp;ipc, &amp;amp;rt,
msg-&amp;gt;msg_flags);
err = PTR_ERR(skb);
if (!IS_ERR_OR_NULL(skb))
err = udp_send_skb(skb, fl4);
goto out;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ip_make_skb&lt;/code> 函数将创建一个 skb，其中需要考虑到很多的事情，例如：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/Maximum_transmission_unit">MTU&lt;/a>&lt;/li>
&lt;li>UDP corking（如果启用）&lt;/li>
&lt;li>UDP Fragmentation Offloading（&lt;a href="https://wiki.linuxfoundation.org/networking/ufo">UFO&lt;/a>）&lt;/li>
&lt;li>Fragmentation（分片）：如果硬件不支持 UFO，但是要传输的数据大于 MTU，需要软件做分片&lt;/li>
&lt;/ol>
&lt;p>大多数网络设备驱动程序不支持 UFO，因为网络硬件本身不支持此功能。我们来看下这段代码，先看 corking 禁用的情况，启用的情况我们更后面再看。&lt;/p>
&lt;h4 id="ip_make_skb">&lt;code>ip_make_skb&lt;/code>&lt;/h4>
&lt;p>定义在&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">net/ipv4/ip_output.c&lt;/a>，这个函数有点复杂。
构建 skb 的时候，&lt;code>ip_make_skb&lt;/code> 依赖的底层代码需要使用一个 corking 变量和一个 queue 变量 ，skb 将通过 queue 变量传入。如果 socket 未被 cork，则会传入一个假的 corking 变量和一个 空队列。
我们来看看假 corking 变量和空队列是如何初始化的：&lt;/p>
&lt;pre>&lt;code>struct sk_buff *ip_make_skb(struct sock *sk, /* more args */)
{
struct inet_cork cork;
struct sk_buff_head queue;
int err;
if (flags &amp;amp; MSG_PROBE)
return NULL;
__skb_queue_head_init(&amp;amp;queue);
cork.flags = 0;
cork.addr = 0;
cork.opt = NULL;
err = ip_setup_cork(sk, &amp;amp;cork, /* more args */);
if (err)
return ERR_PTR(err);
&lt;/code>&lt;/pre>
&lt;p>如上所示，cork 和 queue 都是在栈上分配的，&lt;code>ip_make_skb&lt;/code> 根本不需要它。 &lt;code>ip_setup_cork&lt;/code> 初始化 cork 变量。接下来，调用&lt;code>__ip_append_data&lt;/code> 并传入 cork 和 queue 变 量：&lt;/p>
&lt;pre>&lt;code>err = __ip_append_data(sk, fl4, &amp;amp;queue, &amp;amp;cork,
&amp;amp;current-&amp;gt;task_frag, getfrag,
from, length, transhdrlen, flags);
&lt;/code>&lt;/pre>
&lt;p>我们将在后面看到这个函数是如何工作的，因为不管 socket 是否被 cork，最后都会执行它。
现在，我们只需要知道&lt;code>__ip_append_data&lt;/code> 将创建一个 skb，向其追加数据，并将该 skb 添加 到传入的 queue 变量中。如果追加数据失败，则调用&lt;code>__ip_flush_pending_frame&lt;/code> 丢弃数据 并向上返回错误（指针类型）：&lt;/p>
&lt;pre>&lt;code>if (err) {
__ip_flush_pending_frames(sk, &amp;amp;queue, &amp;amp;cork);
return ERR_PTR(err);
}
&lt;/code>&lt;/pre>
&lt;p>最后，如果没有发生错误，&lt;code>__ip_make_skb&lt;/code> 将 skb 出队，添加 IP 选项，并返回一个准备好传 递给更底层发送的 skb：&lt;/p>
&lt;pre>&lt;code>return __ip_make_skb(sk, fl4, &amp;amp;queue, &amp;amp;cork);
&lt;/code>&lt;/pre>
&lt;h4 id="发送数据">发送数据&lt;/h4>
&lt;p>如果没有错误，skb 就会交给 &lt;code>udp_send_skb&lt;/code>，后者会继续将其传给下一层协议，IP 协议：&lt;/p>
&lt;pre>&lt;code>err = PTR_ERR(skb);
if (!IS_ERR_OR_NULL(skb))
err = udp_send_skb(skb, fl4);
goto out;
&lt;/code>&lt;/pre>
&lt;p>如果有错误，错误计数就会有相应增加。后面的“错误计数”部分会详细介绍。&lt;/p>
&lt;h3 id="5110-没有被-cork-的数据时的慢路径">5.1.10 没有被 cork 的数据时的慢路径&lt;/h3>
&lt;p>如果使用了 UDP corking，但之前没有数据被 cork，则慢路径开始：&lt;/p>
&lt;ol>
&lt;li>对 socket 加锁&lt;/li>
&lt;li>检查应用程序是否有 bug：已经被 cork 的 socket 是否再次被 cork&lt;/li>
&lt;li>设置该 UDP flow 的一些参数，为 corking 做准备&lt;/li>
&lt;li>将要发送的数据追加到现有数据&lt;/li>
&lt;/ol>
&lt;p>&lt;code>udp_sendmsg&lt;/code> 代码继续向下看，就是这一逻辑：&lt;/p>
&lt;pre>&lt;code>lock_sock(sk);
if (unlikely(up-&amp;gt;pending)) {
/* The socket is already corked while preparing it. */
/* ... which is an evident application bug. --ANK */
release_sock(sk);
LIMIT_NETDEBUG(KERN_DEBUG pr_fmt(&amp;quot;cork app bug 2\n&amp;quot;));
err = -EINVAL;
goto out;
}
/*
* Now cork the socket to pend data.
*/
fl4 = &amp;amp;inet-&amp;gt;cork.fl.u.ip4;
fl4-&amp;gt;daddr = daddr;
fl4-&amp;gt;saddr = saddr;
fl4-&amp;gt;fl4_dport = dport;
fl4-&amp;gt;fl4_sport = inet-&amp;gt;inet_sport;
up-&amp;gt;pending = AF_INET;
do_append_data:
up-&amp;gt;len += ulen;
err = ip_append_data(sk, fl4, getfrag, msg-&amp;gt;msg_iov, ulen,
sizeof(struct udphdr), &amp;amp;ipc, &amp;amp;rt,
corkreq ? msg-&amp;gt;msg_flags|MSG_MORE : msg-&amp;gt;msg_flags);
&lt;/code>&lt;/pre>
&lt;h4 id="ip_append_data">&lt;code>ip_append_data&lt;/code>&lt;/h4>
&lt;p>这个函数简单封装了&lt;code>__ip_append_data&lt;/code>，在调用后者之前，做了两件重要的事情：&lt;/p>
&lt;ol>
&lt;li>检查是否从用户传入了 &lt;code>MSG_PROBE&lt;/code> 标志。该标志表示用户不想真正发送数据，只是做路 径探测（例如，确定&lt;a href="https://en.wikipedia.org/wiki/Path_MTU_Discovery">PMTU&lt;/a>）&lt;/li>
&lt;li>检查 socket 的发送队列是否为空。如果为空，意味着没有 cork 数据等待处理，因此调用 &lt;code>ip_setup_cork&lt;/code> 来设置 corking&lt;/li>
&lt;/ol>
&lt;p>一旦处理了上述条件，就调用&lt;code>__ip_append_data&lt;/code> 函数，该函数包含用于将数据处理成数据 包的大量逻辑。&lt;/p>
&lt;h4 id="__ip_append_data">&lt;code>__ip_append_data&lt;/code>&lt;/h4>
&lt;p>如果 socket 是 corked，则从 &lt;code>ip_append_data&lt;/code> 调用此函数；如果 socket 未被 cork，则从 &lt;code>ip_make_skb&lt;/code> 调用此函数。在任何一种情况下，函数都将分配一个新缓冲区来存储传入 的数据，或者将数据附加到现有数据中。
这种工作的方式围绕 socket 的发送队列。等待发送的现有数据（例如，如果 socket 被 cork） 将在队列中有一个对应条目，可以被追加数据。
这个函数很复杂;它执行很多计算以确定如何构造传递给下面的网络层的 skb。
该函数的重点包括：&lt;/p>
&lt;ol>
&lt;li>如果硬件支持，则处理 UDP Fragmentation Offload（UFO）。绝大多数网络硬件不支持 UFO。如果你的网卡驱动程序支持它，它将设置 &lt;code>NETIF_F_UFO&lt;/code> 标记位&lt;/li>
&lt;li>处理支持分散/收集（ &lt;a href="https://en.wikipedia.org/wiki/Vectored_I/O">scatter/gather&lt;/a>）IO 的网卡。许多 卡都支持此功能，并使用 &lt;code>NETIF_F_SG&lt;/code> 标志进行通告。支持该特性的网卡可以处理数据 被分散到多个 buffer 的数据包;内核不需要花时间将多个缓冲区合并成一个缓冲区中。避 免这种额外的复制会提升性能，大多数网卡都支持此功能&lt;/li>
&lt;li>通过调用 &lt;code>sock_wmalloc&lt;/code> 跟踪发送队列的大小。当分配新的 skb 时，skb 的大小由创建它 的 socket 计费（charge），并计入 socket 发送队列的已分配字节数。如果发送队列已经 没有足够的空间（超过计费限制），则 skb 并分配失败并返回错误。我们将在下面的调优 部分中看到如何设置 socket 发送队列大小（txqueuelen）&lt;/li>
&lt;li>更新错误统计信息。此函数中的任何错误都会增加“discard”计数。我们将在下面的监控部分中 看到如何读取此值&lt;/li>
&lt;/ol>
&lt;p>函数执行成功后返回 0，以及一个适用于网络设备传输的 skb。
在 unorked 情况下，持有 skb 的 queue 被作为参数传递给上面描述的&lt;code>__ip_make_skb&lt;/code>，在那里 它被出队并通过 &lt;code>udp_send_skb&lt;/code> 发送到更底层。
在 cork 的情况下，&lt;code>__ip_append_data&lt;/code> 的返回值向上传递。数据位于发送队列中，直到 &lt;code>udp_sendmsg&lt;/code> 确定是时候调用 &lt;code>udp_push_pending_frames&lt;/code> 来完成 skb，后者会进一步调用 &lt;code>udp_send_skb&lt;/code>。&lt;/p>
&lt;h4 id="flushing-corked-sockets">Flushing corked sockets&lt;/h4>
&lt;p>现在，&lt;code>udp_sendmsg&lt;/code> 会继续，检查&lt;code>__ip_append_skb&lt;/code> 的返回值（错误码）：&lt;/p>
&lt;pre>&lt;code>if (err)
udp_flush_pending_frames(sk);
else if (!corkreq)
err = udp_push_pending_frames(sk);
else if (unlikely(skb_queue_empty(&amp;amp;sk-&amp;gt;sk_write_queue)))
up-&amp;gt;pending = 0;
release_sock(sk);
&lt;/code>&lt;/pre>
&lt;p>我们来看看每个情况：&lt;/p>
&lt;ol>
&lt;li>如果出现错误（错误为非零），则调用 &lt;code>udp_flush_pending_frames&lt;/code>，这将取消 cork 并从 socket 的发送队列中删除所有数据&lt;/li>
&lt;li>如果在未指定 &lt;code>MSG_MORE&lt;/code> 的情况下发送此数据，则调用 &lt;code>udp_push_pending_frames&lt;/code>，它将数据传递到更下面的网络层&lt;/li>
&lt;li>如果发送队列为空，请将 socket 标记为不再 cork&lt;/li>
&lt;/ol>
&lt;p>如果追加操作完成并且有更多数据要进入 cork，则代码将做一些清理工作，并返回追加数据的长度：&lt;/p>
&lt;pre>&lt;code>ip_rt_put(rt);
if (free)
kfree(ipc.opt);
if (!err)
return len;
&lt;/code>&lt;/pre>
&lt;p>这就是内核如何处理 corked UDP sockets 的。&lt;/p>
&lt;h3 id="5111-error-accounting">5.1.11 Error accounting&lt;/h3>
&lt;p>如果：&lt;/p>
&lt;ol>
&lt;li>non-corking 快速路径创建 skb 失败，或 &lt;code>udp_send_skb&lt;/code> 返回错误，或&lt;/li>
&lt;li>&lt;code>ip_append_data&lt;/code> 无法将数据附加到 corked UDP socket，或&lt;/li>
&lt;li>当 &lt;code>udp_push_pending_frames&lt;/code> 调用 &lt;code>udp_send_skb&lt;/code> 发送 corked skb 时后者返回错误&lt;/li>
&lt;/ol>
&lt;p>仅当返回的错误是 &lt;code>ENOBUFS&lt;/code>（内核无可用内存）或 socket 已设置 &lt;code>SOCK_NOSPACE&lt;/code>（发送队 列已满）时，&lt;code>SNDBUFERRORS&lt;/code> 统计信息才会增加：&lt;/p>
&lt;pre>&lt;code>/*
* ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space. Reporting
* ENOBUFS might not be good (it's not tunable per se), but otherwise
* we don't have a good statistic (IpOutDiscards but it can be too many
* things). We could add another new stat but at least for now that
* seems like overkill.
*/
if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &amp;amp;sk-&amp;gt;sk_socket-&amp;gt;flags)) {
UDP_INC_STATS_USER(sock_net(sk),
UDP_MIB_SNDBUFERRORS, is_udplite);
}
return err;
&lt;/code>&lt;/pre>
&lt;p>我们接下来会在监控小节里看到如何读取这些计数。&lt;/p>
&lt;h2 id="52-udp_send_skb">5.2 &lt;code>udp_send_skb&lt;/code>&lt;/h2>
&lt;p>&lt;code>udp_sendmsg&lt;/code> 通过调用 &lt;code>udp_send_skb&lt;/code> 函数将 skb 送到下一网络层，在本例中是 IP 协议层。 这个函数做了一些重要的事情：&lt;/p>
&lt;ol>
&lt;li>向 skb 添加 UDP 头&lt;/li>
&lt;li>处理校验和：软件校验和，硬件校验和或无校验和（如果禁用）&lt;/li>
&lt;li>调用 &lt;code>ip_send_skb&lt;/code> 将 skb 发送到 IP 协议层&lt;/li>
&lt;li>更新发送成功或失败的统计计数器&lt;/li>
&lt;/ol>
&lt;p>让我们来看看。首先，创建 UDP 头：&lt;/p>
&lt;pre>&lt;code>static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)
{
/* useful variables ... */
/*
* Create a UDP header
*/
uh = udp_hdr(skb);
uh-&amp;gt;source = inet-&amp;gt;inet_sport;
uh-&amp;gt;dest = fl4-&amp;gt;fl4_dport;
uh-&amp;gt;len = htons(len);
uh-&amp;gt;check = 0;
&lt;/code>&lt;/pre>
&lt;p>接下来，处理校验和。有几种情况：&lt;/p>
&lt;ol>
&lt;li>首先处理&lt;a href="https://en.wikipedia.org/wiki/UDP-Lite">UDP-Lite&lt;/a>校验和&lt;/li>
&lt;li>接下来，如果 socket 校验和选项被关闭（&lt;code>setsockopt&lt;/code> 带 &lt;code>SO_NO_CHECK&lt;/code> 参数），它将被标记为校 验和关闭&lt;/li>
&lt;li>接下来，如果硬件支持 UDP 校验和，则将调用 &lt;code>udp4_hwcsum&lt;/code> 来设置它。请注意，如果数 据包是分段的，内核将在软件中生成校验和，你可以在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L720-L763">udp4_hwcsum&lt;/a> 的源代码中看到这一点&lt;/li>
&lt;li>最后，通过调用 &lt;code>udp_csum&lt;/code> 生成软件校验和&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>if (is_udplite) /* UDP-Lite */
csum = udplite_csum(skb);
else if (sk-&amp;gt;sk_no_check == UDP_CSUM_NOXMIT) { /* UDP csum disabled */
skb-&amp;gt;ip_summed = CHECKSUM_NONE;
goto send;
} else if (skb-&amp;gt;ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */
udp4_hwcsum(skb, fl4-&amp;gt;saddr, fl4-&amp;gt;daddr);
goto send;
} else
csum = udp_csum(skb);
&lt;/code>&lt;/pre>
&lt;p>接下来，添加了&lt;a href="https://en.wikipedia.org/wiki/User_Datagram_Protocol#IPv4_Pseudo_Header">伪头  &lt;/a>：&lt;/p>
&lt;pre>&lt;code>uh-&amp;gt;check = csum_tcpudp_magic(fl4-&amp;gt;saddr, fl4-&amp;gt;daddr, len,
sk-&amp;gt;sk_protocol, csum);
if (uh-&amp;gt;check == 0)
uh-&amp;gt;check = CSUM_MANGLED_0;
&lt;/code>&lt;/pre>
&lt;p>如果校验和为 0，则根据&lt;a href="https://tools.ietf.org/html/rfc768">RFC 768&lt;/a>，校验为全 1（ transmitted as all ones (the equivalent in one’s complement arithmetic)）。最 后，将 skb 传递给 IP 协议层并增加统计计数：&lt;/p>
&lt;pre>&lt;code>send:
err = ip_send_skb(sock_net(sk), skb);
if (err) {
if (err == -ENOBUFS &amp;amp;&amp;amp; !inet-&amp;gt;recverr) {
UDP_INC_STATS_USER(sock_net(sk),
UDP_MIB_SNDBUFERRORS, is_udplite);
err = 0;
}
} else
UDP_INC_STATS_USER(sock_net(sk),
UDP_MIB_OUTDATAGRAMS, is_udplite);
return err;
&lt;/code>&lt;/pre>
&lt;p>如果 &lt;code>ip_send_skb&lt;/code> 成功，将更新 &lt;code>OUTDATAGRAMS&lt;/code> 统计。如果 IP 协议层报告错误，并且错误 是 &lt;code>ENOBUFS&lt;/code>（内核缺少内存）而且错误 queue（&lt;code>inet-&amp;gt;recverr&lt;/code>）没有启用，则更新 &lt;code>SNDBUFERRORS&lt;/code>。
在继续讨论 IP 协议层之前，让我们先看看如何在 Linux 内核中监视和调优 UDP 协议层。&lt;/p>
&lt;h2 id="53-监控udp-层统计">5.3 监控：UDP 层统计&lt;/h2>
&lt;p>两个非常有用的获取 UDP 协议统计文件：&lt;/p>
&lt;ul>
&lt;li>&lt;code>/proc/net/snmp&lt;/code>&lt;/li>
&lt;li>&lt;code>/proc/net/udp&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="procnetsnmp">/proc/net/snmp&lt;/h3>
&lt;p>监控 UDP 协议层统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/snmp | grep Udp\:
Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors
Udp: 16314 0 0 17161 0 0
&lt;/code>&lt;/pre>
&lt;p>要准确地理解这些计数，你需要仔细地阅读内核代码。一些类型的错误计数并不是只出现在 一种计数中，而可能是出现在多个计数中。&lt;/p>
&lt;ul>
&lt;li>&lt;code>InDatagrams&lt;/code>: Incremented when recvmsg was used by a userland program to read datagram. Also incremented when a UDP packet is encapsulated and sent back for processing.&lt;/li>
&lt;li>&lt;code>NoPorts&lt;/code>: Incremented when UDP packets arrive destined for a port where no program is listening.&lt;/li>
&lt;li>&lt;code>InErrors&lt;/code>: Incremented in several cases: no memory in the receive queue, when a bad checksum is seen, and if sk_add_backlog fails to add the datagram.&lt;/li>
&lt;li>&lt;code>OutDatagrams&lt;/code>: Incremented when a UDP packet is handed down without error to the IP protocol layer to be sent.&lt;/li>
&lt;li>&lt;code>RcvbufErrors&lt;/code>: Incremented when sock_queue_rcv_skb reports that no memory is available; this happens if sk-&amp;gt;sk_rmem_alloc is greater than or equal to sk-&amp;gt;sk_rcvbuf.&lt;/li>
&lt;li>&lt;code>SndbufErrors&lt;/code>: Incremented if the IP protocol layer reported an error when trying to send the packet and no error queue has been setup. Also incremented if no send queue space or kernel memory are available.&lt;/li>
&lt;li>&lt;code>InCsumErrors&lt;/code>: Incremented when a UDP checksum failure is detected. Note that in all cases I could find, InCsumErrors is incremented at the same time as InErrors. Thus, InErrors - InCsumErros should yield the count of memory related errors on the receive side.&lt;/li>
&lt;/ul>
&lt;p>注意，UDP 协议层发现的某些错误会出现在其他协议层的统计信息中。一个例子：路由错误 。 &lt;code>udp_sendmsg&lt;/code> 发现的路由错误将导致 IP 协议层的 &lt;code>OutNoRoutes&lt;/code> 统计增加。&lt;/p>
&lt;h3 id="procnetudp">/proc/net/udp&lt;/h3>
&lt;p>监控 UDP socket 统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/udp
sl local_address rem_address st tx_queue rx_queue tr tm-&amp;gt;when retrnsmt uid timeout inode ref pointer drops
515: 00000000:B346 00000000:0000 07 00000000:00000000 00:00000000 00000000 104 0 7518 2 0000000000000000 0
558: 00000000:0371 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7408 2 0000000000000000 0
588: 0100007F:038F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7511 2 0000000000000000 0
769: 00000000:0044 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7673 2 0000000000000000 0
812: 00000000:006F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7407 2 0000000000000000 0
&lt;/code>&lt;/pre>
&lt;p>每一列的意思：&lt;/p>
&lt;ul>
&lt;li>&lt;code>sl&lt;/code>: Kernel hash slot for the socket&lt;/li>
&lt;li>&lt;code>local_address&lt;/code>: Hexadecimal local address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>rem_address&lt;/code>: Hexadecimal remote address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>st&lt;/code>: The state of the socket. Oddly enough, the UDP protocol layer seems to use some TCP socket states. In the example above, 7 is TCP_CLOSE.&lt;/li>
&lt;li>&lt;code>tx_queue&lt;/code>: The amount of memory allocated in the kernel for outgoing UDP datagrams.&lt;/li>
&lt;li>&lt;code>rx_queue&lt;/code>: The amount of memory allocated in the kernel for incoming UDP datagrams.&lt;/li>
&lt;li>&lt;code>tr&lt;/code>, tm-&amp;gt;when, retrnsmt: These fields are unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>uid&lt;/code>: The effective user id of the user who created this socket.&lt;/li>
&lt;li>&lt;code>timeout&lt;/code>: Unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>inode&lt;/code>: The inode number corresponding to this socket. You can use this to help you determine which user process has this socket open. Check /proc/[pid]/fd, which will contain symlinks to socket[:inode].&lt;/li>
&lt;li>&lt;code>ref&lt;/code>: The current reference count for the socket.&lt;/li>
&lt;li>&lt;code>pointer&lt;/code>: The memory address in the kernel of the struct sock.&lt;/li>
&lt;li>&lt;code>drops&lt;/code>: The number of datagram drops associated with this socket. Note that this does not include any drops related to sending datagrams (on corked UDP sockets or otherwise); this is only incremented in receive paths as of the kernel version examined by this blog post.&lt;/li>
&lt;/ul>
&lt;p>打印这些计数的代码在&lt;a href="https://github.com/torvalds/linux/blob/master/net/ipv4/udp.c#L2396-L2431">net/ipv4/udp.c&lt;/a>。&lt;/p>
&lt;h2 id="54-调优socket-发送队列内存大小">5.4 调优：socket 发送队列内存大小&lt;/h2>
&lt;p>发送队列（也叫“写队列”）的最大值可以通过设置 &lt;code>net.core.wmem_max sysctl&lt;/code> 进行修改。&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.wmem_max=8388608
&lt;/code>&lt;/pre>
&lt;p>&lt;code>sk-&amp;gt;sk_write_queue&lt;/code> 用 &lt;code>net.core.wmem_default&lt;/code> 初始化， 这个值也可以调整。
调整初始发送 buffer 大小：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.wmem_default=8388608
&lt;/code>&lt;/pre>
&lt;p>也可以通过从应用程序调用 &lt;code>setsockopt&lt;/code> 并传递 &lt;code>SO_SNDBUF&lt;/code> 来设置 &lt;code>sk-&amp;gt;sk_write_queue&lt;/code> 。通过 &lt;code>setsockopt&lt;/code> 设置的最大值是 &lt;code>net.core.wmem_max&lt;/code>。
不过，可以通过 &lt;code>setsockopt&lt;/code> 并传递 &lt;code>SO_SNDBUFFORCE&lt;/code> 来覆盖 &lt;code>net.core.wmem_max&lt;/code> 限制， 这需要 &lt;code>CAP_NET_ADMIN&lt;/code> 权限。
每次调用&lt;code>__ip_append_data&lt;/code> 分配 skb 时，&lt;code>sk-&amp;gt;sk_wmem_alloc&lt;/code> 都会递增。正如我们所看到 的，UDP 数据报传输速度很快，通常不会在发送队列中花费太多时间。&lt;/p>
&lt;h1 id="6-ip-协议层">6 IP 协议层&lt;/h1>
&lt;p>UDP 协议层通过调用 &lt;code>ip_send_skb&lt;/code> 将 skb 交给 IP 协议层，所以我们从这里开始，探索一下 IP 协议层。&lt;/p>
&lt;h2 id="61-ip_send_skb">6.1 &lt;code>ip_send_skb&lt;/code>&lt;/h2>
&lt;p>&lt;code>ip_send_skb&lt;/code> 函数定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_output.c#L1367-L1380">net/ipv4/ip_output.c&lt;/a> 中，非常简短。它只是调用 &lt;code>ip_local_out&lt;/code>，如果调用失败，就更新相应的错误计数。让 我们来看看：&lt;/p>
&lt;pre>&lt;code>int ip_send_skb(struct net *net, struct sk_buff *skb)
{
int err;
err = ip_local_out(skb);
if (err) {
if (err &amp;gt; 0)
err = net_xmit_errno(err);
if (err)
IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);
}
return err;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>net_xmit_errno&lt;/code> 函数将低层错误转换为 IP 和 UDP 协议层所能理解的错误。如果发生错误， IP 协议计数器 &lt;code>OutDiscards&lt;/code> 会递增。稍后我们将看到读取哪些文件可以获取此统计信 息。现在，让我们继续，看看 &lt;code>ip_local_out&lt;/code> 带我们去哪。&lt;/p>
&lt;h2 id="62-ip_local_out-and-__ip_local_out">6.2 &lt;code>ip_local_out&lt;/code> and &lt;code>__ip_local_out&lt;/code>&lt;/h2>
&lt;p>幸运的是，&lt;code>ip_local_out&lt;/code> 和&lt;code>__ip_local_out&lt;/code> 都很简单。&lt;code>ip_local_out&lt;/code> 只需调用 &lt;code>__ip_local_out&lt;/code>，如果返回值为 1，则调用路由层 &lt;code>dst_output&lt;/code> 发送数据包：&lt;/p>
&lt;pre>&lt;code>int ip_local_out(struct sk_buff *skb)
{
int err;
err = __ip_local_out(skb);
if (likely(err == 1))
err = dst_output(skb);
return err;
}
&lt;/code>&lt;/pre>
&lt;p>我们来看看&lt;code>__ip_local_out&lt;/code> 的代码：&lt;/p>
&lt;pre>&lt;code>int __ip_local_out(struct sk_buff *skb)
{
struct iphdr *iph = ip_hdr(skb);
iph-&amp;gt;tot_len = htons(skb-&amp;gt;len);
ip_send_check(iph);
return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,
skb_dst(skb)-&amp;gt;dev, dst_output);
}
&lt;/code>&lt;/pre>
&lt;p>可以看到，该函数首先做了两件重要的事情：&lt;/p>
&lt;ol>
&lt;li>设置 IP 数据包的长度&lt;/li>
&lt;li>调用 &lt;code>ip_send_check&lt;/code> 来计算要写入 IP 头的校验和。&lt;code>ip_send_check&lt;/code> 函数将进一步调用 名为 &lt;code>ip_fast_csum&lt;/code> 的函数来计算校验和。在 x86 和 x86_64 体系结构上，此函数用汇编实 现，代码：&lt;a href="https://github.com/torvalds/linux/blob/v3.13/arch/x86/include/asm/checksum_64.h#L40-L73">64 位实现&lt;/a> 和&lt;a href="https://github.com/torvalds/linux/blob/v3.13/arch/x86/include/asm/checksum_32.h#L63-L98">32 位实现&lt;/a> 。&lt;/li>
&lt;/ol>
&lt;p>接下来，IP 协议层将通过调用 &lt;code>nf_hook&lt;/code> 进入 netfilter，其返回值将传递回 &lt;code>ip_local_out&lt;/code> 。 如果 &lt;code>nf_hook&lt;/code> 返回 1，则表示允许数据包通过，并且调用者应该自己发送数据包。这正 是我们在上面看到的情况：&lt;code>ip_local_out&lt;/code> 检查返回值 1 时，自己通过调用 &lt;code>dst_output&lt;/code> 发 送数据包。&lt;/p>
&lt;h2 id="63-netfilter-and-nf_hook">6.3 netfilter and nf_hook&lt;/h2>
&lt;p>简洁起见，我决定跳过对 netfilter，iptables 和 conntrack 的深入研究。如果你想深入了解 netfilter 的代码实现，可以从 &lt;code>include/linux/netfilter.h&lt;/code>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netfilter.h#L142-L147">这里  &lt;/a>和 &lt;code>net/netfilter/core.c&lt;/code>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/netfilter/core.c#L168-L209">这里  &lt;/a>开 始。
简短版本是：&lt;code>nf_hook&lt;/code> 只是一个 wrapper，它调用 &lt;code>nf_hook_thresh&lt;/code>，首先检查是否有为这 个&lt;strong>协议族&lt;/strong>和&lt;strong>hook 类型&lt;/strong>（这里分别为 &lt;code>NFPROTO_IPV4&lt;/code> 和 &lt;code>NF_INET_LOCAL_OUT&lt;/code>）安装 的过滤器，然后将返回到 IP 协议层，避免深入到 netfilter 或更下面，比如 iptables 和 conntrack。
请记住：如果你有非常多或者非常复杂的 netfilter 或 iptables 规则，那些规则将在触发 &lt;code>sendmsg&lt;/code> 系统调的用户进程的上下文中执行。如果对这个用户进程设置了 CPU 亲和性，相应 的 CPU 将花费系统时间（system time）处理出站（outbound）iptables 规则。如果你在做性 能回归测试，那可能要考虑根据系统的负载，将相应的用户进程绑到到特定的 CPU，或者是 减少 netfilter/iptables 规则的复杂度，以减少对性能测试的影响。
出于讨论目的，我们假设 &lt;code>nf_hook&lt;/code> 返回 1，表示调用者（在这种情况下是 IP 协议层）应该 自己发送数据包。&lt;/p>
&lt;h2 id="64-目的路由缓存">6.4 目的（路由）缓存&lt;/h2>
&lt;p>dst 代码在 Linux 内核中实现&lt;strong>协议无关&lt;/strong>的目标缓存。为了继续学习发送 UDP 数据报的流程 ，我们需要了解 dst 条目是如何被设置的，首先来看 dst 条目和路由是如何生成的。 目标缓 存，路由和邻居子系统，任何一个都可以拿来单独详细的介绍。我们不深入细节，只是快速 地看一下它们是如何组合到一起的。
我们上面看到的代码调用了 &lt;code>dst_output(skb)&lt;/code>。 此函数只是查找关联到这个 skb 的 dst 条目 ，然后调用 &lt;code>output&lt;/code> 方法。代码如下：&lt;/p>
&lt;pre>&lt;code>/* Output packet to network from transport. */
static inline int dst_output(struct sk_buff *skb)
{
return skb_dst(skb)-&amp;gt;output(skb);
}
&lt;/code>&lt;/pre>
&lt;p>看起来很简单，但是 &lt;code>output&lt;/code> 方法之前是如何关联到 dst 条目的？
首先很重要的一点，目标缓存条目是以多种不同方式添加的。到目前为止，我们已经在代码 中看到的一种方法是从 &lt;code>udp_sendmsg&lt;/code> 调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L2252-L2267">ip_route_output_flow&lt;/a> 。&lt;code>ip_route_output_flow&lt;/code> 函数调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L1990-L2173">__ip_route_output_key&lt;/a> ，后者进而调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/route.c#L1868-L1988">__mkroute_output&lt;/a> 。 &lt;code>__mkroute_output&lt;/code> 函数创建路由和目标缓存条目。当它执行创建操作时，它会判断哪 个 &lt;code>output&lt;/code> 方法适合此 dst。大多数时候，这个函数是 &lt;code>ip_output&lt;/code>。&lt;/p>
&lt;h2 id="65-ip_output">6.5 &lt;code>ip_output&lt;/code>&lt;/h2>
&lt;p>在 UDP IPv4 情况下，上面的 &lt;code>output&lt;/code> 方法指向的是 &lt;code>ip_output&lt;/code>。&lt;code>ip_output&lt;/code> 函数很简单：&lt;/p>
&lt;pre>&lt;code>int ip_output(struct sk_buff *skb)
{
struct net_device *dev = skb_dst(skb)-&amp;gt;dev;
IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb-&amp;gt;len);
skb-&amp;gt;dev = dev;
skb-&amp;gt;protocol = htons(ETH_P_IP);
return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,
ip_finish_output,
!(IPCB(skb)-&amp;gt;flags &amp;amp; IPSKB_REROUTED));
}
&lt;/code>&lt;/pre>
&lt;p>首先，更新 &lt;code>IPSTATS_MIB_OUT&lt;/code> 统计计数。&lt;code>IP_UPD_PO_STATS&lt;/code> 宏将更新字节数和包数统计。 我们将在后面的部分中看到如何获取 IP 协议层统计信息以及它们各自的含义。接下来，设置 要发送此 skb 的设备，以及协议。
最后，通过调用 &lt;code>NF_HOOK_COND&lt;/code> 将控制权交给 netfilter。查看 &lt;code>NF_HOOK_COND&lt;/code> 的函数原型 有助于更清晰地解释它如何工作。来自 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netfilter.h#L177-L188">include/linux/netfilter.h&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>static inline int
NF_HOOK_COND(uint8_t pf, unsigned int hook, struct sk_buff *skb,
struct net_device *in, struct net_device *out,
int (*okfn)(struct sk_buff *), bool cond)
&lt;/code>&lt;/pre>
&lt;p>&lt;code>NF_HOOK_COND&lt;/code> 通过检查传入的条件来工作。在这里条件是&lt;code>!(IPCB(skb)-&amp;gt;flags &amp;amp; IPSKB_REROUTED&lt;/code>。如果此条件为真，则 skb 将发送给 netfilter。如果 netfilter 允许包通过 ，&lt;code>okfn&lt;/code> 回调函数将被调用。在这里，&lt;code>okfn&lt;/code> 是 &lt;code>ip_finish_output&lt;/code>。&lt;/p>
&lt;h2 id="66-ip_finish_output">6.6 &lt;code>ip_finish_output&lt;/code>&lt;/h2>
&lt;pre>&lt;code>static int ip_finish_output(struct sk_buff *skb)
{
#if defined(CONFIG_NETFILTER) &amp;amp;&amp;amp; defined(CONFIG_XFRM)
/* Policy lookup after SNAT yielded a new policy */
if (skb_dst(skb)-&amp;gt;xfrm != NULL) {
IPCB(skb)-&amp;gt;flags |= IPSKB_REROUTED;
return dst_output(skb);
}
#endif
if (skb-&amp;gt;len &amp;gt; ip_skb_dst_mtu(skb) &amp;amp;&amp;amp; !skb_is_gso(skb))
return ip_fragment(skb, ip_finish_output2);
else
return ip_finish_output2(skb);
}
&lt;/code>&lt;/pre>
&lt;p>如果内核启用了 netfilter 和数据包转换（XFRM），则更新 skb 的标志并通过 &lt;code>dst_output&lt;/code> 将 其发回。
更常见的两种情况是：&lt;/p>
&lt;ol>
&lt;li>如果数据包的长度大于 MTU 并且分片不会 offload 到设备，则会调用 &lt;code>ip_fragment&lt;/code> 在发送之前对数据包进行分片&lt;/li>
&lt;li>否则，数据包将直接发送到 ip_finish_output2&lt;/li>
&lt;/ol>
&lt;p>在继续我们的内核之前，让我们简单地谈谈 Path MTU Discovery。&lt;/p>
&lt;h3 id="path-mtu-discovery">Path MTU Discovery&lt;/h3>
&lt;p>Linux 提供了一个功能，我迄今为止一直避免提及：&lt;a href="https://en.wikipedia.org/wiki/Path_MTU_Discovery">路径 MTU 发现  &lt;/a>。此功能允许内核自动确定 路由的最大传输单元（ &lt;a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit">MTU&lt;/a> ）。发送小于或等于该路由的 MTU 的包意味着可以避免 IP 分片，这是推荐设置，因为数 据包分片会消耗系统资源，而避免分片看起来很容易：只需发送足够小的不需要分片的数据 包。
你可以在应用程序中通过调用 &lt;code>setsockopt&lt;/code> 带 &lt;code>SOL_IP&lt;/code> 和 &lt;code>IP_MTU_DISCOVER&lt;/code> 选项，在 packet 级别来调整路径 MTU 发现设置，相应的合法值参考 IP 协议的&lt;a href="http://man7.org/linux/man-pages/man7/ip.7.html">man page&lt;/a>。例如，你可能想设置的值是 ：&lt;code>IP_PMTUDISC_DO&lt;/code>，表示“始终执行路径 MTU 发现”。更高级的网络应用程序或诊断工具可 能选择自己实现&lt;a href="https://www.ietf.org/rfc/rfc4821.txt">RFC 4821&lt;/a>，以在应用程序启动 时针对特定的路由做 PMTU。在这种情况下，你可以使用 &lt;code>IP_PMTUDISC_PROBE&lt;/code> 选项告诉内核 设置“Do not Fragment”位，这就会允许你发送大于 PMTU 的数据。
应用程序可以通过调用 &lt;code>getsockopt&lt;/code> 带 &lt;code>SOL_IP&lt;/code> 和 &lt;code>IP_MTU&lt;/code> 选项来查看当前 PMTU。可以使 用它指导应用程序在发送之前，构造 UDP 数据报的大小。
如果已启用 PMTU 发现，则发送大于 PMTU 的 UDP 数据将导致应用程序收到 &lt;code>EMSGSIZE&lt;/code> 错误。 这种情况下，应用程序只能减小 packet 大小重试。
强烈建议启用 PTMU 发现，因此我将不再详细描述 IP 分片的代码。当我们查看 IP 协议层统计信 息时，我将解释所有统计信息，包括与分片相关的统计信息。其中许多计数都在 &lt;code>ip_fragment&lt;/code> 中更新的。不管分片与否，代码最后都会调到 &lt;code>ip_finish_output2&lt;/code>，所以让 我们继续。&lt;/p>
&lt;h2 id="67-ip_finish_output2">6.7 &lt;code>ip_finish_output2&lt;/code>&lt;/h2>
&lt;p>IP 分片后调用 &lt;code>ip_finish_output2&lt;/code>，另外 &lt;code>ip_finish_output&lt;/code> 也会直接调用它。这个函数 在将包发送到邻居缓存之前处理各种统计计数器。让我们看看它是如何工作的：&lt;/p>
&lt;pre>&lt;code>static inline int ip_finish_output2(struct sk_buff *skb)
{
/* variable declarations */
if (rt-&amp;gt;rt_type == RTN_MULTICAST) {
IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTMCAST, skb-&amp;gt;len);
} else if (rt-&amp;gt;rt_type == RTN_BROADCAST)
IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTBCAST, skb-&amp;gt;len);
/* Be paranoid, rather than too clever. */
if (unlikely(skb_headroom(skb) &amp;lt; hh_len &amp;amp;&amp;amp; dev-&amp;gt;header_ops)) {
struct sk_buff *skb2;
skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(dev));
if (skb2 == NULL) {
kfree_skb(skb);
return -ENOMEM;
}
if (skb-&amp;gt;sk)
skb_set_owner_w(skb2, skb-&amp;gt;sk);
consume_skb(skb);
skb = skb2;
}
&lt;/code>&lt;/pre>
&lt;p>如果与此数据包关联的路由是多播类型，则使用 &lt;code>IP_UPD_PO_STATS&lt;/code> 宏来增加 &lt;code>OutMcastPkts&lt;/code> 和 &lt;code>OutMcastOctets&lt;/code> 计数。如果广播路由，则会增加 &lt;code>OutBcastPkts&lt;/code> 和 &lt;code>OutBcastOctets&lt;/code> 计数。
接下来，确保 skb 结构有足够的空间容纳需要添加的任何链路层头。如果空间不够，则调用 &lt;code>skb_realloc_headroom&lt;/code> 分配额外的空间，并且新的 skb 的费用（charge）记在相关的 socket 上。&lt;/p>
&lt;pre>&lt;code>rcu_read_lock_bh();
nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)-&amp;gt;daddr);
neigh = __ipv4_neigh_lookup_noref(dev, nexthop);
if (unlikely(!neigh))
neigh = __neigh_create(&amp;amp;arp_tbl, &amp;amp;nexthop, dev, false);
&lt;/code>&lt;/pre>
&lt;p>继续，查询路由层找到下一跳，再根据下一跳信息查找邻居缓存。如果未找到，则 调用&lt;code>__neigh_create&lt;/code> 创建一个邻居。例如，第一次将数据发送到另一 台主机的时候，就是这种情况。请注意，创建邻居缓存的时候带了 &lt;code>arp_tbl&lt;/code>（ &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L160-L187">net/ipv4/arp.c&lt;/a> 中定义）参数。其他系统（如 IPv6 或 &lt;a href="https://en.wikipedia.org/wiki/DECnet">DECnet&lt;/a>）维护自己的 ARP 表，并将不同的变量 传给&lt;code>__neigh_create&lt;/code>。 这篇文章的目的并不是要详细介绍邻居缓存，但注意如果创建， 会导致缓存表增大。本文后面会介绍有关邻居缓存的更多详细信息。 邻居缓存会导出一组 统计信息，以便可以衡量这种增长。有关详细信息，请参阅下面的监控部分。&lt;/p>
&lt;pre>&lt;code>if (!IS_ERR(neigh)) {
int res = dst_neigh_output(dst, neigh, skb);
rcu_read_unlock_bh();
return res;
}
rcu_read_unlock_bh();
net_dbg_ratelimited(&amp;quot;%s: No header cache and no neighbour!\n&amp;quot;,
__func__);
kfree_skb(skb);
return -EINVAL;
}
&lt;/code>&lt;/pre>
&lt;p>最后，如果创建邻居缓存成功，则调用 &lt;code>dst_neigh_output&lt;/code> 继续传递 skb；否则，释放 skb 并返 回 &lt;code>EINVAL&lt;/code>，这会向上传递，导致 &lt;code>OutDiscards&lt;/code> 在 &lt;code>ip_send_skb&lt;/code> 中递增。让我们继续在 &lt;code>dst_neigh_output&lt;/code> 中接近 Linux 内核的 netdevice 子系统。&lt;/p>
&lt;h2 id="68-dst_neigh_output">6.8 &lt;code>dst_neigh_output&lt;/code>&lt;/h2>
&lt;p>&lt;code>dst_neigh_output&lt;/code> 函数做了两件重要的事情。首先，回想一下之前在本文中我 们看到，如果用户调用 &lt;code>sendmsg&lt;/code> 并通过辅助消息指定 &lt;code>MSG_CONFIRM&lt;/code> 参数，则会设置一个标 志位以指示目标高速缓存条目仍然有效且不应进行垃圾回收。这个检查就是在这个函数里面 做的，并且邻居上的 &lt;code>confirm&lt;/code> 字段设置为当前的 jiffies 计数。&lt;/p>
&lt;pre>&lt;code>static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n,
struct sk_buff *skb)
{
const struct hh_cache *hh;
if (dst-&amp;gt;pending_confirm) {
unsigned long now = jiffies;
dst-&amp;gt;pending_confirm = 0;
/* avoid dirtying neighbour */
if (n-&amp;gt;confirmed != now)
n-&amp;gt;confirmed = now;
}
&lt;/code>&lt;/pre>
&lt;p>其次，检查邻居的状态并调用适当的 &lt;code>output&lt;/code> 函数。让我们看一下这些条件，并尝试了解发 生了什么：&lt;/p>
&lt;pre>&lt;code>hh = &amp;amp;n-&amp;gt;hh;
if ((n-&amp;gt;nud_state &amp;amp; NUD_CONNECTED) &amp;amp;&amp;amp; hh-&amp;gt;hh_len)
return neigh_hh_output(hh, skb);
else
return n-&amp;gt;output(n, skb);
}
&lt;/code>&lt;/pre>
&lt;p>邻居被认为是 &lt;code>NUD_CONNECTED&lt;/code>，如果它满足以下一个或多个条件：&lt;/p>
&lt;ol>
&lt;li>&lt;code>NUD_PERMANENT&lt;/code>：静态路由&lt;/li>
&lt;li>&lt;code>NUD_NOARP&lt;/code>：不需要 ARP 请求（例如，目标是多播或广播地址，或环回设备）&lt;/li>
&lt;li>&lt;code>NUD_REACHABLE&lt;/code>：邻居是“可达的。”只要&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L905-L923">成功处理了&lt;/a>ARP 请求，目标就会被标记为可达&lt;/li>
&lt;/ol>
&lt;p>进一步，如果“硬件头”（hh）被缓存（之前已经发送过数据，并生成了缓存），将调 用 &lt;code>neigh_hh_output&lt;/code>。
否则，调用 &lt;code>output&lt;/code> 函数。
以上两种情况，最后都会到 &lt;code>dev_queue_xmit&lt;/code>，它将 skb 发送给 Linux 网络设备子系统，在它 进入设备驱动程序层之前将对其进行更多处理。让我们沿着 &lt;code>neigh_hh_output&lt;/code> 和 &lt;code>n-&amp;gt;output&lt;/code> 代码继续向下，直到达到 &lt;code>dev_queue_xmit&lt;/code>。&lt;/p>
&lt;h2 id="69-neigh_hh_output">6.9 &lt;code>neigh_hh_output&lt;/code>&lt;/h2>
&lt;p>如果目标是 &lt;code>NUD_CONNECTED&lt;/code> 并且硬件头已被缓存，则将调用 &lt;code>neigh_hh_output&lt;/code>，在将 skb 移交 给 &lt;code>dev_queue_xmit&lt;/code> 之前执行一小部分处理。 我们来看看 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/net/neighbour.h#L336-L356">include/net/neighbour.h&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>static inline int neigh_hh_output(const struct hh_cache *hh, struct sk_buff *skb)
{
unsigned int seq;
int hh_len;
do {
seq = read_seqbegin(&amp;amp;hh-&amp;gt;hh_lock);
hh_len = hh-&amp;gt;hh_len;
if (likely(hh_len &amp;lt;= HH_DATA_MOD)) {
/* this is inlined by gcc */
memcpy(skb-&amp;gt;data - HH_DATA_MOD, hh-&amp;gt;hh_data, HH_DATA_MOD);
} else {
int hh_alen = HH_DATA_ALIGN(hh_len);
memcpy(skb-&amp;gt;data - hh_alen, hh-&amp;gt;hh_data, hh_alen);
}
} while (read_seqretry(&amp;amp;hh-&amp;gt;hh_lock, seq));
skb_push(skb, hh_len);
return dev_queue_xmit(skb);
}
&lt;/code>&lt;/pre>
&lt;p>这个函数理解有点难，部分原因是&lt;a href="https://en.wikipedia.org/wiki/Seqlock">seqlock&lt;/a>这 个东西，它用于在缓存的硬件头上做读/写锁。可以将上面的 &lt;code>do {} while ()&lt;/code>循环想象成 一个简单的重试机制，它将尝试在循环中执行，直到成功。
循环里处理硬件头的长度对齐。这是必需的，因为某些硬件头（如&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/ieee80211.h#L210-L218">IEEE 802.11&lt;/a> 头）大于 &lt;code>HH_DATA_MOD&lt;/code>（16 字节）。
将头数据复制到 skb 后，&lt;code>skb_push&lt;/code> 将更新 skb 内指向数据缓冲区的指针。最后调用 &lt;code>dev_queue_xmit&lt;/code> 将 skb 传递给 Linux 网络设备子系统。&lt;/p>
&lt;h2 id="610-n-output">6.10 &lt;code>n-&amp;gt;output&lt;/code>&lt;/h2>
&lt;p>如果目标不是 &lt;code>NUD_CONNECTED&lt;/code> 或硬件头尚未缓存，则代码沿 &lt;code>n-&amp;gt;output&lt;/code> 路径向下。 neigbour 结构上的 &lt;code>output&lt;/code> 指针指向哪个函数？这得看情况。要了解这是如何设置的，我们 需要更多地了解邻居缓存的工作原理。
&lt;code>struct neighbour&lt;/code> 包含几个重要字段：我们在上面看到的 &lt;code>nud_state&lt;/code> 字段，&lt;code>output&lt;/code> 函数和 &lt;code>ops&lt;/code> 结构。回想一下，我们之前看到如果在缓存中找不到现有条目，会从 &lt;code>ip_finish_output2&lt;/code> 调用&lt;code>__neigh_create&lt;/code> 创建一个。当调用&lt;code>__neigh_creaet&lt;/code> 时，将分配邻居，其 &lt;code>output&lt;/code> 函 数&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/neighbour.c#L294">最初&lt;/a> 设置为 &lt;code>neigh_blackhole&lt;/code>。随着&lt;code>__neigh_create&lt;/code> 代码的进行，它将根据邻居的状态修改 &lt;code>output&lt;/code> 值以指向适当的发送方法。
例如，当代码确定是“已连接的”邻居时，&lt;code>neigh_connect&lt;/code> 会将 &lt;code>output&lt;/code> 设置为 &lt;code>neigh-&amp;gt;ops-&amp;gt;connected_output&lt;/code>。或者，当代码怀疑邻居可能已关闭时，&lt;code>neigh_suspect&lt;/code> 会将 &lt;code>output&lt;/code> 设置为 &lt;code>neigh-&amp;gt;ops-&amp;gt;output&lt;/code>（例如，如果已超过 &lt;code>/proc/sys/net/ipv4/neigh/default/delay_first_probe_time&lt;/code> 自发送探测以来的 &lt;code>delay_first_probe_time&lt;/code> 秒）。
换句话说：&lt;code>neigh-&amp;gt;output&lt;/code> 会被设置为 &lt;code>neigh-&amp;gt;ops_connected_output&lt;/code> 或 &lt;code>neigh-&amp;gt;ops-&amp;gt;output&lt;/code>，具体取决于邻居的状态。&lt;code>neigh-&amp;gt;ops&lt;/code> 来自哪里？
分配邻居后，调用 &lt;code>arp_constructor&lt;/code>（ &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L220-L313">net/ipv4/arp.c&lt;/a> ）来设置 &lt;code>struct neighbor&lt;/code> 的某些字段。特别是，此函数会检查与此邻居关联的设备是否 导出来一个 &lt;code>struct header_ops&lt;/code> 实例（&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ethernet/eth.c#L342-L348">以太网设备是这样做的  &lt;/a>）， 该结构体有一个 &lt;code>cache&lt;/code> 方法。
&lt;code>neigh-&amp;gt;ops&lt;/code> 设置为 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/arp.c#L138-L144">net/ipv4/arp&lt;/a> 中定义的以下实例：&lt;/p>
&lt;pre>&lt;code>static const struct neigh_ops arp_hh_ops = {
.family = AF_INET,
.solicit = arp_solicit,
.error_report = arp_error_report,
.output = neigh_resolve_output,
.connected_output = neigh_resolve_output,
};
&lt;/code>&lt;/pre>
&lt;p>所以，不管 neighbor 是不是“已连接的”，或者邻居缓存代码是否怀疑连接“已关闭”， &lt;code>neigh_resolve_output&lt;/code> 最终都会被赋给 &lt;code>neigh-&amp;gt;output&lt;/code>。当执行到 &lt;code>n-&amp;gt;output&lt;/code> 时就会调 用它。&lt;/p>
&lt;h3 id="neigh_resolve_output">neigh_resolve_output&lt;/h3>
&lt;p>此函数的目的是解析未连接的邻居，或已连接但没有缓存硬件头的邻居。我们来看看这个 函数是如何工作的：&lt;/p>
&lt;pre>&lt;code>/* Slow and careful. */
int neigh_resolve_output(struct neighbour *neigh, struct sk_buff *skb)
{
struct dst_entry *dst = skb_dst(skb);
int rc = 0;
if (!dst)
goto discard;
if (!neigh_event_send(neigh, skb)) {
int err;
struct net_device *dev = neigh-&amp;gt;dev;
unsigned int seq;
&lt;/code>&lt;/pre>
&lt;p>代码首先进行一些基本检查，然后调用 &lt;code>neigh_event_send&lt;/code>。 &lt;code>neigh_event_send&lt;/code> 函数是 &lt;code>__neigh_event_send&lt;/code> 的简单封装，后者干大部分脏话累活。可以在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/neighbour.c#L964-L1028">net/core/neighbour.c&lt;/a> 中读&lt;code>__neigh_event_send&lt;/code> 的源代码，从大的层面看，三种情况：&lt;/p>
&lt;ol>
&lt;li>&lt;code>NUD_NONE&lt;/code> 状态（默认状态）的邻居：假设 &lt;code>/proc/sys/net/ipv4/neigh/default/app_solicit&lt;/code> 和 &lt;code>/proc/sys/net/ipv4/neigh/default/mcast_solicit&lt;/code> 配置允许发送探测（如果不是， 则将状态标记为 &lt;code>NUD_FAILED&lt;/code>），将导致立即发送 ARP 请求。邻居状态将更新为 &lt;code>NUD_INCOMPLETE&lt;/code>&lt;/li>
&lt;li>&lt;code>NUD_STALE&lt;/code> 状态的邻居：将更新为 &lt;code>NUD_DELAYED&lt;/code> 并且将设置计时器以稍后探测它们（ 稍后是现在的时间+&lt;code>/proc/sys/net/ipv4/neigh/default/delay_first_probe_time&lt;/code> 秒 ）&lt;/li>
&lt;li>检查 &lt;code>NUD_INCOMPLETE&lt;/code> 状态的邻居（包括上面第一种情形），以确保未解析邻居的排 队 packet 的数量小于等于&lt;code>/proc/sys/net/ipv4/neigh/default/unres_qlen&lt;/code>。如果超过 ，则数据包会出列并丢弃，直到小于等于 proc 中的值。 统计信息中有个计数器会因此 更新&lt;/li>
&lt;/ol>
&lt;p>如果需要 ARP 探测，ARP 将立即被发送。&lt;code>__neigh_event_send&lt;/code> 将返回 0，表示邻居被视为“已 连接”或“已延迟”，否则返回 1。返回值 0 允许 &lt;code>neigh_resolve_output&lt;/code> 继续：&lt;/p>
&lt;pre>&lt;code>if (dev-&amp;gt;header_ops-&amp;gt;cache &amp;amp;&amp;amp; !neigh-&amp;gt;hh.hh_len)
neigh_hh_init(neigh, dst);
&lt;/code>&lt;/pre>
&lt;p>如果邻居关联的设备的协议实现（在我们的例子中是以太网）支持缓存硬件头，并且当前没 有缓存，&lt;code>neigh_hh_init&lt;/code> 将缓存它。&lt;/p>
&lt;pre>&lt;code>do {
__skb_pull(skb, skb_network_offset(skb));
seq = read_seqbegin(&amp;amp;neigh-&amp;gt;ha_lock);
err = dev_hard_header(skb, dev, ntohs(skb-&amp;gt;protocol),
neigh-&amp;gt;ha, NULL, skb-&amp;gt;len);
} while (read_seqretry(&amp;amp;neigh-&amp;gt;ha_lock, seq));
&lt;/code>&lt;/pre>
&lt;p>接下来，seqlock 锁控制对邻居的硬件地址字段（&lt;code>neigh-&amp;gt;ha&lt;/code>）的访问。 &lt;code>dev_hard_header&lt;/code> 为 skb 创建以太网头时将读取该字段。
之后是错误检查：&lt;/p>
&lt;pre>&lt;code>if (err &amp;gt;= 0)
rc = dev_queue_xmit(skb);
else
goto out_kfree_skb;
}
&lt;/code>&lt;/pre>
&lt;p>如果以太网头写入成功，将调用 &lt;code>dev_queue_xmit&lt;/code> 将 skb 传递给 Linux 网络设备子系统进行发 送。如果出现错误，goto 将删除 skb，设置并返回错误码：&lt;/p>
&lt;pre>&lt;code>out:
return rc;
discard:
neigh_dbg(1, &amp;quot;%s: dst=%p neigh=%p\n&amp;quot;, __func__, dst, neigh);
out_kfree_skb:
rc = -EINVAL;
kfree_skb(skb);
goto out;
}
EXPORT_SYMBOL(neigh_resolve_output);
&lt;/code>&lt;/pre>
&lt;p>在我们进入 Linux 网络设备子系统之前，让我们看看一些用于监控和转换 IP 协议层的文件。&lt;/p>
&lt;h2 id="611-监控-ip-层">6.11 监控: IP 层&lt;/h2>
&lt;h3 id="procnetsnmp-1">/proc/net/snmp&lt;/h3>
&lt;pre>&lt;code>$ cat /proc/net/snmp
Ip: Forwarding DefaultTTL InReceives InHdrErrors InAddrErrors ForwDatagrams InUnknownProtos InDiscards InDelivers OutRequests OutDiscards OutNoRoutes ReasmTimeout ReasmReqds ReasmOKs ReasmFails FragOKs FragFails FragCreates
Ip: 1 64 25922988125 0 0 15771700 0 0 25898327616 22789396404 12987882 51 1 10129840 2196520 1 0 0 0
...
&lt;/code>&lt;/pre>
&lt;p>这个文件包扩多种协议的统计，IP 层的在最前面，每一列代表什么有说明。
前面我们已经看到 IP 协议层有一些地方会更新计数器。这些计数器的类型是 C 枚举类型，定 义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/snmp.h#L10-L59">include/uapi/linux/snmp.h&lt;/a>:&lt;/p>
&lt;pre>&lt;code>enum
{
IPSTATS_MIB_NUM = 0,
/* frequently written fields in fast path, kept in same cache line */
IPSTATS_MIB_INPKTS, /* InReceives */
IPSTATS_MIB_INOCTETS, /* InOctets */
IPSTATS_MIB_INDELIVERS, /* InDelivers */
IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */
IPSTATS_MIB_OUTPKTS, /* OutRequests */
IPSTATS_MIB_OUTOCTETS, /* OutOctets */
/* ... */
&lt;/code>&lt;/pre>
&lt;p>一些有趣的统计：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OutRequests&lt;/code>: Incremented each time an IP packet is attempted to be sent. It appears that this is incremented for every send, successful or not.&lt;/li>
&lt;li>&lt;code>OutDiscards&lt;/code>: Incremented each time an IP packet is discarded. This can happen if appending data to the skb (for corked sockets) fails, or if the layers below IP return an error.&lt;/li>
&lt;li>&lt;code>OutNoRoute&lt;/code>: Incremented in several places, for example in the UDP protocol layer (udp_sendmsg) if no route can be generated for a given destination. Also incremented when an application calls “connect” on a UDP socket but no route can be found.&lt;/li>
&lt;li>&lt;code>FragOKs&lt;/code>: Incremented once per packet that is fragmented. For example, a packet split into 3 fragments will cause this counter to be incremented once.&lt;/li>
&lt;li>&lt;code>FragCreates&lt;/code>: Incremented once per fragment that is created. For example, a packet split into 3 fragments will cause this counter to be incremented thrice.&lt;/li>
&lt;li>&lt;code>FragFails&lt;/code>: Incremented if fragmentation was attempted, but is not permitted (because the “Don’t Fragment” bit is set). Also incremented if outputting the fragment fails.&lt;/li>
&lt;/ul>
&lt;p>其他（接收数据部分）的统计可以见本文的姊妹篇：&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#monitoring-ip-protocol-layer-statistics">原文  &lt;/a>，&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">中文翻译版&lt;/a>。&lt;/p>
&lt;h3 id="procnetnetstat">/proc/net/netstat&lt;/h3>
&lt;pre>&lt;code>$ cat /proc/net/netstat | grep IpExt
IpExt: InNoRoutes InTruncatedPkts InMcastPkts OutMcastPkts InBcastPkts OutBcastPkts InOctets OutOctets InMcastOctets OutMcastOctets InBcastOctets OutBcastOctets InCsumErrors InNoECTPkts InECT0Pktsu InCEPkts
IpExt: 0 0 0 0 277959 0 14568040307695 32991309088496 0 0 58649349 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>格式与前面的类似，除了每列的名称都有 &lt;code>IpExt&lt;/code> 前缀之外。
一些有趣的统计：&lt;/p>
&lt;ul>
&lt;li>&lt;code>OutMcastPkts&lt;/code>: Incremented each time a packet destined for a multicast address is sent.&lt;/li>
&lt;li>&lt;code>OutBcastPkts&lt;/code>: Incremented each time a packet destined for a broadcast address is sent.&lt;/li>
&lt;li>&lt;code>OutOctects&lt;/code>: The number of packet bytes output.&lt;/li>
&lt;li>&lt;code>OutMcastOctets&lt;/code>: The number of multicast packet bytes output.&lt;/li>
&lt;li>&lt;code>OutBcastOctets&lt;/code>: The number of broadcast packet bytes output.&lt;/li>
&lt;/ul>
&lt;p>其他（接收数据部分）的统计可以见本文的姊妹篇：&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#monitoring-ip-protocol-layer-statistics">原文  &lt;/a>，&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">中文翻译版&lt;/a>。
注意这些计数分别在 IP 层的不同地方被更新。由于代码一直在更新，重复计数或者计数错误 的 bug 可能会引入。如果这些计数对你非常重要，强烈建议你阅读内核的相应源码，确定它 们是在哪里被更新的，以及更新的对不对，是不是有 bug。&lt;/p>
&lt;h1 id="7-linux-netdevice-子系统">7 Linux netdevice 子系统&lt;/h1>
&lt;p>在继续跟进 &lt;code>dev_queue_xmit&lt;/code> 发送数据包之前，让我们花点时间介绍几个将在下一部分中出 现的重要概念。&lt;/p>
&lt;h2 id="71-linux-traffic-control流量控制">7.1 Linux traffic control（流量控制）&lt;/h2>
&lt;p>Linux 支持称为流量控制（&lt;a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html">traffic control&lt;/a>）的功能。此功能 允许系统管理员控制数据包如何从机器发送出去。本文不会深入探讨 Linux 流量控制 的各个方面的细节。&lt;a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/">这篇文档&lt;/a>对流量 控制系统、它如何控制流量，及其其特性进行了深入的介绍。
这里介绍一些值得一提的概念，使后面的代码更容易理解。
流量控制系统包含几组不同的 queue system，每种有不同的排队特征。各个排队系统通常称 为 qdisc，也称为排队规则。你可以将 qdisc 视为&lt;strong>调度程序&lt;/strong>; qdisc 决定数据包的发送时 间和方式。
在 Linux 上，每个 device 都有一个与之关联的默认 qdisc。对于仅支持单发送队列的网卡，使 用默认的 qdisc &lt;code>pfifo_fast&lt;/code>。支持多个发送队列的网卡使用 mq 的默认 qdisc。可以运行 &lt;code>tc qdisc&lt;/code> 来查看系统 qdisc 信息。
某些设备支持硬件流量控制，这允许管理员将流量控制 offload 到网络硬件，节省系统的 CPU 资源。
现在已经介绍了这些概念，让我们从 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2890-L2894">net/core/dev.c&lt;/a> 继续 &lt;code>dev_queue_xmit&lt;/code>。&lt;/p>
&lt;h2 id="72-dev_queue_xmit-and-__dev_queue_xmit">7.2 &lt;code>dev_queue_xmit&lt;/code> and &lt;code>__dev_queue_xmit&lt;/code>&lt;/h2>
&lt;p>&lt;code>dev_queue_xmit&lt;/code> 简单封装了&lt;code>__dev_queue_xmit&lt;/code>:&lt;/p>
&lt;pre>&lt;code>int dev_queue_xmit(struct sk_buff *skb)
{
return __dev_queue_xmit(skb, NULL);
}
EXPORT_SYMBOL(dev_queue_xmit);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>__dev_queue_xmit&lt;/code> 才是干脏活累活的地方。我们&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2808-L2825">一段段  &lt;/a>来看：&lt;/p>
&lt;pre>&lt;code>static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)
{
struct net_device *dev = skb-&amp;gt;dev;
struct netdev_queue *txq;
struct Qdisc *q;
int rc = -ENOMEM;
skb_reset_mac_header(skb);
/* Disable soft irqs for various locks below. Also
* stops preemption for RCU.
*/
rcu_read_lock_bh();
skb_update_prio(skb);
&lt;/code>&lt;/pre>
&lt;p>开始的逻辑：&lt;/p>
&lt;ol>
&lt;li>声明变量&lt;/li>
&lt;li>调用 &lt;code>skb_reset_mac_header&lt;/code>，准备发送 skb。这会重置 skb 内部的指针，使得 ether 头可 以被访问&lt;/li>
&lt;li>调用 &lt;code>rcu_read_lock_bh&lt;/code>，为接下来的读操作加锁。更多关于使用 RCU 安全访问数据的信 息，可以参考&lt;a href="https://www.kernel.org/doc/Documentation/RCU/checklist.txt">这里&lt;/a>&lt;/li>
&lt;li>调用 &lt;code>skb_update_prio&lt;/code>，如果启用了&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/cgroups/net_prio.txt">网络优先级 cgroup&lt;/a>，这会设置 skb 的优先级&lt;/li>
&lt;/ol>
&lt;p>现在，我们来看更复杂的部分：&lt;/p>
&lt;pre>&lt;code>txq = netdev_pick_tx(dev, skb, accel_priv);
&lt;/code>&lt;/pre>
&lt;p>这会选择发送队列。本文后面会看到，一些网卡支持多发送队列。我们来看这是如何工作的。&lt;/p>
&lt;h3 id="721-netdev_pick_tx">7.2.1 &lt;code>netdev_pick_tx&lt;/code>&lt;/h3>
&lt;p>&lt;code>netdev_pick_tx&lt;/code> 定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/flow_dissector.c#L397-L417">net/core/flow_dissector.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>struct netdev_queue *netdev_pick_tx(struct net_device *dev,
struct sk_buff *skb,
void *accel_priv)
{
int queue_index = 0;
if (dev-&amp;gt;real_num_tx_queues != 1) {
const struct net_device_ops *ops = dev-&amp;gt;netdev_ops;
if (ops-&amp;gt;ndo_select_queue)
queue_index = ops-&amp;gt;ndo_select_queue(dev, skb,
accel_priv);
else
queue_index = __netdev_pick_tx(dev, skb);
if (!accel_priv)
queue_index = dev_cap_txqueue(dev, queue_index);
}
skb_set_queue_mapping(skb, queue_index);
return netdev_get_tx_queue(dev, queue_index);
}
&lt;/code>&lt;/pre>
&lt;p>如上所示，如果网络设备仅支持单个 TX 队列，则会跳过复杂的代码，直接返回单个 TX 队列。 大多高端服务器上使用的设备都有多个 TX 队列。具有多个 TX 队列的设备有两种情况：&lt;/p>
&lt;ol>
&lt;li>驱动程序实现 &lt;code>ndo_select_queue&lt;/code>，以硬件或 feature-specific 的方式更智能地选择 TX 队列&lt;/li>
&lt;li>驱动程序没有实现 &lt;code>ndo_select_queue&lt;/code>，这种情况需要内核自己选择设备&lt;/li>
&lt;/ol>
&lt;p>从 3.13 内核开始，没有多少驱动程序实现 &lt;code>ndo_select_queue&lt;/code>。bnx2x 和 ixgbe 驱动程序实 现了此功能，但仅用于以太网光纤通道（&lt;a href="https://en.wikipedia.org/wiki/Fibre_Channel_over_Ethernet">FCoE&lt;/a>）。鉴于此，我们假设网络设备没有实现 &lt;code>ndo_select_queue&lt;/code> 和/或没有使用 FCoE。在这种情况下，内核将使用&lt;code>__netdev_pick_tx&lt;/code> 选择 tx 队列。
一旦&lt;code>__netdev_pick_tx&lt;/code> 确定了队列号，&lt;code>skb_set_queue_mapping&lt;/code> 将缓存该值（稍后将在 流量控制代码中使用），&lt;code>netdev_get_tx_queue&lt;/code> 将查找并返回指向该队列的指针。让我们 看一下&lt;code>__netdev_pick_tx&lt;/code> 在返回&lt;code>__dev_queue_xmit&lt;/code> 之前的工作原理。&lt;/p>
&lt;h3 id="722-__netdev_pick_tx">7.2.2 &lt;code>__netdev_pick_tx&lt;/code>&lt;/h3>
&lt;p>我们来看内核如何选择 TX 队列。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/flow_dissector.c#L375-L395">net/core/flow_dissector.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)
{
struct sock *sk = skb-&amp;gt;sk;
int queue_index = sk_tx_queue_get(sk);
if (queue_index &amp;lt; 0 || skb-&amp;gt;ooo_okay ||
queue_index &amp;gt;= dev-&amp;gt;real_num_tx_queues) {
int new_index = get_xps_queue(dev, skb);
if (new_index &amp;lt; 0)
new_index = skb_tx_hash(dev, skb);
if (queue_index != new_index &amp;amp;&amp;amp; sk &amp;amp;&amp;amp;
rcu_access_pointer(sk-&amp;gt;sk_dst_cache))
sk_tx_queue_set(sk, new_index);
queue_index = new_index;
}
return queue_index;
}
&lt;/code>&lt;/pre>
&lt;p>代码首先调用 &lt;code>sk_tx_queue_get&lt;/code> 检查发送队列是否已经缓存在 socket 上，如果尚未缓存， 则返回-1。
下一个 if 语句检查是否满足以下任一条件：&lt;/p>
&lt;ol>
&lt;li>&lt;code>queue_index &amp;lt; 0&lt;/code>：表示尚未设置 TX queue 的情况&lt;/li>
&lt;li>&lt;code>ooo_okay&lt;/code> 标志是否非零：如果不为 0，则表示现在允许无序（out of order）数据包。 协议层必须正确地地设置此标志。当 flow 的所有 outstanding（需要确认的？）数据包都 已确认时，TCP 协议层将设置此标志。当发生这种情况时，内核可以为此数据包选择不同 的 TX 队列。UDP 协议层不设置此标志 - 因此 UDP 数据包永远不会将 &lt;code>ooo_okay&lt;/code> 设置为非零 值。&lt;/li>
&lt;li>TX queue index 大于 TX queue 数量：如果用户最近通过 ethtool 更改了设备上的队列数， 则会发生这种情况。稍后会详细介绍。&lt;/li>
&lt;/ol>
&lt;p>以上任何一种情况，都表示没有找到合适的 TX queue，因此接下来代码会进入慢路径以继续 寻找合适的发送队列。首先调用 &lt;code>get_xps_queue&lt;/code>，它会使用一个由用户配置的 TX queue 到 CPU 的映射，这称为 XPS（Transmit Packet Steering ，发送数据包控制），我们将更详细 地了解 XPS 是什么以及它如何工作。
如果内核不支持 XPS，或者系统管理员未配置 XPS，或者配置的映射引用了无效队列， &lt;code>get_xps_queue&lt;/code> 返回-1，则代码将继续调用 &lt;code>skb_tx_hash&lt;/code>。
一旦 XPS 或内核使用 &lt;code>skb_tx_hash&lt;/code> 自动选择了发送队列，&lt;code>sk_tx_queue_set&lt;/code> 会将队列缓存 在 socket 对象上，然后返回。让我们看看 XPS，以及 &lt;code>skb_tx_hash&lt;/code> 在继续调用 &lt;code>dev_queue_xmit&lt;/code> 之前是如何工作的。&lt;/p>
&lt;h4 id="transmit-packet-steering-xps">Transmit Packet Steering (XPS)&lt;/h4>
&lt;p>发送数据包控制（XPS）是一项功能，允许系统管理员配置哪些 CPU 可以处理网卡的哪些发送 队列。XPS 的主要目的是&lt;strong>避免处理发送请求时的锁竞争&lt;/strong>。使用 XPS 还可以减少缓存驱逐， 避免&lt;a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA&lt;/a>机器上的远程 内存访问等。
可以查看内核有关 XPS 的&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L364-L422">文档  &lt;/a>了解其如何工作的更多信息。我们后面会介绍如何调整系统的 XPS，现在，你只需要知道 配置 XPS，系统管理员需要定义 TX queue 到 CPU 的映射（bitmap 形式）。
上面代码中，&lt;code>get_xps_queue&lt;/code> 将查询这个用户指定的映射，以确定应使用哪个发送 队列。如果 &lt;code>get_xps_queue&lt;/code> 返回-1，则将改为使用 &lt;code>skb_tx_hash&lt;/code>。&lt;/p>
&lt;h4 id="skb_tx_hash">&lt;code>skb_tx_hash&lt;/code>&lt;/h4>
&lt;p>如果 XPS 未包含在内核中，或 XPS 未配置，或配置的队列不可用（可能因为用户调整了队列数 ），&lt;code>skb_tx_hash&lt;/code> 将接管以确定应在哪个队列上发送数据。准确理解 &lt;code>skb_tx_hash&lt;/code> 的工作 原理非常重要，具体取决于你的发送负载。请注意，这段代码已经随时间做过一些更新，因 此如果你使用的内核版本与本文不同，则应直接查阅相应版本的 j 内核源代码。
让我们看看它是如何工作的，来自 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netdevice.h#L2331-L2340">include/linux/netdevice.h&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>/*
* Returns a Tx hash for the given packet when dev-&amp;gt;real_num_tx_queues is used
* as a distribution range limit for the returned value.
*/
static inline u16 skb_tx_hash(const struct net_device *dev,
const struct sk_buff *skb)
{
return __skb_tx_hash(dev, skb, dev-&amp;gt;real_num_tx_queues);
}
&lt;/code>&lt;/pre>
&lt;p>直接调用了&lt;code> __skb_tx_hash&lt;/code>, &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/flow_dissector.c#L239-L271">net/core/flow_dissector.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>/*
* Returns a Tx hash based on the given packet descriptor a Tx queues' number
* to be used as a distribution range.
*/
u16 __skb_tx_hash(const struct net_device *dev, const struct sk_buff *skb,
unsigned int num_tx_queues)
{
u32 hash;
u16 qoffset = 0;
u16 qcount = num_tx_queues;
if (skb_rx_queue_recorded(skb)) {
hash = skb_get_rx_queue(skb);
while (unlikely(hash &amp;gt;= num_tx_queues))
hash -= num_tx_queues;
return hash;
}
&lt;/code>&lt;/pre>
&lt;p>这个函数中的第一个 if 是一个有趣的短路。函数名 &lt;code>skb_rx_queue_recorded&lt;/code> 有点误导。skb 有一个 &lt;code>queue_mapping&lt;/code> 字段，rx 和 tx 都会用到这个字段。无论如何，如果系统正在接收数 据包并将其转发到其他地方，则此 if 语句都为 &lt;code>true&lt;/code>。否则，代码将继续向下：&lt;/p>
&lt;pre>&lt;code>if (dev-&amp;gt;num_tc) {
u8 tc = netdev_get_prio_tc_map(dev, skb-&amp;gt;priority);
qoffset = dev-&amp;gt;tc_to_txq[tc].offset;
qcount = dev-&amp;gt;tc_to_txq[tc].count;
}
&lt;/code>&lt;/pre>
&lt;p>要理解这段代码，首先要知道，程序可以设置 socket 上发送的数据的优先级。这可以通过 &lt;code>setsockopt&lt;/code> 带 &lt;code>SOL_SOCKET&lt;/code> 和 &lt;code>SO_PRIORITY&lt;/code> 选项来完成。有关 &lt;code>SO_PRIORITY&lt;/code> 的更多信息 ，请参见&lt;a href="http://man7.org/linux/man-pages/man7/socket.7.html">socket (7) man page&lt;/a>。
请注意，如果使用 &lt;code>setsockopt&lt;/code> 带 &lt;code>IP_TOS&lt;/code> 选项来设置在 socket 上发送的 IP 包的 TOS 标志（ 或者作为辅助消息传递给 &lt;code>sendmsg&lt;/code>，在数据包级别设置），内核会将其转换为 &lt;code>skb-&amp;gt;priority&lt;/code>。
如前所述，一些网络设备支持基于硬件的流量控制系统。&lt;strong>如果 num_tc 不为零，则表示此设 备支持基于硬件的流量控制&lt;/strong>。这种情况下，将查询一个&lt;strong>packet priority 到该硬件支持 的流量控制&lt;/strong>的映射，根据此映射选择适当的流量类型（traffic class）。
接下来，将计算出该 traffic class 的 TX queue 的范围，它将用于确定发送队列。
如果 &lt;code>num_tc&lt;/code> 为零（网络设备不支持硬件流量控制），则 &lt;code>qcount&lt;/code> 和 &lt;code>qoffset&lt;/code> 变量分 别设置为发送队列数和 0。
使用 &lt;code>qcount&lt;/code> 和 &lt;code>qoffset&lt;/code>，将计算发送队列的 index：&lt;/p>
&lt;pre>&lt;code>if (skb-&amp;gt;sk &amp;amp;&amp;amp; skb-&amp;gt;sk-&amp;gt;sk_hash)
hash = skb-&amp;gt;sk-&amp;gt;sk_hash;
else
hash = (__force u16) skb-&amp;gt;protocol;
hash = __flow_hash_1word(hash);
return (u16) (((u64) hash * qcount) &amp;gt;&amp;gt; 32) + qoffset;
}
EXPORT_SYMBOL(__skb_tx_hash);
&lt;/code>&lt;/pre>
&lt;p>最后，通过&lt;code>__netdev_pick_tx&lt;/code> 返回选出的 TX queue index。&lt;/p>
&lt;h2 id="73-继续__dev_queue_xmit">7.3 继续&lt;code>__dev_queue_xmit&lt;/code>&lt;/h2>
&lt;p>至此已经选到了合适的发送队列。
继续&lt;code>__dev_queue_xmit can continue&lt;/code>:&lt;/p>
&lt;pre>&lt;code>q = rcu_dereference_bh(txq-&amp;gt;qdisc);
#ifdef CONFIG_NET_CLS_ACT
skb-&amp;gt;tc_verd = SET_TC_AT(skb-&amp;gt;tc_verd, AT_EGRESS);
#endif
trace_net_dev_queue(skb);
if (q-&amp;gt;enqueue) {
rc = __dev_xmit_skb(skb, q, dev, txq);
goto out;
}
&lt;/code>&lt;/pre>
&lt;p>首先获取与此队列关联的 qdisc。回想一下，之前我们看到单发送队列设备的默认类型是 &lt;code>pfifo_fast&lt;/code> qdisc，而对于多队列设备，默认类型是 &lt;code>mq&lt;/code> qdisc。
接下来，如果内核中已启用数据包分类 API，则代码会为 packet 分配 traffic class。 接下 来，检查 disc 是否有合适的队列来存放 packet。像 &lt;code>noqueue&lt;/code> 这样的 qdisc 没有队列。 如果 有队列，则代码调用&lt;code>__dev_xmit_skb&lt;/code> 继续处理数据，然后跳转到此函数的末尾。我们很快 就会看到&lt;code>__dev_xmit_skb&lt;/code>。现在，让我们看看如果没有队列会发生什么，从一个非常有用 的注释开始：&lt;/p>
&lt;pre>&lt;code>/* The device has no queue. Common case for software devices:
loopback, all the sorts of tunnels...
Really, it is unlikely that netif_tx_lock protection is necessary
here. (f.e. loopback and IP tunnels are clean ignoring statistics
counters.)
However, it is possible, that they rely on protection
made by us here.
Check this and shot the lock. It is not prone from deadlocks.
Either shot noqueue qdisc, it is even simpler 8)
*/
if (dev-&amp;gt;flags &amp;amp; IFF_UP) {
int cpu = smp_processor_id(); /* ok because BHs are off */
&lt;/code>&lt;/pre>
&lt;p>正如注释所示，&lt;strong>唯一可以拥有”没有队列的 qdisc”的设备是环回设备和隧道设备&lt;/strong>。如果 设备当前处于运行状态，则获取当前 CPU，然后判断此设备队列上的发送锁是否由此 CPU 拥有 ：&lt;/p>
&lt;pre>&lt;code>if (txq-&amp;gt;xmit_lock_owner != cpu) {
if (__this_cpu_read(xmit_recursion) &amp;gt; RECURSION_LIMIT)
goto recursion_alert;
&lt;/code>&lt;/pre>
&lt;p>如果发送锁不由此 CPU 拥有，则在此处检查 per-CPU 计数器变量 &lt;code>xmit_recursion&lt;/code>，判断其是 否超过 &lt;code>RECURSION_LIMIT&lt;/code>。 一个程序可能会在这段代码这里持续发送数据，然后被抢占， 调度程序选择另一个程序来运行。第二个程序也可能驻留在此持续发送数据。因此， &lt;code>xmit_recursion&lt;/code> 计数器用于确保在此处竞争发送数据的程序不超过 &lt;code>RECURSION_LIMIT&lt;/code> 个 。
我们继续：&lt;/p>
&lt;pre>&lt;code>HARD_TX_LOCK(dev, txq, cpu);
if (!netif_xmit_stopped(txq)) {
__this_cpu_inc(xmit_recursion);
rc = dev_hard_start_xmit(skb, dev, txq);
__this_cpu_dec(xmit_recursion);
if (dev_xmit_complete(rc)) {
HARD_TX_UNLOCK(dev, txq);
goto out;
}
}
HARD_TX_UNLOCK(dev, txq);
net_crit_ratelimited(&amp;quot;Virtual device %s asks to queue packet!\n&amp;quot;,
dev-&amp;gt;name);
} else {
/* Recursion is detected! It is possible,
* unfortunately
*/
recursion_alert:
net_crit_ratelimited(&amp;quot;Dead loop on virtual device %s, fix it urgently!\n&amp;quot;,
dev-&amp;gt;name);
}
}
&lt;/code>&lt;/pre>
&lt;p>接下来的代码首先尝试获取发送锁，然后检查要使用的设备的发送队列是否被停用。如果没 有停用，则更新 &lt;code>xmit_recursion&lt;/code> 计数，然后将数据向下传递到更靠近发送的设备。我们稍 后会更详细地看到 &lt;code>dev_hard_start_xmit&lt;/code>。
或者，如果当前 CPU 是发送锁定的拥有者，或者如果 &lt;code>RECURSION_LIMIT&lt;/code> 被命中，则不进行发 送，而会打印告警日志。
函数剩余部分的代码设置错误码并返回。
由于我们对真正的以太网设备感兴趣，让我们来看一下之前就需要跟进去的 &lt;code>__dev_xmit_skb&lt;/code> 函数，这是发送主线上的函数。&lt;/p>
&lt;h2 id="74-__dev_xmit_skb">7.4 &lt;code>__dev_xmit_skb&lt;/code>&lt;/h2>
&lt;p>现在我们带着排队规则 &lt;code>qdisc&lt;/code>、网络设备 &lt;code>dev&lt;/code> 和发送队列 &lt;code>txq&lt;/code> 三个变量来到 &lt;code>__dev_xmit_skb&lt;/code>， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2684-L2745">net/core/dev.c&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
struct net_device *dev,
struct netdev_queue *txq)
{
spinlock_t *root_lock = qdisc_lock(q);
bool contended;
int rc;
qdisc_pkt_len_init(skb);
qdisc_calculate_pkt_len(skb, q);
/*
* Heuristic to force contended enqueues to serialize on a
* separate lock before trying to get qdisc main lock.
* This permits __QDISC_STATE_RUNNING owner to get the lock more often
* and dequeue packets faster.
*/
contended = qdisc_is_running(q);
if (unlikely(contended))
spin_lock(&amp;amp;q-&amp;gt;busylock);
&lt;/code>&lt;/pre>
&lt;p>代码首先使用 &lt;code>qdisc_pkt_len_init&lt;/code> 和 &lt;code>qdisc_calculate_pkt_len&lt;/code> 来计算数据的准确长度 ，稍后 qdisc 会用到该值。 对于硬件 offload（例如 UFO）这是必需的，因为添加的额外的头 信息，硬件 offload 的时候回用到。
接下来，使用另一个锁来帮助减少 qdisc 主锁上的竞争（我们稍后会看到这第二个锁）。 如 果 qdisc 当前正在运行，那么试图发送的其他程序将在 qdisc 的 &lt;code>busylock&lt;/code> 上竞争。 这允许 运行 qdisc 的程序在处理数据包的同时，与较少量的程序竞争第二个主锁。随着竞争者数量 的减少，这种技巧增加了吞吐量。&lt;a href="https://github.com/torvalds/linux/commit/79640a4ca6955e3ebdb7038508fa7a0cd7fa5527">原始 commit 描述  &lt;/a>。 接下来是主锁：&lt;/p>
&lt;pre>&lt;code>spin_lock(root_lock);
&lt;/code>&lt;/pre>
&lt;p>接下来处理 3 种可能情况：&lt;/p>
&lt;ol>
&lt;li>如果 qdisc 已停用&lt;/li>
&lt;li>如果 qdisc 允许数据包 bypass 排队系统，并且没有其他包要发送，并且 qdisc 当前没有运 行。允许包 bypass 所谓的**“work-conserving qdisc” - 那些用于流量整形（traffic reshaping）目的并且不会引起发送延迟的 qdisc**&lt;/li>
&lt;li>所有其他情况&lt;/li>
&lt;/ol>
&lt;p>让我们来看看每种情况下发生什么，从 qdisc 停用开始：&lt;/p>
&lt;pre>&lt;code>if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &amp;amp;q-&amp;gt;state))) {
kfree_skb(skb);
rc = NET_XMIT_DROP;
&lt;/code>&lt;/pre>
&lt;p>这很简单。 如果 qdisc 停用，则释放数据并将返回代码设置为 &lt;code>NET_XMIT_DROP&lt;/code>。
接下来，如果 qdisc 允许数据包 bypass，并且没有其他包要发送，并且 qdisc 当前没有运行：&lt;/p>
&lt;pre>&lt;code>} else if ((q-&amp;gt;flags &amp;amp; TCQ_F_CAN_BYPASS) &amp;amp;&amp;amp; !qdisc_qlen(q) &amp;amp;&amp;amp;
qdisc_run_begin(q)) {
/*
* This is a work-conserving queue; there are no old skbs
* waiting to be sent out; and the qdisc is not running -
* xmit the skb directly.
*/
if (!(dev-&amp;gt;priv_flags &amp;amp; IFF_XMIT_DST_RELEASE))
skb_dst_force(skb);
qdisc_bstats_update(q, skb);
if (sch_direct_xmit(skb, q, dev, txq, root_lock)) {
if (unlikely(contended)) {
spin_unlock(&amp;amp;q-&amp;gt;busylock);
contended = false;
}
__qdisc_run(q);
} else
qdisc_run_end(q);
rc = NET_XMIT_SUCCESS;
&lt;/code>&lt;/pre>
&lt;p>这个 if 语句有点复杂，如果满足以下所有条件，则整个语句的计算结果为 true：&lt;/p>
&lt;ol>
&lt;li>&lt;code>q-&amp;gt; flags＆TCQ_F_CAN_BYPASS&lt;/code>：qdisc 允许数据包绕过排队系统。对于所谓的“ work-conserving” qdiscs 这会是 &lt;code>true&lt;/code>；即，允许 packet bypass 流量整形 qdisc。 &lt;code>pfifo_fast&lt;/code> qdisc 允许数据包 bypass&lt;/li>
&lt;li>&lt;code>!qdisc_qlen(q)&lt;/code>：qdisc 的队列中没有待发送的数据&lt;/li>
&lt;li>&lt;code>qdisc_run_begin(p)&lt;/code>：如果 qdisc 未运行，此函数将设置 qdisc 的状态为“running”并返 回 &lt;code>true&lt;/code>，如果 qdisc 已在运行，则返回 &lt;code>false&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>如果以上三个条件都为 &lt;code>true&lt;/code>，那么：&lt;/p>
&lt;ul>
&lt;li>检查 &lt;code>IFF_XMIT_DST_RELEASE&lt;/code> 标志，此标志允许内核释放 skb 的目标缓存。如果标志已禁用，将强制对 skb 进行引用计数&lt;/li>
&lt;li>调用 &lt;code>qdisc_bstats_update&lt;/code> 更新 qdisc 发送的字节数和包数统计&lt;/li>
&lt;li>调用 &lt;code>sch_direct_xmit&lt;/code> 用于发送数据包。我们将很快深入研究 &lt;code>sch_direct_xmit&lt;/code>，因为慢路径也会调用到它&lt;/li>
&lt;/ul>
&lt;p>&lt;code>sch_direct_xmit&lt;/code> 的返回值有两种情况：&lt;/p>
&lt;ol>
&lt;li>队列不为空（返回&amp;gt; 0）。在这种情况下，&lt;code>busylock&lt;/code> 将被释放，然后调用&lt;code>__qdisc_run&lt;/code> 重新启动 qdisc 处理&lt;/li>
&lt;li>队列为空（返回 0）。在这种情况下，&lt;code>qdisc_run_end&lt;/code> 用于关闭 qdisc 处理&lt;/li>
&lt;/ol>
&lt;p>在任何一种情况下，都会返回 &lt;code>NET_XMIT_SUCCESS&lt;/code>，这不是太糟糕。
让我们检查最后一种情况：&lt;/p>
&lt;pre>&lt;code>} else {
skb_dst_force(skb);
rc = q-&amp;gt;enqueue(skb, q) &amp;amp; NET_XMIT_MASK;
if (qdisc_run_begin(q)) {
if (unlikely(contended)) {
spin_unlock(&amp;amp;q-&amp;gt;busylock);
contended = false;
}
__qdisc_run(q);
}
}
&lt;/code>&lt;/pre>
&lt;p>在所有其他情况下：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>skb_dst_force&lt;/code> 强制对 skb 的目标缓存进行引用计数&lt;/li>
&lt;li>调用 qdisc 的 &lt;code>enqueue&lt;/code> 方法将数据入队，保存函数返回值&lt;/li>
&lt;li>调用 &lt;code>qdisc_run_begin(p)&lt;/code>将 qdisc 标记为正在运行。如果它尚未运行（&lt;code>contended == false&lt;/code>），则释放 &lt;code>busylock&lt;/code>，然后调用&lt;code>__qdisc_run(p)&lt;/code>启动 qdisc 处理&lt;/li>
&lt;/ol>
&lt;p>函数最后释放相应的锁，并返回状态码：&lt;/p>
&lt;pre>&lt;code>spin_unlock(root_lock);
if (unlikely(contended))
spin_unlock(&amp;amp;q-&amp;gt;busylock);
return rc;
&lt;/code>&lt;/pre>
&lt;h2 id="75-调优-transmit-packet-steering-xps">7.5 调优: Transmit Packet Steering (XPS)&lt;/h2>
&lt;p>使用 XPS 需要在内核配置中启用它（Ubuntu 上内核 3.13.0 有 XPS），并提供一个位掩码，用于 描述&lt;strong>CPU 和 TX queue 的对应关系&lt;/strong>。
这些位掩码类似于 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#receive-packet-steering-rps">RPS&lt;/a> 位掩码，你可以在内核&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L147-L150">文档  &lt;/a>中找到有关这些位掩码的一些资料。
简而言之，要修改的位掩码位于以下位置：&lt;/p>
&lt;pre>&lt;code>/sys/class/net/DEVICE_NAME/queues/QUEUE/xps_cpus
&lt;/code>&lt;/pre>
&lt;p>因此，对于 eth0 和 TX queue 0，你需要使用十六进制数修改文件： &lt;code>/sys/class/net/eth0/queues/tx-0/xps_cpus&lt;/code>，制定哪些 CPU 应处理来自 eth0 的发送队列 0 的发送过程。另外，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L412-L422">文档  &lt;/a>指出，在某些配置中可能不需要 XPS。&lt;/p>
&lt;h1 id="8-queuing-disciplines排队规则">8 Queuing Disciplines（排队规则）&lt;/h1>
&lt;p>至此，我们需要先看一些 qdisc 代码。本文不打算涵盖 TX 所有选项的具体细节。 如果对此感兴趣，可以查看&lt;a href="http://lartc.org/howto/index.html">这篇&lt;/a>很棒的指南。
接下来将查看&lt;strong>通用的数据包调度程序&lt;/strong>（generic packet scheduler）是如何工作的 。特别地，我们将分析 &lt;code>qdisc_run_begin()&lt;/code>、&lt;code>qdisc_run_end()&lt;/code>、&lt;code>__ qdisc_run()&lt;/code> 和 &lt;code>sch_direct_xmit()&lt;/code> 函数是如何一层层将数据传递给驱动程序的。
从 &lt;code>qdisc_run_begin()&lt;/code> 的工作原理开始。&lt;/p>
&lt;h2 id="81-qdisc_run_begin-and-qdisc_run_end仅设置-qdisc-状态位">8.1 &lt;code>qdisc_run_begin()&lt;/code> and &lt;code>qdisc_run_end()&lt;/code>：仅设置 qdisc 状态位&lt;/h2>
&lt;p>定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/net/sch_generic.h#L101-L107">include/net/sch_generic.h&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static inline bool qdisc_run_begin(struct Qdisc *qdisc)
{
if (qdisc_is_running(qdisc))
return false;
qdisc-&amp;gt;__state |= __QDISC___STATE_RUNNING;
return true;
}
static inline void qdisc_run_end(struct Qdisc *qdisc)
{
qdisc-&amp;gt;__state &amp;amp;= ~__QDISC___STATE_RUNNING;
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>qdisc_run_begin()&lt;/code> 检查 qdisc 是否设置了&lt;code>__QDISC___STATE_RUNNING&lt;/code> 状态 位。如果设置了，直接返回 &lt;code>false&lt;/code>；否则，设置此状态位，然后返回 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>&lt;code>qdisc_run_end()&lt;/code> 执行相反的操作，清除此状态位。&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，这两个函数都&lt;strong>只是设置状态位，并没有真正干活&lt;/strong>。真正的处理过程是从 &lt;code>__qdisc_run()&lt;/code> 开始的。&lt;/p>
&lt;h2 id="82-__qdisc_run真正的-qdisc-执行入口">8.2 &lt;code>__qdisc_run()&lt;/code>：真正的 qdisc 执行入口&lt;/h2>
&lt;p>这个函数乍看非常简单，甚至让人产生错觉：&lt;/p>
&lt;pre>&lt;code>void __qdisc_run(struct Qdisc *q)
{
int quota = weight_p;
while (qdisc_restart(q)) { // 从队列取出一个 skb 并发送，剩余队列不为空时返回非零，见 8.3
// 如果发生下面情况之一，则延后处理：
// 1. quota 用尽
// 2. 其他进程需要 CPU
if (--quota &amp;lt;= 0 || need_resched()) {
__netif_schedule(q);
break;
}
}
qdisc_run_end(q); // 清除 RUNNING 状态位
}
&lt;/code>&lt;/pre>
&lt;p>函数首先获取 &lt;code>weight_p&lt;/code>，这个变量通常是通过 sysctl 设置的，收包路径也会用到。我们稍 后会看到如何调整此值。这个循环做两件事：&lt;/p>
&lt;ol>
&lt;li>在 &lt;code>while&lt;/code> 循环中调用 &lt;code>qdisc_restart()&lt;/code>，直到它返回 &lt;code>false&lt;/code>（或触发下面的中断）。&lt;/li>
&lt;li>判断是否还有 quota，或 &lt;code>need_resched()&lt;/code> 是否返回 &lt;code>true&lt;/code>。其中任何一个为真， 将调用 &lt;code>__netif_schedule()&lt;/code> 然后跳出循环。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注意：用户程序调用 &lt;code>sendmsg&lt;/code> &lt;strong>系统调用之后，内核便接管了执行过程，一路执行到 这里;用户程序一直在累积系统时间（system time）&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>如果用户程序在内核中用完其 time quota，&lt;code>need_resched()&lt;/code> 将返回 &lt;code>true&lt;/code>。&lt;/li>
&lt;li>如果仍有 quota，且用户程序的时间片尚未使用，则将再次调用 &lt;code>qdisc_restart()&lt;/code>。&lt;/li>
&lt;/ul>
&lt;p>先来看看 &lt;code>qdisc_restart(q)&lt;/code>是如何工作的，然后将深入研究&lt;code>__netif_schedule(q)&lt;/code>。&lt;/p>
&lt;h2 id="83-qdisc_restart从-qdisc-队列中取包发送给网络驱动">8.3 &lt;code>qdisc_restart&lt;/code>：从 qdisc 队列中取包，发送给网络驱动&lt;/h2>
&lt;p>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L156-L192">qdisc_restart()&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/*
* NOTE: Called under qdisc_lock(q) with locally disabled BH.
*
* __QDISC_STATE_RUNNING guarantees only one CPU can process
* this qdisc at a time. qdisc_lock(q) serializes queue accesses for this queue.
*
* netif_tx_lock serializes accesses to device driver.
*
* qdisc_lock(q) and netif_tx_lock are mutually exclusive,
* if one is grabbed, another must be free.
*
* Returns to the caller:
* 0 - queue is empty or throttled.
* &amp;gt;0 - queue is not empty.
*/
static inline int qdisc_restart(struct Qdisc *q)
{
struct sk_buff *skb = dequeue_skb(q);
if (!skb)
return 0;
spinlock_t *root_lock = qdisc_lock(q);
struct net_device *dev = qdisc_dev(q);
struct netdev_queue *txq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));
return sch_direct_xmit(skb, q, dev, txq, root_lock);
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>qdisc_restart()&lt;/code> 函数开头的注释非常有用，描述了用到的三个锁：&lt;/p>
&lt;ol>
&lt;li>&lt;code>__QDISC_STATE_RUNNING&lt;/code> 保证了同一时间只有一个 CPU 可以处理这个 qdisc。&lt;/li>
&lt;li>&lt;code>qdisc_lock(q)&lt;/code> 将&lt;strong>访问此 qdisc&lt;/strong> 的操作顺序化。&lt;/li>
&lt;li>&lt;code>netif_tx_lock&lt;/code> 将&lt;strong>访问设备驱动&lt;/strong>的操作顺序化。&lt;/li>
&lt;/ol>
&lt;p>函数逻辑：&lt;/p>
&lt;ol>
&lt;li>首先调用 &lt;code>dequeue_skb()&lt;/code> 从 qdisc 中取出要发送的 skb。如果队列为空，返回 0， 这将导致上层的 &lt;code>qdisc_restart()&lt;/code> 返回 &lt;code>false&lt;/code>，继而退出 &lt;code>while&lt;/code> 循环。&lt;/li>
&lt;li>如果 skb 不为空，接下来获取 qdisc 队列锁，然后找到相关的发送设备 &lt;code>dev&lt;/code> 和发送 队列 &lt;code>txq&lt;/code>，最后带着这些参数调用 &lt;code>sch_direct_xmit()&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>先来看 &lt;code>dequeue_skb()&lt;/code>，然后再回到 &lt;code>sch_direct_xmit()&lt;/code>。&lt;/p>
&lt;h3 id="831-dequeue_skb从-qdisc-队列取待发送-skb">8.3.1 &lt;code>dequeue_skb()&lt;/code>：从 qdisc 队列取待发送 skb&lt;/h3>
&lt;p>定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L59-L78">net/sched/sch_generic.c&lt;/a>。&lt;/p>
&lt;pre>&lt;code>static inline struct sk_buff *dequeue_skb(struct Qdisc *q)
{
struct sk_buff *skb = q-&amp;gt;gso_skb; // 待发送包
struct netdev_queue *txq = q-&amp;gt;dev_queue; // 之前发送失败的包所在的队列
if (unlikely(skb)) {
/* check the reason of requeuing without tx lock first */
txq = netdev_get_tx_queue(txq-&amp;gt;dev, skb_get_queue_mapping(skb));
if (!netif_xmit_frozen_or_stopped(txq)) {
q-&amp;gt;gso_skb = NULL;
q-&amp;gt;q.qlen--;
} else
skb = NULL;
} else {
if (!(q-&amp;gt;flags &amp;amp; TCQ_F_ONETXQUEUE) || !netif_xmit_frozen_or_stopped(txq))
skb = q-&amp;gt;dequeue(q);
}
return skb;
&lt;/code>&lt;/pre>
&lt;p>函数首先声明一个 &lt;code>struct sk_buff *skb&lt;/code> 变量，这是接下来要处理的数据。这个变量后 面会依不同情况而被赋不同的值，最后作为返回值返回给调用方。
变量 &lt;code>skb&lt;/code> 初始化为 qdisc 的 &lt;code>gso_skb&lt;/code> 字段，这是&lt;strong>之前由于发送失败而重新入队的数据&lt;/strong>。
接下来分为两种情况，根据 &lt;code>skb = q-&amp;gt;gso_skb&lt;/code> 是否为空：&lt;/p>
&lt;ol>
&lt;li>如果不为空，会将之前重新入队的 skb 出队，作为待处理数据返回。
&lt;ol>
&lt;li>检查发送队列是否已停止。&lt;/li>
&lt;li>如果队列未停止，则 &lt;code>gso_skb&lt;/code> 字段置空，队列长度减 1，返回 skb。&lt;/li>
&lt;li>如果队列已停止，则 &lt;code>gso_skb&lt;/code> 不动，返回空。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>如果为空（即之前没有数据重新入队），则从要处理的 qdisc 中取出一个新 skb，作为待处理数据返回。进入另一个 tricky 的 if 语句，如果：
&lt;ol>
&lt;li>qdisc 不是单发送队列，或&lt;/li>
&lt;li>发送队列未停止工作&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>则调用 qdisc 的 &lt;code>dequeue()&lt;/code> 方法获取新数据并返回。dequeue 的内部实现依 qdisc 的实现和功能而有所不同。&lt;/li>
&lt;/ol>
&lt;p>该函数最后返回变量 &lt;code>skb&lt;/code>，这是接下来要处理的数据包。&lt;/p>
&lt;h3 id="832-sch_direct_xmit发送给网卡驱动">8.3.2 &lt;code>sch_direct_xmit()&lt;/code>：发送给网卡驱动&lt;/h3>
&lt;p>现在来到 &lt;code>sch_direct_xmit()&lt;/code>（定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L109-L154">net/sched/sch_generic.c&lt;/a> ），这是将数据向下发送到网络设备的重要一步。&lt;/p>
&lt;pre>&lt;code>/*
* Transmit one skb, and handle the return status as required. Holding the
* __QDISC_STATE_RUNNING bit guarantees that only one CPU can execute this
* function.
*
* Returns to the caller:
* 0 - queue is empty or throttled.
* &amp;gt;0 - queue is not empty.
*/
int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,
struct net_device *dev, struct netdev_queue *txq,
spinlock_t *root_lock)
{
int ret = NETDEV_TX_BUSY;
spin_unlock(root_lock);
if (!netif_xmit_frozen_or_stopped(txq))
ret = dev_hard_start_xmit(skb, dev, txq);
spin_lock(root_lock);
if (dev_xmit_complete(ret)) { // 1. 驱动发送成功
ret = qdisc_qlen(q); // 将 qdisc 队列的剩余长度作为返回值
} else if (ret == NETDEV_TX_LOCKED) { // 2. 驱动获取发送锁失败
ret = handle_dev_cpu_collision(skb, txq, q);
} else { // 3. 驱动发送“正忙”，当前无法发送
ret = dev_requeue_skb(skb, q); // 将数据重新入队，等下次发送。
}
if (ret &amp;amp;&amp;amp; netif_xmit_frozen_or_stopped(txq))
ret = 0;
return ret;
&lt;/code>&lt;/pre>
&lt;p>这段代码首先释放 qdisc（发送队列）锁，然后获取（设备驱动的）发送锁。
接下来，如果发送队列没有停止，就会调用 &lt;code>dev_hard_start_xmit()&lt;/code>。稍后将看到， 后者会把数据从 Linux 内核的网络设备子系统发送到设备驱动程序。
&lt;code>dev_hard_start_xmit()&lt;/code> 执行之后，（或因发送队列停止而跳过执行），队列的发送锁就会被释放。
接下来，再次获取此 qdisc 的锁，然后通过调用 &lt;code>dev_xmit_complete()&lt;/code> 检查 &lt;code>dev_hard_start_xmit()&lt;/code> 的返回值。&lt;/p>
&lt;ol>
&lt;li>如果 &lt;code>dev_xmit_complete()&lt;/code> 返回 &lt;code>true&lt;/code>，数据已成功发送，则将 qdisc 队列长度设置为返回值，否则&lt;/li>
&lt;li>如果 &lt;code>dev_hard_start_xmit()&lt;/code> 返回的是 &lt;code>NETDEV_TX_LOCKED&lt;/code>，调用 &lt;code>handle_dev_cpu_collision()&lt;/code> 来处理锁竞争。
当驱动程序锁定发送队列失败时，支持 &lt;code>NETIF_F_LLTX&lt;/code> 功能的设备会返回 &lt;code>NETDEV_TX_LOCKED&lt;/code>。 稍后会仔细研究 &lt;code>handle_dev_cpu_collision&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>现在，让我们继续关注 &lt;code>sch_direct_xmit()&lt;/code> 并查看，以上两种情况都不满足时的情况。 如果发送失败，而且不是以上两种情况，那还有第三种可能：由于 &lt;code>NETDEV_TX_BUSY&lt;/code>。驱动 程序返回 &lt;code>NETDEV_TX_BUSY&lt;/code> 表示设备或驱动程序“正忙”，数据现在无法发送。这种情 况下，调用 &lt;code>dev_requeue_skb()&lt;/code> 将数据重新入队，等下次发送。
来深入地看一下 &lt;code>handle_dev_cpu_collision()&lt;/code> 和 &lt;code>dev_requeue_skb()&lt;/code>。&lt;/p>
&lt;h3 id="833-handle_dev_cpu_collision">8.3.3 &lt;code>handle_dev_cpu_collision()&lt;/code>&lt;/h3>
&lt;p>定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L80-L107">net/sched/sch_generic.c&lt;/a>，处理两种情况：&lt;/p>
&lt;ol>
&lt;li>发送锁由当前 CPU 保持&lt;/li>
&lt;li>发送锁由其他 CPU 保存&lt;/li>
&lt;/ol>
&lt;p>第一种情况认为是配置问题，打印一条警告。
第二种情况，更新统计计数器 &lt;code>cpu_collision&lt;/code>，通过 &lt;code>dev_requeue_skb&lt;/code> 将数据重新入队 以便稍后发送。回想一下，我们在 &lt;code>dequeue_skb&lt;/code> 中看到了专门处理重新入队的 skb 的代码。
代码很简短，可以快速阅读：&lt;/p>
&lt;pre>&lt;code>static inline int handle_dev_cpu_collision(struct sk_buff *skb,
struct netdev_queue *dev_queue,
struct Qdisc *q)
{
int ret;
if (unlikely(dev_queue-&amp;gt;xmit_lock_owner == smp_processor_id())) {
/*
* Same CPU holding the lock. It may be a transient
* configuration error, when hard_start_xmit() recurses. We
* detect it by checking xmit owner and drop the packet when
* deadloop is detected. Return OK to try the next skb.
*/
kfree_skb(skb);
net_warn_ratelimited(&amp;quot;Dead loop on netdevice %s, fix it urgently!\n&amp;quot;,
dev_queue-&amp;gt;dev-&amp;gt;name);
ret = qdisc_qlen(q);
} else {
/*
* Another cpu is holding lock, requeue &amp;amp; delay xmits for
* some time.
*/
__this_cpu_inc(softnet_data.cpu_collision);
ret = dev_requeue_skb(skb, q);
}
return ret;
}
&lt;/code>&lt;/pre>
&lt;p>接下来看看 &lt;code>dev_requeue_skb&lt;/code> 做了什么，后面会看到，&lt;code>sch_direct_xmit&lt;/code> 会调用它.&lt;/p>
&lt;h3 id="834-dev_requeue_skb重新压入-qdisc-队列等待下次发送">8.3.4 &lt;code>dev_requeue_skb()&lt;/code>：重新压入 qdisc 队列，等待下次发送&lt;/h3>
&lt;p>这个函数很简短，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/sched/sch_generic.c#L39-L57">net/sched/sch_generic.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/* Modifications to data participating in scheduling must be protected with
* qdisc_lock(qdisc) spinlock.
*
* The idea is the following:
* - enqueue, dequeue are serialized via qdisc root lock
* - ingress filtering is also serialized via qdisc root lock
* - updates to tree and tree walking are only done under the rtnl mutex.
*/
static inline int dev_requeue_skb(struct sk_buff *skb, struct Qdisc *q)
{
skb_dst_force(skb); // skb 上强制增加一次引用计数
q-&amp;gt;gso_skb = skb; // 回想一下，dequeue_skb() 中取出一个 skb 时会检查该字段
q-&amp;gt;qstats.requeues++; // 更新 `requeue` 计数
q-&amp;gt;q.qlen++; // 更新 qdisc 队列长度
__netif_schedule(q); // 触发 softirq
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>接下来再回忆一遍我们一步步到达这里的过程，然后查看 &lt;code>__netif_schedule()&lt;/code>。&lt;/p>
&lt;h2 id="84-复习__qdisc_run-主逻辑">8.4 复习：&lt;code>__qdisc_run()&lt;/code> 主逻辑&lt;/h2>
&lt;p>回想一下，我们是从 &lt;code>__qdisc_run()&lt;/code> 开始到达这里的：&lt;/p>
&lt;pre>&lt;code>void __qdisc_run(struct Qdisc *q)
{
int quota = weight_p;
while (qdisc_restart(q)) { // dequeue skb, send it
if (--quota &amp;lt;= 0 || need_resched()) {// Ordered by possible occurrence: Postpone processing if
__netif_schedule(q); // 1. we've exceeded packet quota
break; // 2. another process needs the CPU
}
}
qdisc_run_end(q);
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>while&lt;/code> 循环调用 &lt;code>qdisc_restart()&lt;/code>，后者取出一个 skb，然后尝试通过 &lt;code>sch_direct_xmit()&lt;/code> 来发送；&lt;code>sch_direct_xmit&lt;/code> 调用 &lt;code>dev_hard_start_xmit&lt;/code> 来向驱动 程序进行实际发送。任何无法发送的 skb 都重新入队，将在 &lt;code>NET_TX&lt;/code> softirq 中进行 发送。
发送过程的下一步是查看 &lt;code>dev_hard_start_xmit()&lt;/code>，了解如何调用驱动程序来发送数据。但 在此之前，应该先查看 &lt;code>__netif_schedule()&lt;/code> 以完全理解 &lt;code>__qdisc_run()&lt;/code> 和 &lt;code>dev_requeue_skb()&lt;/code> 的工作方式。&lt;/p>
&lt;h3 id="841-__netif_schedule">8.4.1 &lt;code>__netif_schedule&lt;/code>&lt;/h3>
&lt;p>现在来看 &lt;code>__netif_schedule()&lt;/code>， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2127-L2146">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>void __netif_schedule(struct Qdisc *q)
{
if (!test_and_set_bit(__QDISC_STATE_SCHED, &amp;amp;q-&amp;gt;state))
__netif_reschedule(q);
}
EXPORT_SYMBOL(__netif_schedule);
static inline void __netif_reschedule(struct Qdisc *q)
{
struct softnet_data *sd;
unsigned long flags;
local_irq_save(flags); // 保存硬中断状态，并禁用硬中断（IRQ）
sd = &amp;amp;__get_cpu_var(softnet_data); // 获取当前 CPU 的 struct softnet_data 实例
q-&amp;gt;next_sched = NULL;
*sd-&amp;gt;output_queue_tailp = q; // 将 qdisc 添加到 softnet_data 的 output 队列中
sd-&amp;gt;output_queue_tailp = &amp;amp;q-&amp;gt;next_sched;
raise_softirq_irqoff(NET_TX_SOFTIRQ); // 重要步骤：触发 NET_TX_SOFTIRQ 类型软中断（softirq）
local_irq_restore(flags); // 恢复 IRQ 状态并重新启用硬中断
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>test_and_set_bit()&lt;/code> 检查 &lt;code>q-&amp;gt;state&lt;/code> 中的 &lt;code>__QDISC_STATE_SCHED&lt;/code> 位，如果为该位为 0，会将其置 1。 如果置位成功（意味着之前处于非 &lt;code>__QDISC_STATE_SCHED&lt;/code> 状态），代码将调用 &lt;code>__netif_reschedule()&lt;/code>，这个函数不长，但做的事情非常重要。&lt;/p>
&lt;blockquote>
&lt;p>更多有关 &lt;code>struct softnet_data&lt;/code> 初始化的内容，可参考我们之前关于网络栈接收数据的 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#linux-network-device-subsystem">文章&lt;/a>。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>__netif_reschedule()&lt;/code> 中的重要步骤是 &lt;code>raise_softirq_irqoff()&lt;/code>，它触发一次 &lt;code>NET_TX_SOFTIRQ&lt;/code> 类型 softirq。 softirqs 及其注册过程也包含在我们之前的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#softirqs">文章  &lt;/a>中。简单来说，可以认为 &lt;strong>softirqs 是以很高优先级在执行的内核线程，并代表内核处理数据&lt;/strong>， 用于网络数据的收发处理（incoming 和 outgoing）。
正如在&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">上一篇&lt;/a>文章中看到的，&lt;code>NET_TX_SOFTIRQ&lt;/code> softirq 有一个注册的回调函数 &lt;code>net_tx_action()&lt;/code>，这意味着有一个内核线程将会执行 &lt;code>net_tx_action()&lt;/code>。该线程偶尔会被暂 停（pause），&lt;code>raise_softirq_irqoff()&lt;/code> 会恢复（resume）其执行。让我们看一下 &lt;code>net_tx_action()&lt;/code> 的作用，以便了解内核如何处理发送数据请求。&lt;/p>
&lt;h3 id="842-net_tx_action">8.4.2 &lt;code>net_tx_action()&lt;/code>&lt;/h3>
&lt;p>定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3297-L3353">net/core/dev.c&lt;/a> ，由两个 if 组成，分别处理 executing CPU 的 &lt;strong>&lt;code>softnet_data&lt;/code> 实例的两个 queue&lt;/strong>：&lt;/p>
&lt;ol>
&lt;li>completion queue&lt;/li>
&lt;li>output queue&lt;/li>
&lt;/ol>
&lt;p>让我们分别来看这两种情况，注意，&lt;strong>这段代码在 softirq 上下文中作为一个独立的内核线 程执行&lt;/strong>。网络栈发送侧的&lt;strong>热路径中不适合执行的代码，将被延后（defer），然 后由执行 &lt;code>net_tx_action()&lt;/code> 的线程处理&lt;/strong>。&lt;/p>
&lt;h3 id="843-net_tx_action-completion-queue待释放-skb-队列">8.4.3 &lt;code>net_tx_action()&lt;/code> completion queue：待释放 skb 队列&lt;/h3>
&lt;p>&lt;code>softnet_data&lt;/code> 的 completion queue 存放&lt;strong>等待释放的 skb&lt;/strong>。函数 &lt;code>dev_kfree_skb_irq&lt;/code> 可以将 skbs 添加到队列中以便稍后释放。设备驱动程序通常使用它来推迟释放已经发送成功的 skbs。驱动 程序推迟释放 skb 的原因是，释放内存可能需要时间，而且有些代码（如 hardirq 处理程序） 需要尽可能快的执行并返回。
看一下 &lt;code>net_tx_action&lt;/code> 第一段代码，该代码处理 completion queue 中等待释放的 skb：&lt;/p>
&lt;pre>&lt;code>if (sd-&amp;gt;completion_queue) {
struct sk_buff *clist;
local_irq_disable();
clist = sd-&amp;gt;completion_queue;
sd-&amp;gt;completion_queue = NULL;
local_irq_enable();
while (clist) {
struct sk_buff *skb = clist;
clist = clist-&amp;gt;next;
__kfree_skb(skb);
}
}
&lt;/code>&lt;/pre>
&lt;p>如果 completion queue 非空，&lt;code>while&lt;/code> 循环将遍历这个列表并&lt;code>__kfree_skb&lt;/code> 释放每个 skb 占 用的内存。&lt;strong>牢记，此代码在一个名为 softirq 的独立“线程”中运行 - 它并没有占用用 户程序的系统时间（system time）&lt;/strong>。&lt;/p>
&lt;h3 id="844-net_tx_action-output-queue待发送-skb-队列">8.4.4 &lt;code>net_tx_action&lt;/code> output queue：待发送 skb 队列&lt;/h3>
&lt;p>output queue 存储 &lt;strong>待发送的 skb&lt;/strong>。如前所述，&lt;code>__netif_reschedule()&lt;/code> 将数据添加到 output queue 中，通常从&lt;code>__netif_schedule&lt;/code> 调用过来。
目前，我们看到 &lt;code>__netif_schedule()&lt;/code> 函数在两个地方被调用：&lt;/p>
&lt;ol>
&lt;li>&lt;code>dev_requeue_skb()&lt;/code>：如果驱动程序返回 &lt;code>NETDEV_TX_BUSY&lt;/code> 或者存在 CPU 冲突，可以调用此函数。&lt;/li>
&lt;li>&lt;code>__qdisc_run()&lt;/code>：一旦超出 quota 或者需要 reschedule，会调用&lt;code>__netif_schedule&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>这个函数会将 qdisc 添加到 softnet_data 的 output queue 进行处理。 这里将输出队列处理代码拆分为三个块。
我们来看看第一块：&lt;/p>
&lt;pre>&lt;code>if (sd-&amp;gt;output_queue) { // 如果 output queue 上有 qdisc
struct Qdisc *head;
local_irq_disable();
head = sd-&amp;gt;output_queue; // 将 head 指向第一个 qdisc
sd-&amp;gt;output_queue = NULL;
sd-&amp;gt;output_queue_tailp = &amp;amp;sd-&amp;gt;output_queue; // 更新队尾指针
local_irq_enable();
&lt;/code>&lt;/pre>
&lt;p>如果 output queue 上有 qdisc，则将 &lt;code>head&lt;/code> 变量指向第一个 qdisc，并 更新队尾指针。
接下来，一个 &lt;strong>&lt;code>while&lt;/code> 循环开始遍历 qdsics 列表&lt;/strong>：&lt;/p>
&lt;pre>&lt;code>while (head) {
struct Qdisc *q = head;
head = head-&amp;gt;next_sched;
spinlock_t *root_lock = qdisc_lock(q);
if (spin_trylock(root_lock)) { // 非阻塞：尝试获取 qdisc root lock
smp_mb__before_clear_bit();
clear_bit(__QDISC_STATE_SCHED, &amp;amp;q-&amp;gt;state); // 清除 q-&amp;gt;state SCHED 状态位
qdisc_run(q); // 执行 qdisc 规则，这会设置 q-&amp;gt;state 的 RUNNING 状态位
spin_unlock(root_lock); // 释放 qdisc 锁
} else {
if (!test_bit(__QDISC_STATE_DEACTIVATED, &amp;amp;q-&amp;gt;state)) { // qdisc 还在运行
__netif_reschedule(q); // 重新放入 queue，稍后继续尝试获取 root lock
} else { // qdisc 已停止运行，清除 SCHED 状态位
smp_mb__before_clear_bit();
clear_bit(__QDISC_STATE_SCHED, &amp;amp;q-&amp;gt;state);
}
}
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>spin_trylock()&lt;/code> 获得 root lock 后，&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>clear_bit()&lt;/code> 清除 qdisc 的 &lt;code>__QDISC_STATE_SCHED&lt;/code> 状态位。&lt;/li>
&lt;li>然后执行 &lt;code>qdisc_run()&lt;/code>，这会将 &lt;code>__QDISC___STATE_RUNNING&lt;/code> 状态位置 1，并执行&lt;code>__qdisc_run()&lt;/code>。&lt;/li>
&lt;/ol>
&lt;p>这里很重要。从系统调用开始的发送过程代表 applition 执行，花费的是系统时间；但接 下来它将转入 softirq 上下文中执行（这个 qdisc 的 skb 之前没有被发送出去发），花 费的是 softirq 时间。这种区分非常重要，因为这&lt;strong>直接影响着应用程序的 CPU 使用量监 控&lt;/strong>，尤其是发送大量数据的应用。换一种陈述方式：&lt;/p>
&lt;ol>
&lt;li>无论发送完成还是驱动程序返回错误，程序的系统时间都包括调用驱动程序发送数据所花的时间。&lt;/li>
&lt;li>如果驱动层发送失败（例如，设备忙于发送其他内容），则会将 qdisc 添加到 output queue，稍后由 softirq 线程处理。在这种情况下，将会额外花费一些 softirq（ &lt;code>si&lt;/code>）时间在发送数据上。&lt;/li>
&lt;/ol>
&lt;p>因此，发送数据花费的总时间是下面二者之和：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>系统调用的系统时间&lt;/strong>（sys time）&lt;/li>
&lt;li>&lt;strong>&lt;code>NET_TX&lt;/code> 类型的 softirq 时间&lt;/strong>（softirq time）&lt;/li>
&lt;/ol>
&lt;p>如果 &lt;code>spin_trylock()&lt;/code> 失败，则检查 qdisc 是否已经停止运行（&lt;code>__QDISC_STATE_DEACTIVATED&lt;/code> 状态位），两种情况：&lt;/p>
&lt;ol>
&lt;li>qdisc 未停用：调用 &lt;code>__netif_reschedule()&lt;/code>，这会将 qdisc 放回到原 queue 中，稍后再次尝试获取 qdisc 锁。&lt;/li>
&lt;li>qdisc 已停用：清除 &lt;code>__QDISC_STATE_SCHED&lt;/code> 状态位。&lt;/li>
&lt;/ol>
&lt;h2 id="85-最终来到-dev_hard_start_xmit">8.5 最终来到 &lt;code>dev_hard_start_xmit&lt;/code>&lt;/h2>
&lt;p>至此，我们已经穿过了整个网络栈，最终来到 &lt;code>dev_hard_start_xmit&lt;/code>。也许你是从 &lt;code>sendmsg&lt;/code> 系统调用直接到达这里的，或者你是通过 qdisc 上的 softirq 线程处理网络数据来 到这里的。&lt;code>dev_hard_start_xmit&lt;/code> 将调用设备驱动程序来实际执行发送操作。
这个函数处理两种主要情况：&lt;/p>
&lt;ol>
&lt;li>已经准备好要发送的数据，或&lt;/li>
&lt;li>需要 segmentation offloading 的数据&lt;/li>
&lt;/ol>
&lt;p>先看第一种情况，要发送的数据已经准备好的情况。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L2541-L2652">net/code/dev.c&lt;/a> ：&lt;/p>
&lt;pre>&lt;code>int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
struct netdev_queue *txq)
{
const struct net_device_ops *ops = dev-&amp;gt;netdev_ops;
int rc = NETDEV_TX_OK;
unsigned int skb_len;
if (likely(!skb-&amp;gt;next)) {
netdev_features_t features;
/*
* If device doesn't need skb-&amp;gt;dst, release it right now while
* its hot in this cpu cache
*/
if (dev-&amp;gt;priv_flags &amp;amp; IFF_XMIT_DST_RELEASE)
skb_dst_drop(skb);
features = netif_skb_features(skb);
&lt;/code>&lt;/pre>
&lt;p>代码首先获取设备的回调函数集合 &lt;code>ops&lt;/code>，后面让驱动程序做一些发送数据的工作时会用到 。检查 &lt;code>skb-&amp;gt;next&lt;/code> 以确定此数据不是已分片数据的一部分，然后继续执行以下两项操作：
首先，检查设备是否设置了 &lt;code>IFF_XMIT_DST_RELEASE&lt;/code> 标志。这个版本的内核中的任何“真实” 以太网设备都不使用此标志，但环回设备和其他一些软件设备使用。如果启用此特性，则可 以减少目标高速缓存条目上的引用计数，因为驱动程序不需要它。
接下来，&lt;code>netif_skb_features&lt;/code> 获取设备支持的功能列表，并根据数据的协议类型（ &lt;code>dev-&amp;gt;protocol&lt;/code>）对特性列表进行一些修改。例如，如果设备支持此协议的校验和计算， 则将对 skb 进行相应的标记。 VLAN tag（如果已设置）也会导致功能标记被修改。
接下来，将检查 vlan 标记，如果设备无法 offload VLAN tag，将通过&lt;code>__vlan_put_tag&lt;/code> 在软 件中执行此操作：&lt;/p>
&lt;pre>&lt;code>if (vlan_tx_tag_present(skb) &amp;amp;&amp;amp;
!vlan_hw_offload_capable(features, skb-&amp;gt;vlan_proto)) {
skb = __vlan_put_tag(skb, skb-&amp;gt;vlan_proto,
vlan_tx_tag_get(skb));
if (unlikely(!skb))
goto out;
skb-&amp;gt;vlan_tci = 0;
}
&lt;/code>&lt;/pre>
&lt;p>然后，检查数据以确定这是不是 encapsulation （隧道封装）offload 请求，例如， &lt;a href="https://en.wikipedia.org/wiki/Generic_Routing_Encapsulation">GRE&lt;/a>。 在这种情况 下，feature flags 将被更新，以添加任何特定于设备的硬件封装功能：&lt;/p>
&lt;pre>&lt;code>/* If encapsulation offload request, verify we are testing
* hardware encapsulation features instead of standard
* features for the netdev
*/
if (skb-&amp;gt;encapsulation)
features &amp;amp;= dev-&amp;gt;hw_enc_features;
&lt;/code>&lt;/pre>
&lt;p>接下来，&lt;code>netif_needs_gso&lt;/code> 用于确定 skb 是否需要分片。 如果需要，但设备不支持，则 &lt;code>netif_needs_gso&lt;/code> 将返回 &lt;code>true&lt;/code>，表示分片应在软件中进行。 在这种情况下，调用 &lt;code>dev_gso_segment&lt;/code> 进行分片，代码将跳转到 gso 以发送数据包。我们稍后会看到 GSO 路径。&lt;/p>
&lt;pre>&lt;code>if (netif_needs_gso(skb, features)) {
if (unlikely(dev_gso_segment(skb, features)))
goto out_kfree_skb;
if (skb-&amp;gt;next)
goto gso;
}
&lt;/code>&lt;/pre>
&lt;p>如果数据不需要分片，则处理一些其他情况。 首先，数据是否需要顺序化？ 也就是说，如 果数据分布在多个缓冲区中，设备是否支持发送网络数据，还是首先需要将它们组合成单个 有序缓冲区？ 绝大多数网卡不需要在发送之前将数据顺序化，因此在几乎所有情况下， &lt;code>skb_needs_linearize&lt;/code> 将为 &lt;code>false&lt;/code> 然后被跳过。&lt;/p>
&lt;pre>&lt;code>else {
if (skb_needs_linearize(skb, features) &amp;amp;&amp;amp;
__skb_linearize(skb))
goto out_kfree_skb;
&lt;/code>&lt;/pre>
&lt;p>从接下来的一段注释我们可以了解到，下面的代码判断数据包是否仍然需要计算校验和。 如果设备不支持计算校验和，则在这里通过软件计算：&lt;/p>
&lt;pre>&lt;code>/* If packet is not checksummed and device does not
* support checksumming for this protocol, complete
* checksumming here.
*/
if (skb-&amp;gt;ip_summed == CHECKSUM_PARTIAL) {
if (skb-&amp;gt;encapsulation)
skb_set_inner_transport_header(skb,
skb_checksum_start_offset(skb));
else
skb_set_transport_header(skb,
skb_checksum_start_offset(skb));
if (!(features &amp;amp; NETIF_F_ALL_CSUM) &amp;amp;&amp;amp;
skb_checksum_help(skb))
goto out_kfree_skb;
}
}
&lt;/code>&lt;/pre>
&lt;p>再往前，我们来到了 packet taps（tap 是包过滤器的安插点，例如抓包执行的地方）。回想 一下在&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#netifreceiveskbcore-special-box-delivers-data-to-packet-taps-and-protocol-layers">接收数据的文章  &lt;/a>中，我们看到了数据包是如何传递给 tap（如 &lt;a href="http://www.tcpdump.org/manpages/pcap.3pcap.html">PCAP&lt;/a>）的。 该函数中的下一个代 码块将要发送的数据包传递给 tap（如果有的话）：&lt;/p>
&lt;pre>&lt;code>if (!list_empty(&amp;amp;ptype_all))
dev_queue_xmit_nit(skb, dev);
&lt;/code>&lt;/pre>
&lt;p>最终，调用驱动的 &lt;code>ops&lt;/code> 里面的发送回调函数 &lt;code>ndo_start_xmit&lt;/code> 将数据包传给网卡设备：&lt;/p>
&lt;pre>&lt;code>skb_len = skb-&amp;gt;len;
rc = ops-&amp;gt;ndo_start_xmit(skb, dev);
trace_net_dev_xmit(skb, rc, dev, skb_len);
if (rc == NETDEV_TX_OK)
txq_trans_update(txq);
return rc;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ndo_start_xmit&lt;/code> 的返回值表示发送成功与否，并作为这个函数的返回值被返回给更上层。 我们看到了这个返回值将如何影响上层：数据可能会被此时的 qdisc 重新入队，因此 稍后尝试再次发送。
我们来看看 GSO 的 case。如果此函数的前面部分完成了分片，或者之前已经完成了分片但是 上次发送失败，则会进入下面的代码：&lt;/p>
&lt;pre>&lt;code>gso:
do {
struct sk_buff *nskb = skb-&amp;gt;next;
skb-&amp;gt;next = nskb-&amp;gt;next;
nskb-&amp;gt;next = NULL;
if (!list_empty(&amp;amp;ptype_all))
dev_queue_xmit_nit(nskb, dev);
skb_len = nskb-&amp;gt;len;
rc = ops-&amp;gt;ndo_start_xmit(nskb, dev);
trace_net_dev_xmit(nskb, rc, dev, skb_len);
if (unlikely(rc != NETDEV_TX_OK)) {
if (rc &amp;amp; ~NETDEV_TX_MASK)
goto out_kfree_gso_skb;
nskb-&amp;gt;next = skb-&amp;gt;next;
skb-&amp;gt;next = nskb;
return rc;
}
txq_trans_update(txq);
if (unlikely(netif_xmit_stopped(txq) &amp;amp;&amp;amp; skb-&amp;gt;next))
return NETDEV_TX_BUSY;
} while (skb-&amp;gt;next);
&lt;/code>&lt;/pre>
&lt;p>你可能已经猜到，此 &lt;code>while&lt;/code> 循环会遍历分片生成的 skb 列表。
每个数据包将被：&lt;/p>
&lt;ul>
&lt;li>传给包过滤器（tap，如果有的话）&lt;/li>
&lt;li>通过 &lt;code>ndo_start_xmit&lt;/code> 传递给驱动程序进行发送&lt;/li>
&lt;/ul>
&lt;p>设备驱动 &lt;code>ndo_start_xmit()&lt;/code>返回错误时，会进行一些错误处理，并将错误返回给更上层。 未发送的 skbs 可能会被重新入队以便稍后再次发送。
该函数的最后一部分做一些清理工作，在上面发生错误时释放一些资源：&lt;/p>
&lt;pre>&lt;code>out_kfree_gso_skb:
if (likely(skb-&amp;gt;next == NULL)) {
skb-&amp;gt;destructor = DEV_GSO_CB(skb)-&amp;gt;destructor;
consume_skb(skb);
return rc;
}
out_kfree_skb:
kfree_skb(skb);
out:
return rc;
}
EXPORT_SYMBOL_GPL(dev_hard_start_xmit);
&lt;/code>&lt;/pre>
&lt;p>在继续进入到设备驱动程序之前，先来看一些和前面分析过的代码有关的监控和调优的内容。&lt;/p>
&lt;h2 id="86-monitoring-qdiscs">8.6 Monitoring qdiscs&lt;/h2>
&lt;h3 id="using-the-tc-command-line-tool">Using the tc command line tool&lt;/h3>
&lt;p>使用 &lt;code>tc&lt;/code> 工具监控 qdisc 统计：&lt;/p>
&lt;pre>&lt;code>$ tc -s qdisc show dev eth1
qdisc mq 0: root
Sent 31973946891907 bytes 2298757402 pkt (dropped 0, overlimits 0 requeues 1776429)
backlog 0b 0p requeues 1776429
&lt;/code>&lt;/pre>
&lt;p>网络设备的 qdisc 统计对于监控系统发送数据包的运行状况至关重要。你可以通过运行命令 行工具 tc 来查看状态。 上面的示例显示了如何检查 eth1 的统计信息。&lt;/p>
&lt;ul>
&lt;li>&lt;code>bytes&lt;/code>: The number of bytes that were pushed down to the driver for transmit.&lt;/li>
&lt;li>&lt;code>pkt&lt;/code>: The number of packets that were pushed down to the driver for transmit.&lt;/li>
&lt;li>&lt;code>dropped&lt;/code>: The number of packets that were dropped by the qdisc. This can happen if transmit queue length is not large enough to fit the data being queued to it.&lt;/li>
&lt;li>&lt;code>overlimits&lt;/code>: Depends on the queuing discipline, but can be either the number of packets that could not be enqueued due to a limit being hit, and/or the number of packets which triggered a throttling event when dequeued.&lt;/li>
&lt;li>&lt;code>requeues&lt;/code>: Number of times dev_requeue_skb has been called to requeue an skb. Note that an skb which is requeued multiple times will bump this counter each time it is requeued.&lt;/li>
&lt;li>&lt;code>backlog&lt;/code>: Number of bytes currently on the qdisc’s queue. This number is usually bumped each time a packet is enqueued.&lt;/li>
&lt;/ul>
&lt;p>一些 qdisc 还会导出额外的统计信息。每个 qdisc 都不同，对同一个 counter 可能会累积不同 的次数。你需要查看相应 qdisc 的源代码，弄清楚每个 counter 是在哪里、什么条件下被更新 的，如果这些数据对你非常重要，那你必须这么谨慎。&lt;/p>
&lt;h2 id="87-tuning-qdiscs">8.7 Tuning qdiscs&lt;/h2>
&lt;h3 id="调整__qdisc_run-处理权重">调整&lt;code>__qdisc_run&lt;/code> 处理权重&lt;/h3>
&lt;p>你可以调整前面看到的&lt;code>__qdisc_run&lt;/code> 循环的权重（上面看到的 &lt;code>quota&lt;/code> 变量），这将导致 &lt;code>__netif_schedule&lt;/code> 更多的被调用执行。 结果将是当前 qdisc 将被更多的添加到当前 CPU 的 output_queue，最终会使发包所占的时间变多。
例如：调整所有 qdisc 的&lt;code>__qdisc_run&lt;/code> 权重：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.dev_weight=600
&lt;/code>&lt;/pre>
&lt;h3 id="增加发送队列长度">增加发送队列长度&lt;/h3>
&lt;p>每个网络设备都有一个可以修改的 txqueuelen。 大多数 qdisc 在将数据插入到其发送队列之 前，会检查 txqueuelen 是否足够（表示的是字节数？）。 你可以调整这个参数以增加 qdisc 队列的字节数。
Example: increase the &lt;code>txqueuelen&lt;/code> of &lt;code>eth0&lt;/code> to &lt;code>10000&lt;/code>.&lt;/p>
&lt;pre>&lt;code>$ sudo ifconfig eth0 txqueuelen 10000
&lt;/code>&lt;/pre>
&lt;p>默认值是 1000，你可以通过 ifconfig 命令的输出，查看每个网络设备的 txqueuelen。&lt;/p>
&lt;h1 id="9-网络设备驱动">9 网络设备驱动&lt;/h1>
&lt;p>我们即将结束我们的网络栈之旅。
要理解数据包的发送过程，有一个重要的概念。大多数设备和驱动程序通过两个阶段处理数 据包发送：&lt;/p>
&lt;ol>
&lt;li>合理地组织数据，然后触发设备通过 DMA 从 RAM 中读取数据并将其发送到网络中&lt;/li>
&lt;li>发送完成后，设备发出中断，驱动程序解除映射缓冲区、释放内存或清除其状态&lt;/li>
&lt;/ol>
&lt;p>第二阶段通常称为“发送完成”（transmit completion）阶段。我们将对以上两阶段进行研 究，先从第一个开始：发送阶段。
之前已经看到，&lt;code>dev_hard_start_xmit&lt;/code> 通过调用 &lt;code>ndo_start_xmit&lt;/code>（保持一个锁）来发送 数据，所以接下来先看驱动程序是如何注册 &lt;code>ndo_start_xmit&lt;/code> 的，然后再深入理解该函数的 工作原理。
与上篇&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">Linux 网络栈监控和调优：接收数据&lt;/a> 一样，我们将拿 &lt;code>igb&lt;/code> 驱动作为例子。&lt;/p>
&lt;h2 id="91-驱动回调函数注册">9.1 驱动回调函数注册&lt;/h2>
&lt;p>驱动程序实现了一系列方法来支持设备操作，例如：&lt;/p>
&lt;ol>
&lt;li>发送数据（&lt;code>ndo_start_xmit&lt;/code>）&lt;/li>
&lt;li>获取统计信息（&lt;code>ndo_get_stats64&lt;/code>）&lt;/li>
&lt;li>处理设备 &lt;code>ioctl&lt;/code>s（&lt;code>ndo_do_ioctl&lt;/code>）&lt;/li>
&lt;/ol>
&lt;p>这些方法通过一个 &lt;code>struct net_device_ops&lt;/code> 实例导出。让我们来看看&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1905-L1928">igb 驱动程序  &lt;/a>中这些操作：&lt;/p>
&lt;pre>&lt;code>static const struct net_device_ops igb_netdev_ops = {
.ndo_open = igb_open,
.ndo_stop = igb_close,
.ndo_start_xmit = igb_xmit_frame,
.ndo_get_stats64 = igb_get_stats64,
/* ... more fields ... */
};
&lt;/code>&lt;/pre>
&lt;p>这个 &lt;code>igb_netdev_ops&lt;/code> 变量在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2090">&lt;code>igb_probe&lt;/code>&lt;/a> 函数中注册给设备：&lt;/p>
&lt;pre>&lt;code>static int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
/* ... lots of other stuff ... */
netdev-&amp;gt;netdev_ops = &amp;amp;igb_netdev_ops;
/* ... more code ... */
}
&lt;/code>&lt;/pre>
&lt;p>正如我们在上一节中看到的，更上层的代码将通过设备的 &lt;code>netdev_ops&lt;/code> 字段 调用适当的回调函数。想了解更多关于 PCI 设备是如何启动的，以及何时/何处调用 &lt;code>igb_probe&lt;/code>，请查看我们之前文章中的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#initialization">驱动程序初始化  &lt;/a>部分。&lt;/p>
&lt;h2 id="92-通过-ndo_start_xmit-发送数据">9.2 通过 &lt;code>ndo_start_xmit&lt;/code> 发送数据&lt;/h2>
&lt;p>上层的网络栈通过 &lt;code>struct net_device_ops&lt;/code> 实例里的回调函数，调用驱动程序来执行各种 操作。正如我们之前看到的，qdisc 代码调用 &lt;code>ndo_start_xmit&lt;/code> 将数据传递给驱动程序进行 发送。对于大多数硬件设备，都是在保持一个锁时调用 &lt;code>ndo_start_xmit&lt;/code> 函数。
在 igb 设备驱动程序中，&lt;code>ndo_start_xmit&lt;/code> 字段初始化为 &lt;code>igb_xmit_frame&lt;/code> 函数，所以 我们接下来从 &lt;code>igb_xmit_frame&lt;/code> 开始，查看该驱动程序是如何发送数据的。跟随 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L4664-L4741">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a> ，并记得以下代码在整个执行过程中都 hold 着一个锁：&lt;/p>
&lt;pre>&lt;code>netdev_tx_t igb_xmit_frame_ring(struct sk_buff *skb,
struct igb_ring *tx_ring)
{
struct igb_tx_buffer *first;
int tso;
u32 tx_flags = 0;
u16 count = TXD_USE_COUNT(skb_headlen(skb));
__be16 protocol = vlan_get_protocol(skb);
u8 hdr_len = 0;
/* need: 1 descriptor per page * PAGE_SIZE/IGB_MAX_DATA_PER_TXD,
* + 1 desc for skb_headlen/IGB_MAX_DATA_PER_TXD,
* + 2 desc gap to keep tail from touching head,
* + 1 desc for context descriptor,
* otherwise try next time
*/
if (NETDEV_FRAG_PAGE_MAX_SIZE &amp;gt; IGB_MAX_DATA_PER_TXD) {
unsigned short f;
for (f = 0; f &amp;lt; skb_shinfo(skb)-&amp;gt;nr_frags; f++)
count += TXD_USE_COUNT(skb_shinfo(skb)-&amp;gt;frags[f].size);
} else {
count += skb_shinfo(skb)-&amp;gt;nr_frags;
}
&lt;/code>&lt;/pre>
&lt;p>函数首先使用 &lt;code>TXD_USER_COUNT&lt;/code> 宏来计算发送 skb 所需的描述符数量，用 &lt;code>count&lt;/code> 变量表示。然后根据分片情况，对 &lt;code>count&lt;/code> 进行相应调整。&lt;/p>
&lt;pre>&lt;code>if (igb_maybe_stop_tx(tx_ring, count + 3)) {
/* this is a hard error */
return NETDEV_TX_BUSY;
}
&lt;/code>&lt;/pre>
&lt;p>然后驱动程序调用内部函数 &lt;code>igb_maybe_stop_tx&lt;/code>，检查 TX Queue 以确保有足够可用的描 述符。如果没有，则返回 &lt;code>NETDEV_TX_BUSY&lt;/code>。正如我们之前在 qdisc 代码中看到的那样，这 将导致 qdisc 将 skb 重新入队以便稍后重试。&lt;/p>
&lt;pre>&lt;code>/* record the location of the first descriptor for this packet */
first = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[tx_ring-&amp;gt;next_to_use];
first-&amp;gt;skb = skb;
first-&amp;gt;bytecount = skb-&amp;gt;len;
first-&amp;gt;gso_segs = 1;
&lt;/code>&lt;/pre>
&lt;p>然后，获取 TX Queue 中下一个可用缓冲区信息，用 &lt;code>struct igb_tx_buffer *first&lt;/code> 表 示，这个信息稍后将用于设置缓冲区描述符。数据包 &lt;code>skb&lt;/code> 指针及其大小 &lt;code>skb-&amp;gt;len&lt;/code> 也存储到 &lt;code>first&lt;/code>。&lt;/p>
&lt;pre>&lt;code>skb_tx_timestamp(skb);
&lt;/code>&lt;/pre>
&lt;p>接下来代码调用 &lt;code>skb_tx_timestamp&lt;/code>，获取基于软件的发送时间戳。应用程序可以 使用发送时间戳来确定数据包通过网络栈的发送路径所花费的时间。
某些设备还支持硬件时间戳，这允许系统将打时间戳任务 offload 到设备。程序员因此可以 获得更准确的时间戳，因为它更接近于硬件实际发送的时间。
某些网络设备可以使用&lt;a href="https://events.linuxfoundation.org/sites/events/files/slides/lcjp14_ichikawa_0.pdf">Precision Time Protocol&lt;/a> （PTP，精确时间协议）在硬件中为数据包加时间戳。驱动程序处理用户的硬件时间戳请求。
我们现在看到这个代码：&lt;/p>
&lt;pre>&lt;code>if (unlikely(skb_shinfo(skb)-&amp;gt;tx_flags &amp;amp; SKBTX_HW_TSTAMP)) {
struct igb_adapter *adapter = netdev_priv(tx_ring-&amp;gt;netdev);
if (!(adapter-&amp;gt;ptp_tx_skb)) {
skb_shinfo(skb)-&amp;gt;tx_flags |= SKBTX_IN_PROGRESS;
tx_flags |= IGB_TX_FLAGS_TSTAMP;
adapter-&amp;gt;ptp_tx_skb = skb_get(skb);
adapter-&amp;gt;ptp_tx_start = jiffies;
if (adapter-&amp;gt;hw.mac.type == e1000_82576)
schedule_work(&amp;amp;adapter-&amp;gt;ptp_tx_work);
}
}
&lt;/code>&lt;/pre>
&lt;p>上面的 if 语句检查 &lt;code>SKBTX_HW_TSTAMP&lt;/code> 标志，该标志表示用户请求了硬件时间戳。接下来检 查是否设置了 &lt;code>ptp_tx_skb&lt;/code>。一次只能给一个数据包加时间戳，因此给正在打时间戳的 skb 上设置了 &lt;code>SKBTX_IN_PROGRESS&lt;/code> 标志。然后更新 &lt;code>tx_flags&lt;/code>，将 &lt;code>IGB_TX_FLAGS_TSTAMP&lt;/code> 标志 置位。&lt;code>tx_flags&lt;/code> 变量稍后将被复制到缓冲区信息结构中。
当前的 &lt;code>jiffies&lt;/code> 值赋给 &lt;code>ptp_tx_start&lt;/code>。驱动程序中的其他代码将使用这个值， 以确保 TX 硬件打时间戳不会 hang 住。最后，如果这是一个 82576 以太网硬件网卡，将用 &lt;code>schedule_work&lt;/code> 函数启动&lt;a href="http://www.makelinux.net/ldd3/chp-7-sect-6">工作队列&lt;/a>。&lt;/p>
&lt;pre>&lt;code>if (vlan_tx_tag_present(skb)) {
tx_flags |= IGB_TX_FLAGS_VLAN;
tx_flags |= (vlan_tx_tag_get(skb) &amp;lt;&amp;lt; IGB_TX_FLAGS_VLAN_SHIFT);
}
&lt;/code>&lt;/pre>
&lt;p>上面的代码将检查 skb 的 &lt;code>vlan_tci&lt;/code> 字段是否设置了，如果是，将设置 &lt;code>IGB_TX_FLAGS_VLAN&lt;/code> 标记，并保存 VLAN ID。&lt;/p>
&lt;pre>&lt;code>/* record initial flags and protocol */
first-&amp;gt;tx_flags = tx_flags;
first-&amp;gt;protocol = protocol;
&lt;/code>&lt;/pre>
&lt;p>最后将 &lt;code>tx_flags&lt;/code> 和 &lt;code>protocol&lt;/code> 值都保存到 &lt;code>first&lt;/code> 变量里面。&lt;/p>
&lt;pre>&lt;code>tso = igb_tso(tx_ring, first, &amp;amp;hdr_len);
if (tso &amp;lt; 0)
goto out_drop;
else if (!tso)
igb_tx_csum(tx_ring, first);
&lt;/code>&lt;/pre>
&lt;p>接下来，驱动程序调用其内部函数 &lt;code>igb_tso&lt;/code>，判断 skb 是否需要分片。如果需要 ，缓冲区信息变量（&lt;code>first&lt;/code>）将更新标志位，以提示硬件需要做 TSO。
如果不需要 TSO，则 &lt;code>igb_tso&lt;/code> 返回 0；否则返回 1。 如果返回 0，则将调用 &lt;code>igb_tx_csum&lt;/code> 来 处理校验和 offload 信息（是否需要 offload，是否支持此协议的 offload）。 &lt;code>igb_tx_csum&lt;/code> 函数将检查 skb 的属性，修改 &lt;code>first&lt;/code> 变量中的一些标志位，以表示需要校验 和 offload。&lt;/p>
&lt;pre>&lt;code>igb_tx_map(tx_ring, first, hdr_len);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>igb_tx_map&lt;/code> 函数准备给设备发送的数据。我们后面会仔细查看这个函数。&lt;/p>
&lt;pre>&lt;code>/* Make sure there is space in the ring for the next send. */
igb_maybe_stop_tx(tx_ring, DESC_NEEDED);
return NETDEV_TX_OK;
&lt;/code>&lt;/pre>
&lt;p>发送结束之后，驱动要检查确保有足够的描述符用于下一次发送。如果不够，TX Queue 将被 关闭。最后返回 &lt;code>NETDEV_TX_OK&lt;/code> 给上层（qdisc 代码）。&lt;/p>
&lt;pre>&lt;code>out_drop:
igb_unmap_and_free_tx_resource(tx_ring, first);
return NETDEV_TX_OK;
}
&lt;/code>&lt;/pre>
&lt;p>最后是一些错误处理代码，只有当 &lt;code>igb_tso&lt;/code> 遇到某种错误时才会触发此代码。 &lt;code>igb_unmap_and_free_tx_resource&lt;/code> 用于清理数据。在这种情况下也返回 &lt;code>NETDEV_TX_OK&lt;/code> 。发送没有成功，但驱动程序释放了相关资源，没有什么需要做的了。请注意，在这种情 况下，此驱动程序不会增加 drop 计数，但或许它应该增加。&lt;/p>
&lt;h2 id="93-igb_tx_map">9.3 &lt;code>igb_tx_map&lt;/code>&lt;/h2>
&lt;p>&lt;code>igb_tx_map&lt;/code> 函数处理将 skb 数据映射到 RAM 的 DMA 区域的细节。它还会更新设备 TX Queue 的 尾部指针，从而触发设备“被唤醒”，从 RAM 获取数据并开始发送。
让我们简单地看一下这个&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L4501-L4627">函数  &lt;/a>的工作原理：&lt;/p>
&lt;pre>&lt;code>static void igb_tx_map(struct igb_ring *tx_ring,
struct igb_tx_buffer *first,
const u8 hdr_len)
{
struct sk_buff *skb = first-&amp;gt;skb;
/* ... other variables ... */
u32 tx_flags = first-&amp;gt;tx_flags;
u32 cmd_type = igb_tx_cmd_type(skb, tx_flags);
u16 i = tx_ring-&amp;gt;next_to_use;
tx_desc = IGB_TX_DESC(tx_ring, i);
igb_tx_olinfo_status(tx_ring, tx_desc, tx_flags, skb-&amp;gt;len - hdr_len);
size = skb_headlen(skb);
data_len = skb-&amp;gt;data_len;
dma = dma_map_single(tx_ring-&amp;gt;dev, skb-&amp;gt;data, size, DMA_TO_DEVICE);
&lt;/code>&lt;/pre>
&lt;p>上面的代码所做的一些事情：&lt;/p>
&lt;ol>
&lt;li>声明变量并初始化&lt;/li>
&lt;li>使用 &lt;code>IGB_TX_DESC&lt;/code> 获取下一个可用描述符的指针&lt;/li>
&lt;li>&lt;code>igb_tx_olinfo_status&lt;/code> 函数更新 &lt;code>tx_flags&lt;/code>，并将它们复制到描述符（&lt;code>tx_desc&lt;/code>）中&lt;/li>
&lt;li>计算 skb 头长度和数据长度&lt;/li>
&lt;li>调用 &lt;code>dma_map_single&lt;/code> 为 &lt;code>skb-&amp;gt;data&lt;/code> 构造内存映射，以允许设备通过 DMA 从 RAM 中读取数据&lt;/li>
&lt;/ol>
&lt;p>接下来是驱动程序中的一个&lt;strong>非常长的循环，用于为 skb 的每个分片生成有效映射&lt;/strong>。具体如何 做的细节并不是特别重要，但如下步骤值得一提：&lt;/p>
&lt;ul>
&lt;li>驱动程序遍历该数据包的所有分片&lt;/li>
&lt;li>当前描述符有其数据的 DMA 地址信息&lt;/li>
&lt;li>如果分片的大小大于单个 IGB 描述符可以发送的大小，则构造多个描述符指向可 DMA 区域的块，直到描述符指向整个分片&lt;/li>
&lt;li>更新描述符迭代器&lt;/li>
&lt;li>更新剩余长度&lt;/li>
&lt;li>当没有剩余分片或者已经消耗了整个数据长度时，循环终止&lt;/li>
&lt;/ul>
&lt;p>下面提供循环的代码以供以上描述参考。这里的代码进一步向读者说明，&lt;strong>如果可能的话，避 免分片是一个好主意&lt;/strong>。分片需要大量额外的代码来处理网络栈的每一层，包括驱动层。&lt;/p>
&lt;pre>&lt;code>tx_buffer = first;
for (frag = &amp;amp;skb_shinfo(skb)-&amp;gt;frags[0];; frag++) {
if (dma_mapping_error(tx_ring-&amp;gt;dev, dma))
goto dma_error;
/* record length, and DMA address */
dma_unmap_len_set(tx_buffer, len, size);
dma_unmap_addr_set(tx_buffer, dma, dma);
tx_desc-&amp;gt;read.buffer_addr = cpu_to_le64(dma);
while (unlikely(size &amp;gt; IGB_MAX_DATA_PER_TXD)) {
tx_desc-&amp;gt;read.cmd_type_len =
cpu_to_le32(cmd_type ^ IGB_MAX_DATA_PER_TXD);
i++;
tx_desc++;
if (i == tx_ring-&amp;gt;count) {
tx_desc = IGB_TX_DESC(tx_ring, 0);
i = 0;
}
tx_desc-&amp;gt;read.olinfo_status = 0;
dma += IGB_MAX_DATA_PER_TXD;
size -= IGB_MAX_DATA_PER_TXD;
tx_desc-&amp;gt;read.buffer_addr = cpu_to_le64(dma);
}
if (likely(!data_len))
break;
tx_desc-&amp;gt;read.cmd_type_len = cpu_to_le32(cmd_type ^ size);
i++;
tx_desc++;
if (i == tx_ring-&amp;gt;count) {
tx_desc = IGB_TX_DESC(tx_ring, 0);
i = 0;
}
tx_desc-&amp;gt;read.olinfo_status = 0;
size = skb_frag_size(frag);
data_len -= size;
dma = skb_frag_dma_map(tx_ring-&amp;gt;dev, frag, 0,
size, DMA_TO_DEVICE);
tx_buffer = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[i];
}
&lt;/code>&lt;/pre>
&lt;p>所有需要的描述符都已建好，且 &lt;code>skb&lt;/code> 的所有数据都映射到 DMA 地址后，驱动就会 进入到它的最后一步，触发一次发送：&lt;/p>
&lt;pre>&lt;code>/* write last descriptor with RS and EOP bits */
cmd_type |= size | IGB_TXD_DCMD;
tx_desc-&amp;gt;read.cmd_type_len = cpu_to_le32(cmd_type);
&lt;/code>&lt;/pre>
&lt;p>对最后一个描述符设置 &lt;code>RS&lt;/code> 和 &lt;code>EOP&lt;/code> 位，以提示设备这是最后一个描述符了。&lt;/p>
&lt;pre>&lt;code>netdev_tx_sent_queue(txring_txq(tx_ring), first-&amp;gt;bytecount);
/* set the timestamp */
first-&amp;gt;time_stamp = jiffies;
&lt;/code>&lt;/pre>
&lt;p>调用 &lt;code>netdev_tx_sent_queue&lt;/code> 函数，同时带着将发送的字节数作为参数。这个函数是 byte query limit（字节查询限制）功能的一部分，我们将在稍后详细介绍。当前的 jiffies 存 储到 &lt;code>first&lt;/code> 的时间戳字段。
接下来，有点 tricky：&lt;/p>
&lt;pre>&lt;code>/* Force memory writes to complete before letting h/w know there
* are new descriptors to fetch. (Only applicable for weak-ordered
* memory model archs, such as IA-64).
*
* We also need this memory barrier to make certain all of the
* status bits have been updated before next_to_watch is written.
*/
wmb();
/* set next_to_watch value indicating a packet is present */
first-&amp;gt;next_to_watch = tx_desc;
i++;
if (i == tx_ring-&amp;gt;count)
i = 0;
tx_ring-&amp;gt;next_to_use = i;
writel(i, tx_ring-&amp;gt;tail);
/* we need this if more than one processor can write to our tail
* at a time, it synchronizes IO on IA64/Altix systems
*/
mmiowb();
return;
&lt;/code>&lt;/pre>
&lt;p>上面的代码做了一些重要的事情：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>wmb&lt;/code> 函数强制完成内存写入。这通常称作**“写屏障”**（write barrier） ，是通过 CPU 平台相关的特殊指令完成的。这对某些 CPU 架构非常重要，因为如果触发 设备启动 DMA 时不能确保所有内存写入已经完成，那设备可能从 RAM 中读取不一致 状态的数据。&lt;a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">这篇文章&lt;/a>和&lt;a href="http://www.cs.utexas.edu/~pingali/CS378/2012fa/lectures/consistency.pdf">这个课程&lt;/a>深 入探讨了内存顺序的细节&lt;/li>
&lt;li>设置 &lt;code>next_to_watch&lt;/code> 字段，它将在 completion 阶段后期使用&lt;/li>
&lt;li>更新计数，并且 TX Queue 的 &lt;code>next_to_use&lt;/code> 字段设置为下一个可用的描述符。使用 &lt;code>writel&lt;/code> 函数更新 TX Queue 的尾部。&lt;code>writel&lt;/code> 向&lt;a href="https://en.wikipedia.org/wiki/Memory-mapped_I/O">内存映射 I/O&lt;/a>地址写入一个 &lt;code>long&lt;/code> 型数据 ，这里地址是 &lt;code>tx_ring-&amp;gt;tail&lt;/code>（一个硬件地址），要写入的值是 &lt;code>i&lt;/code>。这次写操作会让 设备知道其他数据已经准备好，可以通过 DMA 从 RAM 中读取并写入网络&lt;/li>
&lt;li>最后，调用 &lt;code>mmiowb&lt;/code> 函数。它执行特定于 CPU 体系结构的指令，对内存映射的 写操作进行排序。它也是一个写屏障，用于内存映射的 I/O 写&lt;/li>
&lt;/ol>
&lt;p>想了解更多关于 &lt;code>wmb&lt;/code>，&lt;code>mmiowb&lt;/code> 以及何时使用它们的信息，可以阅读 Linux 内核中一些包含 内存屏障的优秀&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/memory-barriers.txt">文档&lt;/a> 。
最后，代码包含了一些错误处理。只有 DMA API（将 skb 数据地址映射到 DMA 地址）返回错误 时，才会执行此代码。&lt;/p>
&lt;pre>&lt;code>dma_error:
dev_err(tx_ring-&amp;gt;dev, &amp;quot;TX DMA map failed\n&amp;quot;);
/* clear dma mappings for failed tx_buffer_info map */
for (;;) {
tx_buffer = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[i];
igb_unmap_and_free_tx_resource(tx_ring, tx_buffer);
if (tx_buffer == first)
break;
if (i == 0)
i = tx_ring-&amp;gt;count;
i--;
}
tx_ring-&amp;gt;next_to_use = i;
&lt;/code>&lt;/pre>
&lt;p>在继续跟进“发送完成”（transmit completion）过程之前，让我们来看下之前跳过了的一 个东西：dynamic queue limits（动态队列限制）。&lt;/p>
&lt;h3 id="dynamic-queue-limits-dql">Dynamic Queue Limits (DQL)&lt;/h3>
&lt;p>正如在本文中看到的，&lt;strong>数据在逐步接近网络设备的过程中，花费了大量时间在 不同阶段的 Queue 里面&lt;/strong>。队列越大，在队列中所花费的时间就越多。
解决这个问题的一种方式是&lt;strong>背压&lt;/strong>（back pressure）。动态队列限制（DQL）系统是一种 机制，驱动程序可以使用该机制向网络系统（network system）施加反压，以避免设备 无法发送时有过多的数据积压在队列。
要使用 DQL，驱动需要在其发送和完成例程（transmit and completion routines）中调用 几次简单的 API。DQL 内部算法判断何时数据已足够多，达到此阈值后，DQL 将暂时禁用 TX Queue，从而对网络系统产生背压。当足够的数据已发送完后，DQL 再自动重新启用 该队列。
&lt;a href="https://www.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf">这里&lt;/a> 给出了 DQL 的一些性能数据及 DQL 内部算法的说明。
我们刚刚看到的 &lt;code>netdev_tx_sent_queue&lt;/code> 函数就是 DQL API 一部分。当数据排 队到设备进行发送时，将调用此函数。发送完成后，驱动程序调用 &lt;code>netdev_tx_completed_queue&lt;/code>。在内部，这两个函数都将调用 DQL 库（在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/lib/dynamic_queue_limits.c">lib/dynamic_queue_limits.c&lt;/a> 和 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/dynamic_queue_limits.h">include/linux/dynamic_queue_limits.h&lt;/a> ），以判断是否禁用、重新启用 DQL，或保持配置不动。
DQL 在 sysfs 中导出了一些统计信息和调优参数。调整 DQL 不是必需的；算法自己会随着时间 变化调整其参数。尽管如此，为了完整性，我们稍后会看到如何监控和调整 DQL。&lt;/p>
&lt;h2 id="94-发送完成transmit-completions">9.4 发送完成（Transmit completions）&lt;/h2>
&lt;p>设备发送数据之后会产生一个中断，表示发送已完成。然后，设备驱动程序可以调度一些长 时间运行的工作，例如解除 DMA 映射、释放数据。这是如何工作的取决于不同设备。对于 &lt;code>igb&lt;/code> 驱动程序（及其关联设备），发送完成和数据包接收所触发的 IRQ 是相同的。这意味着 对于 &lt;code>igb&lt;/code> 驱动程序，&lt;code>NET_RX&lt;/code> 既用于处理发送完成，又用于处理数据包接收。
让我重申一遍，以强调这一点的重要性：&lt;strong>你的设备可能会发出与“接收到数据包时触发的中 断”相同的中断来表示“数据包发送已完成”&lt;/strong>。如果是这种情况，则 &lt;code>NET_RX&lt;/code> softirq 会被用于 处理&lt;strong>数据包接收&lt;/strong>和&lt;strong>发送完成&lt;/strong>两种情况。
由于两个操作共享相同的 IRQ，因此只能注册一个 IRQ 处理函数来处理这两种情况。 回忆以下收到网络数据时的流程：&lt;/p>
&lt;ol>
&lt;li>收到网络数据&lt;/li>
&lt;li>网络设备触发 IRQ&lt;/li>
&lt;li>驱动的 IRQ 处理程序执行，清除 IRQ 并运行 softIRQ（如果尚未运行）。这里触发的 softIRQ 是 &lt;code>NET_RX&lt;/code> 类型&lt;/li>
&lt;li>softIRQ 本质上作为单独的内核线程，执行 NAPI 轮询循环&lt;/li>
&lt;li>只要有足够的预算，NAPI 轮询循环就一直接收数据包&lt;/li>
&lt;li>每次处理数据包后，预算都会减少，直到没有更多数据包要处理、预算达到 0 或时间片已过期为止&lt;/li>
&lt;/ol>
&lt;p>在 igb（和 ixgbe）驱动中，上面的步骤 5 在处理接收数据之前会先处理发送完成（TX completion）。请记住，&lt;strong>根据驱动程序的实现，处理发送完成和接收数据的函数可能共享一 份处理预算&lt;/strong>。igb 和 ixgbe 驱动程序分别跟踪发送完成和接收数据包的预算，因此处理发送完 成不一定会消耗完 RX 预算。
也就是说，整个 NAPI 轮询循环在 hard code 时间片内运行。这意味着如果要处理大量的 TX 完成 ，TX 完成可能会比处理接收数据时占用更多的时间片。对于在高负载环境中运行网络硬 件的人来说，这可能是一个重要的考虑因素。
让我们看看 igb 驱动程序在实际是如何实现的。&lt;/p>
&lt;h3 id="941-transmit-completion-irq">9.4.1 Transmit completion IRQ&lt;/h3>
&lt;p>收包过程我们已经在&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">数据接收部分的博客&lt;/a> 中介绍过，这里不再赘述，只给出相应链接。
那么，让我们从头开始：&lt;/p>
&lt;ol>
&lt;li>网络设备&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#bringing-a-network-device-up">启用&lt;/a>（bring up）&lt;/li>
&lt;li>IRQ 处理函数完成&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#register-an-interrupt-handler">注册&lt;/a>&lt;/li>
&lt;li>用户程序将数据发送到 socket。数据穿过网络栈，最后被网络设备从内存中取出并发送&lt;/li>
&lt;li>设备完成数据发送并触发 IRQ 表示发送完成&lt;/li>
&lt;li>驱动程序的 IRQ 处理函数开始&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#interrupt-handler">处理中断&lt;/a>&lt;/li>
&lt;li>IRQ 处理程序调用 &lt;code>napi_schedule&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#napi-and-napischedule">NAPI 代码&lt;/a>触发 &lt;code>NET_RX&lt;/code> 类型 softirq&lt;/li>
&lt;li>&lt;code>NET_RX&lt;/code> 类型 sofitrq 的中断处理函数 &lt;code>net_rx_action&lt;/code>&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#network-data-processing-begins">开始执行&lt;/a>&lt;/li>
&lt;li>&lt;code>net_rx_action&lt;/code> 函数调用驱动程序注册的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#napi-poll-function-and-weight">NAPI 轮询函数&lt;/a>&lt;/li>
&lt;li>NAPI 轮询函数 &lt;code>igb_poll&lt;/code>&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#igbpoll">开始运行&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>poll 函数 &lt;code>igb_poll&lt;/code> 同时处理接收数据包和发送完成（transmit completion）逻辑。让我 们深入研究这个函数的代码，看看发生了什么。&lt;/p>
&lt;h3 id="942-igb_poll">9.4.2 &lt;code>igb_poll&lt;/code>&lt;/h3>
&lt;p>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/**
* igb_poll - NAPI Rx polling callback
* @napi: napi polling structure
* @budget: count of how many packets we should handle
**/
static int igb_poll(struct napi_struct *napi, int budget)
{
struct igb_q_vector *q_vector = container_of(napi,
struct igb_q_vector,
napi);
bool clean_complete = true;
#ifdef CONFIG_IGB_DCA
if (q_vector-&amp;gt;adapter-&amp;gt;flags &amp;amp; IGB_FLAG_DCA_ENABLED)
igb_update_dca(q_vector);
#endif
if (q_vector-&amp;gt;tx.ring)
clean_complete = igb_clean_tx_irq(q_vector);
if (q_vector-&amp;gt;rx.ring)
clean_complete &amp;amp;= igb_clean_rx_irq(q_vector, budget);
/* If all work not completed, return budget and keep polling */
if (!clean_complete)
return budget;
/* If not enough Rx work done, exit the polling mode */
napi_complete(napi);
igb_ring_irq_enable(q_vector);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>函数按顺序执行以下操作：&lt;/p>
&lt;ol>
&lt;li>如果在内核中启用了直接缓存访问（&lt;a href="https://lwn.net/Articles/247493/">DCA&lt;/a>）功能 ，则更新 CPU 缓存（预热，warm up），后续对 RX Ring Buffer 的访问将命中 CPU 缓存。可以在接 收数据博客的 Extras 部分中阅读&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#direct-cache-access-dca">有关 DCA 的更多信息&lt;/a>&lt;/li>
&lt;li>调用 &lt;code>igb_clean_tx_irq&lt;/code> 执行发送完成操作&lt;/li>
&lt;li>调用 &lt;code>igb_clean_rx_irq&lt;/code> 处理收到的数据包&lt;/li>
&lt;li>最后，检查 &lt;code>clean_complete&lt;/code> 变量，判断是否还有更多工作可以完成。如果是，则返 回预算。如果是这种情况，&lt;code>net_rx_action&lt;/code> 会将此 NAPI 实例移动到轮询列表的末尾， 以便稍后再次处理&lt;/li>
&lt;/ol>
&lt;p>要了解 &lt;code>igb_clean_rx_irq&lt;/code> 如何工作的，请阅读上一篇博客文章的&lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/#igbcleanrxirq">这一部分  &lt;/a>。
本文主要关注发送方面，因此我们将继续研究上面的 &lt;code>igb_clean_tx_irq&lt;/code> 如何工作。&lt;/p>
&lt;h3 id="943-igb_clean_tx_irq">9.4.3 &lt;code>igb_clean_tx_irq&lt;/code>&lt;/h3>
&lt;p>来看一下这个函数的实现， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L6020-L6189">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>。
这个函数有点长，分成几部分来看：&lt;/p>
&lt;pre>&lt;code>static bool igb_clean_tx_irq(struct igb_q_vector *q_vector)
{
struct igb_adapter *adapter = q_vector-&amp;gt;adapter;
struct igb_ring *tx_ring = q_vector-&amp;gt;tx.ring;
struct igb_tx_buffer *tx_buffer;
union e1000_adv_tx_desc *tx_desc;
unsigned int total_bytes = 0, total_packets = 0;
unsigned int budget = q_vector-&amp;gt;tx.work_limit;
unsigned int i = tx_ring-&amp;gt;next_to_clean;
if (test_bit(__IGB_DOWN, &amp;amp;adapter-&amp;gt;state))
return true;
&lt;/code>&lt;/pre>
&lt;p>该函数首先初始化一些变量，其中比较重要的是预算（变量 &lt;code>budget&lt;/code>） ，初始化为此队列的 &lt;code>tx.work_limit&lt;/code>。在 igb 驱动程序中，&lt;code>tx.work_limit&lt;/code> 初始化为 hard code 值 &lt;code>IGB_DEFAULT_TX_WORK&lt;/code>（128）。
值得注意的是，虽然我们现在看到的 TX 完成代码与 RX 处理在同一个 &lt;code>NET_RX&lt;/code> softirq 中运行 ，但 igb 驱动的 TX 和 RX 函数&lt;strong>不共享处理预算&lt;/strong>。由于整个轮询函数在同一时间片内运行，因此 每次 &lt;code>igb_poll&lt;/code> 运行不会出现 RX 或 TX 饥饿，只要调用 &lt;code>igb_poll&lt;/code>，两者都将被处理。
继续前进，代码检查网络设备是否已关闭。如果是，则返回 &lt;code>true&lt;/code> 并退出 &lt;code>igb_clean_tx_irq&lt;/code>。&lt;/p>
&lt;pre>&lt;code>tx_buffer = &amp;amp;tx_ring-&amp;gt;tx_buffer_info[i];
tx_desc = IGB_TX_DESC(tx_ring, i);
i -= tx_ring-&amp;gt;count;
&lt;/code>&lt;/pre>
&lt;p>接下来：&lt;/p>
&lt;ol>
&lt;li>&lt;code>tx_buffer&lt;/code> 变量初始化为 &lt;code>tx_ring-&amp;gt;next_to_clean&lt;/code>（其本身被初始化为 0）&lt;/li>
&lt;li>&lt;code>tx_desc&lt;/code> 变量初始化为相关描述符的指针&lt;/li>
&lt;li>计数器 &lt;code>i&lt;/code> 减去 TX Queue 的大小。可以调整此值（我们将在调优部分中看到），但初始化为 &lt;code>IGB_DEFAULT_TXD&lt;/code>（256）&lt;/li>
&lt;/ol>
&lt;p>接下来，循环开始。它包含一些有用的注释，用于解释每个步骤中发生的情况：&lt;/p>
&lt;pre>&lt;code>do {
union e1000_adv_tx_desc *eop_desc = tx_buffer-&amp;gt;next_to_watch;
/* if next_to_watch is not set then there is no work pending */
if (!eop_desc)
break;
/* prevent any other reads prior to eop_desc */
read_barrier_depends();
/* if DD is not set pending work has not been completed */
if (!(eop_desc-&amp;gt;wb.status &amp;amp; cpu_to_le32(E1000_TXD_STAT_DD)))
break;
/* clear next_to_watch to prevent false hangs */
tx_buffer-&amp;gt;next_to_watch = NULL;
/* update the statistics for this packet */
total_bytes += tx_buffer-&amp;gt;bytecount;
total_packets += tx_buffer-&amp;gt;gso_segs;
/* free the skb */
dev_kfree_skb_any(tx_buffer-&amp;gt;skb);
/* unmap skb header data */
dma_unmap_single(tx_ring-&amp;gt;dev,
dma_unmap_addr(tx_buffer, dma),
dma_unmap_len(tx_buffer, len),
DMA_TO_DEVICE);
/* clear tx_buffer data */
tx_buffer-&amp;gt;skb = NULL;
dma_unmap_len_set(tx_buffer, len, 0);
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>首先将 &lt;code>eop_desc&lt;/code>（eop = end of packet）设置为发送缓冲区 &lt;code>tx_buffer&lt;/code> 的 &lt;code>next_to_watch&lt;/code>，后者是在我们之前看到的发送代码中设置的&lt;/li>
&lt;li>如果 &lt;code>eop_desc&lt;/code> 为 &lt;code>NULL&lt;/code>，则表示没有待处理的工作&lt;/li>
&lt;li>接下来调用 &lt;code>read_barrier_depends&lt;/code> 函数，该函数执行此 CPU 体系结构相关的指令，通过屏障防止其他任何读操作&lt;/li>
&lt;li>接下来，检查描述符 &lt;code>eop_desc&lt;/code> 的状态位。如果 &lt;code>E1000_TXD_STAT_DD&lt;/code> 未设置，则表示发送尚未完成，因此跳出循环&lt;/li>
&lt;li>清除 &lt;code>tx_buffer-&amp;gt;next_to_watch&lt;/code>。驱动中的 watchdog 定时器将监视此字段以判断发送是否 hang 住。清除此字段将不会触发 watchdog&lt;/li>
&lt;li>统计发送的总字节数和包数，这些计数将被复制到驱动的相应计数中&lt;/li>
&lt;li>释放 skb&lt;/li>
&lt;li>调用 &lt;code>dma_unmap_single&lt;/code> 取消 skb 数据区映射&lt;/li>
&lt;li>&lt;code>tx_buffer-&amp;gt;skb&lt;/code> 设置为 &lt;code>NULL&lt;/code>，解除 &lt;code>tx_buffer&lt;/code> 映射&lt;/li>
&lt;/ol>
&lt;p>接下来，在上面的循环内部开始了另一个循环：&lt;/p>
&lt;pre>&lt;code>/* clear last DMA location and unmap remaining buffers */
while (tx_desc != eop_desc) {
tx_buffer++;
tx_desc++;
i++;
if (unlikely(!i)) {
i -= tx_ring-&amp;gt;count;
tx_buffer = tx_ring-&amp;gt;tx_buffer_info;
tx_desc = IGB_TX_DESC(tx_ring, 0);
}
/* unmap any remaining paged data */
if (dma_unmap_len(tx_buffer, len)) {
dma_unmap_page(tx_ring-&amp;gt;dev,
dma_unmap_addr(tx_buffer, dma),
dma_unmap_len(tx_buffer, len),
DMA_TO_DEVICE);
dma_unmap_len_set(tx_buffer, len, 0);
}
}
&lt;/code>&lt;/pre>
&lt;p>这个内层循环会遍历每个发送描述符，直到 &lt;code>tx_desc&lt;/code> 等于 &lt;code>eop_desc&lt;/code>，并会解除被其他描 述符引用的被 DMA 映射的数据。
外层循环继续：&lt;/p>
&lt;pre>&lt;code>/* move us one more past the eop_desc for start of next pkt */
tx_buffer++;
tx_desc++;
i++;
if (unlikely(!i)) {
i -= tx_ring-&amp;gt;count;
tx_buffer = tx_ring-&amp;gt;tx_buffer_info;
tx_desc = IGB_TX_DESC(tx_ring, 0);
}
/* issue prefetch for next Tx descriptor */
prefetch(tx_desc);
/* update budget accounting */
budget--;
} while (likely(budget));
&lt;/code>&lt;/pre>
&lt;p>外层循环递增迭代器，更新 budget，然后检查是否要进入下一次循环。&lt;/p>
&lt;pre>&lt;code>netdev_tx_completed_queue(txring_txq(tx_ring),
total_packets, total_bytes);
i += tx_ring-&amp;gt;count;
tx_ring-&amp;gt;next_to_clean = i;
u64_stats_update_begin(&amp;amp;tx_ring-&amp;gt;tx_syncp);
tx_ring-&amp;gt;tx_stats.bytes += total_bytes;
tx_ring-&amp;gt;tx_stats.packets += total_packets;
u64_stats_update_end(&amp;amp;tx_ring-&amp;gt;tx_syncp);
q_vector-&amp;gt;tx.total_bytes += total_bytes;
q_vector-&amp;gt;tx.total_packets += total_packets;
&lt;/code>&lt;/pre>
&lt;p>这段代码：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>netdev_tx_completed_queue&lt;/code>，它是上面解释的 DQL API 的一部分。如果处理了足够的发送完成，这可能会重新启用 TX Queue&lt;/li>
&lt;li>更新各处的统计信息，以便用户可以访问它们，我们稍后会看到&lt;/li>
&lt;/ol>
&lt;p>代码继续，首先检查是否设置了 &lt;code>IGB_RING_FLAG_TX_DETECT_HANG&lt;/code> 标志。每次运行定时器 回调函数时，watchdog 定时器都会设置此标志，以强制定期检查 TX Queue。如果该标志被设 置了，则代码将检查 TX Queue 是否 hang 住：&lt;/p>
&lt;pre>&lt;code>if (test_bit(IGB_RING_FLAG_TX_DETECT_HANG, &amp;amp;tx_ring-&amp;gt;flags)) {
struct e1000_hw *hw = &amp;amp;adapter-&amp;gt;hw;
/* Detect a transmit hang in hardware, this serializes the
* check with the clearing of time_stamp and movement of i
*/
clear_bit(IGB_RING_FLAG_TX_DETECT_HANG, &amp;amp;tx_ring-&amp;gt;flags);
if (tx_buffer-&amp;gt;next_to_watch &amp;amp;&amp;amp;
time_after(jiffies, tx_buffer-&amp;gt;time_stamp +
(adapter-&amp;gt;tx_timeout_factor * HZ)) &amp;amp;&amp;amp;
!(rd32(E1000_STATUS) &amp;amp; E1000_STATUS_TXOFF)) {
/* detected Tx unit hang */
dev_err(tx_ring-&amp;gt;dev,
&amp;quot;Detected Tx Unit Hang\n&amp;quot;
&amp;quot; Tx Queue &amp;lt;%d&amp;gt;\n&amp;quot;
&amp;quot; TDH &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot; TDT &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot; next_to_use &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot; next_to_clean &amp;lt;%x&amp;gt;\n&amp;quot;
&amp;quot;buffer_info[next_to_clean]\n&amp;quot;
&amp;quot; time_stamp &amp;lt;%lx&amp;gt;\n&amp;quot;
&amp;quot; next_to_watch &amp;lt;%p&amp;gt;\n&amp;quot;
&amp;quot; jiffies &amp;lt;%lx&amp;gt;\n&amp;quot;
&amp;quot; desc.status &amp;lt;%x&amp;gt;\n&amp;quot;,
tx_ring-&amp;gt;queue_index,
rd32(E1000_TDH(tx_ring-&amp;gt;reg_idx)),
readl(tx_ring-&amp;gt;tail),
tx_ring-&amp;gt;next_to_use,
tx_ring-&amp;gt;next_to_clean,
tx_buffer-&amp;gt;time_stamp,
tx_buffer-&amp;gt;next_to_watch,
jiffies,
tx_buffer-&amp;gt;next_to_watch-&amp;gt;wb.status);
netif_stop_subqueue(tx_ring-&amp;gt;netdev,
tx_ring-&amp;gt;queue_index);
/* we are about to reset, no point in enabling stuff */
return true;
}
&lt;/code>&lt;/pre>
&lt;p>上面的 if 语句检查：&lt;/p>
&lt;ol>
&lt;li>&lt;code>tx_buffer-&amp;gt;next_to_watch&lt;/code> 已设置，并且&lt;/li>
&lt;li>当前 jiffies 大于 &lt;code>tx_buffer&lt;/code> 发送路径上记录的 &lt;code>time_stamp&lt;/code> 加上超时因子，并且&lt;/li>
&lt;li>设备的发送状态寄存器未设置 &lt;code>E1000_STATUS_TXOFF&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>如果这三个条件都为真，则会打印一个错误，表明已检测到挂起。&lt;code>netif_stop_subqueue&lt;/code> 用于关闭队列，最后函数返回 true。
让我们继续阅读代码，看看如果没有发送挂起检查会发生什么，或者如果有，但没有检测到 挂起：&lt;/p>
&lt;pre>&lt;code>#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)
if (unlikely(total_packets &amp;amp;&amp;amp;
netif_carrier_ok(tx_ring-&amp;gt;netdev) &amp;amp;&amp;amp;
igb_desc_unused(tx_ring) &amp;gt;= TX_WAKE_THRESHOLD)) {
/* Make sure that anybody stopping the queue after this
* sees the new next_to_clean.
*/
smp_mb();
if (__netif_subqueue_stopped(tx_ring-&amp;gt;netdev,
tx_ring-&amp;gt;queue_index) &amp;amp;&amp;amp;
!(test_bit(__IGB_DOWN, &amp;amp;adapter-&amp;gt;state))) {
netif_wake_subqueue(tx_ring-&amp;gt;netdev,
tx_ring-&amp;gt;queue_index);
u64_stats_update_begin(&amp;amp;tx_ring-&amp;gt;tx_syncp);
tx_ring-&amp;gt;tx_stats.restart_queue++;
u64_stats_update_end(&amp;amp;tx_ring-&amp;gt;tx_syncp);
}
}
return !!budget;
&lt;/code>&lt;/pre>
&lt;p>在上面的代码中，如果先前已禁用，则驱动程序将重新启动 TX Queue。 它首先检查：&lt;/p>
&lt;ol>
&lt;li>是否有数据包处理完成（&lt;code>total_packets&lt;/code> 非零）&lt;/li>
&lt;li>调用 &lt;code>netif_carrier_ok&lt;/code>，确保设备没有被关闭&lt;/li>
&lt;li>TX Queue 中未使用的描述符数量大于等于 &lt;code>TX_WAKE_THRESHOLD&lt;/code>（我的 x86_64 系统上此阈值为 42）&lt;/li>
&lt;/ol>
&lt;p>如果满足以上所有条件，则执行&lt;strong>写屏障&lt;/strong>（&lt;code>smp_mb&lt;/code>）。
接下来检查另一组条件。如果：&lt;/p>
&lt;ol>
&lt;li>队列停止了&lt;/li>
&lt;li>设备未关闭&lt;/li>
&lt;/ol>
&lt;p>则调用 &lt;code>netif_wake_subqueue&lt;/code> 唤醒 TX Queue，并向更高层发信号通知它们可能需要将数据 再次入队。&lt;code>restart_queue&lt;/code> 统计计数器递增。我们接下来会看到如何阅读这个值。
最后，返回一个布尔值。如果有任何剩余的未使用预算，则返回 true，否则为 false。在 &lt;code>igb_poll&lt;/code> 中检查此值以确定返回 &lt;code>net_rx_action&lt;/code> 的内容。&lt;/p>
&lt;h3 id="944-igb_poll-返回值">9.4.4 &lt;code>igb_poll&lt;/code> 返回值&lt;/h3>
&lt;p>&lt;code>igb_poll&lt;/code> 函数通过以下逻辑决定返回什么值给 &lt;code>net_rx_action&lt;/code>：&lt;/p>
&lt;pre>&lt;code>if (q_vector-&amp;gt;tx.ring)
clean_complete = igb_clean_tx_irq(q_vector);
if (q_vector-&amp;gt;rx.ring)
clean_complete &amp;amp;= igb_clean_rx_irq(q_vector, budget);
/* If all work not completed, return budget and keep polling */
if (!clean_complete)
return budget;
&lt;/code>&lt;/pre>
&lt;p>换句话说，如果：&lt;/p>
&lt;ol>
&lt;li>&lt;code>igb_clean_tx_irq&lt;/code> 清除了所有&lt;strong>待发送&lt;/strong>数据包，且未用完其 TX 预算（transmit completion budget），并且&lt;/li>
&lt;li>&lt;code>igb_clean_rx_irq&lt;/code> 清除了所有&lt;strong>接收到的&lt;/strong>数据包，且未用完其 RX 预算（packet processing budget）&lt;/li>
&lt;/ol>
&lt;p>那么，最后将返回整个预算值（包括 igb 在内的大多数驱动程序 hard code 为 64）；否则，如果 RX 或 TX 处理中的任何用完了其 budget（因为还有更多工作要做），则调用 &lt;code>napi_complete&lt;/code> 禁用 NAPI 并返回 0：&lt;/p>
&lt;pre>&lt;code>/* If not enough Rx work done, exit the polling mode */
napi_complete(napi);
igb_ring_irq_enable(q_vector);
return 0;
}
&lt;/code>&lt;/pre>
&lt;h2 id="95-监控网络设备">9.5 监控网络设备&lt;/h2>
&lt;p>监控网络设备有多种方式，每种方式提供的监控粒度和复杂度各不相同。我们先从最粗 大粒度开始，然后逐步到最细的粒度。&lt;/p>
&lt;h3 id="951-使用-ethtool--s-命令">9.5.1 使用 &lt;code>ethtool -S&lt;/code> 命令&lt;/h3>
&lt;p>Ubuntu 安装 ethtool：&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get install ethtool.
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ethtool -S &amp;lt;NIC&amp;gt;&lt;/code>可以打印设备的收发统计信息（例如，发送错误）：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -S eth0
NIC statistics:
rx_packets: 597028087
tx_packets: 5924278060
rx_bytes: 112643393747
tx_bytes: 990080156714
rx_broadcast: 96
tx_broadcast: 116
rx_multicast: 20294528
....
&lt;/code>&lt;/pre>
&lt;p>监控这个数据不是太容易，因为并无统一的标准规定&lt;code>-S&lt;/code> 应该打印出哪些字段。不同的设备 ，甚至是相同设备的不同版本，都可能打印出名字不同但意思相同的字段。
你首先需要检查里面的“drop”、“buffer”、“miss”、“errors”等字段，然后查看驱动程序的 代码，以确定哪些计数是在软件里更新的（例如，内存不足时更新），哪些是直接来自硬件 寄存器更新的。如果是硬件寄存器值，那你需要查看网卡的 data sheet，确定这个计数真正 表示什么，因为 ethtool 给出的很多字段都是有误导性的（misleading）。&lt;/p>
&lt;h3 id="952-使用-sysfs">9.5.2 使用 &lt;code>sysfs&lt;/code>&lt;/h3>
&lt;p>sysfs 也提供了很多统计值，但比网卡层的统计更上层一些。
例如，你可以通过 &lt;code>cat &amp;lt;file&amp;gt;&lt;/code>的方式查看 eth0 接收的丢包数。
示例：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/net/eth0/statistics/tx_aborted_errors
2
&lt;/code>&lt;/pre>
&lt;p>每个 counter 对应一个文件，包括 &lt;code>tx_aborted_errors&lt;/code>, &lt;code>tx_carrier_errors&lt;/code>, &lt;code>tx_compressed&lt;/code>, &lt;code>tx_dropped&lt;/code>,等等。
**不幸的是，每个值代表什么是由驱动决定的，因此，什么时候更新它们，在什么条件下更新 ，都是驱动决定的。**例如，你可能已经注意到，对于同一种错误，有的驱动将其视为 drop ，而有的驱动将其视为 miss。
如果这些值对你非常重要，那你必须阅读驱动代码和网卡 data sheet，以确定每个值真正代 表什么。&lt;/p>
&lt;h3 id="953-使用procnetdev">9.5.3 使用&lt;code>/proc/net/dev&lt;/code>&lt;/h3>
&lt;p>&lt;code>/proc/net/dev&lt;/code> 提供了更高一层的统计，它给系统中的每个网络设备一个统计摘要。&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/dev
Inter-| Receive | Transmit
face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed
eth0: 110346752214 597737500 0 2 0 0 0 20963860 990024805984 6066582604 0 0 0 0 0 0
lo: 428349463836 1579868535 0 0 0 0 0 0 428349463836 1579868535 0 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>这里打印出来的字段是上面 sysfs 里字段的一个子集，可以作为通用 general reference。
上面的建议在这里同样适用，即： 如果这些值对你非常重要，那你必须阅读驱动代码和网卡 data sheet，以确定每个值真正代 表什么。&lt;/p>
&lt;h2 id="96-监控-dql">9.6 监控 DQL&lt;/h2>
&lt;p>可以通过&lt;code>/sys/class/net/&amp;lt;NIC&amp;gt;/queues/tx-&amp;lt;QUEUE_ID&amp;gt;/byte_queue_limits/&lt;/code> 监控网络设备的动态队列限制（DQL）信息。&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/net/eth0/queues/tx-0/byte_queue_limits/inflight
350
&lt;/code>&lt;/pre>
&lt;p>文件包括：&lt;/p>
&lt;ol>
&lt;li>&lt;code>hold_time&lt;/code>: Initialized to HZ (a single hertz). If the queue has been full for hold_time, then the maximum size is decreased.&lt;/li>
&lt;li>&lt;code>inflight&lt;/code>: This value is equal to (number of packets queued - number of packets completed). It is the current number of packets being transmit for which a completion has not been processed.&lt;/li>
&lt;li>&lt;code>limit_max&lt;/code>: A hardcoded value, set to DQL_MAX_LIMIT (1879048192 on my x86_64 system).&lt;/li>
&lt;li>&lt;code>limit_min&lt;/code>: A hardcoded value, set to 0.&lt;/li>
&lt;li>&lt;code>limit&lt;/code>: A value between limit_min and limit_max which represents the current maximum number of objects which can be queued.&lt;/li>
&lt;/ol>
&lt;p>在修改这些值之前，强烈建议先阅读&lt;a href="https://www.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf">这些资料  &lt;/a>，以更深入地了解其算法。&lt;/p>
&lt;h2 id="97-调优网络设备">9.7 调优网络设备&lt;/h2>
&lt;h3 id="971-查询-tx-queue-数量">9.7.1 查询 TX Queue 数量&lt;/h3>
&lt;p>如果网络及其驱动支持多 TX Queue，那可以用 ethtool 调整 TX queue（也叫 TX channel）的数量。
查看网卡 TX Queue 数量：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Pre-set maximums:
RX: 0
TX: 0
Other: 0
Combined: 8
Current hardware settings:
RX: 0
TX: 0
Other: 0
Combined: 4
&lt;/code>&lt;/pre>
&lt;p>这里显示了（由驱动和硬件）预设的最大值，以及当前值。
注意：不是所有设备驱动都支持这个选项。
如果你的网卡不支持，会遇到以下错误：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Cannot get device channel parameters
: Operation not supported
&lt;/code>&lt;/pre>
&lt;p>这表示设备驱动没有实现 ethtool 的 &lt;code>get_channels&lt;/code> 方法，这可能是由于网卡不支持调整 queue 数量，不支持多 TX Queue，或者驱动版本太旧导致不支持此操作。&lt;/p>
&lt;h3 id="972-调整-tx-queue-数量">9.7.2 调整 TX queue 数量&lt;/h3>
&lt;p>&lt;code>ethtool -L&lt;/code> 可以修改 TX Queue 数量。
注意：一些设备及其驱动只支持 combined queue，这种情况下一个 TX queue 和和一个 RX queue 绑定到一起的。前面的例子中我们已经看到了。
例子：设置收发队列数量为 8：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 combined 8
&lt;/code>&lt;/pre>
&lt;p>如果你的设备和驱动支持分别设置 TX queue 和 RX queue 的数量，那你可以分别设置。&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 tx 8
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，调整以上设置会导致网卡先 down 再 up，经过这个网卡的连接会断掉 。如果只是一次性改动，那这可能不是太大问题。&lt;/p>
&lt;h3 id="973-调整-tx-queue-大小">9.7.3 调整 TX queue 大小&lt;/h3>
&lt;p>一些设备及其驱动支持修改 TX queue 大小，这是如何实现的取决于具体的硬件，但是， ethtool 提供了一个通用的接口可以调整这个大小。由于 DQL 在更高层面处理数据排队的问题 ，因此调整队列大小可能不会产生明显的影响。然而，你可能还是想要将 TX queue 调到最大 ，然后再把剩下的事情交给 DQL：
&lt;code>ethtool -g&lt;/code> 查看队列当前的大小：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX: 4096
RX Mini: 0
RX Jumbo: 0
TX: 4096
Current hardware settings:
RX: 512
RX Mini: 0
RX Jumbo: 0
TX: 512
&lt;/code>&lt;/pre>
&lt;p>以上显示硬件支持最大 4096 个接收和发送描述符，但当前只使用了 512 个。
&lt;code>-G&lt;/code> 修改 queue 大小：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -G eth0 tx 4096
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，调整以上设置会导致网卡先 down 再 up，经过这个网卡的连接会断掉 。如果只是一次性改动，那这可能不是太大问题。&lt;/p>
&lt;h1 id="10-网络栈之旅结束">10 网络栈之旅：结束&lt;/h1>
&lt;p>至此，你已经知道关于 Linux 如何发送数据包的全部内容了：从用户程序直到驱动，以及反 方向。&lt;/p>
&lt;h1 id="11-extras">11 Extras&lt;/h1>
&lt;h2 id="111-减少-arp-流量-msg_confirm">11.1 减少 ARP 流量 (MSG_CONFIRM)&lt;/h2>
&lt;p>&lt;code>send&lt;/code>, &lt;code>sendto&lt;/code> 和 &lt;code>sendmsg&lt;/code> 系统调用都支持一个 &lt;code>flags&lt;/code> 参数。如果你调用的时候传递了 &lt;code>MSG_CONFIRM&lt;/code> flag，它会使内核里的 &lt;code>dst_neigh_output&lt;/code> 函数更新邻居（ARP）缓存的时 间戳。所导致的结果是，相应的邻居缓存不会被垃圾回收。这会减少发出的 ARP 请求的数量 。&lt;/p>
&lt;h2 id="112-udp-corking软木塞">11.2 UDP Corking（软木塞）&lt;/h2>
&lt;p>在查看 UDP 协议栈的时候我们深入地研究过了 UDP corking 这个选项。如果你想在应用中使用 这个选项，可以在调用 &lt;code>setsockopt&lt;/code> 设置 IPPROTO_UDP 类型 socket 的时候，将 UDP_CORK 标记 位置 1。&lt;/p>
&lt;h2 id="113-打时间戳">11.3 打时间戳&lt;/h2>
&lt;p>本文已经看到，网络栈可以收集发送包的时间戳信息。我们在文章中已经看到了软 件部分哪里可以设置时间戳；而一些网卡甚至还支持硬件时间戳。
如果你想看内核网络栈给收包增加了多少延迟，那这个特性非常有用。
内核&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/timestamping.txt">关于时间戳的文档&lt;/a> 非常优秀，甚至还包括一个&lt;a href="https://github.com/torvalds/linux/tree/v3.13/Documentation/networking/timestamping">示例程序和相应的 Makefile&lt;/a>，有兴趣的话可以上手试试。
使用 &lt;code>ethtool -T&lt;/code> 可以查看网卡和驱动支持哪种打时间戳方式：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -T eth0
Time stamping parameters for eth0:
Capabilities:
software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
PTP Hardware Clock: none
Hardware Transmit Timestamp Modes: none
Hardware Receive Filter Modes: none
&lt;/code>&lt;/pre>
&lt;p>从上面这个信息看，该网卡不支持硬件打时间戳。但这个系统上的软件打时间戳，仍然可以 帮助我判断内核在接收路径上到底带来多少延迟。&lt;/p>
&lt;h1 id="12-结论">12 结论&lt;/h1>
&lt;p>Linux 网络栈很复杂。
我们已经看到，即使是 &lt;code>NET_RX&lt;/code> 这样看起来极其简单的（名字），也不是按照我们（字面上 ）理解的方式在运行，虽然名字带 RX，但其实发送数据也在 &lt;code>NET_RX&lt;/code> 软中断处理函数中被处 理。
这揭示了我认为的问题的核心：&lt;strong>不深入阅读和理解网络栈，就不可能优化和监控它&lt;/strong>。 &lt;strong>你监控不了你没有深入理解的代码&lt;/strong>。&lt;/p>
&lt;h1 id="13-额外帮助">13 额外帮助&lt;/h1>
&lt;p>需要一些额外的关于网络栈的指导(navigating the network stack)？对本文有疑问，或有 相关内容本文没有提到？以上问题，都可以发邮件给&lt;a href="http://arthurchiao.art/blog/tuning-stack-tx-zh/support@packagecloud.io">我们&lt;/a>， 以便我们知道如何提供帮助。&lt;/p></description></item><item><title>Docs: 数据包接收过程详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E6%8E%A5%E6%94%B6%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/%E6%95%B0%E6%8D%AE%E5%8C%85%E6%8E%A5%E6%94%B6%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="译-linux-网络栈监控和调优接收数据2016">[译] Linux 网络栈监控和调优：接收数据（2016）&lt;/h1>
&lt;p>Published at 2018-12-05 | Last Update 2020-03-29&lt;/p>
&lt;h2 id="译者序">译者序&lt;/h2>
&lt;p>本文翻译自 2016 年的一篇英文博客 &lt;a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data&lt;/a>。&lt;strong>如果能看懂英文，建议阅读原文，或者和本文对照看。&lt;/strong>
这篇文章写的是 &lt;strong>“Linux networking stack”&lt;/strong>，这里的 ”stack“ 指的不仅仅是内核协议 栈，而是包括内核协议栈在内的、从数据包到达物理网卡到最终被用户态程序收起的整个路 径。所以文章有三方面，交织在一起，看起来非常累（但是很过瘾）：&lt;/p>
&lt;ol>
&lt;li>原理及代码实现：网络各层，包括驱动、硬中断、软中断、内核协议栈、socket 等等&lt;/li>
&lt;li>监控：对代码中的重要计数进行监控，一般在 &lt;code>/proc&lt;/code> 或 &lt;code>/sys&lt;/code> 下面有对应输出&lt;/li>
&lt;li>调优：修改网络配置参数&lt;/li>
&lt;/ol>
&lt;p>本文的另一个特色是，几乎所有讨论的内核代码，都在相应的地方给出了 github 上的链接 ，具体到行。
网络栈非常复杂，原文太长又没有任何章节号，看起来非常累。因此本文翻译时添加了适当 的章节号，以期按图索骥。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>2020 更新：&lt;/strong>
基于 Prometheus+Grafana 监控网络栈：&lt;a href="http://arthurchiao.art/blog/monitoring-network-stack/">Monitoring Network Stack&lt;/a>。
以下是翻译。&lt;/p>
&lt;hr>
&lt;h2 id="太长不读tl-dr">太长不读（TL; DR）&lt;/h2>
&lt;p>本文介绍了 Linux 内核是如何&lt;strong>收包&lt;/strong>（receive packets）的，包是怎样从网络栈到达用 户空间程序的，以及如何&lt;strong>监控&lt;/strong>（monitoring）和&lt;strong>调优&lt;/strong>（tuning）这一路径上的各个 网络栈组件。
这篇文章的姊妹篇 &lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">Monitoring and Tuning the Linux Networking Stack: Sending Data&lt;/a>。
这篇文章的图文注释版 &lt;a href="https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/">the Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data&lt;/a>。
想对 Linux 网络栈进行监控或调优，必须对它的行为（what exactly is happening）和原 理有深入的理解，而这是离不开读内核源码的。希望本文可以给那些正准备投身于此的人提 供一份参考。
&lt;strong>特别鸣谢&lt;/strong>
特别感谢 &lt;a href="https://privateinternetaccess.com/">Private Internet Access&lt;/a> 的各位同 僚。公司雇佣我们做一些包括本文主题在内的网络研究，并非常慷慨地允许我们将研究成果 以文章的形式发表。
本文基于在 &lt;a href="https://privateinternetaccess.com/">Private Internet Access&lt;/a> 时的研 究成果，最开始以 &lt;a href="https://www.privateinternetaccess.com/blog/2016/01/linux-networking-stack-from-the-ground-up-part-1/">5 篇连载  &lt;/a>的形式出现。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%AF%91%E8%80%85%E5%BA%8F">译者序&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E5%A4%AA%E9%95%BF%E4%B8%8D%E8%AF%BBtl-dr">太长不读（TL; DR）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%9B%91%E6%8E%A7%E5%92%8C%E8%B0%83%E4%BC%98%E5%B8%B8%E8%A7%84%E5%BB%BA%E8%AE%AE">1 网络栈监控和调优：常规建议&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#2-%E6%94%B6%E5%8C%85%E8%BF%87%E7%A8%8B%E4%BF%AF%E7%9E%B0">2 收包过程俯瞰&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#3-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8">3 网络设备驱动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#31-%E5%88%9D%E5%A7%8B%E5%8C%96">3.1 初始化&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#pci-%E5%88%9D%E5%A7%8B%E5%8C%96">PCI 初始化&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#32-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%88%9D%E5%A7%8B%E5%8C%96">3.2 网络设备初始化&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E6%9B%B4%E5%A4%9A-pci-%E9%A9%B1%E5%8A%A8%E4%BF%A1%E6%81%AF">更多 PCI 驱动信息&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#33-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%90%AF%E5%8A%A8">3.3 网络设备启动&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#331-struct-net_device_ops">3.3.1 &lt;code>struct net_device_ops&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#332-ethtool-%E5%87%BD%E6%95%B0%E6%B3%A8%E5%86%8C">3.3.2 &lt;code>ethtool&lt;/code> 函数注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#333-%E8%BD%AF%E4%B8%AD%E6%96%ADirq">3.3.3 软中断（IRQ）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#334-napi">3.3.4 NAPI&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#napi">NAPI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#335-igb-%E9%A9%B1%E5%8A%A8%E7%9A%84-napi-%E5%88%9D%E5%A7%8B%E5%8C%96">3.3.5 &lt;code>igb&lt;/code> 驱动的 NAPI 初始化&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#34-%E5%90%AF%E7%94%A8%E7%BD%91%E5%8D%A1-bring-a-network-device-up">3.4 启用网卡 (Bring A Network Device Up)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#341-%E5%87%86%E5%A4%87%E4%BB%8E%E7%BD%91%E7%BB%9C%E6%8E%A5%E6%94%B6%E6%95%B0%E6%8D%AE">3.4.1 准备从网络接收数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#342-enable-napi">3.4.2 Enable NAPI&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#343-%E6%B3%A8%E5%86%8C%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0">3.4.3 注册中断处理函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#344-enable-interrupts">3.4.4 Enable Interrupts&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#35-%E7%BD%91%E5%8D%A1%E7%9B%91%E6%8E%A7">3.5 网卡监控&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#351-using-ethtool--s">3.5.1 Using &lt;code>ethtool -S&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#352-using-sysfs">3.5.2 Using &lt;code>sysfs&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#353-using-procnetdev">3.5.3 Using &lt;code>/proc/net/dev&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#36-%E7%BD%91%E5%8D%A1%E8%B0%83%E4%BC%98">3.6 网卡调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#361-%E6%9F%A5%E7%9C%8B-rx-%E9%98%9F%E5%88%97%E6%95%B0%E9%87%8F">3.6.1 查看 RX 队列数量&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#362-%E8%B0%83%E6%95%B4-rx-queues">3.6.2 调整 RX queues&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#363-%E8%B0%83%E6%95%B4-rx-queue-%E7%9A%84%E5%A4%A7%E5%B0%8F">3.6.3 调整 RX queue 的大小&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#364-%E8%B0%83%E6%95%B4-rx-queue-%E7%9A%84%E6%9D%83%E9%87%8Dweight">3.6.4 调整 RX queue 的权重（weight）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#365-%E8%B0%83%E6%95%B4-rx-%E5%93%88%E5%B8%8C%E5%AD%97%E6%AE%B5-for-network-flows">3.6.5 调整 RX 哈希字段 for network flows&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#366-ntuple-filtering-for-steering-network-flows">3.6.6 ntuple filtering for steering network flows&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#4-%E8%BD%AF%E4%B8%AD%E6%96%ADsoftirq">4 软中断（SoftIRQ）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#41-%E8%BD%AF%E4%B8%AD%E6%96%AD%E6%98%AF%E4%BB%80%E4%B9%88">4.1 软中断是什么&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#42-ksoftirqd">4.2 &lt;code>ksoftirqd&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#43-__do_softirq">4.3 &lt;code>__do_softirq&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#44-%E7%9B%91%E6%8E%A7">4.4 监控&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#5-linux-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%AD%90%E7%B3%BB%E7%BB%9F">5 Linux 网络设备子系统&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#51-%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%AD%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96">5.1 网络设备子系统的初始化&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#struct-softnet_data-%E5%8F%98%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96">&lt;code>struct softnet_data&lt;/code> 变量初始化&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#softirq-handler-%E5%88%9D%E5%A7%8B%E5%8C%96">SoftIRQ Handler 初始化&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#52-%E6%95%B0%E6%8D%AE%E6%9D%A5%E4%BA%86">5.2 数据来了&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#521-%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0">5.2.1 中断处理函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#522-napi-%E5%92%8C-napi_schedule">5.2.2 NAPI 和 &lt;code>napi_schedule&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#523-%E5%85%B3%E4%BA%8E-cpu-%E5%92%8C%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E4%B8%80%E7%82%B9%E7%AC%94%E8%AE%B0">5.2.3 关于 CPU 和网络数据处理的一点笔记&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#524-%E7%9B%91%E6%8E%A7%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%88%B0%E8%BE%BE">5.2.4 监控网络数据到达&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%A1%AC%E4%B8%AD%E6%96%AD%E8%AF%B7%E6%B1%82">硬中断请求&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#525-%E6%95%B0%E6%8D%AE%E6%8E%A5%E6%94%B6%E8%B0%83%E4%BC%98">5.2.5 数据接收调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E4%B8%AD%E6%96%AD%E5%90%88%E5%B9%B6interrupt-coalescing">中断合并（Interrupt coalescing）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E6%95%B4%E7%A1%AC%E4%B8%AD%E6%96%AD%E4%BA%B2%E5%92%8C%E6%80%A7irq-affinities">调整硬中断亲和性（IRQ affinities）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#53-%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%BC%80%E5%A7%8B">5.3 网络数据处理：开始&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#531-net_rx_action-%E5%A4%84%E7%90%86%E5%BE%AA%E7%8E%AF">5.3.1 &lt;code>net_rx_action&lt;/code> 处理循环&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#532-napi-poll-%E5%87%BD%E6%95%B0%E5%8F%8A%E6%9D%83%E9%87%8D">5.3.2 NAPI poll 函数及权重&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#533-napi-%E5%92%8C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%90%88%E7%BA%A6contract">5.3.3 NAPI 和设备驱动的合约（contract）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#534-finishing-the-net_rx_action-loop">5.3.4 Finishing the &lt;code>net_rx_action&lt;/code> loop&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#535-%E5%88%B0%E8%BE%BE-limit-%E6%97%B6%E9%80%80%E5%87%BA%E5%BE%AA%E7%8E%AF">5.3.5 到达 limit 时退出循环&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#536-napi-poll">5.3.6 NAPI &lt;code>poll&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#igb_poll">&lt;code>igb_poll&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#igb_clean_rx_irq">&lt;code>igb_clean_rx_irq&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#537-%E7%9B%91%E6%8E%A7%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">5.3.7 监控网络数据处理&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#procnetsoftnet_stat">&lt;code>/proc/net/softnet_stat&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#538-%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E8%B0%83%E4%BC%98">5.3.8 网络数据处理调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E6%95%B4-net_rx_action-budget">调整 &lt;code>net_rx_action&lt;/code> budget&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#54-grogeneric-receive-offloading">5.4 GRO（Generic Receive Offloading）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E4%BD%BF%E7%94%A8-ethtool-%E4%BF%AE%E6%94%B9-gro-%E9%85%8D%E7%BD%AE">使用 ethtool 修改 GRO 配置&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#55-napi_gro_receive">5.5 &lt;code>napi_gro_receive&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#dev_gro_receive">&lt;code>dev_gro_receive&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#56-napi_skb_finish">5.6 &lt;code>napi_skb_finish&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#6-rps-receive-packet-steering">6 RPS (Receive Packet Steering)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#rps-%E8%B0%83%E4%BC%98">RPS 调优&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#7-rfs-receive-flow-steering">7 RFS (Receive Flow Steering)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98%E6%89%93%E5%BC%80-rfs">调优：打开 RFS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#8-arfs-hardware-accelerated-rfs">8 aRFS (Hardware accelerated RFS)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98-%E5%90%AF%E7%94%A8-arfs">调优: 启用 aRFS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#9-%E4%BB%8E-netif_receive_skb-%E8%BF%9B%E5%85%A5%E5%8D%8F%E8%AE%AE%E6%A0%88">9 从 &lt;code>netif_receive_skb&lt;/code> 进入协议栈&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#91-%E8%B0%83%E4%BC%98-%E6%94%B6%E5%8C%85%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3rx-packet-timestamping">9.1 调优: 收包打时间戳（RX packet timestamping）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#10-netif_receive_skb">10 &lt;code>netif_receive_skb&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#101-%E4%B8%8D%E4%BD%BF%E7%94%A8-rps%E9%BB%98%E8%AE%A4">10.1 不使用 RPS（默认）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#102-%E4%BD%BF%E7%94%A8-rps">10.2 使用 RPS&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#103-enqueue_to_backlog">10.3 &lt;code>enqueue_to_backlog&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#flow-limits">Flow limits&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%91%E6%8E%A7%E7%94%B1%E4%BA%8E-input_pkt_queue-%E6%89%93%E6%BB%A1%E6%88%96-flow-limit-%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%A2%E5%8C%85">监控：由于 &lt;code>input_pkt_queue&lt;/code> 打满或 flow limit 导致的丢包&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98">调优&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-adjusting-netdev_max_backlog-to-prevent-drops">Tuning: Adjusting netdev_max_backlog to prevent drops&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-adjust-the-napi-weight-of-the-backlog-poll-loop">Tuning: Adjust the NAPI weight of the backlog poll loop&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-enabling-flow-limits-and-tuning-flow-limit-hash-table-size">Tuning: Enabling flow limits and tuning flow limit hash table size&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#104-%E5%A4%84%E7%90%86-backlog-%E9%98%9F%E5%88%97napi-poller">10.4 处理 backlog 队列：NAPI poller&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#105-process_backlog">10.5 &lt;code>process_backlog&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#106-__netif_receive_skb_core%E5%B0%86%E6%95%B0%E6%8D%AE%E9%80%81%E5%88%B0%E6%8A%93%E5%8C%85%E7%82%B9tap%E6%88%96%E5%8D%8F%E8%AE%AE%E5%B1%82">10.6 &lt;code>__netif_receive_skb_core&lt;/code>：将数据送到抓包点（tap）或协议层&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#107-%E9%80%81%E5%88%B0%E6%8A%93%E5%8C%85%E7%82%B9tap">10.7 送到抓包点（tap）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#108-%E9%80%81%E5%88%B0%E5%8D%8F%E8%AE%AE%E5%B1%82">10.8 送到协议层&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#11-%E5%8D%8F%E8%AE%AE%E5%B1%82%E6%B3%A8%E5%86%8C">11 协议层注册&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#111-ip-%E5%8D%8F%E8%AE%AE%E5%B1%82">11.1 IP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1111-ip_rcv">11.1.1 &lt;code>ip_rcv&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#netfilter-and-iptables">netfilter and iptables&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1112-ip_rcv_finish">11.1.2 &lt;code>ip_rcv_finish&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98-%E6%89%93%E5%BC%80%E6%88%96%E5%85%B3%E9%97%AD-ip-%E5%8D%8F%E8%AE%AE%E7%9A%84-early-demux-%E9%80%89%E9%A1%B9">调优: 打开或关闭 IP 协议的 early demux 选项&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1113-ip_local_deliver">11.1.3 &lt;code>ip_local_deliver&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1114-ip_local_deliver_finish">11.1.4 &lt;code>ip_local_deliver_finish&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#monitoring-ip-protocol-layer-statistics">Monitoring: IP protocol layer statistics&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#112-%E9%AB%98%E5%B1%82%E5%8D%8F%E8%AE%AE%E6%B3%A8%E5%86%8C">11.2 高层协议注册&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#113-udp-%E5%8D%8F%E8%AE%AE%E5%B1%82">11.3 UDP 协议层&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1131-udp_rcv">11.3.1 &lt;code>udp_rcv&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1132-__udp4_lib_rcv">11.3.2 &lt;code>__udp4_lib_rcv&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1133-udp_queue_rcv_skb">11.3.3 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1334-sk_rcvqueues_full">13.3.4 &lt;code>sk_rcvqueues_full&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E8%B0%83%E4%BC%98-socket-receive-queue-memory">调优: Socket receive queue memory&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1135-udp_queue_rcv_skb">11.3.5 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1137-__udp_queue_rcv_skb">11.3.7 &lt;code>__udp_queue_rcv_skb&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#1138-monitoring-udp-protocol-layer-statistics">11.3.8 Monitoring: UDP protocol layer statistics&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%91%E6%8E%A7-udp-%E5%8D%8F%E8%AE%AE%E7%BB%9F%E8%AE%A1procnetsnmp">监控 UDP 协议统计：&lt;code>/proc/net/snmp&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%91%E6%8E%A7-udp-socket-%E7%BB%9F%E8%AE%A1procnetudp">监控 UDP socket 统计：&lt;code>/proc/net/udp&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#114-%E5%B0%86%E6%95%B0%E6%8D%AE%E6%94%BE%E5%88%B0-socket-%E9%98%9F%E5%88%97">11.4 将数据放到 socket 队列&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#12-%E5%85%B6%E4%BB%96">12 其他&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#121-%E6%89%93%E6%97%B6%E9%97%B4%E6%88%B3-timestamping">12.1 打时间戳 (timestamping)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#122-socket-%E4%BD%8E%E5%BB%B6%E8%BF%9F%E9%80%89%E9%A1%B9busy-polling">12.2 socket 低延迟选项：busy polling&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#123-netpoll%E7%89%B9%E6%AE%8A%E7%BD%91%E7%BB%9C%E5%9C%BA%E6%99%AF%E6%94%AF%E6%8C%81">12.3 Netpoll：特殊网络场景支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#124-so_incoming_cpu">12.4 &lt;code>SO_INCOMING_CPU&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#125-dma-%E5%BC%95%E6%93%8E">12.5 DMA 引擎&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#intels-io-acceleration-technology-ioat">Intel’s I/O Acceleration Technology (IOAT)&lt;/a>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#%E7%9B%B4%E6%8E%A5%E7%BC%93%E5%AD%98%E8%AE%BF%E9%97%AE-dca-direct-cache-access">直接缓存访问 (DCA, Direct cache access)&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#monitoring-ioat-dma-engine">Monitoring IOAT DMA engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#tuning-ioat-dma-engine">Tuning IOAT DMA engine&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#13-%E6%80%BB%E7%BB%93">13 总结&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#14-%E9%A2%9D%E5%A4%96%E8%AE%A8%E8%AE%BA%E5%92%8C%E5%B8%AE%E5%8A%A9">14 额外讨论和帮助&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/#15-%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0">15 相关文章&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="1-网络栈监控和调优常规建议">1 网络栈监控和调优：常规建议&lt;/h1>
&lt;p>网络栈很复杂，没有一种适用于所有场景的通用方式。如果网络的性能和健康状况对你来说 非常重要，那你别无选择，只能投入大量的时间、精力和资源去深入理解系统的各个部分是 如何工作的。
理想情况下，你应该考虑在网络栈的各个层级测量丢包状况，这样就可以缩小范围，确定哪 个组件需要调优。
&lt;strong>然而，这也是一些网络管理员开始走偏的地方&lt;/strong>：他们想当然地认为通过一波 &lt;code>sysctl&lt;/code> 或 &lt;code>/proc&lt;/code> 操作就可以解决问题，并且认为这些配置适用于所有场景。在某些场景下，可能确实 如此；但是，考虑到整个系统是如此细微而精巧地交织在一起的，如果想做有意义的监控和调优 ，就必须得在更深层次搞清系统是如何工作的。否则，你虽然可以使用默认配置，并在 相当长的时间内运行良好，但终会到某个时间点，你不得不（投时间、精力和资源研究这些 配置，然后）做优化。
本文中的一些示例配置仅为了方便理解（效果），并不作为任何特定配置或默认配置的建议 。在做任何配置改动之前，你应该有一个能够对系统进行监控的框架，以查看变更是否带来 预期的效果。
对远程连接上的机器进行网络变更是相当危险的，机器很可能失联。另外，不要在生产环境 直接调整这些配置；如果可能的话，在新机器上改配置，然后将机器灰度上线到生产。&lt;/p>
&lt;h1 id="2-收包过程俯瞰">2 收包过程俯瞰&lt;/h1>
&lt;p>本文将拿 &lt;strong>Intel I350&lt;/strong> 网卡的 &lt;code>igb&lt;/code> 驱动作为参考，网卡的 data sheet 这里可以下 载 &lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF&lt;/a> （警告：文件很大）。
从比较高的层次看，一个数据包从被网卡接收到进入 socket 接收队列的整个过程如下：&lt;/p>
&lt;ol>
&lt;li>加载网卡驱动，初始化&lt;/li>
&lt;li>包从外部网络进入网卡&lt;/li>
&lt;li>网卡（通过 DMA）将包 copy 到内核内存中的 ring buffer&lt;/li>
&lt;li>产生硬件中断，通知系统收到了一个包&lt;/li>
&lt;li>驱动调用 NAPI，如果轮询（poll）还没开始，就开始轮询&lt;/li>
&lt;li>&lt;code>ksoftirqd&lt;/code> 进程调用 NAPI 的 &lt;code>poll&lt;/code> 函数从 ring buffer 收包（&lt;code>poll&lt;/code> 函数是网卡 驱动在初始化阶段注册的；每个 CPU 上都运行着一个 &lt;code>ksoftirqd&lt;/code> 进程，在系统启动期 间就注册了）&lt;/li>
&lt;li>ring buffer 里包对应的内存区域解除映射（unmapped）&lt;/li>
&lt;li>（通过 DMA 进入）内存的数据包以 &lt;code>skb&lt;/code> 的形式被送至更上层处理&lt;/li>
&lt;li>如果 packet steering 功能打开，或者网卡有多队列，网卡收到的包会被分发到多个 CPU&lt;/li>
&lt;li>包从队列进入协议层&lt;/li>
&lt;li>协议层处理包&lt;/li>
&lt;li>包从协议层进入相应 socket 的接收队列&lt;/li>
&lt;/ol>
&lt;p>接下来会详细介绍这个过程。
协议层分析我们将会关注 IP 和 UDP 层，其他协议层可参考这个过程。&lt;/p>
&lt;h1 id="3-网络设备驱动">3 网络设备驱动&lt;/h1>
&lt;p>本文基于 Linux 3.13。
准确地理解 Linux 内核的收包过程是一件非常有挑战性的事情。我们需要仔细研究网卡驱 动的工作原理，才能对网络栈的相应部分有更加清晰的理解。
本文将拿 &lt;code>ibg&lt;/code> 驱动作为例子，它是常见的 Intel I350 网卡的驱动。先来看网卡 驱动是如何工作的。&lt;/p>
&lt;h2 id="31-初始化">3.1 初始化&lt;/h2>
&lt;p>驱动会使用 &lt;code>module_init&lt;/code> 向内核注册一个初始化函数，当驱动被加载时，内核会调用这个函数。
这个初始化函数（&lt;code>igb_init_module&lt;/code>）的代码见 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L676-L697">&lt;code>drivers/net/ethernet/intel/igb/igb_main.c&lt;/code>&lt;/a>.
过程非常简单直接：&lt;/p>
&lt;pre>&lt;code>/**
* igb_init_module - Driver Registration Routine
*
* igb_init_module is the first routine called when the driver is
* loaded. All it does is register with the PCI subsystem.
**/
static int __init igb_init_module(void)
{
int ret;
pr_info(&amp;quot;%s - version %s\n&amp;quot;, igb_driver_string, igb_driver_version);
pr_info(&amp;quot;%s\n&amp;quot;, igb_copyright);
/* ... */
ret = pci_register_driver(&amp;amp;igb_driver);
return ret;
}
module_init(igb_init_module);
&lt;/code>&lt;/pre>
&lt;p>初始化的大部分工作在 &lt;code>pci_register_driver&lt;/code> 里面完成，下面来细看。&lt;/p>
&lt;h3 id="pci-初始化">PCI 初始化&lt;/h3>
&lt;p>Intel I350 网卡是 &lt;a href="https://en.wikipedia.org/wiki/PCI_Express">PCI express&lt;/a> 设备。 PCI 设备通过 &lt;a href="https://en.wikipedia.org/wiki/PCI_configuration_space#Standardized_registers">PCI Configuration Space&lt;/a> 里面的寄存器识别自己。
当设备驱动编译时，&lt;code>MODULE_DEVICE_TABLE&lt;/code> 宏（定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/module.h#L145-L146">&lt;code>include/module.h&lt;/code>&lt;/a>） 会导出一个 &lt;strong>PCI 设备 ID 列表&lt;/strong>（a table of PCI device IDs），驱动据此识别它可以 控制的设备，内核也会依据这个列表对不同设备加载相应驱动。
&lt;code>igb&lt;/code> 驱动的设备表和 PCI 设备 ID 分别见： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L79-L117">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a> 和&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/e1000_hw.h#L41-L75">drivers/net/ethernet/intel/igb/e1000_hw.h&lt;/a>。&lt;/p>
&lt;pre>&lt;code>static DEFINE_PCI_DEVICE_TABLE(igb_pci_tbl) = {
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS), board_82575 },
{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS), board_82575 },
/* ... */
};
MODULE_DEVICE_TABLE(pci, igb_pci_tbl);
&lt;/code>&lt;/pre>
&lt;p>前面提到，驱动初始化的时候会调用 &lt;code>pci_register_driver&lt;/code>，这个函数会将该驱动的各 种回调方法注册到一个 &lt;code>struct pci_driver&lt;/code> 变量，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L238-L249">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>static struct pci_driver igb_driver = {
.name = igb_driver_name,
.id_table = igb_pci_tbl,
.probe = igb_probe,
.remove = igb_remove,
/* ... */
};
&lt;/code>&lt;/pre>
&lt;h2 id="32-网络设备初始化">3.2 网络设备初始化&lt;/h2>
&lt;p>通过 PCI ID 识别设备后，内核就会为它选择合适的驱动。每个 PCI 驱动注册了一个 &lt;code>probe()&lt;/code> 方法，内核会对每个设备依次调用其驱动的 &lt;code>probe&lt;/code> 方法，一旦找到一个合适的 驱动，就不会再为这个设备尝试其他驱动。
很多驱动都需要大量代码来使得设备 ready，具体做的事情各有差异。典型的过程：&lt;/p>
&lt;ol>
&lt;li>启用 PCI 设备&lt;/li>
&lt;li>请求（requesting）内存范围和 IO 端口&lt;/li>
&lt;li>设置 DMA 掩码&lt;/li>
&lt;li>注册设备驱动支持的 ethtool 方法（后面介绍）&lt;/li>
&lt;li>注册所需的 watchdog（例如，e1000e 有一个检测设备是否僵死的 watchdog）&lt;/li>
&lt;li>其他和具体设备相关的事情，例如一些 workaround，或者特定硬件的非常规处理&lt;/li>
&lt;li>创建、初始化和注册一个 &lt;code>struct net_device_ops&lt;/code> 类型变量，这个变量包含了用于设 备相关的回调函数，例如打开设备、发送数据到网络、设置 MAC 地址等&lt;/li>
&lt;li>创建、初始化和注册一个更高层的 &lt;code>struct net_device&lt;/code> 类型变量（一个变量就代表了 一个设备）&lt;/li>
&lt;/ol>
&lt;p>我们来简单看下 &lt;code>igb&lt;/code> 驱动的 &lt;code>igb_probe&lt;/code> 包含哪些过程。下面的代码来自 &lt;code>igb_probe&lt;/code>，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>err = pci_enable_device_mem(pdev);
/* ... */
err = dma_set_mask_and_coherent(&amp;amp;pdev-&amp;gt;dev, DMA_BIT_MASK(64));
/* ... */
err = pci_request_selected_regions(pdev, pci_select_bars(pdev,
IORESOURCE_MEM),
igb_driver_name);
pci_enable_pcie_error_reporting(pdev);
pci_set_master(pdev);
pci_save_state(pdev);
&lt;/code>&lt;/pre>
&lt;h3 id="更多-pci-驱动信息">更多 PCI 驱动信息&lt;/h3>
&lt;p>详细的 PCI 驱动讨论不在本文范围，如果想进一步了解，推荐如下材料： &lt;a href="http://free-electrons.com/doc/pci-drivers.pdf">分享&lt;/a>， &lt;a href="http://wiki.osdev.org/PCI">wiki&lt;/a>， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/PCI/pci.txt">Linux Kernel Documentation: PCI&lt;/a>。&lt;/p>
&lt;h2 id="33-网络设备启动">3.3 网络设备启动&lt;/h2>
&lt;p>&lt;code>igb_probe&lt;/code> 做了很多重要的设备初始化工作。除了 PCI 相关的，还有如下一些通用网络 功能和网络设备相关的工作：&lt;/p>
&lt;ol>
&lt;li>注册 &lt;code>struct net_device_ops&lt;/code> 变量&lt;/li>
&lt;li>注册 ethtool 相关的方法&lt;/li>
&lt;li>从网卡获取默认 MAC 地址&lt;/li>
&lt;li>设置 &lt;code>net_device&lt;/code> 特性标记&lt;/li>
&lt;/ol>
&lt;p>我们逐一看下这些过程，后面会用到。&lt;/p>
&lt;h3 id="331-struct-net_device_ops">3.3.1 &lt;code>struct net_device_ops&lt;/code>&lt;/h3>
&lt;p>网络设备相关的操作函数都注册到这个类型的变量中。&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>static const struct net_device_ops igb_netdev_ops = {
.ndo_open = igb_open,
.ndo_stop = igb_close,
.ndo_start_xmit = igb_xmit_frame,
.ndo_get_stats64 = igb_get_stats64,
.ndo_set_rx_mode = igb_set_rx_mode,
.ndo_set_mac_address = igb_set_mac,
.ndo_change_mtu = igb_change_mtu,
.ndo_do_ioctl = igb_ioctl,
/* ... */
&lt;/code>&lt;/pre>
&lt;p>这个变量会在 &lt;code>igb_probe()&lt;/code>中赋给 &lt;code>struct net_device&lt;/code> 中的 &lt;code>netdev_ops&lt;/code> 字段：&lt;/p>
&lt;pre>&lt;code>static int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
{
...
netdev-&amp;gt;netdev_ops = &amp;amp;igb_netdev_ops;
}
&lt;/code>&lt;/pre>
&lt;h3 id="332-ethtool-函数注册">3.3.2 &lt;code>ethtool&lt;/code> 函数注册&lt;/h3>
&lt;p>&lt;a href="https://www.kernel.org/pub/software/network/ethtool/">&lt;code>ethtool&lt;/code>&lt;/a> 是一个命令行工 具，可以查看和修改网络设备的一些配置，常用于收集网卡统计数据。在 Ubuntu 上，可以 通过 &lt;code>apt-get install ethtool&lt;/code> 安装。
&lt;code>ethtool&lt;/code> 通过 &lt;a href="http://man7.org/linux/man-pages/man2/ioctl.2.html">ioctl&lt;/a> 和设备驱 动通信。内核实现了一个通用 &lt;code>ethtool&lt;/code> 接口，网卡驱动实现这些接口，就可以被 &lt;code>ethtool&lt;/code> 调用。当 &lt;code>ethtool&lt;/code> 发起一个系统调用之后，内核会找到对应操作的回调函数 。回调实现了各种简单或复杂的函数，简单的如改变一个 flag 值，复杂的包括调整网卡硬 件如何运行。
相关实现见：&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_ethtool.c">igb_ethtool.c&lt;/a>。&lt;/p>
&lt;h3 id="333-软中断irq">3.3.3 软中断（IRQ）&lt;/h3>
&lt;p>当一个数据帧通过 DMA 写到 RAM（内存）后，网卡是如何通知其他系统这个包可以被处理 了呢？
传统的方式是，网卡会产生一个硬件中断（IRQ），通知数据包到了。有&lt;strong>三种常见的硬中 断类型&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>MSI-X&lt;/li>
&lt;li>MSI&lt;/li>
&lt;li>legacy IRQ&lt;/li>
&lt;/ul>
&lt;p>稍后详细介绍到。
先来思考这样一个问题：如果有大量的数据包到达，就会产生大量的硬件中断。CPU 忙于处 理硬件中断的时候，可用于处理其他任务的时间就会减少。
NAPI（New API）是一种新的机制，可以减少产生的硬件中断的数量（但不能完全消除硬中断 ）。&lt;/p>
&lt;h3 id="334-napi">3.3.4 NAPI&lt;/h3>
&lt;h4 id="napi">NAPI&lt;/h4>
&lt;p>NAPI 接收数据包的方式和传统方式不同，它允许设备驱动注册一个 &lt;code>poll&lt;/code> 方法，然后调 用这个方法完成收包。
NAPI 的使用方式：&lt;/p>
&lt;ol>
&lt;li>驱动打开 NAPI 功能，默认处于未工作状态（没有在收包）&lt;/li>
&lt;li>数据包到达，网卡通过 DMA 写到内存&lt;/li>
&lt;li>网卡触发一个硬中断，&lt;strong>中断处理函数开始执行&lt;/strong>&lt;/li>
&lt;li>软中断（softirq，稍后介绍），唤醒 NAPI 子系统。这会触发&lt;strong>在一个单独的线程里， 调用驱动注册的 &lt;code>poll&lt;/code> 方法收包&lt;/strong>&lt;/li>
&lt;li>驱动禁止网卡产生新的硬件中断。这样做是为了 NAPI 能够在收包的时候不会被新的中 断打扰&lt;/li>
&lt;li>一旦没有包需要收了，NAPI 关闭，网卡的硬中断重新开启&lt;/li>
&lt;li>转步骤 2&lt;/li>
&lt;/ol>
&lt;p>和传统方式相比，NAPI 一次中断会接收多个包，因此可以减少硬件中断的数量。
&lt;code>poll&lt;/code> 方法是通过调用 &lt;code>netif_napi_add&lt;/code> 注册到 NAPI 的，同时还可以指定权重 &lt;code>weight&lt;/code>，大部分驱动都 hardcode 为 64。后面会进一步解释这个 weight 以及 hardcode 64。
通常来说，驱动在初始化的时候注册 NAPI poll 方法。&lt;/p>
&lt;h3 id="335-igb-驱动的-napi-初始化">3.3.5 &lt;code>igb&lt;/code> 驱动的 NAPI 初始化&lt;/h3>
&lt;p>&lt;code>igb&lt;/code> 驱动的初始化过程是一个很长的调用链：&lt;/p>
&lt;ol>
&lt;li>&lt;code>igb_probe&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_sw_init&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_sw_init&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_init_interrupt_scheme&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_init_interrupt_scheme&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_alloc_q_vectors&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_alloc_q_vectors&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>igb_alloc_q_vector&lt;/code>&lt;/li>
&lt;li>&lt;code>igb_alloc_q_vector&lt;/code> &lt;code>-&amp;gt;&lt;/code> &lt;code>netif_napi_add&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>从较高的层面来看，这个调用过程会做以下事情：&lt;/p>
&lt;ol>
&lt;li>如果支持 &lt;code>MSI-X&lt;/code>，调用 &lt;code>pci_enable_msix&lt;/code> 打开它&lt;/li>
&lt;li>计算和初始化一些配置，包括网卡收发队列的数量&lt;/li>
&lt;li>调用 &lt;code>igb_alloc_q_vector&lt;/code> 创建每个发送和接收队列&lt;/li>
&lt;li>&lt;code>igb_alloc_q_vector&lt;/code> 会进一步调用 &lt;code>netif_napi_add&lt;/code> 注册 poll 方法到 NAPI 变量&lt;/li>
&lt;/ol>
&lt;p>我们来看下 &lt;code>igb_alloc_q_vector&lt;/code> 是如何注册 poll 方法和私有数据的： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1145-L1271">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static int igb_alloc_q_vector(struct igb_adapter *adapter,
int v_count, int v_idx,
int txr_count, int txr_idx,
int rxr_count, int rxr_idx)
{
/* ... */
/* allocate q_vector and rings */
q_vector = kzalloc(size, GFP_KERNEL);
if (!q_vector)
return -ENOMEM;
/* initialize NAPI */
netif_napi_add(adapter-&amp;gt;netdev, &amp;amp;q_vector-&amp;gt;napi, igb_poll, 64);
/* ... */
&lt;/code>&lt;/pre>
&lt;p>&lt;code>q_vector&lt;/code> 是新分配的队列，&lt;code>igb_poll&lt;/code> 是 poll 方法，当它收包的时候，会通过 这个接收队列找到关联的 NAPI 变量（&lt;code>q_vector-&amp;gt;napi&lt;/code>）。
这里很重要，后面我们介绍从驱动到网络协议栈的 flow（根据 IP 头信息做哈希，哈希相 同的属于同一个 flow）时会看到。&lt;/p>
&lt;h2 id="34-启用网卡-bring-a-network-device-up">3.4 启用网卡 (Bring A Network Device Up)&lt;/h2>
&lt;p>回忆前面我们提到的 &lt;code>structure net_device_ops&lt;/code> 变量，它包含网卡启用、发包、设置 mac 地址等回调函数（函数指针）。
当启用一个网卡时（例如，通过 &lt;code>ifconfig eth0 up&lt;/code>），&lt;code>net_device_ops&lt;/code> 的 &lt;code>ndo_open&lt;/code> 方法会被调用。它通常会做以下事情：&lt;/p>
&lt;ol>
&lt;li>分配 RX、TX 队列内存&lt;/li>
&lt;li>打开 NAPI 功能&lt;/li>
&lt;li>注册中断处理函数&lt;/li>
&lt;li>打开（enable）硬中断&lt;/li>
&lt;li>其他&lt;/li>
&lt;/ol>
&lt;p>&lt;code>igb&lt;/code> 驱动中，这个方法对应的是 &lt;code>igb_open&lt;/code> 函数。&lt;/p>
&lt;h3 id="341-准备从网络接收数据">3.4.1 准备从网络接收数据&lt;/h3>
&lt;p>今天的大部分网卡都&lt;strong>使用 DMA 将数据直接写到内存&lt;/strong>，接下来&lt;strong>操作系统可以直接从里 面读取&lt;/strong>。实现这一目的所使用的数据结构是 ring buffer（环形缓冲区）。
要实现这一功能，设备驱动必须和操作系统合作，&lt;strong>预留（reserve）出一段内存来给网卡 使用&lt;/strong>。预留成功后，网卡知道了这块内存的地址，接下来收到的包就会放到这里，进而被 操作系统取走。
由于这块内存区域是有限的，如果数据包的速率非常快，单个 CPU 来不及取走这些包，新 来的包就会被丢弃。这时候，Receive Side Scaling（RSS，接收端扩展）或者多队列（ multiqueue）一类的技术可能就会排上用场。
一些网卡有能力将接收到的包写到&lt;strong>多个不同的内存区域，每个区域都是独立的接收队列&lt;/strong>。这 样操作系统就可以利用多个 CPU（硬件层面）并行处理收到的包。只有部分网卡支持这个功 能。
Intel I350 网卡支持多队列，我们可以在 &lt;code>igb&lt;/code> 的驱动里看出来。&lt;code>igb&lt;/code> 驱动启用的时候 ，最开始做的事情之一就是调用 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2801-L2804">&lt;code>igb_setup_all_rx_resources&lt;/code>&lt;/a> 函数。这个函数会对每个 RX 队列调用 &lt;code>igb_setup_rx_resources&lt;/code>, 里面会管理 DMA 的内存.
如果对其原理感兴趣，可以进一步查看 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/DMA-API-HOWTO.txt">Linux kernel’s DMA API HOWTO&lt;/a> 。
RX 队列的数量和大小可以通过 ethtool 进行配置，调整这两个参数会对收包或者丢包产生可见影响。
网卡通过对 packet 头（例如源地址、目的地址、端口等）做哈希来决定将 packet 放到 哪个 RX 队列。只有很少的网卡支持调整哈希算法。如果支持的话，那你可以根据算法将特定 的 flow 发到特定的队列，甚至可以做到在硬件层面直接将某些包丢弃。
一些网卡支持调整 RX 队列的权重，你可以有意地将更多的流量发到指定的 queue。
后面会介绍如何对这些参数进行调优。&lt;/p>
&lt;h3 id="342-enable-napi">3.4.2 Enable NAPI&lt;/h3>
&lt;p>前面看到了驱动如何注册 NAPI &lt;code>poll&lt;/code> 方法，但是，一般直到网卡被启用之后，NAPI 才被启用。
启用 NAPI 很简单，调用 &lt;code>napi_enable&lt;/code> 函数就行，这个函数会设置 NAPI 变量（&lt;code>struct napi_struct&lt;/code>）中一个表示是否启用的标志位。前面说到，NAPI 启用后并不是立即开始工 作（而是等硬中断触发）。
对于 &lt;code>igb&lt;/code>，驱动初始化或者通过 ethtool 修改 queue 数量或大小的时候，会启用每个 &lt;code>q_vector&lt;/code> 的 NAPI 变量。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2833-L2834">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>for (i = 0; i &amp;lt; adapter-&amp;gt;num_q_vectors; i++)
napi_enable(&amp;amp;(adapter-&amp;gt;q_vector[i]-&amp;gt;napi));
&lt;/code>&lt;/pre>
&lt;h3 id="343-注册中断处理函数">3.4.3 注册中断处理函数&lt;/h3>
&lt;p>启用 NAPI 之后，下一步就是注册中断处理函数。设备有多种方式触发一个中断：&lt;/p>
&lt;ul>
&lt;li>MSI-X&lt;/li>
&lt;li>MSI&lt;/li>
&lt;li>legacy interrupts&lt;/li>
&lt;/ul>
&lt;p>设备驱动的实现也因此而异。
驱动必须判断出设备支持哪种中断方式，然后注册相应的中断处理函数，这些函数在中断发 生的时候会被执行。
一些驱动，例如 &lt;code>igb&lt;/code>，会试图为每种中断类型注册一个中断处理函数，如果注册失败，就 尝试下一种（没测试过的）类型。
&lt;strong>MSI-X 中断是比较推荐的方式，尤其是对于支持多队列的网卡&lt;/strong>。因为每个 RX 队列有独 立的 MSI-X 中断，因此可以被不同的 CPU 处理（通过 &lt;code>irqbalance&lt;/code> 方式，或者修改 &lt;code>/proc/irq/IRQ_NUMBER/smp_affinity&lt;/code>）。我们后面会看到，处理中断的 CPU 也是随后处 理这个包的 CPU。这样的话，从网卡硬件中断的层面就可以设置让收到的包被不同的 CPU 处理。
如果不支持 MSI-X，那 MSI 相比于传统中断方式仍然有一些优势，驱动仍然会优先考虑它。 这个 &lt;a href="https://en.wikipedia.org/wiki/Message_Signaled_Interrupts">wiki&lt;/a> 介绍了更多 关于 MSI 和 MSI-X 的信息。
在 &lt;code>igb&lt;/code> 驱动中，函数 &lt;code>igb_msix_ring&lt;/code>, &lt;code>igb_intr_msi&lt;/code>, &lt;code>igb_intr&lt;/code> 分别是 MSI-X, MSI, 和传统中断方式的中断处理函数。
这段代码显式了驱动是如何尝试各种中断类型的， &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1360-L1413">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static int igb_request_irq(struct igb_adapter *adapter)
{
struct net_device *netdev = adapter-&amp;gt;netdev;
struct pci_dev *pdev = adapter-&amp;gt;pdev;
int err = 0;
if (adapter-&amp;gt;msix_entries) {
err = igb_request_msix(adapter);
if (!err)
goto request_done;
/* fall back to MSI */
/* ... */
}
/* ... */
if (adapter-&amp;gt;flags &amp;amp; IGB_FLAG_HAS_MSI) {
err = request_irq(pdev-&amp;gt;irq, igb_intr_msi, 0,
netdev-&amp;gt;name, adapter);
if (!err)
goto request_done;
/* fall back to legacy interrupts */
/* ... */
}
err = request_irq(pdev-&amp;gt;irq, igb_intr, IRQF_SHARED,
netdev-&amp;gt;name, adapter);
if (err)
dev_err(&amp;amp;pdev-&amp;gt;dev, &amp;quot;Error %d getting interrupt\n&amp;quot;, err);
request_done:
return err;
}
&lt;/code>&lt;/pre>
&lt;p>这就是 &lt;code>igb&lt;/code> 驱动注册中断处理函数的过程，这个函数在一个包到达网卡触发一个硬 件中断时就会被执行。&lt;/p>
&lt;h3 id="344-enable-interrupts">3.4.4 Enable Interrupts&lt;/h3>
&lt;p>到这里，几乎所有的准备工作都就绪了。唯一剩下的就是打开硬中断，等待数据包进来。 打开硬中断的方式因硬件而异，&lt;code>igb&lt;/code> 驱动是在 &lt;code>__igb_open&lt;/code> 里调用辅助函数 &lt;code>igb_irq_enable&lt;/code> 完成的。
中断通过写寄存器的方式打开：&lt;/p>
&lt;pre>&lt;code>static void igb_irq_enable(struct igb_adapter *adapter)
{
/* ... */
wr32(E1000_IMS, IMS_ENABLE_MASK | E1000_IMS_DRSTA);
wr32(E1000_IAM, IMS_ENABLE_MASK | E1000_IMS_DRSTA);
/* ... */
}
&lt;/code>&lt;/pre>
&lt;p>现在，网卡已经启用了。驱动可能还会做一些额外的事情，例如启动定时器，工作队列（ work queue），或者其他硬件相关的设置。这些工作做完后，网卡就可以收包了。
接下来看一下如何监控和调优网卡。&lt;/p>
&lt;h2 id="35-网卡监控">3.5 网卡监控&lt;/h2>
&lt;p>监控网络设备有几种不同的方式，每种方式的监控粒度（granularity）和复杂度不同。我 们先从最粗的粒度开始，逐步细化。&lt;/p>
&lt;h3 id="351-using-ethtool--s">3.5.1 Using &lt;code>ethtool -S&lt;/code>&lt;/h3>
&lt;p>&lt;code>ethtool -S&lt;/code> 可以查看网卡统计信息（例如丢弃的包数量）：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -S eth0
NIC statistics:
rx_packets: 597028087
tx_packets: 5924278060
rx_bytes: 112643393747
tx_bytes: 990080156714
rx_broadcast: 96
tx_broadcast: 116
rx_multicast: 20294528
....
&lt;/code>&lt;/pre>
&lt;p>监控这些数据比较困难。因为用命令行获取很容易，但是以上字段并没有一个统一的标准。 不同的驱动，甚至同一驱动的不同版本可能字段都会有差异。
你可以先粗略的查看 “drop”, “buffer”, “miss” 等字样。然后，在驱动的源码里找到对应的 更新这些字段的地方，这可能是在软件层面更新的，也有可能是在硬件层面通过寄存器更新 的。如果是通过硬件寄存器的方式，你就得查看网卡的 data sheet（说明书），搞清楚这个 寄存器代表什么。ethtoool 给出的这些字段名，有一些是有误导性的（misleading）。&lt;/p>
&lt;h3 id="352-using-sysfs">3.5.2 Using &lt;code>sysfs&lt;/code>&lt;/h3>
&lt;p>sysfs 也提供了统计信息，但相比于网卡层的统计，要更上层一些。
例如，获取 eth0 的接收端 drop 的数量：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/net/eth0/statistics/rx_dropped
2
&lt;/code>&lt;/pre>
&lt;p>不同类型的统计分别位于&lt;code> /sys/class/net/&amp;lt;NIC&amp;gt;/statistics/&lt;/code> 下面的不同文件，包括 &lt;code>collisions&lt;/code>, &lt;code>rx_dropped&lt;/code>, &lt;code>rx_errors&lt;/code>, &lt;code>rx_missed_errors&lt;/code> 等等。
不幸的是，每种类型代表什么意思，是由驱动来决定的，因此也是由驱动决定何时以及在哪 里更新这些计数的。你可能会发现一些驱动将一些特定类型的错误归类为 drop，而另外 一些驱动可能将它们归类为 miss。
这些值至关重要，因此你需要查看对应的网卡驱动，搞清楚它们真正代表什么。&lt;/p>
&lt;h3 id="353-using-procnetdev">3.5.3 Using &lt;code>/proc/net/dev&lt;/code>&lt;/h3>
&lt;p>&lt;code>/proc/net/dev&lt;/code> 提供了更高一层的网卡统计。&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/dev
Inter-| Receive | Transmit
face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed
eth0: 110346752214 597737500 0 2 0 0 0 20963860 990024805984 6066582604 0 0 0 0 0 0
lo: 428349463836 1579868535 0 0 0 0 0 0 428349463836 1579868535 0 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>这个文件里显式的统计只是 sysfs 里面的一个子集，但适合作为一个常规的统计参考。
前面的警告（caveat）也适用于此：如果这些数据对你非常重要，那你必须得查看内核源码 、驱动源码和驱动手册，搞清楚每个字段真正代表什么意思，计数是如何以及何时被更新的。&lt;/p>
&lt;h2 id="36-网卡调优">3.6 网卡调优&lt;/h2>
&lt;h3 id="361-查看-rx-队列数量">3.6.1 查看 RX 队列数量&lt;/h3>
&lt;p>如果网卡及其驱动支持 RSS/多队列，那你可以会调整 RX queue（也叫 RX channel）的数量。 这可以用 ethtool 完成。
查看 RX queue 数量：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Pre-set maximums:
RX: 0
TX: 0
Other: 0
Combined: 8
Current hardware settings:
RX: 0
TX: 0
Other: 0
Combined: 4
&lt;/code>&lt;/pre>
&lt;p>这里可以看到允许的最大值（网卡及驱动限制），以及当前设置的值。
注意：不是所有网卡驱动都支持这个操作。如果你的网卡不支持，会看到如下类似的错误：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -l eth0
Channel parameters for eth0:
Cannot get device channel parameters
: Operation not supported
&lt;/code>&lt;/pre>
&lt;p>这意味着驱动没有实现 ethtool 的 &lt;code>get_channels&lt;/code> 方法。可能的原因包括：该网卡不支 持调整 RX queue 数量，不支持 RSS/multiqueue，或者驱动没有更新来支持此功能。&lt;/p>
&lt;h3 id="362-调整-rx-queues">3.6.2 调整 RX queues&lt;/h3>
&lt;p>&lt;code>ethtool -L&lt;/code> 可以修改 RX queue 数量。
注意：一些网卡和驱动只支持 combined queue，这种模式下，RX queue 和 TX queue 是一 对一绑定的，上面的例子我们看到的就是这种。
设置 combined 类型网卡的收发队列为 8 个：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 combined 8
&lt;/code>&lt;/pre>
&lt;p>如果你的网卡支持独立的 RX 和 TX 队列数量，那你可以只修改 RX queue 数量：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -L eth0 rx 8
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，修改以上配置会使网卡先 down 再 up，因此会造成丢包。请酌情 使用。&lt;/p>
&lt;h2 id="363-调整-rx-queue-的大小">3.6.3 调整 RX queue 的大小&lt;/h2>
&lt;p>一些网卡和驱动也支持修改 RX queue 的大小。底层是如何工作的，随硬件而异，但幸运的是 ，ethtool 提供了一个通用的接口来做这件事情。增加 RX queue 的大小可以在流量很大的时 候缓解丢包问题，但是，只调整这个还不够，软件层面仍然可能会丢包，因此还需要其他的 一些调优才能彻底的缓解或解决丢包问题。
&lt;code>ethtool -g&lt;/code> 可以查看 queue 的大小。&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX: 4096
RX Mini: 0
RX Jumbo: 0
TX: 4096
Current hardware settings:
RX: 512
RX Mini: 0
RX Jumbo: 0
TX: 512
&lt;/code>&lt;/pre>
&lt;p>以上显式网卡支持最多 4096 个接收和发送 descriptor（描述符，简单理解，存放的是指 向包的指针），但是现在只用到了 512 个。
用 &lt;code>ethtool -G&lt;/code> 修改 queue 大小：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -G eth0 rx 4096
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，修改以上配置会使网卡先 down 再 up，因此会造成丢包。请酌情 使用。&lt;/p>
&lt;h3 id="364-调整-rx-queue-的权重weight">3.6.4 调整 RX queue 的权重（weight）&lt;/h3>
&lt;p>一些网卡支持给不同的 queue 设置不同的权重，以此分发不同数量的网卡包到不同的队列。
如果你的网卡支持以下功能，那你可以使用：&lt;/p>
&lt;ol>
&lt;li>网卡支持 flow indirection（flow 重定向，flow 是什么前面提到过）&lt;/li>
&lt;li>网卡驱动实现了 &lt;code>get_rxfh_indir_size&lt;/code> 和 &lt;code>get_rxfh_indir&lt;/code> 方法&lt;/li>
&lt;li>使用的 ethtool 版本足够新，支持 &lt;code>-x&lt;/code> 和 &lt;code>-X&lt;/code> 参数来设置 indirection table&lt;/li>
&lt;/ol>
&lt;p>&lt;code>ethtool -x&lt;/code> 检查 flow indirection 设置：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -x eth0
RX flow hash indirection table for eth3 with 2 RX ring(s):
0: 0 1 0 1 0 1 0 1
8: 0 1 0 1 0 1 0 1
16: 0 1 0 1 0 1 0 1
24: 0 1 0 1 0 1 0 1
&lt;/code>&lt;/pre>
&lt;p>第一列是哈希值索引，是该行的第一个哈希值；冒号后面的是每个哈希值对于的 queue，例 如，第一行分别是哈希值 0，1，2，3，4，5，6，7，对应的 packet 应该分别被放到 RX queue 0，1，0，1，0，1，0，1。
例子：在前两个 RX queue 之间均匀的分发（接收到的包）：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -X eth0 equal 2
&lt;/code>&lt;/pre>
&lt;p>例子：用 &lt;code>ethtool -X&lt;/code> 设置自定义权重：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -X eth0 weight 6 2
&lt;/code>&lt;/pre>
&lt;p>以上命令分别给 rx queue 0 和 rx queue 1 不同的权重：6 和 2，因此 queue 0 接收到的数量更 多。注意 queue 一般是和 CPU 绑定的，因此这也意味着相应的 CPU 也会花更多的时间片在收包 上。
一些网卡还支持修改计算 hash 时使用哪些字段。&lt;/p>
&lt;h3 id="365-调整-rx-哈希字段-for-network-flows">3.6.5 调整 RX 哈希字段 for network flows&lt;/h3>
&lt;p>可以用 ethtool 调整 RSS 计算哈希时所使用的字段。
例子：查看 UDP RX flow 哈希所使用的字段：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -n eth0 rx-flow-hash udp4
UDP over IPV4 flows use these fields for computing Hash flow key:
IP SA
IP DA
&lt;/code>&lt;/pre>
&lt;p>可以看到只用到了源 IP（SA：Source Address）和目的 IP。
我们接下来修改一下，加入源端口和目的端口：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -N eth0 rx-flow-hash udp4 sdfn
&lt;/code>&lt;/pre>
&lt;p>&lt;code>sdfn&lt;/code> 的具体含义解释起来有点麻烦，请查看 ethtool 的帮助（man page）。
调整 hash 所用字段是有用的，而 &lt;code>ntuple&lt;/code> 过滤对于更加细粒度的 flow control 更加有用。&lt;/p>
&lt;h3 id="366-ntuple-filtering-for-steering-network-flows">3.6.6 ntuple filtering for steering network flows&lt;/h3>
&lt;p>一些网卡支持 “ntuple filtering” 特性。该特性允许用户（通过 ethtool ）指定一些参数来 在硬件上过滤收到的包，然后将其直接放到特定的 RX queue。例如，用户可以指定到特定目 端口的 TCP 包放到 RX queue 1。
Intel 的网卡上这个特性叫 Intel Ethernet Flow Director，其他厂商可能也有他们的名字 ，这些都是出于市场宣传原因，底层原理是类似的。
我们后面会看到，ntuple filtering 是一个叫 Accelerated Receive Flow Steering (aRFS) 功能的核心部分之一，后者使得 ntuple filtering 的使用更加方便。
这个特性适用的场景：最大化数据本地性（data locality），以增加 CPU 处理网络数据时的 缓存命中率。例如，考虑运行在 80 口的 web 服务器：&lt;/p>
&lt;ol>
&lt;li>webserver 进程运行在 80 口，并绑定到 CPU 2&lt;/li>
&lt;li>和某个 RX queue 关联的硬中断绑定到 CPU 2&lt;/li>
&lt;li>目的端口是 80 的 TCP 流量通过 ntuple filtering 绑定到 CPU 2&lt;/li>
&lt;li>接下来所有到 80 口的流量，从数据包进来到数据到达用户程序的整个过程，都由 CPU 2 处理&lt;/li>
&lt;li>仔细监控系统的缓存命中率、网络栈的延迟等信息，以验证以上配置是否生效&lt;/li>
&lt;/ol>
&lt;p>检查 ntuple filtering 特性是否打开：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -k eth0
Offload parameters for eth0:
...
ntuple-filters: off
receive-hashing: on
&lt;/code>&lt;/pre>
&lt;p>可以看到，上面的 ntuple 是关闭的。
打开：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -K eth0 ntuple on
&lt;/code>&lt;/pre>
&lt;p>打开 ntuple filtering 功能，并确认打开之后，可以用 &lt;code>ethtool -u&lt;/code> 查看当前的 ntuple rules：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -u eth0
40 RX rings available
Total 0 rules
&lt;/code>&lt;/pre>
&lt;p>可以看到当前没有 rules。
我们来加一条：目的端口是 80 的放到 RX queue 2：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -U eth0 flow-type tcp4 dst-port 80 action 2
&lt;/code>&lt;/pre>
&lt;p>也可以用 ntuple filtering 在硬件层面直接 drop 某些 flow 的包。当特定 IP 过来的流 量太大时，这种功能可能会派上用场。更多关于 ntuple 的信息，参考 ethtool man page。
&lt;code>ethtool -S &amp;lt;DEVICE&amp;gt;&lt;/code> 的输出统计里，Intel 的网卡有 &lt;code>fdir_match&lt;/code> 和 &lt;code>fdir_miss&lt;/code> 两项， 是和 ntuple filtering 相关的。关于具体的、详细的统计计数，需要查看相应网卡的设备驱 动和 data sheet。&lt;/p>
&lt;h1 id="4-软中断softirq">4 软中断（SoftIRQ）&lt;/h1>
&lt;p>在查看网络栈之前，让我们先开个小差，看下内核里一个叫 SoftIRQ 的东西。&lt;/p>
&lt;h2 id="41-软中断是什么">4.1 软中断是什么&lt;/h2>
&lt;p>内核的软中断系统是一种&lt;strong>在硬中断处理上下文（驱动中）之外执行代码&lt;/strong>的机制。&lt;strong>硬中 断处理函数（handler）执行时，会屏蔽部分或全部（新的）硬中断&lt;/strong>。中断被屏蔽的时间 越长，丢失事件的可能性也就越大。所以，&lt;strong>所有耗时的操作都应该从硬中断处理逻辑中剥 离出来&lt;/strong>，硬中断因此能尽可能快地执行，然后再重新打开硬中断。
内核中也有其他机制将耗时操作转移出去，不过对于网络栈，我们接下来只看软中断这种方 式。
可以把软中断系统想象成一系列&lt;strong>内核线程&lt;/strong>（每个 CPU 一个），这些线程执行针对不同 事件注册的处理函数（handler）。如果你用过 &lt;code>top&lt;/code> 命令，可能会注意到 &lt;code>ksoftirqd/0&lt;/code> 这个内核线程，其表示这个软中断线程跑在 CPU 0 上。
内核子系统（比如网络）能通过 &lt;code>open_softirq()&lt;/code> 注册软中断处理函数。接下来会看到 网络系统是如何注册它的处理函数的。
现在先来学习一下软中断是如何工作的。&lt;/p>
&lt;h2 id="42-ksoftirqd">4.2 &lt;code>ksoftirqd&lt;/code>&lt;/h2>
&lt;p>软中断对分担硬中断的工作量至关重要，因此软中断线程在&lt;strong>内核启动的很早阶段&lt;/strong>就 &lt;code>spawn&lt;/code> 出来了。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/kernel/softirq.c#L743-L758">&lt;code>kernel/softirq.c&lt;/code>&lt;/a> 展示了 &lt;code>ksoftirqd&lt;/code> 系统是如何初始化的：&lt;/p>
&lt;pre>&lt;code>static struct smp_hotplug_thread softirq_threads = {
.store = &amp;amp;ksoftirqd,
.thread_should_run = ksoftirqd_should_run,
.thread_fn = run_ksoftirqd,
.thread_comm = &amp;quot;ksoftirqd/%u&amp;quot;,
};
static __init int spawn_ksoftirqd(void)
{
register_cpu_notifier(&amp;amp;cpu_nfb);
BUG_ON(smpboot_register_percpu_thread(&amp;amp;softirq_threads));
return 0;
}
early_initcall(spawn_ksoftirqd);
&lt;/code>&lt;/pre>
&lt;p>看到注册了两个回调函数: &lt;code>ksoftirqd_should_run&lt;/code> 和 &lt;code>run_ksoftirqd&lt;/code>。这两个函数都会从 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/kernel/smpboot.c#L94-L163">&lt;code>kernel/smpboot.c&lt;/code>&lt;/a> 里调用，作为事件处理循环的一部分。
&lt;code>kernel/smpboot.c&lt;/code> 里面的代码首先调用 &lt;code>ksoftirqd_should_run&lt;/code> 判断是否有 pending 的软 中断，如果有，就执行 &lt;code>run_ksoftirqd&lt;/code>，后者做一些 bookeeping 工作，然后调用 &lt;code>__do_softirq&lt;/code>。&lt;/p>
&lt;h2 id="43-__do_softirq">4.3 &lt;code>__do_softirq&lt;/code>&lt;/h2>
&lt;p>&lt;code>__do_softirq&lt;/code> 做的几件事情：&lt;/p>
&lt;ul>
&lt;li>判断哪个 softirq 被 pending&lt;/li>
&lt;li>计算 softirq 时间，用于统计&lt;/li>
&lt;li>更新 softirq 执行相关的统计数据&lt;/li>
&lt;li>执行 pending softirq 的处理函数&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>查看 CPU 利用率时，&lt;code>si&lt;/code> 字段对应的就是 softirq&lt;/strong>，度量（从硬中断转移过来的）软 中断的 CPU 使用量。&lt;/p>
&lt;h2 id="44-监控">4.4 监控&lt;/h2>
&lt;p>软中断的信息可以从 &lt;code>/proc/softirqs&lt;/code> 读取：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/softirqs
CPU0 CPU1 CPU2 CPU3
HI: 0 0 0 0
TIMER: 2831512516 1337085411 1103326083 1423923272
NET_TX: 15774435 779806 733217 749512
NET_RX: 1671622615 1257853535 2088429526 2674732223
BLOCK: 1800253852 1466177 1791366 634534
BLOCK_IOPOLL: 0 0 0 0
TASKLET: 25 0 0 0
SCHED: 2642378225 1711756029 629040543 682215771
HRTIMER: 2547911 2046898 1558136 1521176
RCU: 2056528783 4231862865 3545088730 844379888
&lt;/code>&lt;/pre>
&lt;p>监控这些数据可以得到软中断的执行频率信息。
例如，&lt;code>NET_RX&lt;/code> 一行显示的是软中断在 CPU 间的分布。如果分布非常不均匀，那某一列的 值就会远大于其他列，这预示着下面要介绍的 Receive Packet Steering / Receive Flow Steering 可能会派上用场。但也要注意：不要太相信这个数值，&lt;code>NET_RX&lt;/code> 太高并不一定都 是网卡触发的，下面会看到其他地方也有可能触发之。
调整其他网络配置时，可以留意下这个指标的变动。
现在，让我们进入网络栈部分，跟踪一个包是如何被接收的。&lt;/p>
&lt;h1 id="5-linux-网络设备子系统">5 Linux 网络设备子系统&lt;/h1>
&lt;p>我们已经知道了网络驱动和软中断是如何工作的，接下来看 Linux 网络设备子系统是如何 初始化的。然后我们就可以从一个包到达网卡开始跟踪它的整个路径。&lt;/p>
&lt;h2 id="51-网络设备子系统的初始化">5.1 网络设备子系统的初始化&lt;/h2>
&lt;p>网络设备（netdev）的初始化在 &lt;code>net_dev_init&lt;/code>，里面有些东西很有意思。&lt;/p>
&lt;h3 id="struct-softnet_data-变量初始化">&lt;code>struct softnet_data&lt;/code> 变量初始化&lt;/h3>
&lt;p>&lt;code>net_dev_init&lt;/code> 为每个 CPU 创建一个 &lt;code>struct softnet_data&lt;/code> 变量。这些变量包含一些 指向重要信息的指针：&lt;/p>
&lt;ul>
&lt;li>需要注册到这个 CPU 的 NAPI 变量列表&lt;/li>
&lt;li>数据处理 backlog&lt;/li>
&lt;li>处理权重&lt;/li>
&lt;li>receive offload 变量列表&lt;/li>
&lt;li>receive packet steering 设置&lt;/li>
&lt;/ul>
&lt;p>接下来随着逐步进入网络栈，我们会一一查看这些功能。&lt;/p>
&lt;h3 id="softirq-handler-初始化">SoftIRQ Handler 初始化&lt;/h3>
&lt;p>&lt;code>net_dev_init&lt;/code> 分别为接收和发送数据注册了一个软中断处理函数。&lt;/p>
&lt;pre>&lt;code>static int __init net_dev_init(void)
{
/* ... */
open_softirq(NET_TX_SOFTIRQ, net_tx_action);
open_softirq(NET_RX_SOFTIRQ, net_rx_action);
/* ... */
}
&lt;/code>&lt;/pre>
&lt;p>后面会看到驱动的中断处理函数是如何触发 &lt;code>net_rx_action&lt;/code> 这个为 &lt;code>NET_RX_SOFTIRQ&lt;/code> 软中断注册的处理函数的。&lt;/p>
&lt;h2 id="52-数据来了">5.2 数据来了&lt;/h2>
&lt;p>终于，网络数据来了！
如果 RX 队列有足够的描述符（descriptors），包会&lt;strong>通过 DMA 写到 RAM&lt;/strong>。设备然后发 起对应于它的中断（或者在 MSI-X 的场景，中断和包达到的 RX 队列绑定）。&lt;/p>
&lt;h3 id="521-中断处理函数">5.2.1 中断处理函数&lt;/h3>
&lt;p>一般来说，中断处理函数应该将尽可能多的处理逻辑移出（到软中断），这至关重要，因为 发起一个中断后，其他的中断就会被屏蔽。
我们来看一下 MSI-X 中断处理函数的代码，它展示了中断处理函数是如何尽量简单的。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">igb_main.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>static irqreturn_t igb_msix_ring(int irq, void *data)
{
struct igb_q_vector *q_vector = data;
/* Write the ITR value calculated from the previous interrupt. */
igb_write_itr(q_vector);
napi_schedule(&amp;amp;q_vector-&amp;gt;napi);
return IRQ_HANDLED;
}
&lt;/code>&lt;/pre>
&lt;p>这个中断处理函数非常简短，只做了 2 个很快的操作就返回了。
首先，它调用 &lt;code>igb_write_itr&lt;/code> 更新一个硬件寄存器。对这个例子，这个寄存器是记录硬件 中断频率的。
这个寄存器和一个叫 &lt;strong>“Interrupt Throttling”（也叫 “Interrupt Coalescing”）的硬件 特性&lt;/strong>相关，这个特性可以平滑传送到 CPU 的中断数量。我们接下来会看到，ethtool 是 怎么样提供了一个机制用于&lt;strong>调整 IRQ 触发频率&lt;/strong>的。
第二，触发 &lt;code>napi_schedule&lt;/code>，如果 NAPI 的处理循环还没开始的话，这会唤醒它。注意， 这个处理循环是在软中断中执行的，而不是硬中断。
这段代码展示了硬中断尽量简短为何如此重要；为我们接下来理解多核 CPU 的接收逻辑很有 帮助。&lt;/p>
&lt;h3 id="522-napi-和-napi_schedule">5.2.2 NAPI 和 &lt;code>napi_schedule&lt;/code>&lt;/h3>
&lt;p>接下来看从硬件中断中调用的 &lt;code>napi_schedule&lt;/code> 是如何工作的。
注意，NAPI 存在的意义是&lt;strong>无需硬件中断通知就可以接收网络数据&lt;/strong>。前面提到， NAPI 的轮询循环（poll loop）是受硬件中断触发而跑起来的。换句话说，NAPI 功能启用了 ，但是默认是没有工作的，直到第一个包到达的时候，网卡触发的一个硬件将它唤醒。后面 会看到，也还有其他的情况，NAPI 功能也会被关闭，直到下一个硬中断再次将它唤起。
&lt;code>napi_schedule&lt;/code> 只是一个简单的封装，内层调用 &lt;code>__napi_schedule&lt;/code>。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/**
* __napi_schedule - schedule for receive
* @n: entry to schedule
*
* The entry's receive function will be scheduled to run
*/
void __napi_schedule(struct napi_struct *n)
{
unsigned long flags;
local_irq_save(flags);
____napi_schedule(&amp;amp;__get_cpu_var(softnet_data), n);
local_irq_restore(flags);
}
EXPORT_SYMBOL(__napi_schedule);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>__get_cpu_var&lt;/code> 用于获取属于这个 CPU 的 &lt;code>structure softnet_data&lt;/code> 变量。
&lt;code>____napi_schedule&lt;/code>, &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/* Called with irq disabled */
static inline void ____napi_schedule(struct softnet_data *sd,
struct napi_struct *napi)
{
list_add_tail(&amp;amp;napi-&amp;gt;poll_list, &amp;amp;sd-&amp;gt;poll_list);
__raise_softirq_irqoff(NET_RX_SOFTIRQ);
}
&lt;/code>&lt;/pre>
&lt;p>这段代码了做了两个重要的事情：&lt;/p>
&lt;ol>
&lt;li>将（从驱动的中断函数中传来的）&lt;code>napi_struct&lt;/code> 变量，添加到 poll list，后者 attach 到这个 CPU 上的 &lt;code>softnet_data&lt;/code>&lt;/li>
&lt;li>&lt;code>__raise_softirq_irqoff&lt;/code> 触发一个 &lt;code>NET_RX_SOFTIRQ&lt;/code> 类型软中断。这会触发执行 &lt;code>net_rx_action&lt;/code>（如果没有正在执行），后者是网络设备初始化的时候注册的&lt;/li>
&lt;/ol>
&lt;p>接下来会看到，软中断处理函数 &lt;code>net_rx_action&lt;/code> 会调用 NAPI 的 poll 函数来收包。&lt;/p>
&lt;h3 id="523-关于-cpu-和网络数据处理的一点笔记">5.2.3 关于 CPU 和网络数据处理的一点笔记&lt;/h3>
&lt;p>注意到目前为止，我们从硬中断处理函数中转移到软中断处理函数的逻辑，都是使用的本 CPU 变量。
驱动的硬中断处理函数做的事情很少，但软中断将会在和硬中断相同的 CPU 上执行。&lt;strong>这就 是为什么给每个 CPU 一个特定的硬中断非常重要：这个 CPU 不仅处理这个硬中断，而且通 过 NAPI 处理接下来的软中断来收包&lt;/strong>。
后面我们会看到，Receive Packet Steering 可以将软中断分给其他 CPU。&lt;/p>
&lt;h3 id="524-监控网络数据到达">5.2.4 监控网络数据到达&lt;/h3>
&lt;h4 id="硬中断请求">硬中断请求&lt;/h4>
&lt;p>注意：由于某些驱动在 NAPI 运行时会关闭硬中断，因此只监控硬中断无法得到网络处理健 康状况的全景试图，硬中断监控只是整个监控方案的重要组成部分。
读取硬中断统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/interrupts
CPU0 CPU1 CPU2 CPU3
0: 46 0 0 0 IR-IO-APIC-edge timer
1: 3 0 0 0 IR-IO-APIC-edge i8042
30: 3361234770 0 0 0 IR-IO-APIC-fasteoi aacraid
64: 0 0 0 0 DMAR_MSI-edge dmar0
65: 1 0 0 0 IR-PCI-MSI-edge eth0
66: 863649703 0 0 0 IR-PCI-MSI-edge eth0-TxRx-0
67: 986285573 0 0 0 IR-PCI-MSI-edge eth0-TxRx-1
68: 45 0 0 0 IR-PCI-MSI-edge eth0-TxRx-2
69: 394 0 0 0 IR-PCI-MSI-edge eth0-TxRx-3
NMI: 9729927 4008190 3068645 3375402 Non-maskable interrupts
LOC: 2913290785 1585321306 1495872829 1803524526 Local timer interrupts
&lt;/code>&lt;/pre>
&lt;p>可以看到有多少包进来、硬件中断频率，RX 队列被哪个 CPU 处理等信息。这里只能看到硬中 断数量，不能看出实际多少数据被接收或处理，因为一些驱动在 NAPI 收包时会关闭硬中断。 进一步，使用 Interrupt Coalescing 时也会影响这个统计。监控这个指标能帮你判断出你设 置的 Interrupt Coalescing 是不是在工作。
为了使监控更加完整，需要同时监控 &lt;code>/proc/softirqs&lt;/code> (前面提到)和 &lt;code>/proc&lt;/code>。&lt;/p>
&lt;h3 id="525-数据接收调优">5.2.5 数据接收调优&lt;/h3>
&lt;h4 id="中断合并interrupt-coalescing">中断合并（Interrupt coalescing）&lt;/h4>
&lt;p>中断合并会将多个中断事件放到一起，累积到一定阈值后才向 CPU 发起中断请求。
这可以防止&lt;strong>中断风暴&lt;/strong>，提升吞吐，降低 CPU 使用量，但延迟也变大；中断数量过多则相反。
历史上，早期的 igb、e1000 版本，以及其他的都包含一个叫 InterruptThrottleRate 参数， 最近的版本已经被 ethtool 可配置的参数取代。&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off TX: off
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0
...
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ethtool&lt;/code> 提供了用于中断合并相关的通用接口。但切记，不是所有的设备都支持完整的配 置。你需要查看驱动文档或代码来确定哪些支持，哪些不支持。ethtool 的文档说：&lt;strong>“ 驱动没有实现的接口将会被静默忽略”&lt;/strong>。
某些驱动支持一个有趣的特性：“自适应 RX/TX 硬中断合并”。这个特性一般是在硬件实现的 。驱动通常需要做一些额外的工作来告诉网卡需要打开这个特性（前面的 igb 驱动代码里有 涉及）。
自适应 RX/TX 硬中断合并带来的效果是：带宽比较低时降低延迟，带宽比较高时提升吞吐。
用 &lt;code>ethtool -C&lt;/code> 打开自适应 RX IRQ 合并：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -C eth0 adaptive-rx on
&lt;/code>&lt;/pre>
&lt;p>还可以用 &lt;code>ethtool -C&lt;/code> 更改其他配置。常用的包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>rx-usecs&lt;/code>: How many usecs to delay an RX interrupt after a packet arrives.&lt;/li>
&lt;li>&lt;code>rx-frames&lt;/code>: Maximum number of data frames to receive before an RX interrupt.&lt;/li>
&lt;li>&lt;code>rx-usecs-irq&lt;/code>: How many usecs to delay an RX interrupt while an interrupt is being serviced by the host.&lt;/li>
&lt;li>&lt;code>rx-frames-irq&lt;/code>: Maximum number of data frames to receive before an RX interrupt is generated while the system is servicing an interrupt.&lt;/li>
&lt;/ul>
&lt;p>请注意你的硬件可能只支持以上列表的一个子集，具体请参考相应的驱动说明或源码。
不幸的是，通常并没有一个很好的文档来说明这些选项，最全的文档很可能是头文件。每个选项的解释见 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/ethtool.h#L184-L255">include/uapi/linux/ethtool.h&lt;/a> 。
注意：虽然硬中断合并看起来是个不错的优化项，但需要网络栈的其他一些 部分做针对性调整。只合并硬中断很可能并不会带来多少收益。&lt;/p>
&lt;h4 id="调整硬中断亲和性irq-affinities">调整硬中断亲和性（IRQ affinities）&lt;/h4>
&lt;p>If your NIC supports RSS / multiqueue or if you are attempting to optimize for data locality, you may wish to use a specific set of CPUs for handling interrupts generated by your NIC.
Setting specific CPUs allows you to segment which CPUs will be used for processing which IRQs. These changes may affect how upper layers operate, as we’ve seen for the networking stack.
If you do decide to adjust your IRQ affinities, you should first check if you running the irqbalance daemon. This daemon tries to automatically balance IRQs to CPUs and it may overwrite your settings. If you are running irqbalance, you should either disable irqbalance or use the –banirq in conjunction with IRQBALANCE_BANNED_CPUS to let irqbalance know that it shouldn’t touch a set of IRQs and CPUs that you want to assign yourself.
Next, you should check the file /proc/interrupts for a list of the IRQ numbers for each network RX queue for your NIC.
Finally, you can adjust the which CPUs each of those IRQs will be handled by modifying /proc/irq/IRQ_NUMBER/smp_affinity for each IRQ number.
You simply write a hexadecimal bitmask to this file to instruct the kernel which CPUs it should use for handling the IRQ.
Example: Set the IRQ affinity for IRQ 8 to CPU 0&lt;/p>
&lt;pre>&lt;code>$ sudo bash -c 'echo 1 &amp;gt; /proc/irq/8/smp_affinity'
&lt;/code>&lt;/pre>
&lt;h2 id="53-网络数据处理开始">5.3 网络数据处理：开始&lt;/h2>
&lt;p>一旦软中断代码判断出有 softirq 处于 pending 状态，就会开始处理，&lt;strong>执行 &lt;code>net_rx_action&lt;/code>，网络数据处理就此开始&lt;/strong>。
我们来看一下 &lt;code>net_rx_action&lt;/code> 的循环部分，理解它是如何工作的。哪个部分可以调优， 哪个可以监控。&lt;/p>
&lt;h3 id="531-net_rx_action-处理循环">5.3.1 &lt;code>net_rx_action&lt;/code> 处理循环&lt;/h3>
&lt;p>&lt;strong>&lt;code>net_rx_action&lt;/code> 从包所在的内存开始处理，包是被设备通过 DMA 直接送到内存的。&lt;/strong> 函数遍历本 CPU 队列的 NAPI 变量列表，依次出队并操作之。处理逻辑考虑任务量（work ）和执行时间两个因素：&lt;/p>
&lt;ol>
&lt;li>跟踪记录工作量预算（work budget），预算可以调整&lt;/li>
&lt;li>记录消耗的时间&lt;/li>
&lt;/ol>
&lt;p>&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4380-L4383">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>while (!list_empty(&amp;amp;sd-&amp;gt;poll_list)) {
struct napi_struct *n;
int work, weight;
/* If softirq window is exhausted then punt.
* Allow this to run for 2 jiffies since which will allow
* an average latency of 1.5/HZ.
*/
if (unlikely(budget &amp;lt;= 0 || time_after_eq(jiffies, time_limit)))
goto softnet_break;
&lt;/code>&lt;/pre>
&lt;p>这里可以看到内核是如何防止处理数据包过程霸占整个 CPU 的，其中 budget 是该 CPU 的 所有 NAPI 变量的总预算。这也是多队列网卡应该精心调整 IRQ Affinity 的原因。回忆前 面讲的，&lt;strong>处理硬中断的 CPU 接下来会处理相应的软中断&lt;/strong>，进而执行上面包含 budget 的 这段逻辑。
多网卡多队列可能会出现这样的情况：多个 NAPI 变量注册到同一个 CPU 上。每个 CPU 上 的所有 NAPI 变量共享一份 budget。
如果没有足够的 CPU 来分散网卡硬中断，可以考虑增加 &lt;code>net_rx_action&lt;/code> 允许每个 CPU 处理更多包。增加 budget 可以增加 CPU 使用量（&lt;code>top&lt;/code> 等命令看到的 &lt;code>sitime&lt;/code> 或 &lt;code>si&lt;/code> 部分），但可以减少延迟，因为数据处理更加及时。
Note: the CPU will still be bounded by a time limit of 2 jiffies, regardless of the assigned budget.&lt;/p>
&lt;h3 id="532-napi-poll-函数及权重">5.3.2 NAPI poll 函数及权重&lt;/h3>
&lt;p>回忆前面，网络设备驱动使用 &lt;code>netif_napi_add&lt;/code> 注册 poll 方法，&lt;code>igb&lt;/code> 驱动有如下代码：&lt;/p>
&lt;pre>&lt;code>/* initialize NAPI */
netif_napi_add(adapter-&amp;gt;netdev, &amp;amp;q_vector-&amp;gt;napi, igb_poll, 64);
&lt;/code>&lt;/pre>
&lt;p>这注册了一个 NAPI 变量，hardcode 64 的权重。我们来看在 &lt;code>net_rx_action&lt;/code> 处理循环 中这个值是如何使用的。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4322-L4338">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>weight = n-&amp;gt;weight;
work = 0;
if (test_bit(NAPI_STATE_SCHED, &amp;amp;n-&amp;gt;state)) {
work = n-&amp;gt;poll(n, weight);
trace_napi_poll(n);
}
WARN_ON_ONCE(work &amp;gt; weight);
budget -= work;
&lt;/code>&lt;/pre>
&lt;p>其中的 &lt;code>n&lt;/code> 是 &lt;code>struct napi&lt;/code> 的变量。其中的 &lt;code>poll&lt;/code> 指向 &lt;code>igb_poll&lt;/code>。&lt;code>poll()&lt;/code> 返回 处理的数据帧数量，budget 会减去这个值。
所以，假设驱动使用 weight 值 64（Linux 3.13.0 的所有驱动都是 hardcode 这个值） ，设置 budget 默认值 300，那系统将在如下条件之一停止数据处理：&lt;/p>
&lt;ol>
&lt;li>&lt;code>igb_poll&lt;/code> 函数被调用了最多 5 次（如果没有数据需要处理，那次数就会很少）&lt;/li>
&lt;li>时间经过了至少 2 个 jiffies&lt;/li>
&lt;/ol>
&lt;h3 id="533-napi-和设备驱动的合约contract">5.3.3 NAPI 和设备驱动的合约（contract）&lt;/h3>
&lt;p>NAPI 子系统和设备驱动之间的合约，最重要的一点是关闭 NAPI 的条件。具体如下：&lt;/p>
&lt;ol>
&lt;li>如果驱动的 &lt;code>poll&lt;/code> 方法用完了它的全部 weight（默认 hardcode 64），那 它&lt;strong>不要更改&lt;/strong> NAPI 状态。接下来 &lt;code>net_rx_action&lt;/code> loop 会做的&lt;/li>
&lt;li>如果驱动的 &lt;code>poll&lt;/code> 方法没有用完全部 weight，那它&lt;strong>必须关闭&lt;/strong> NAPI。下次有硬件 中断触发，驱动的硬件处理函数调用 &lt;code>napi_schedule&lt;/code> 时，NAPI 会被重新打开&lt;/li>
&lt;/ol>
&lt;p>接下来先看 &lt;code>net_rx_action&lt;/code> 如何处理合约的第一部分，然后看 &lt;code>poll&lt;/code> 方法如何处理第 二部分。&lt;/p>
&lt;h3 id="534-finishing-the-net_rx_action-loop">5.3.4 Finishing the &lt;code>net_rx_action&lt;/code> loop&lt;/h3>
&lt;p>&lt;code>net_rx_action&lt;/code> 循环的最后一部分代码处理前面提到的 &lt;strong>NAPI 合约的第一部分&lt;/strong>。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4342-L4363">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/* Drivers must not modify the NAPI state if they
* consume the entire weight. In such cases this code
* still &amp;quot;owns&amp;quot; the NAPI instance and therefore can
* move the instance around on the list at-will.
*/
if (unlikely(work == weight)) {
if (unlikely(napi_disable_pending(n))) {
local_irq_enable();
napi_complete(n);
local_irq_disable();
} else {
if (n-&amp;gt;gro_list) {
/* flush too old packets
* If HZ &amp;lt; 1000, flush all packets.
*/
local_irq_enable();
napi_gro_flush(n, HZ &amp;gt;= 1000);
local_irq_disable();
}
list_move_tail(&amp;amp;n-&amp;gt;poll_list, &amp;amp;sd-&amp;gt;poll_list);
}
}
&lt;/code>&lt;/pre>
&lt;p>如果全部 &lt;code>work&lt;/code> 都用完了，&lt;code>net_rx_action&lt;/code> 会面临两种情况：&lt;/p>
&lt;ol>
&lt;li>网络设备需要关闭（例如，用户敲了 &lt;code>ifconfig eth0 down&lt;/code> 命令）&lt;/li>
&lt;li>如果设备不需要关闭，那检查是否有 GRO（后面会介绍）列表。如果时钟 tick rate &lt;code>&amp;gt;= 1000&lt;/code>，所有最近被更新的 GRO network flow 都会被 flush。将这个 NAPI 变量移 到 list 末尾，这个循环下次再进入时，处理的就是下一个 NAPI 变量&lt;/li>
&lt;/ol>
&lt;p>这就是包处理循环如何唤醒驱动注册的 &lt;code>poll&lt;/code> 方法进行包处理的过程。接下来会看到， &lt;code>poll&lt;/code> 方法会收割网络数据，发送到上层栈进行处理。&lt;/p>
&lt;h3 id="535-到达-limit-时退出循环">5.3.5 到达 limit 时退出循环&lt;/h3>
&lt;p>&lt;code>net_rx_action&lt;/code> 下列条件之一退出循环：&lt;/p>
&lt;ol>
&lt;li>这个 CPU 上注册的 poll 列表已经没有 NAPI 变量需要处理(&lt;code>!list_empty(&amp;amp;sd-&amp;gt;poll_list)&lt;/code>)&lt;/li>
&lt;li>剩余的 &lt;code>budget &amp;lt;= 0&lt;/code>&lt;/li>
&lt;li>已经满足 2 个 jiffies 的时间限制&lt;/li>
&lt;/ol>
&lt;p>代码：&lt;/p>
&lt;pre>&lt;code>/* If softirq window is exhausted then punt.
* Allow this to run for 2 jiffies since which will allow
* an average latency of 1.5/HZ.
*/
if (unlikely(budget &amp;lt;= 0 || time_after_eq(jiffies, time_limit)))
goto softnet_break;
&lt;/code>&lt;/pre>
&lt;p>如果跟踪 &lt;code>softnet_break&lt;/code>，会发现很有意思的东西：
From net/core/dev.c:&lt;/p>
&lt;pre>&lt;code>softnet_break:
sd-&amp;gt;time_squeeze++;
__raise_softirq_irqoff(NET_RX_SOFTIRQ);
goto out;
&lt;/code>&lt;/pre>
&lt;p>&lt;code>softnet_data&lt;/code> 变量更新统计信息，软中断的 &lt;code>NET_RX_SOFTIRQ&lt;/code> 被关闭。
&lt;code>time_squeeze&lt;/code> 字段记录的是满足如下条件的次数：&lt;code>net_rx_action&lt;/code> 有很多 &lt;code>work&lt;/code> 要做但 是 budget 用完了，或者 work 还没做完但时间限制到了。这对理解网络处理的瓶颈至关重要 。我们后面会看到如何监控这个值。关闭 &lt;code>NET_RX_SOFTIRQ&lt;/code> 是为了释放 CPU 时间给其他任务 用。这行代码是有意义的，因为只有我们有更多工作要做（还没做完）的时候才会执行到这里， 我们主动让出 CPU，不想独占太久。
然后执行到了 &lt;code>out&lt;/code> 标签所在的代码。另外还有一种条件也会跳转到 &lt;code>out&lt;/code> 标签：所有 NAPI 变量都处理完了，换言之，budget 数量大于网络包数量，所有驱动都已经关闭 NAPI ，没有什么事情需要 &lt;code>net_rx_action&lt;/code> 做了。
&lt;code>out&lt;/code> 代码段在从 &lt;code>net_rx_action&lt;/code> 返回之前做了一件重要的事情：调用 &lt;code>net_rps_action_and_irq_enable&lt;/code>。Receive Packet Steering 功能打开时这个函数 有重要作用：唤醒其他 CPU 处理网络包。
我们后面会看到 RPS 是如何工作的。现在先看看怎样监控 &lt;code>net_rx_action&lt;/code> 处理循环的 健康状态，以及进入 NAPI &lt;code>poll&lt;/code> 的内部，这样才能更好的理解网络栈。&lt;/p>
&lt;h3 id="536-napi-poll">5.3.6 NAPI &lt;code>poll&lt;/code>&lt;/h3>
&lt;p>回忆前文，驱动程序会分配一段内存用于 DMA，将数据包写到内存。就像这段内存是由驱动 程序分配的一样，驱动程序也负责解绑（unmap）这些内存，读取数据，将数据送到网络栈 。
我们看下 &lt;code>igb&lt;/code> 驱动如何实现这一过程的。&lt;/p>
&lt;h4 id="igb_poll">&lt;code>igb_poll&lt;/code>&lt;/h4>
&lt;p>可以看到 &lt;code>igb_poll&lt;/code> 代码其实相当简单。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018">drivers/net/ethernet/intel/igb/igb_main.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/**
* igb_poll - NAPI Rx polling callback
* @napi: napi polling structure
* @budget: count of how many packets we should handle
**/
static int igb_poll(struct napi_struct *napi, int budget)
{
struct igb_q_vector *q_vector = container_of(napi,
struct igb_q_vector,
napi);
bool clean_complete = true;
#ifdef CONFIG_IGB_DCA
if (q_vector-&amp;gt;adapter-&amp;gt;flags &amp;amp; IGB_FLAG_DCA_ENABLED)
igb_update_dca(q_vector);
#endif
/* ... */
if (q_vector-&amp;gt;rx.ring)
clean_complete &amp;amp;= igb_clean_rx_irq(q_vector, budget);
/* If all work not completed, return budget and keep polling */
if (!clean_complete)
return budget;
/* If not enough Rx work done, exit the polling mode */
napi_complete(napi);
igb_ring_irq_enable(q_vector);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>几件有意思的事情：&lt;/p>
&lt;ul>
&lt;li>如果内核 &lt;a href="https://lwn.net/Articles/247493/">DCA&lt;/a>（Direct Cache Access）功能打 开了，CPU 缓存是热的，对 RX ring 的访问会命中 CPU cache。更多 DCA 信息见本文 “ Extra” 部分&lt;/li>
&lt;li>然后执行 &lt;code>igb_clean_rx_irq&lt;/code>，这里做的事情非常多，我们后面看&lt;/li>
&lt;li>然后执行 &lt;code>clean_complete&lt;/code>，判断是否仍然有 work 可以做。如果有，就返回 budget（ 回忆，这里是 hardcode 64）。在之前我们已经看到，&lt;code>net_rx_action&lt;/code> 会将这个 NAPI 变量移动到 poll 列表的末尾&lt;/li>
&lt;li>如果所有 &lt;code>work&lt;/code> 都已经完成，驱动通过调用 &lt;code>napi_complete&lt;/code> 关闭 NAPI，并通过调用 &lt;code>igb_ring_irq_enable&lt;/code> 重新进入可中断状态。下次中断到来的时候回重新打开 NAPI&lt;/li>
&lt;/ul>
&lt;p>我们来看 &lt;code>igb_clean_rx_irq&lt;/code> 如何将网络数据送到网络栈。&lt;/p>
&lt;h4 id="igb_clean_rx_irq">&lt;code>igb_clean_rx_irq&lt;/code>&lt;/h4>
&lt;p>&lt;code>igb_clean_rx_irq&lt;/code> 方法是一个循环，每次处理一个包，直到 budget 用完，或者没有数 据需要处理了。
做的几件重要事情：&lt;/p>
&lt;ol>
&lt;li>分配额外的 buffer 用于接收数据，因为已经用过的 buffer 被 clean out 了。一次分配 &lt;code>IGB_RX_BUFFER_WRITE (16)&lt;/code>个。&lt;/li>
&lt;li>从 RX 队列取一个 buffer，保存到一个 &lt;code>skb&lt;/code> 类型的变量中&lt;/li>
&lt;li>判断这个 buffer 是不是一个包的最后一个 buffer。如果是，继续处理；如果不是，继续 从 buffer 列表中拿出下一个 buffer，加到 skb。当数据帧的大小比一个 buffer 大的时候， 会出现这种情况&lt;/li>
&lt;li>验证数据的 layout 和头信息是正确的&lt;/li>
&lt;li>更新 &lt;code>skb-&amp;gt;len&lt;/code>，表示这个包已经处理的字节数&lt;/li>
&lt;li>设置 &lt;code>skb&lt;/code> 的 hash, checksum, timestamp, VLAN id, protocol 字段。hash， checksum，timestamp，VLAN ID 信息是硬件提供的，如果硬件报告 checksum error， &lt;code>csum_error&lt;/code> 统计就会增加。如果 checksum 通过了，数据是 UDP 或者 TCP 数据，&lt;code>skb&lt;/code> 就会 被标记成 &lt;code>CHECKSUM_UNNECESSARY&lt;/code>&lt;/li>
&lt;li>构建的 skb 经 &lt;code>napi_gro_receive()&lt;/code>进入协议栈&lt;/li>
&lt;li>更新处理过的包的统计信息&lt;/li>
&lt;li>循环直至处理的包数量达到 budget&lt;/li>
&lt;/ol>
&lt;p>循环结束的时候，这个函数设置收包的数量和字节数统计信息。
接下来在进入协议栈之前，我们先开两个小差：首先是看一些如何监控和调优软中断，其次 是介绍 GRO。有了这个两个背景，后面（通过 &lt;code>napi_gro_receive&lt;/code> 进入）协议栈部分会更容易理解。&lt;/p>
&lt;h3 id="537-监控网络数据处理">5.3.7 监控网络数据处理&lt;/h3>
&lt;h4 id="procnetsoftnet_stat">&lt;code>/proc/net/softnet_stat&lt;/code>&lt;/h4>
&lt;p>前面看到，如果 budget 或者 time limit 到了而仍有包需要处理，那 &lt;code>net_rx_action&lt;/code> 在退出 循环之前会更新统计信息。这个信息存储在该 CPU 的 &lt;code>struct softnet_data&lt;/code> 变量中。
这些统计信息打到了&lt;code>/proc/net/softnet_stat&lt;/code>，但不幸的是，关于这个的文档很少。每一 列代表什么并没有标题，而且列的内容会随着内核版本可能发生变化。
在内核 3.13.0 中，你可以阅读内核源码，查看每一列分别对应什么。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/net-procfs.c#L161-L165">net/core/net-procfs.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>seq_printf(seq,
&amp;quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&amp;quot;,
sd-&amp;gt;processed, sd-&amp;gt;dropped, sd-&amp;gt;time_squeeze, 0,
0, 0, 0, 0, /* was fastroute */
sd-&amp;gt;cpu_collision, sd-&amp;gt;received_rps, flow_limit_count);
&lt;/code>&lt;/pre>
&lt;p>其中一些的名字让人很困惑，而且在你意想不到的地方更新。在接下来的网络栈分析说，我 们会举例说明其中一些字段是何时、在哪里被更新的。前面我们已经看到了 &lt;code>squeeze_time&lt;/code> 是在 &lt;code>net_rx_action&lt;/code> 在被更新的，到此时，如下数据你应该能看懂了：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/softnet_stat
6dcad223 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6f0e1565 00000000 00000002 00000000 00000000 00000000 00000000 00000000 00000000 00000000
660774ec 00000000 00000003 00000000 00000000 00000000 00000000 00000000 00000000 00000000
61c99331 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6794b1b3 00000000 00000005 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6488cb92 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
&lt;/code>&lt;/pre>
&lt;p>关于&lt;code>/proc/net/softnet_stat&lt;/code> 的重要细节:&lt;/p>
&lt;ol>
&lt;li>每一行代表一个 &lt;code>struct softnet_data&lt;/code> 变量。因为每个 CPU 只有一个该变量，所以每行 其实代表一个 CPU&lt;/li>
&lt;li>每列用空格隔开，数值用 16 进制表示&lt;/li>
&lt;li>第一列 &lt;code>sd-&amp;gt;processed&lt;/code>，是处理的网络帧的数量。如果你使用了 ethernet bonding， 那这个值会大于总的网络帧的数量，因为 ethernet bonding 驱动有时会触发网络数据被 重新处理（re-processed）&lt;/li>
&lt;li>第二列，&lt;code>sd-&amp;gt;dropped&lt;/code>，是因为处理不过来而 drop 的网络帧数量。后面会展开这一话题&lt;/li>
&lt;li>第三列，&lt;code>sd-&amp;gt;time_squeeze&lt;/code>，前面介绍过了，由于 budget 或 time limit 用完而退出 &lt;code>net_rx_action&lt;/code> 循环的次数&lt;/li>
&lt;li>接下来的 5 列全是 0&lt;/li>
&lt;li>第九列，&lt;code>sd-&amp;gt;cpu_collision&lt;/code>，是为了发送包而获取锁的时候有冲突的次数&lt;/li>
&lt;li>第十列，&lt;code>sd-&amp;gt;received_rps&lt;/code>，是这个 CPU 被其他 CPU 唤醒去收包的次数&lt;/li>
&lt;li>最后一列，&lt;code>flow_limit_count&lt;/code>，是达到 flow limit 的次数。flow limit 是 RPS 的特性， 后面会稍微介绍一下&lt;/li>
&lt;/ol>
&lt;p>如果你要画图监控这些数据，确保你的列和相应的字段是对的上的，最保险的方式是阅读相 应版本的内核代码。&lt;/p>
&lt;h3 id="538-网络数据处理调优">5.3.8 网络数据处理调优&lt;/h3>
&lt;h4 id="调整-net_rx_action-budget">调整 &lt;code>net_rx_action&lt;/code> budget&lt;/h4>
&lt;p>&lt;code>net_rx_action&lt;/code> budget 表示一个 CPU 单次轮询（&lt;code>poll&lt;/code>）所允许的最大收包数量。单次 poll 收包时，所有注册到这个 CPU 的 NAPI 变量收包数量之和不能大于这个阈值。 调整：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.netdev_budget=600
&lt;/code>&lt;/pre>
&lt;p>如果要保证重启仍然生效，需要将这个配置写到&lt;code>/etc/sysctl.conf&lt;/code>。
Linux 3.13.0 的默认配置是 300。&lt;/p>
&lt;h2 id="54-grogeneric-receive-offloading">5.4 GRO（Generic Receive Offloading）&lt;/h2>
&lt;p>&lt;strong>Large Receive Offloading (LRO) 是一个硬件优化，GRO 是 LRO 的一种软件实现。&lt;/strong>
两种方案的主要思想都是：&lt;strong>通过合并“足够类似”的包来减少传送给网络栈的包数，这有 助于减少 CPU 的使用量&lt;/strong>。例如，考虑大文件传输的场景，包的数量非常多，大部分包都是一 段文件数据。相比于每次都将小包送到网络栈，可以将收到的小包合并成一个很大的包再送 到网络栈。GRO &lt;strong>使协议层只需处理一个 header&lt;/strong>，而将包含大量数据的整个大包送到用 户程序。
这类优化方式的缺点是 &lt;strong>信息丢失&lt;/strong>：包的 option 或者 flag 信息在合并时会丢 失。这也是为什么大部分人不使用或不推荐使用 LRO 的原因。
LRO 的实现，一般来说，对合并包的规则非常宽松。GRO 是 LRO 的软件实现，但是对于包合并 的规则更严苛。
顺便说一下，&lt;strong>如果用 tcpdump 抓包，有时会看到机器收到了看起来不现实的、非常大的包&lt;/strong>， 这很可能是你的系统开启了 GRO。接下来会看到，&lt;strong>tcpdump 的抓包点（捕获包的 tap ）在整个栈的更后面一些，在 GRO 之后&lt;/strong>。&lt;/p>
&lt;h3 id="使用-ethtool-修改-gro-配置">使用 ethtool 修改 GRO 配置&lt;/h3>
&lt;p>&lt;code>-k&lt;/code> 查看 GRO 配置：&lt;/p>
&lt;pre>&lt;code>$ ethtool -k eth0 | grep generic-receive-offload
generic-receive-offload: on
&lt;/code>&lt;/pre>
&lt;p>&lt;code>-K&lt;/code> 修改 GRO 配置：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -K eth0 gro on
&lt;/code>&lt;/pre>
&lt;p>注意：对于大部分驱动，修改 GRO 配置会涉及先 down 再 up 这个网卡，因此这个网卡上的连接 都会中断。&lt;/p>
&lt;h2 id="55-napi_gro_receive">5.5 &lt;code>napi_gro_receive&lt;/code>&lt;/h2>
&lt;p>如果开启了 GRO，&lt;code>napi_gro_receive&lt;/code> 将负责处理网络数据，并将数据送到协议栈，大部分 相关的逻辑在函数 &lt;code>dev_gro_receive&lt;/code> 里实现。&lt;/p>
&lt;h3 id="dev_gro_receive">&lt;code>dev_gro_receive&lt;/code>&lt;/h3>
&lt;p>这个函数首先检查 GRO 是否开启了，如果是，就准备做 GRO。GRO 首先遍历一个 offload filter 列表，如果高层协议认为其中一些数据属于 GRO 处理的范围，就会允许其对数据进行 操作。
协议层以此方式让网络设备层知道，这个 packet 是不是当前正在处理的一个需要做 GRO 的 network flow 的一部分，而且也可以通过这种方式传递一些协议相关的信息。例如，TCP 协 议需要判断是否以及合适应该将一个 ACK 包合并到其他包里。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3844-L3856">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>list_for_each_entry_rcu(ptype, head, list) {
if (ptype-&amp;gt;type != type || !ptype-&amp;gt;callbacks.gro_receive)
continue;
skb_set_network_header(skb, skb_gro_offset(skb));
skb_reset_mac_len(skb);
NAPI_GRO_CB(skb)-&amp;gt;same_flow = 0;
NAPI_GRO_CB(skb)-&amp;gt;flush = 0;
NAPI_GRO_CB(skb)-&amp;gt;free = 0;
pp = ptype-&amp;gt;callbacks.gro_receive(&amp;amp;napi-&amp;gt;gro_list, skb);
break;
}
&lt;/code>&lt;/pre>
&lt;p>如果协议层提示是时候 flush GRO packet 了，那就到下一步处理了。这发生在 &lt;code>napi_gro_complete&lt;/code>，会进一步调用相应协议的 &lt;code>gro_complete&lt;/code> 回调方法，然后调用 &lt;code>netif_receive_skb&lt;/code> 将包送到协议栈。 这个过程见&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3862-L3872">net/core/dev.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>if (pp) {
struct sk_buff *nskb = *pp;
*pp = nskb-&amp;gt;next;
nskb-&amp;gt;next = NULL;
napi_gro_complete(nskb);
napi-&amp;gt;gro_count--;
}
&lt;/code>&lt;/pre>
&lt;p>接下来，如果协议层将这个包合并到一个已经存在的 flow，&lt;code>napi_gro_receive&lt;/code> 就没什么事 情需要做，因此就返回了。如果 packet 没有被合并，而且 GRO 的数量小于 &lt;code>MAX_GRO_SKBS&lt;/code>（ 默认是 8），就会创建一个新的 entry 加到本 CPU 的 NAPI 变量的 &lt;code>gro_list&lt;/code>。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3877-L3886">net/core/dev.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>if (NAPI_GRO_CB(skb)-&amp;gt;flush || napi-&amp;gt;gro_count &amp;gt;= MAX_GRO_SKBS)
goto normal;
napi-&amp;gt;gro_count++;
NAPI_GRO_CB(skb)-&amp;gt;count = 1;
NAPI_GRO_CB(skb)-&amp;gt;age = jiffies;
skb_shinfo(skb)-&amp;gt;gso_size = skb_gro_len(skb);
skb-&amp;gt;next = napi-&amp;gt;gro_list;
napi-&amp;gt;gro_list = skb;
ret = GRO_HELD;
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>这就是 Linux 网络栈中 GRO 的工作原理。&lt;/strong>&lt;/p>
&lt;h2 id="56-napi_skb_finish">5.6 &lt;code>napi_skb_finish&lt;/code>&lt;/h2>
&lt;p>一旦 &lt;code>dev_gro_receive&lt;/code> 完成，&lt;code>napi_skb_finish&lt;/code> 就会被调用，其如果一个 packet 被合并了 ，就释放不用的变量；或者调用 &lt;code>netif_receive_skb&lt;/code> 将数据发送到网络协议栈（因为已经 有 &lt;code>MAX_GRO_SKBS&lt;/code> 个 flow 了，够 GRO 了）。
接下来，是看 &lt;code>netif_receive_skb&lt;/code> 如何将数据交给协议层。但在此之前，我们先看一下 RPS。&lt;/p>
&lt;h1 id="6-rps-receive-packet-steering">6 RPS (Receive Packet Steering)&lt;/h1>
&lt;p>回忆前面我们讨论了网络设备驱动是如何注册 NAPI &lt;code>poll&lt;/code> 方法的。每个 NAPI 变量都会运 行在相应 CPU 的软中断的上下文中。而且，触发硬中断的这个 CPU 接下来会负责执行相应的软 中断处理函数来收包。
换言之，同一个 CPU 既处理硬中断，又处理相应的软中断。
一些网卡（例如 Intel I350）在硬件层支持多队列。这意味着收进来的包会被通过 DMA 放到 位于不同内存的队列上，而不同的队列有相应的 NAPI 变量管理软中断 &lt;code>poll()&lt;/code>过程。因此， 多个 CPU 同时处理从网卡来的中断，处理收包过程。
这个特性被称作 RSS（Receive Side Scaling，接收端扩展）。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222">RPS&lt;/a> （Receive Packet Steering，接收包控制，接收包引导）是 RSS 的一种软件实现。因为是软 件实现的，意味着任何网卡都可以使用这个功能，即便是那些只有一个接收队列的网卡。但 是，因为它是软件实现的，这意味着 RPS 只能在 packet 通过 DMA 进入内存后，RPS 才能开始工 作。
这意味着，RPS 并不会减少 CPU 处理硬件中断和 NAPI &lt;code>poll&lt;/code>（软中断最重要的一部分）的时 间，但是可以在 packet 到达内存后，将 packet 分到其他 CPU，从其他 CPU 进入协议栈。
RPS 的工作原理是对个 packet 做 hash，以此决定分到哪个 CPU 处理。然后 packet 放到每个 CPU 独占的接收后备队列（backlog）等待处理。这个 CPU 会触发一个进程间中断（ &lt;a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">IPI&lt;/a>，Inter-processor Interrupt）向对端 CPU。如果当时对端 CPU 没有在处理 backlog 队列收包，这个进程间中断会 触发它开始从 backlog 收包。&lt;code>/proc/net/softnet_stat&lt;/code> 其中有一列是记录 &lt;code>softnet_data&lt;/code> 变量（也即这个 CPU）收到了多少 IPI（&lt;code>received_rps&lt;/code> 列）。
因此，&lt;code>netif_receive_skb&lt;/code> 或者继续将包送到协议栈，或者交给 RPS，后者会转交给其他 CPU 处理。&lt;/p>
&lt;h2 id="rps-调优">RPS 调优&lt;/h2>
&lt;p>使用 RPS 需要在内核做配置（Ubuntu + Kernel 3.13.0 支持），而且需要一个掩码（ bitmask）指定哪些 CPU 可以处理那些 RX 队列。相关的一些信息可以在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L138-L164">内核文档  &lt;/a>里找到。
bitmask 配置位于：&lt;code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus&lt;/code>。
例如，对于 eth0 的 queue 0，你需要更改&lt;code>/sys/class/net/eth0/queues/rx-0/rps_cpus&lt;/code>。&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L160-L164">  内核文档  &lt;/a>里说，对一些特定的配置下，RPS 没必要了。
注意：打开 RPS 之后，原来不需要处理软中断（softirq）的 CPU 这时也会参与处理。因此相 应 CPU 的 &lt;code>NET_RX&lt;/code> 数量，以及 &lt;code>si&lt;/code> 或 &lt;code>sitime&lt;/code> 占比都会相应增加。你可以对比启用 RPS 前后的 数据，以此来确定你的配置是否生效，以及是否符合预期（哪个 CPU 处理哪个网卡的哪个中 断）。&lt;/p>
&lt;h1 id="7-rfs-receive-flow-steering">7 RFS (Receive Flow Steering)&lt;/h1>
&lt;p>RFS（Receive flow steering）和 RPS 配合使用。RPS 试图在 CPU 之间平衡收包，但是没考虑 数据的本地性问题，如何最大化 CPU 缓存的命中率。RFS 将属于相同 flow 的包送到相同的 CPU 进行处理，可以提高缓存命中率。&lt;/p>
&lt;h2 id="调优打开-rfs">调优：打开 RFS&lt;/h2>
&lt;p>RPS 记录一个全局的 hash table，包含所有 flow 的信息。这个 hash table 的大小可以在 &lt;code>net.core.rps_sock_flow_entries&lt;/code>：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.rps_sock_flow_entries=32768
&lt;/code>&lt;/pre>
&lt;p>其次，你可以设置每个 RX queue 的 flow 数量，对应着 &lt;code>rps_flow_cnt&lt;/code>：
例如，eth0 的 RX queue0 的 flow 数量调整到 2048：&lt;/p>
&lt;pre>&lt;code>$ sudo bash -c 'echo 2048 &amp;gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt'
&lt;/code>&lt;/pre>
&lt;h1 id="8-arfs-hardware-accelerated-rfs">8 aRFS (Hardware accelerated RFS)&lt;/h1>
&lt;p>RFS 可以用硬件加速，网卡和内核协同工作，判断哪个 flow 应该在哪个 CPU 上处理。这需要网 卡和网卡驱动的支持。
如果你的网卡驱动里对外提供一个 &lt;code>ndo_rx_flow_steer&lt;/code> 函数，那就是支持 RFS。&lt;/p>
&lt;h2 id="调优-启用-arfs">调优: 启用 aRFS&lt;/h2>
&lt;p>假如你的网卡支持 aRFS，你可以开启它并做如下配置：&lt;/p>
&lt;ul>
&lt;li>打开并配置 RPS&lt;/li>
&lt;li>打开并配置 RFS&lt;/li>
&lt;li>内核中编译期间指定了 &lt;code>CONFIG_RFS_ACCEL&lt;/code> 选项。Ubuntu kernel 3.13.0 是有的&lt;/li>
&lt;li>打开网卡的 ntuple 支持。可以用 ethtool 查看当前的 ntuple 设置&lt;/li>
&lt;li>配置 IRQ（硬中断）中每个 RX 和 CPU 的对应关系&lt;/li>
&lt;/ul>
&lt;p>以上配置完成后，aRFS 就会自动将 RX queue 数据移动到指定 CPU 的内存，每个 flow 的包都会 到达同一个 CPU，不需要你再通过 ntuple 手动指定每个 flow 的配置了。&lt;/p>
&lt;h1 id="9-从-netif_receive_skb-进入协议栈">9 从 &lt;code>netif_receive_skb&lt;/code> 进入协议栈&lt;/h1>
&lt;p>重新捡起我们前面已经几次提到过的 &lt;code>netif_receive_skb&lt;/code>，这个函数在好几个地方被调用 。两个最重要的地方（前面都看到过了）：&lt;/p>
&lt;ul>
&lt;li>&lt;code>napi_skb_finish&lt;/code>：当 packet 不需要被合并到已经存在的某个 GRO flow 的时候&lt;/li>
&lt;li>&lt;code>napi_gro_complete&lt;/code>：协议层提示需要 flush 当前的 flow 的时候&lt;/li>
&lt;/ul>
&lt;p>提示：&lt;code>netif_receive_skb&lt;/code> 和它调用的函数都运行在软中断处理循环（softirq processing loop）的上下文中，因此这里的时间会记录到 &lt;code>top&lt;/code> 命令看到的 &lt;code>si&lt;/code> 或者 &lt;code>sitime&lt;/code> 字段。
&lt;code>netif_receive_skb&lt;/code> 首先会检查用户有没有设置一个接收时间戳选项（sysctl），这个选 项决定在 packet 在到达 backlog queue 之前还是之后打时间戳。如果启用，那立即打时间戳 ，在 RPS 之前（CPU 和 backlog queue 绑定）；如果没有启用，那只有在它进入到 backlog queue 之后才会打时间戳。如果 RPS 开启了，那这个选项可以将打时间戳的任务分散个其他 CPU，但会带来一些延迟。&lt;/p>
&lt;h2 id="91-调优-收包打时间戳rx-packet-timestamping">9.1 调优: 收包打时间戳（RX packet timestamping）&lt;/h2>
&lt;p>你可以调整包被收到后，何时给它打时间戳。
关闭收包打时间戳：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.netdev_tstamp_prequeue=0
&lt;/code>&lt;/pre>
&lt;p>默认是 1。&lt;/p>
&lt;h1 id="10-netif_receive_skb">10 &lt;code>netif_receive_skb&lt;/code>&lt;/h1>
&lt;p>处理完时间戳后，&lt;code>netif_receive_skb&lt;/code> 会根据 RPS 是否启用来做不同的事情。我们先来看简 单情况，RPS 未启用。&lt;/p>
&lt;h2 id="101-不使用-rps默认">10.1 不使用 RPS（默认）&lt;/h2>
&lt;p>如果 RPS 没启用，会调用&lt;code>__netif_receive_skb&lt;/code>，它做一些 bookkeeping 工作，进而调用 &lt;code>__netif_receive_skb_core&lt;/code>，将数据移动到离协议栈更近一步。
&lt;code>__netif_receive_skb_core&lt;/code> 工作的具体细节我们稍后再看，先看一下 RPS 启用的情况下的 代码调用关系，它也会调到这个函数的。&lt;/p>
&lt;h2 id="102-使用-rps">10.2 使用 RPS&lt;/h2>
&lt;p>如果 RPS 启用了，它会做一些计算，判断使用哪个 CPU 的 backlog queue，这个过程由 &lt;code>get_rps_cpu&lt;/code> 函数完成。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>cpu = get_rps_cpu(skb-&amp;gt;dev, skb, &amp;amp;rflow);
if (cpu &amp;gt;= 0) {
ret = enqueue_to_backlog(skb, cpu, &amp;amp;rflow-&amp;gt;last_qtail);
rcu_read_unlock();
return ret;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>get_rps_cpu&lt;/code> 会考虑前面提到的 RFS 和 aRFS 设置，以此选出一个合适的 CPU，通过调用 &lt;code>enqueue_to_backlog&lt;/code> 将数据放到它的 backlog queue。&lt;/p>
&lt;h2 id="103-enqueue_to_backlog">10.3 &lt;code>enqueue_to_backlog&lt;/code>&lt;/h2>
&lt;p>首先从远端 CPU 的 &lt;code>struct softnet_data&lt;/code> 变量获取 backlog queue 长度。如果 backlog 大于 &lt;code>netdev_max_backlog&lt;/code>，或者超过了 flow limit，直接 drop，并更新 &lt;code>softnet_data&lt;/code> 的 drop 统计。注意这是远端 CPU 的统计。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>qlen = skb_queue_len(&amp;amp;sd-&amp;gt;input_pkt_queue);
if (qlen &amp;lt;= netdev_max_backlog &amp;amp;&amp;amp; !skb_flow_limit(skb, qlen)) {
if (skb_queue_len(&amp;amp;sd-&amp;gt;input_pkt_queue)) {
enqueue:
__skb_queue_tail(&amp;amp;sd-&amp;gt;input_pkt_queue, skb);
input_queue_tail_incr_save(sd, qtail);
return NET_RX_SUCCESS;
}
/* Schedule NAPI for backlog device */
if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;amp;sd-&amp;gt;backlog.state)) {
if (!rps_ipi_queued(sd))
____napi_schedule(sd, &amp;amp;sd-&amp;gt;backlog);
}
goto enqueue;
}
sd-&amp;gt;dropped++;
kfree_skb(skb);
return NET_RX_DROP;
&lt;/code>&lt;/pre>
&lt;p>&lt;code>enqueue_to_backlog&lt;/code> 被调用的地方很少。在基于 RPS 处理包的地方，以及 &lt;code>netif_rx&lt;/code>，会 调用到它。大部分驱动都不应该使用 &lt;code>netif_rx&lt;/code>，而应该是用 &lt;code>netif_receive_skb&lt;/code>。如果 你没用到 RPS，你的驱动也没有使用 &lt;code>netif_rx&lt;/code>，那增大 &lt;code>backlog&lt;/code> 并不会带来益处，因为它 根本没被用到。
注意：检查你的驱动，如果它调用了 &lt;code>netif_receive_skb&lt;/code>，而且你没用 RPS，那增大 &lt;code>netdev_max_backlog&lt;/code> 并不会带来任何性能提升，因为没有数据包会被送到 &lt;code>input_pkt_queue&lt;/code>。
如果 &lt;code>input_pkt_queue&lt;/code> 足够小，而 flow limit（后面会介绍）也还没达到（或者被禁掉了 ），那数据包将会被放到队列。这里的逻辑有点 funny，但大致可以归为为：&lt;/p>
&lt;ul>
&lt;li>如果 backlog 是空的：如果远端 CPU NAPI 变量没有运行，并且 IPI 没有被加到队列，那就 触发一个 IPI 加到队列，然后调用&lt;code>____napi_schedule&lt;/code> 进一步处理&lt;/li>
&lt;li>如果 backlog 非空，或者远端 CPU NAPI 变量正在运行，那就 enqueue 包&lt;/li>
&lt;/ul>
&lt;p>这里使用了 &lt;code>goto&lt;/code>，所以代码看起来有点 tricky。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3201-L3218">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>if (skb_queue_len(&amp;amp;sd-&amp;gt;input_pkt_queue)) {
enqueue:
__skb_queue_tail(&amp;amp;sd-&amp;gt;input_pkt_queue, skb);
input_queue_tail_incr_save(sd, qtail);
rps_unlock(sd);
local_irq_restore(flags);
return NET_RX_SUCCESS;
}
/* Schedule NAPI for backlog device
* We can use non atomic operation since we own the queue lock
*/
if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;amp;sd-&amp;gt;backlog.state)) {
if (!rps_ipi_queued(sd))
____napi_schedule(sd, &amp;amp;sd-&amp;gt;backlog);
}
goto enqueue;
&lt;/code>&lt;/pre>
&lt;h3 id="flow-limits">Flow limits&lt;/h3>
&lt;p>RPS 在不同 CPU 之间分发 packet，但是，如果一个 flow 特别大，会出现单个 CPU 被打爆，而 其他 CPU 无事可做（饥饿）的状态。因此引入了 flow limit 特性，放到一个 backlog 队列的属 于同一个 flow 的包的数量不能超过一个阈值。这可以保证即使有一个很大的 flow 在大量收包 ，小 flow 也能得到及时的处理。
检查 flow limit 的代码，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200">net/core/dev.c&lt;/a>：&lt;/p>
&lt;pre>&lt;code>if (qlen &amp;lt;= netdev_max_backlog &amp;amp;&amp;amp; !skb_flow_limit(skb, qlen)) {
&lt;/code>&lt;/pre>
&lt;p>默认，flow limit 功能是关掉的。要打开 flow limit，你需要指定一个 bitmap（类似于 RPS 的 bitmap）。&lt;/p>
&lt;h3 id="监控由于-input_pkt_queue-打满或-flow-limit-导致的丢包">监控：由于 &lt;code>input_pkt_queue&lt;/code> 打满或 flow limit 导致的丢包&lt;/h3>
&lt;p>在&lt;code>/proc/net/softnet_stat&lt;/code> 里面的 dropped 列计数，包含本节提到的原因导致的 drop。&lt;/p>
&lt;h3 id="调优">调优&lt;/h3>
&lt;h4 id="tuning-adjusting-netdev_max_backlog-to-prevent-drops">Tuning: Adjusting netdev_max_backlog to prevent drops&lt;/h4>
&lt;p>在调整这个值之前，请先阅读前面的“注意”。
如果你使用了 RPS，或者你的驱动调用了 &lt;code>netif_rx&lt;/code>，那增加 &lt;code>netdev_max_backlog&lt;/code> 可以改 善在 &lt;code>enqueue_to_backlog&lt;/code> 里的丢包：
例如：increase backlog to 3000 with sysctl.&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.netdev_max_backlog=3000
&lt;/code>&lt;/pre>
&lt;p>默认值是 1000。&lt;/p>
&lt;h4 id="tuning-adjust-the-napi-weight-of-the-backlog-poll-loop">Tuning: Adjust the NAPI weight of the backlog poll loop&lt;/h4>
&lt;p>&lt;code>net.core.dev_weight&lt;/code> 决定了 backlog poll loop 可以消耗的整体 budget（参考前面更改 &lt;code>net.core.netdev_budget&lt;/code> 的章节）：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.dev_weight=600
&lt;/code>&lt;/pre>
&lt;p>默认值是 64。
记住，backlog 处理逻辑和设备驱动的 &lt;code>poll&lt;/code> 函数类似，都是在软中断（softirq）的上下文 中执行，因此受整体 budget 和处理时间的限制，前面已经分析过了。&lt;/p>
&lt;h4 id="tuning-enabling-flow-limits-and-tuning-flow-limit-hash-table-size">Tuning: Enabling flow limits and tuning flow limit hash table size&lt;/h4>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.flow_limit_table_len=8192
&lt;/code>&lt;/pre>
&lt;p>默认值是 4096.
这只会影响新分配的 flow hash table。所以，如果你想增加 table size 的话，应该在打开 flow limit 功能之前设置这个值。
打开 flow limit 功能的方式是，在&lt;code>/proc/sys/net/core/flow_limit_cpu_bitmap&lt;/code> 中指定一 个 bitmask，和通过 bitmask 打开 RPS 的操作类似。&lt;/p>
&lt;h2 id="104-处理-backlog-队列napi-poller">10.4 处理 backlog 队列：NAPI poller&lt;/h2>
&lt;p>每个 CPU 都有一个 backlog queue，其加入到 NAPI 变量的方式和驱动差不多，都是注册一个 &lt;code>poll&lt;/code> 方法，在软中断的上下文中处理包。此外，还提供了一个 &lt;code>weight&lt;/code>，这也和驱动类似 。
注册发生在网络系统初始化的时候。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L6952-L6955">net/core/dev.c&lt;/a>的 &lt;code>net_dev_init&lt;/code> 函数：&lt;/p>
&lt;pre>&lt;code>sd-&amp;gt;backlog.poll = process_backlog;
sd-&amp;gt;backlog.weight = weight_p;
sd-&amp;gt;backlog.gro_list = NULL;
sd-&amp;gt;backlog.gro_count = 0;
&lt;/code>&lt;/pre>
&lt;p>backlog NAPI 变量和设备驱动 NAPI 变量的不同之处在于，它的 weight 是可以调节的，而设备 驱动是 hardcode 64。在下面的调优部分，我们会看到如何用 sysctl 调整这个设置。&lt;/p>
&lt;h2 id="105-process_backlog">10.5 &lt;code>process_backlog&lt;/code>&lt;/h2>
&lt;p>&lt;code>process_backlog&lt;/code> 是一个循环，它会一直运行直至 &lt;code>weight&lt;/code>（前面介绍了）用完，或者 backlog 里没有数据了。
backlog queue 里的数据取出来，传递给&lt;code>__netif_receive_skb&lt;/code>。这个函数做的事情和 RPS 关闭的情况下做的事情一样。即，&lt;code>__netif_receive_skb&lt;/code> 做一些 bookkeeping 工作，然后调 用&lt;code>__netif_receive_skb_core&lt;/code> 将数据发送给更上面的协议层。
&lt;code>process_backlog&lt;/code> 和 NAPI 之间遵循的合约，和驱动和 NAPI 之间的合约相同：NAPI is disabled if the total weight will not be used. The poller is restarted with the call to &lt;code>____napi_schedule&lt;/code> from &lt;code>enqueue_to_backlog&lt;/code> as described above.
函数返回接收完成的数据帧数量（在代码中是变量 &lt;code>work&lt;/code>），&lt;code>net_rx_action&lt;/code>（前面介绍了 ）将会从 budget（通过 &lt;code>net.core.netdev_budget&lt;/code> 可以调整，前面介绍了）里减去这个值。&lt;/p>
&lt;h2 id="106-__netif_receive_skb_core将数据送到抓包点tap或协议层">10.6 &lt;code>__netif_receive_skb_core&lt;/code>：将数据送到抓包点（tap）或协议层&lt;/h2>
&lt;p>&lt;code>__netif_receive_skb_core&lt;/code> 完成&lt;strong>将数据送到协议栈&lt;/strong>这一繁重工作（the heavy lifting of delivering the data)。在此之前，它会先&lt;strong>检查是否插入了 packet tap（探 测点），这些 tap 是抓包用的&lt;/strong>。例如，&lt;code>AF_PACKET&lt;/code> 地址族就可以插入这些抓包指令， 一般通过 &lt;code>libpcap&lt;/code> 库。
&lt;strong>如果存在抓包点（tap），数据就会先到抓包点，然后才到协议层。&lt;/strong>&lt;/p>
&lt;h2 id="107-送到抓包点tap">10.7 送到抓包点（tap）&lt;/h2>
&lt;p>如果有 packet tap（通常通过 &lt;code>libpcap&lt;/code>），packet 会送到那里。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>list_for_each_entry_rcu(ptype, &amp;amp;ptype_all, list) {
if (!ptype-&amp;gt;dev || ptype-&amp;gt;dev == skb-&amp;gt;dev) {
if (pt_prev)
ret = deliver_skb(skb, pt_prev, orig_dev);
pt_prev = ptype;
}
}
&lt;/code>&lt;/pre>
&lt;p>如果对 packet 如何经过 pcap 有兴趣，可以阅读 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/packet/af_packet.c">net/packet/af_packet.c&lt;/a>。&lt;/p>
&lt;h2 id="108-送到协议层">10.8 送到协议层&lt;/h2>
&lt;p>处理完 taps 之后，&lt;code>__netif_receive_skb_core&lt;/code> 将数据发送到协议层。它会从数据包中取出 协议信息，然后遍历注册在这个协议上的回调函数列表。
可以看&lt;code>__netif_receive_skb_core&lt;/code> 函数，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554">net/core/dev.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>type = skb-&amp;gt;protocol;
list_for_each_entry_rcu(ptype,
&amp;amp;ptype_base[ntohs(type) &amp;amp; PTYPE_HASH_MASK], list) {
if (ptype-&amp;gt;type == type &amp;amp;&amp;amp;
(ptype-&amp;gt;dev == null_or_dev || ptype-&amp;gt;dev == skb-&amp;gt;dev ||
ptype-&amp;gt;dev == orig_dev)) {
if (pt_prev)
ret = deliver_skb(skb, pt_prev, orig_dev);
pt_prev = ptype;
}
}
&lt;/code>&lt;/pre>
&lt;p>上面的 &lt;code>ptype_base&lt;/code> 是一个 hash table，定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L146">net/core/dev.c&lt;/a>中:&lt;/p>
&lt;pre>&lt;code>struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;
&lt;/code>&lt;/pre>
&lt;p>每种协议在上面的 hash table 的一个 slot 里，添加一个过滤器到列表里。这个列表的头用如 下函数获取：&lt;/p>
&lt;pre>&lt;code>static inline struct list_head *ptype_head(const struct packet_type *pt)
{
if (pt-&amp;gt;type == htons(ETH_P_ALL))
return &amp;amp;ptype_all;
else
return &amp;amp;ptype_base[ntohs(pt-&amp;gt;type) &amp;amp; PTYPE_HASH_MASK];
}
&lt;/code>&lt;/pre>
&lt;p>添加的时候用 &lt;code>dev_add_pack&lt;/code> 这个函数。这就是协议层如何注册自身，用于处理相应协议的 网络数据的。
现在，你已经知道了数据是如何从卡进入到协议层的了。&lt;/p>
&lt;h1 id="11-协议层注册">11 协议层注册&lt;/h1>
&lt;p>接下来我们看协议层注册自身的实现。
本文会拿 IP 层作为例子，因为它最常用，大部分读者都很熟悉。&lt;/p>
&lt;h2 id="111-ip-协议层">11.1 IP 协议层&lt;/h2>
&lt;p>IP 层在函数 &lt;code>inet_init&lt;/code> 中将自身注册到 &lt;code>ptype_base&lt;/code> 哈希表。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1788">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>dev_add_pack(&amp;amp;ip_packet_type);
&lt;/code>&lt;/pre>
&lt;p>&lt;code>struct packet_type&lt;/code> 的变量 &lt;code>ip_packet_type&lt;/code> 定义在 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1673-L1676">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static struct packet_type ip_packet_type __read_mostly = {
.type = cpu_to_be16(ETH_P_IP),
.func = ip_rcv,
};
&lt;/code>&lt;/pre>
&lt;p>&lt;code>__netif_receive_skb_core&lt;/code> 会调用 &lt;code>deliver_skb&lt;/code> (前面介绍过了), 后者会调用&lt;code>.func&lt;/code> 方法(这个例子中就是 &lt;code>ip_rcv&lt;/code>)。&lt;/p>
&lt;h3 id="1111-ip_rcv">11.1.1 &lt;code>ip_rcv&lt;/code>&lt;/h3>
&lt;p>&lt;code>ip_rcv&lt;/code> 方法的核心逻辑非常简单直接，此外就是一些数据合法性验证，统计计数器更新等 等。它在最后会以 netfilter 的方式调用 &lt;code>ip_rcv_finish&lt;/code> 方法。这样做的目的是，任何 iptables 规则都能在 packet 刚进入 IP 层协议的时候被应用，在其他处理之前。
我们可以在 &lt;code>ip_rcv&lt;/code> 结束的时候看到交给 netfilter 的代码： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L453-L454">net/ipv4/ip_input.c&lt;/a>&lt;/p>
&lt;pre>&lt;code>return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL, ip_rcv_finish);
&lt;/code>&lt;/pre>
&lt;h4 id="netfilter-and-iptables">netfilter and iptables&lt;/h4>
&lt;p>这里简单介绍下 &lt;code>netfilter&lt;/code>, &lt;code>iptables&lt;/code> 和 &lt;code>conntrack&lt;/code>。
&lt;code>NF_HOOK_THRESH&lt;/code> 会检查是否有 filter 被安装，并会适时地返回到 IP 协议层，避免过深的进 入 netfilter 处理，以及在 netfilter 下面再做 hook 的 iptables 和 conntrack。
注意：&lt;strong>netfilter 或 iptables 规则都是在软中断上下文中执行的&lt;/strong>，数量很多或规则很 复杂时会导致&lt;strong>网络延迟&lt;/strong>。但如果你就是需要一些规则的话，那这个性能损失看起来是无 法避免的。&lt;/p>
&lt;h3 id="1112-ip_rcv_finish">11.1.2 &lt;code>ip_rcv_finish&lt;/code>&lt;/h3>
&lt;p>netfilter 完成对数据的处理之后，就会调用 &lt;code>ip_rcv_finish&lt;/code> —— 当然，前提是 netfilter 没有丢掉这个包。
&lt;code>ip_rcv_finish&lt;/code> 开始的地方做了一个优化。为了能将包送到合适的目的地，需要一个路由 子系统的 &lt;code>dst_entry&lt;/code> 变量。为了获取这个变量，早期的代码调用了 &lt;code>early_demux&lt;/code> 函数，从 这个数据的目的端的高层协议中。
&lt;code>early_demux&lt;/code> 是一个优化项，通过检查相应的变量是否缓存在 &lt;code>socket&lt;/code> 变量上，来路由 这个包所需要的 &lt;code>dst_entry&lt;/code> 变量。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L317-L327">net/ipv4/ip_input.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>if (sysctl_ip_early_demux &amp;amp;&amp;amp; !skb_dst(skb) &amp;amp;&amp;amp; skb-&amp;gt;sk == NULL) {
const struct net_protocol *ipprot;
int protocol = iph-&amp;gt;protocol;
ipprot = rcu_dereference(inet_protos[protocol]);
if (ipprot &amp;amp;&amp;amp; ipprot-&amp;gt;early_demux) {
ipprot-&amp;gt;early_demux(skb);
/* must reload iph, skb-&amp;gt;head might have changed */
iph = ip_hdr(skb);
}
}
&lt;/code>&lt;/pre>
&lt;p>可以看到，这个函数只有在 &lt;code>sysctl_ip_early_demux&lt;/code> 为 &lt;code>true&lt;/code> 的时候才有可能被执行。默 认 &lt;code>early_demux&lt;/code> 是打开的。下一节会介绍如何关闭它，以及为什么你可能会需要关闭它。
如果这个优化打开了，但是并没有命中缓存（例如，这是第一个包），这个包就会被送到内 核的路由子系统，在那里将会计算出一个 &lt;code>dst_entry&lt;/code> 并赋给相应的字段。
路由子系统完成工作后，会更新计数器，然后调用 &lt;code>dst_input(skb)&lt;/code>，后者会进一步调用 &lt;code>dst_entry&lt;/code> 变量中的 &lt;code>input&lt;/code> 方法，这个方法是一个函数指针，有路由子系统初始化。例如 ，如果 packet 的最终目的地是本机（local system），路由子系统会将 &lt;code>ip_local_deliver&lt;/code> 赋 给 &lt;code>input&lt;/code>。&lt;/p>
&lt;h4 id="调优-打开或关闭-ip-协议的-early-demux-选项">调优: 打开或关闭 IP 协议的 early demux 选项&lt;/h4>
&lt;p>关闭 &lt;code>early_demux&lt;/code> 优化：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.ipv4.ip_early_demux=0
&lt;/code>&lt;/pre>
&lt;p>默认是 1，即该功能默认是打开的。
添加这个 &lt;code>sysctl&lt;/code> 开关的原因是，一些用户报告说，在某些场景下 &lt;code>early_demux&lt;/code> 优化会导 致 ~5% 左右的吞吐量下降。&lt;/p>
&lt;h3 id="1113-ip_local_deliver">11.1.3 &lt;code>ip_local_deliver&lt;/code>&lt;/h3>
&lt;p>回忆我们看到的 IP 协议层过程：&lt;/p>
&lt;ol>
&lt;li>调用 &lt;code>ip_rcv&lt;/code> 做一些初始的 bookkeeping&lt;/li>
&lt;li>将包交给 netfilter 处理，同时还有一个回调函数，netfilter 处理完毕后会调用这个函 数&lt;/li>
&lt;li>处理结束的时候，调用 &lt;code>ip_rcv_finish&lt;/code>，将数据包送到协议栈的更上层&lt;/li>
&lt;/ol>
&lt;p>&lt;code>ip_local_deliver&lt;/code> 的逻辑与此类似： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L241-L258">net/ipv4/ip_input.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/*
* Deliver IP Packets to the higher protocol layers.
*/
int ip_local_deliver(struct sk_buff *skb)
{
/*
* Reassemble IP fragments.
*/
if (ip_is_fragment(ip_hdr(skb))) {
if (ip_defrag(skb, IP_DEFRAG_LOCAL_DELIVER))
return 0;
}
return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN, skb, skb-&amp;gt;dev, NULL,
ip_local_deliver_finish);
}
&lt;/code>&lt;/pre>
&lt;p>只要 packet 没有在 netfilter 被 drop，就会调用 &lt;code>ip_local_deliver_finish&lt;/code> 函数。&lt;/p>
&lt;h3 id="1114-ip_local_deliver_finish">11.1.4 &lt;code>ip_local_deliver_finish&lt;/code>&lt;/h3>
&lt;p>&lt;code>ip_local_deliver_finish&lt;/code> 从数据包中读取协议，寻找注册在这个协议上的 &lt;code>struct net_protocol&lt;/code> 变量，并调用该变量中的回调方法。这样将包送到协议栈的更上层。&lt;/p>
&lt;h4 id="monitoring-ip-protocol-layer-statistics">Monitoring: IP protocol layer statistics&lt;/h4>
&lt;p>读取&lt;code>/proc/net/snmp&lt;/code> 获取详细的 IP 协议统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/snmp
Ip: Forwarding DefaultTTL InReceives InHdrErrors InAddrErrors ForwDatagrams InUnknownProtos InDiscards InDelivers OutRequests OutDiscards OutNoRoutes ReasmTimeout ReasmReqds ReasmOKs ReasmFails FragOKs FragFails FragCreates
Ip: 1 64 25922988125 0 0 15771700 0 0 25898327616 22789396404 12987882 51 1 10129840 2196520 1 0 0 0
...
&lt;/code>&lt;/pre>
&lt;p>这个文件包含几个协议层的统计信息。先是 IP 层。
与这些列相关的，IP 层的统计类型都定义在&lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/snmp.h#L10-L59">include/uapi/linux/snmp.h&lt;/a>：&lt;/p>
&lt;pre>&lt;code>enum
{
IPSTATS_MIB_NUM = 0,
/* frequently written fields in fast path, kept in same cache line */
IPSTATS_MIB_INPKTS, /* InReceives */
IPSTATS_MIB_INOCTETS, /* InOctets */
IPSTATS_MIB_INDELIVERS, /* InDelivers */
IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */
IPSTATS_MIB_OUTPKTS, /* OutRequests */
IPSTATS_MIB_OUTOCTETS, /* OutOctets */
/* ... */
&lt;/code>&lt;/pre>
&lt;p>读取&lt;code>/proc/net/netstat&lt;/code> 获取更详细的 IP 层统计：&lt;/p>
&lt;pre>&lt;code>$ cat /proc/net/netstat | grep IpExt
IpExt: InNoRoutes InTruncatedPkts InMcastPkts OutMcastPkts InBcastPkts OutBcastPkts InOctets OutOctets InMcastOctets OutMcastOctets InBcastOctets OutBcastOctets InCsumErrors InNoECTPkts InECT0Pktsu InCEPkts
IpExt: 0 0 0 0 277959 0 14568040307695 32991309088496 0 0 58649349 0 0 0 0 0
&lt;/code>&lt;/pre>
&lt;p>格式和&lt;code>/proc/net/snmp&lt;/code> 类似，除了每列的命字都一 &lt;code>IpExt&lt;/code> 开头之外。
一些有趣的统计：&lt;/p>
&lt;ul>
&lt;li>&lt;code>InReceives&lt;/code>: The total number of IP packets that reached ip_rcv before any data integrity checks.&lt;/li>
&lt;li>&lt;code>InHdrErrors&lt;/code>: Total number of IP packets with corrupted headers. The header was too short, too long, non-existent, had the wrong IP protocol version number, etc.&lt;/li>
&lt;li>&lt;code>InAddrErrors&lt;/code>: Total number of IP packets where the host was unreachable.&lt;/li>
&lt;li>&lt;code>ForwDatagrams&lt;/code>: Total number of IP packets that have been forwarded.&lt;/li>
&lt;li>&lt;code>InUnknownProtos&lt;/code>: Total number of IP packets with unknown or unsupported protocol specified in the header.&lt;/li>
&lt;li>&lt;code>InDiscards&lt;/code>: Total number of IP packets discarded due to memory allocation failure or checksum failure when packets are trimmed.&lt;/li>
&lt;li>&lt;code>InDelivers&lt;/code>: Total number of IP packets successfully delivered to higher protocol layers. Keep in mind that those protocol layers may drop data even if the IP layer does not.&lt;/li>
&lt;li>InCsumErrors: Total number of IP Packets with checksum errors.&lt;/li>
&lt;/ul>
&lt;p>注意这些计数分别在 IP 层的不同地方被更新。由于代码一直在更新，重复计数或者计数错 误的 bug 可能会引入。如果这些计数对你非常重要，强烈建议阅读内核的相应源码，确定 它们是在哪里更新的，以及更新的对不对，是不是有 bug 等等。&lt;/p>
&lt;h2 id="112-高层协议注册">11.2 高层协议注册&lt;/h2>
&lt;p>本文介绍 UDP 处理函数的注册过程，TCP 的注册过程与此一样，并且是在相同的时间注册的。
在 &lt;code>net/ipv4/af_inet.c&lt;/code> 中定义了 UDP、TCP 和 ICMP 协议的回调函数相关的数据结构，IP 层处 理完毕之后会调用相应的回调. From &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1526-L1547">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static const struct net_protocol tcp_protocol = {
.early_demux = tcp_v4_early_demux,
.handler = tcp_v4_rcv,
.err_handler = tcp_v4_err,
.no_policy = 1,
.netns_ok = 1,
};
static const struct net_protocol udp_protocol = {
.early_demux = udp_v4_early_demux,
.handler = udp_rcv,
.err_handler = udp_err,
.no_policy = 1,
.netns_ok = 1,
};
static const struct net_protocol icmp_protocol = {
.handler = icmp_rcv,
.err_handler = icmp_err,
.no_policy = 1,
.netns_ok = 1,
};
&lt;/code>&lt;/pre>
&lt;p>这些变量在 &lt;code>inet&lt;/code> 地址族初始化的时候被注册。 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1720-L1725">net/ipv4/af_inet.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>/*
* Add all the base protocols.
*/
if (inet_add_protocol(&amp;amp;icmp_protocol, IPPROTO_ICMP) &amp;lt; 0)
pr_crit(&amp;quot;%s: Cannot add ICMP protocol\n&amp;quot;, __func__);
if (inet_add_protocol(&amp;amp;udp_protocol, IPPROTO_UDP) &amp;lt; 0)
pr_crit(&amp;quot;%s: Cannot add UDP protocol\n&amp;quot;, __func__);
if (inet_add_protocol(&amp;amp;tcp_protocol, IPPROTO_TCP) &amp;lt; 0)
pr_crit(&amp;quot;%s: Cannot add TCP protocol\n&amp;quot;, __func__);
&lt;/code>&lt;/pre>
&lt;p>接下来我们详细查看 UDP 协议。上面可以看到，UDP 的回调函数是 &lt;code>udp_rcv&lt;/code>。这是从 IP 层进 入 UDP 层的入口。我们就从这里开始探索。&lt;/p>
&lt;h2 id="113-udp-协议层">11.3 UDP 协议层&lt;/h2>
&lt;p>UDP 协议层的实现见&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c">net/ipv4/udp.c&lt;/a>。&lt;/p>
&lt;h3 id="1131-udp_rcv">11.3.1 &lt;code>udp_rcv&lt;/code>&lt;/h3>
&lt;p>这个函数只要一行，调用&lt;code>__udp4_lib_rcv&lt;/code> 接收 UDP 报文。&lt;/p>
&lt;h3 id="1132-__udp4_lib_rcv">11.3.2 &lt;code>__udp4_lib_rcv&lt;/code>&lt;/h3>
&lt;p>&lt;code>__udp4_lib_rcv&lt;/code> 首先对包数据进行合法性检查，获取 UDP 头、UDP 数据报长度、源地址、目 标地址等信息。然后进行其他一些完整性检测和 checksum 验证。
回忆前面的 IP 层内容，在送到更上面一层协议（这里是 UDP）之前，会将一个 &lt;code>dst_entry&lt;/code> 会关联到 &lt;code>skb&lt;/code>。
如果对应的 &lt;code>dst_entry&lt;/code> 找到了，并且有对应的 socket，&lt;code>__udp4_lib_rcv&lt;/code> 会将 packet 放到 &lt;code>socket&lt;/code> 的接收队列：&lt;/p>
&lt;pre>&lt;code>sk = skb_steal_sock(skb);
if (sk) {
struct dst_entry *dst = skb_dst(skb);
int ret;
if (unlikely(sk-&amp;gt;sk_rx_dst != dst))
udp_sk_rx_dst_set(sk, dst);
ret = udp_queue_rcv_skb(sk, skb);
sock_put(sk);
/* a return value &amp;gt; 0 means to resubmit the input, but
* it wants the return to be -protocol, or 0
*/
if (ret &amp;gt; 0)
return -ret;
return 0;
} else {
&lt;/code>&lt;/pre>
&lt;p>如果 &lt;code>early_demux&lt;/code> 中没有关联 socket 信息，那此时会调用&lt;code>__udp4_lib_lookup_skb&lt;/code> 查找对应的 socket。
以上两种情况，最后都会将 packet 放到 socket 的接收队列：&lt;/p>
&lt;pre>&lt;code>ret = udp_queue_rcv_skb(sk, skb);
sock_put(sk);
&lt;/code>&lt;/pre>
&lt;p>如果 socket 没有找到，数据报(datagram)会被丢弃：&lt;/p>
&lt;pre>&lt;code>/* No socket. Drop packet silently, if checksum is wrong */
if (udp_lib_checksum_complete(skb))
goto csum_error;
UDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);
icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
/*
* Hmm. We got an UDP packet to a port to which we
* don't wanna listen. Ignore it.
*/
kfree_skb(skb);
return 0;
&lt;/code>&lt;/pre>
&lt;h3 id="1133-udp_queue_rcv_skb">11.3.3 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;/h3>
&lt;p>这个函数的前面部分所做的工作：&lt;/p>
&lt;ol>
&lt;li>判断和这个数据报关联的 socket 是不是 &lt;a href="https://tools.ietf.org/html/rfc3948">encapsulation&lt;/a> socket。如果是，将 packet 送到该层的处理函数&lt;/li>
&lt;li>判断这个数据报是不是 UDP-Lite 数据报，做一些完整性检测&lt;/li>
&lt;li>验证 UDP 数据报的校验和，如果校验失败，就丢弃&lt;/li>
&lt;/ol>
&lt;p>最后，我们来到了 socket 的接收队列逻辑，判断队列是不是满了： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1548-L1549">net/ipv4/udp.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>if (sk_rcvqueues_full(sk, skb, sk-&amp;gt;sk_rcvbuf))
goto drop;
&lt;/code>&lt;/pre>
&lt;h3 id="1334-sk_rcvqueues_full">13.3.4 &lt;code>sk_rcvqueues_full&lt;/code>&lt;/h3>
&lt;p>定义如下：&lt;/p>
&lt;pre>&lt;code>/*
* Take into account size of receive queue and backlog queue
* Do not take into account this skb truesize,
* to allow even a single big packet to come.
*/
static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb,
unsigned int limit)
{
unsigned int qsize = sk-&amp;gt;sk_backlog.len + atomic_read(&amp;amp;sk-&amp;gt;sk_rmem_alloc);
return qsize &amp;gt; limit;
}
&lt;/code>&lt;/pre>
&lt;p>Tuning these values is a bit tricky as there are many things that can be adjusted.&lt;/p>
&lt;h4 id="调优-socket-receive-queue-memory">调优: Socket receive queue memory&lt;/h4>
&lt;p>上面看到，判断 socket 接收队列是否满了是和 &lt;code>sk-&amp;gt;sk_rcvbuf&lt;/code> 做比较。 这个值可以被两个 sysctl 参数控制：最大值和默认值：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.core.rmem_max=8388608
$ sudo sysctl -w net.core.rmem_default=8388608
&lt;/code>&lt;/pre>
&lt;p>你也可以在你的应用里调用 &lt;code>setsockopt&lt;/code> 带上 &lt;code>SO_RCVBUF&lt;/code> 来修改这个值(&lt;code>sk-&amp;gt;sk_rcvbuf&lt;/code>) ，能设置的最大值不能超过 &lt;code>net.core.rmem_max&lt;/code>。
但是，你也可以 &lt;code>setsockopt&lt;/code> 带上 &lt;code>SO_RCVBUFFORCE&lt;/code> 来覆盖 &lt;code>net.core.rmem_max&lt;/code>，但是执 行应用的用户要有 &lt;code>CAP_NET_ADMIN&lt;/code> 权限。
&lt;code>skb_set_owner_r&lt;/code> 函数设置 UDP 数据包的 owner，并会更新计数器 &lt;code>sk-&amp;gt;sk_rmem_alloc&lt;/code>。 我们接下来会看到。
&lt;code>sk_add_backlog&lt;/code> 函数会更新 &lt;code>sk-&amp;gt;sk_backlog.len&lt;/code> 计数，后面看。&lt;/p>
&lt;h3 id="1135-udp_queue_rcv_skb">11.3.5 &lt;code>udp_queue_rcv_skb&lt;/code>&lt;/h3>
&lt;p>判断 queue 未满之后，就会将数据报放到里面： &lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1554-L1561">net/ipv4/udp.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>bh_lock_sock(sk);
if (!sock_owned_by_user(sk))
rc = __udp_queue_rcv_skb(sk, skb);
else if (sk_add_backlog(sk, skb, sk-&amp;gt;sk_rcvbuf)) {
bh_unlock_sock(sk);
goto drop;
}
bh_unlock_sock(sk);
return rc;
&lt;/code>&lt;/pre>
&lt;p>第一步先判断有没有用户空间的程序正在这个 socket 上进行系统调用。如果没有，就可以调用&lt;code>__udp_queue_rcv_skb&lt;/code> 将数据报放到接收队列；如果有，就调用 &lt;code>sk_add_backlog&lt;/code> 将它放到 backlog 队列。
当用户空间程序释放在这个 socket 上的系统调用时（通过向内核调用 &lt;code>release_sock&lt;/code>），这 个数据报就从 backlog 移动到了接收队列。&lt;/p>
&lt;h3 id="1137-__udp_queue_rcv_skb">11.3.7 &lt;code>__udp_queue_rcv_skb&lt;/code>&lt;/h3>
&lt;p>这个函数调用 &lt;code>sock_queue_rcv_skb&lt;/code> 将数据报送到 socket 接收队列；如果失败，更新统计计数并释放 skb。
&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1431-L1443">net/ipv4/udp.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>rc = sock_queue_rcv_skb(sk, skb);
if (rc &amp;lt; 0) {
int is_udplite = IS_UDPLITE(sk);
/* Note that an ENOMEM error is charged twice */
if (rc == -ENOMEM)
UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,is_udplite);
UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);
kfree_skb(skb);
trace_udp_fail_queue_rcv_skb(rc, sk);
return -1;
}
&lt;/code>&lt;/pre>
&lt;h3 id="1138-monitoring-udp-protocol-layer-statistics">11.3.8 Monitoring: UDP protocol layer statistics&lt;/h3>
&lt;p>以下文件可以获取非常有用的 UDP 统计：&lt;/p>
&lt;pre>&lt;code>/proc/net/snmp
/proc/net/udp
/proc/net/snmp
&lt;/code>&lt;/pre>
&lt;h4 id="监控-udp-协议统计procnetsnmp">监控 UDP 协议统计：&lt;code>/proc/net/snmp&lt;/code>&lt;/h4>
&lt;pre>&lt;code>$ cat /proc/net/snmp | grep Udp\:
Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors
Udp: 16314 0 0 17161 0 0
&lt;/code>&lt;/pre>
&lt;p>Much like the detailed statistics found in this file for the IP protocol, you will need to read the protocol layer source to determine exactly when and where these values are incremented.&lt;/p>
&lt;pre>&lt;code>InDatagrams: Incremented when recvmsg was used by a userland program to read datagram. Also incremented when a UDP packet is encapsulated and sent back for processing.
NoPorts: Incremented when UDP packets arrive destined for a port where no program is listening.
InErrors: Incremented in several cases: no memory in the receive queue, when a bad checksum is seen, and if sk_add_backlog fails to add the datagram.
OutDatagrams: Incremented when a UDP packet is handed down without error to the IP protocol layer to be sent.
RcvbufErrors: Incremented when sock_queue_rcv_skb reports that no memory is available; this happens if sk-&amp;gt;sk_rmem_alloc is greater than or equal to sk-&amp;gt;sk_rcvbuf.
SndbufErrors: Incremented if the IP protocol layer reported an error when trying to send the packet and no error queue has been setup. Also incremented if no send queue space or kernel memory are available.
InCsumErrors: Incremented when a UDP checksum failure is detected. Note that in all cases I could find, InCsumErrors is incrememnted at the same time as InErrors. Thus, InErrors - InCsumErros should yield the count of memory related errors on the receive side.
&lt;/code>&lt;/pre>
&lt;h4 id="监控-udp-socket-统计procnetudp">监控 UDP socket 统计：&lt;code>/proc/net/udp&lt;/code>&lt;/h4>
&lt;pre>&lt;code>$ cat /proc/net/udp
sl local_address rem_address st tx_queue rx_queue tr tm-&amp;gt;when retrnsmt uid timeout inode ref pointer drops
515: 00000000:B346 00000000:0000 07 00000000:00000000 00:00000000 00000000 104 0 7518 2 0000000000000000 0
558: 00000000:0371 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7408 2 0000000000000000 0
588: 0100007F:038F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7511 2 0000000000000000 0
769: 00000000:0044 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7673 2 0000000000000000 0
812: 00000000:006F 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 7407 2 0000000000000000 0
&lt;/code>&lt;/pre>
&lt;p>The first line describes each of the fields in the lines following:&lt;/p>
&lt;ul>
&lt;li>&lt;code>sl&lt;/code>: Kernel hash slot for the socket&lt;/li>
&lt;li>&lt;code>local_address&lt;/code>: Hexadecimal local address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>rem_address&lt;/code>: Hexadecimal remote address of the socket and port number, separated by :.&lt;/li>
&lt;li>&lt;code>st&lt;/code>: The state of the socket. Oddly enough, the UDP protocol layer seems to use some TCP socket states. In the example above, 7 is TCP_CLOSE.&lt;/li>
&lt;li>&lt;code>tx_queue&lt;/code>: The amount of memory allocated in the kernel for outgoing UDP datagrams.&lt;/li>
&lt;li>&lt;code>rx_queue&lt;/code>: The amount of memory allocated in the kernel for incoming UDP datagrams.&lt;/li>
&lt;li>&lt;code>tr&lt;/code>, tm-&amp;gt;when, retrnsmt: These fields are unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>uid&lt;/code>: The effective user id of the user who created this socket.&lt;/li>
&lt;li>&lt;code>timeout&lt;/code>: Unused by the UDP protocol layer.&lt;/li>
&lt;li>&lt;code>inode&lt;/code>: The inode number corresponding to this socket. You can use this to help you determine which user process has this socket open. Check /proc/[pid]/fd, which will contain symlinks to socket[:inode].&lt;/li>
&lt;li>&lt;code>ref&lt;/code>: The current reference count for the socket.&lt;/li>
&lt;li>&lt;code>pointer&lt;/code>: The memory address in the kernel of the struct sock.&lt;/li>
&lt;li>&lt;code>drops&lt;/code>: The number of datagram drops associated with this socket. Note that this does not include any drops related to sending datagrams (on corked UDP sockets or otherwise); this is only incremented in receive paths as of the kernel version examined by this blog post.&lt;/li>
&lt;/ul>
&lt;p>打印这些信息的代码见&lt;a href="https://github.com/torvalds/linux/blob/master/net/ipv4/udp.c#L2396-L2431">net/ipv4/udp.c&lt;/a>.&lt;/p>
&lt;h2 id="114-将数据放到-socket-队列">11.4 将数据放到 socket 队列&lt;/h2>
&lt;p>网络数据通过 &lt;code>sock_queue_rcv&lt;/code> 进入 socket 的接收队列。这个函数在将数据报最终送到接收 队列之前，会做几件事情：&lt;/p>
&lt;ol>
&lt;li>检查 socket 已分配的内存，如果超过了 receive buffer 的大小，丢弃这个包并更新计数&lt;/li>
&lt;li>应用 &lt;code>sk_filter&lt;/code>，这允许 BPF（Berkeley Packet Filter）过滤器在 socket 上被应用&lt;/li>
&lt;li>执行 &lt;code>sk_rmem_scedule&lt;/code>，确保有足够大的 receive buffer 接收这个数据报&lt;/li>
&lt;li>执行 &lt;code>skb_set_owner_r&lt;/code>，这会计算数据报的长度并更新 &lt;code>sk-&amp;gt;sk_rmem_alloc&lt;/code> 计数&lt;/li>
&lt;li>调用&lt;code>__skb_queue_tail&lt;/code> 将数据加到队列尾端&lt;/li>
&lt;/ol>
&lt;p>最后，所有在这个 socket 上等待数据的进程都收到一个通知通过 &lt;code>sk_data_ready&lt;/code> 通知处理 函数。
&lt;strong>这就是一个数据包从到达机器开始，依次穿过协议栈，到达 socket，最终被用户程序读取 的过程。&lt;/strong>&lt;/p>
&lt;h1 id="12-其他">12 其他&lt;/h1>
&lt;p>还有一些值得讨论的地方，放在前面哪里都不太合适，故统一放到这里。&lt;/p>
&lt;h2 id="121-打时间戳-timestamping">12.1 打时间戳 (timestamping)&lt;/h2>
&lt;p>前面提到，网络栈可以收集包的时间戳信息。如果使用了 RPS 功能，有相应的 &lt;code>sysctl&lt;/code> 参数 可以控制何时以及如何收集时间戳；更多关于 RPS、时间戳，以及网络栈在哪里完成这些工 作的内容，请查看前面的章节。一些网卡甚至支持在硬件上打时间戳。
如果你想看内核网络栈给收包增加了多少延迟，那这个特性非常有用。
内核&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/timestamping.txt">关于时间戳的文档&lt;/a> 非常优秀，甚至还包括一个&lt;a href="https://github.com/torvalds/linux/tree/v3.13/Documentation/networking/timestamping">示例程序和相应的 Makefile&lt;/a>，有兴趣的话可以上手试试。
使用 &lt;code>ethtool -T&lt;/code> 可以查看网卡和驱动支持哪种打时间戳方式：&lt;/p>
&lt;pre>&lt;code>$ sudo ethtool -T eth0
Time stamping parameters for eth0:
Capabilities:
software-transmit (SOF_TIMESTAMPING_TX_SOFTWARE)
software-receive (SOF_TIMESTAMPING_RX_SOFTWARE)
software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
PTP Hardware Clock: none
Hardware Transmit Timestamp Modes: none
Hardware Receive Filter Modes: none
&lt;/code>&lt;/pre>
&lt;p>从上面这个信息看，该网卡不支持硬件打时间戳。但这个系统上的软件打时间戳，仍然可以 帮助我判断内核在接收路径上到底带来多少延迟。&lt;/p>
&lt;h2 id="122-socket-低延迟选项busy-polling">12.2 socket 低延迟选项：busy polling&lt;/h2>
&lt;p>socket 有个 &lt;code>SO_BUSY_POLL&lt;/code> 选项，可以让内核在&lt;strong>阻塞式接收&lt;/strong>（blocking receive） 的时候做 busy poll。这个选项会减少延迟，但会增加 CPU 使用量和耗电量。
&lt;strong>重要提示&lt;/strong>：要使用此功能，首先要检查你的设备驱动是否支持。Linux 内核 3.13.0 的 &lt;code>igb&lt;/code> 驱动不支持，但 &lt;code>ixgbe&lt;/code> 驱动支持。如果你的驱动实现(并注册)了 &lt;code>struct net_device_ops&lt;/code>(前面介绍过了)的 &lt;code>ndo_busy_poll&lt;/code> 方法，那它就是支持 &lt;code>SO_BUSY_POLL&lt;/code>。
Intel 有一篇非常好的&lt;a href="http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/open-source-kernel-enhancements-paper.pdf">文章&lt;/a>介绍其原理。
对单个 socket 设置此选项，需要传一个以微秒（microsecond）为单位的时间，内核会 在这个时间内对设备驱动的接收队列做 busy poll。当在这个 socket 上触发一个阻塞式读请 求时，内核会 busy poll 来收数据。
全局设置此选项，可以修改 &lt;code>net.core.busy_poll&lt;/code> 配置（毫秒，microsecond），当 &lt;code>poll&lt;/code> 或 &lt;code>select&lt;/code> 方 法以阻塞方式调用时，busy poll 的时长就是这个值。&lt;/p>
&lt;h2 id="123-netpoll特殊网络场景支持">12.3 Netpoll：特殊网络场景支持&lt;/h2>
&lt;p>Linux 内核提供了一种方式，在内核挂掉（crash）的时候，设备驱动仍然可以接收和发送数 据，相应的 API 被称作 &lt;code>Netpoll&lt;/code>。这个功能在一些特殊的网络场景有用途，比如最著名的两个例子： &lt;a href="http://sysprogs.com/VisualKernel/kgdboe/launch/">&lt;code>kgdb&lt;/code>&lt;/a>和 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/netconsole.txt">&lt;code>netconsole&lt;/code>&lt;/a>。
大部分驱动都支持 &lt;code>Netpoll&lt;/code> 功能。支持此功能的驱动需要实现 &lt;code>struct net_device_ops&lt;/code> 的 &lt;code>ndo_poll_controller&lt;/code> 方法（回调函数，探测驱动模块的时候注册的，前面介绍过）。
当网络设备子系统收包或发包的时候，会首先检查这个包的目的端是不是 &lt;code>netpoll&lt;/code>。
例如，我们来看下&lt;code>__netif_receive_skb_core&lt;/code>，&lt;a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3511-L3514">net/dev/core.c&lt;/a>:&lt;/p>
&lt;pre>&lt;code>static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
{
/* ... */
/* if we've gotten here through NAPI, check netpoll */
if (netpoll_receive_skb(skb))
goto out;
/* ... */
}
&lt;/code>&lt;/pre>
&lt;p>设备驱动收发包相关代码里，关于 &lt;code>netpoll&lt;/code> 的判断逻辑在很前面。
Netpoll API 的消费者可以通过 &lt;code>netpoll_setup&lt;/code> 函数注册 &lt;code>struct netpoll&lt;/code> 变量，后者有收 包和发包的 hook 方法（函数指针）。
如果你对使用 Netpoll API 感兴趣，可以看看 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/netconsole.c">netconsole&lt;/a> 的&lt;a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/netconsole.c">驱动&lt;/a> ，Netpoll API 的头文件 &lt;a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netpoll.h">&lt;code>include/linux/netpoll.h&lt;/code>&lt;/a> ，以及&lt;a href="http://people.redhat.com/~jmoyer/netpoll-linux_kongress-2005.pdf">这个&lt;/a>精 彩的分享。&lt;/p>
&lt;h2 id="124-so_incoming_cpu">12.4 &lt;code>SO_INCOMING_CPU&lt;/code>&lt;/h2>
&lt;p>&lt;code>SO_INCOMING_CPU&lt;/code> 直到 Linux 3.19 才添加, 但它非常有用，所以这里讨论一下。
使用 &lt;code>getsockopt&lt;/code> 带 &lt;code>SO_INCOMING_CPU&lt;/code> 选项，可以判断当前哪个 CPU 在处理这个 socket 的网 络包。你的应用程序可以据此将 socket 交给在期望的 CPU 上运行的线程，增加数据本地性（ data locality）和 CPU 缓存命中率。
在提出 &lt;code>SO_INCOMING_CPU&lt;/code> 的&lt;a href="https://patchwork.ozlabs.org/patch/408257/">邮件列表&lt;/a> 里有一个简单示例框架，展示在什么场景下使用这个功能。&lt;/p>
&lt;h2 id="125-dma-引擎">12.5 DMA 引擎&lt;/h2>
&lt;p>DMA engine (直接内存访问引擎)是一个硬件，允许 CPU 将&lt;strong>很大的复制操作&lt;/strong>（large copy operations）offload（下放）给它。这样 CPU 就从数据拷贝中解放出来，去做其他工作，而 拷贝就交由硬件完成。合理的使用 DMA 引擎（代码要利用到 DMA 特性）可以减少 CPU 的使用量 。
Linux 内核有一个通用的 DMA 引擎接口，DMA engine 驱动实现这个接口即可。更多关于这个接 口的信息可以查看内核源码的&lt;a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/dmaengine.txt">文档  &lt;/a>。
内核支持的 DMA 引擎很多，这里我们拿 Intel 的&lt;a href="https://en.wikipedia.org/wiki/I/O_Acceleration_Technology">IOAT DMA engine&lt;/a>为例来看一下。&lt;/p>
&lt;h3 id="intels-io-acceleration-technology-ioat">Intel’s I/O Acceleration Technology (IOAT)&lt;/h3>
&lt;p>很多服务器都安装了&lt;a href="http://www.intel.com/content/www/us/en/wireless-network/accel-technology.html">Intel I/O AT bundle&lt;/a> ，其中包含了一系列性能优化相关的东西，包括一个硬件 DMA 引擎。可以查看 &lt;code>dmesg&lt;/code> 里面有 没有 &lt;code>ioatdma&lt;/code>，判断这个模块是否被加载，以及它是否找到了支持的硬件。
DMA 引擎在很多地方有用到，例如 TCP 协议栈。
Intel IOAT DMA engine 最早出现在 Linux 2.6.18，但随后 3.13.11.10 就禁用掉了，因为有一些 bug，会导致数据损坏。
&lt;code>3.13.11.10&lt;/code> 版本之前的内核默认是开启的，将来这些版本的内核如果有更新，可能也会禁用掉。&lt;/p>
&lt;h4 id="直接缓存访问-dca-direct-cache-access">直接缓存访问 (DCA, Direct cache access)&lt;/h4>
&lt;p>&lt;a href="http://www.intel.com/content/www/us/en/wireless-network/accel-technology.html">Intel I/O AT bundle&lt;/a> 中的另一个有趣特性是直接缓存访问（DCA）。
该特性允许网络设备（通过各自的驱动）直接将网络数据放到 CPU 缓存上。至于是如何实现 的，随各家驱动而异。对于 &lt;code>igb&lt;/code> 的驱动，你可以查看 &lt;code>igb_update_dca&lt;/code> 和 &lt;code>igb_update_rx_dca&lt;/code> 这两个函数的实现。&lt;code>igb&lt;/code> 驱动使用 DCA，直接写硬件网卡的一个 寄存器。
要使用 DCA 功能，首先检查你的 BIOS 里是否打开了此功能，然后确保 &lt;code>dca&lt;/code> 模块加载了， 还要确保你的网卡和驱动支持 DCA。&lt;/p>
&lt;h4 id="monitoring-ioat-dma-engine">Monitoring IOAT DMA engine&lt;/h4>
&lt;p>如上所说，如果你不怕数据损坏的风险，那你可以使用 &lt;code>ioatdma&lt;/code> 模块。监控上，可以看几 个 sysfs 参数。
例如，监控一个 DMA 通道（channel）总共 offload 的 &lt;code>memcpy&lt;/code> 操作次数：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/dma/dma0chan0/memcpy_count
123205655
&lt;/code>&lt;/pre>
&lt;p>类似的，一个 DMA 通道总共 offload 的字节数：&lt;/p>
&lt;pre>&lt;code>$ cat /sys/class/dma/dma0chan0/bytes_transferred
131791916307
&lt;/code>&lt;/pre>
&lt;h4 id="tuning-ioat-dma-engine">Tuning IOAT DMA engine&lt;/h4>
&lt;p>IOAT DMA engine 只有在包大小超过一定的阈值之后才会使用，这个阈值叫 &lt;code>copybreak&lt;/code>。 之所以要设置阈值是因为，对于小包，设置和使用 DMA 的开销要大于其收益。
调整 DMA engine &lt;code>copybreak&lt;/code> 参数：&lt;/p>
&lt;pre>&lt;code>$ sudo sysctl -w net.ipv4.tcp_dma_copybreak=2048
&lt;/code>&lt;/pre>
&lt;p>默认值是 4096。&lt;/p>
&lt;h1 id="13-总结">13 总结&lt;/h1>
&lt;p>Linux 网络栈很复杂。
对于这样复杂的系统（以及类似的其他系统）， 如果不能在更深的层次理解它正在做什么，就不可能做监控和调优。 当遇到网络问题时，你可能会在网上搜到一些 &lt;code>sysctl.conf&lt;/code> 最优实践一类的东西，然后应 用在自己的系统上，但这并不是网络栈调优的最佳方式。
监控网络栈需要从驱动开始，逐步往上，仔细地在每一层统计网络数据。 这样你才能清楚地看出哪里有丢包（drop），哪里有收包错误（errors），然后根据导致错 误的原因做相应的配置调整。
&lt;strong>不幸的是，这项工作并没有捷径。&lt;/strong>&lt;/p>
&lt;h1 id="14-额外讨论和帮助">14 额外讨论和帮助&lt;/h1>
&lt;p>需要一些额外的关于网络栈的指导(navigating the network stack)？对本文有疑问，或有 相关内容本文没有提到？以上问题，都可以发邮件给&lt;a href="http://arthurchiao.art/blog/tuning-stack-rx-zh/support@packagecloud.io">我们&lt;/a>， 以便我们知道如何提供帮助。&lt;/p>
&lt;h1 id="15-相关文章">15 相关文章&lt;/h1>
&lt;p>如果你喜欢本文，你可能对下面这些底层技术文章也感兴趣：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">Monitoring and Tuning the Linux Networking Stack: Sending Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/">The Definitive Guide to Linux System Calls&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/02/29/how-does-strace-work/">How does strace work?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/03/14/how-does-ltrace-work/">How does ltrace work?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2016/03/21/apt-hash-sum-mismatch/">APT Hash sum mismatch&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2014/10/28/howto-gpg-sign-verify-deb-packages-apt-repositories/">HOWTO: GPG sign and verify deb packages and APT repositories&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.packagecloud.io/eng/2014/11/24/howto-gpg-sign-verify-rpm-packages-yum-repositories/">HOWTO: GPG sign and verify RPM packages and yum repositories&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: 用户空间和内核空间通讯--netlink</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netlink/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4%E9%80%9A%E8%AE%AF--netlink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/linux-%E7%BD%91%E7%BB%9C%E6%A0%88%E7%AE%A1%E7%90%86/netlink/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4%E9%80%9A%E8%AE%AF--netlink/</guid><description>
&lt;p>&lt;strong>目录&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#%E4%BB%80%E4%B9%88%E6%98%AFnetlink">什么是 Netlink&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E9%80%9A%E4%BF%A1%E7%B1%BB%E5%9E%8B">Netlink 通信类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E7%9A%84%E6%B6%88%E6%81%AF%E6%A0%BC%E5%BC%8F">Netlink 的消息格式&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E7%9A%84%E6%B6%88%E6%81%AF%E5%A4%B4">Netlink 的消息头&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E7%9A%84%E6%B6%88%E6%81%AF%E4%BD%93">Netlink 的消息体&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E6%8F%90%E4%BE%9B%E7%9A%84%E9%94%99%E8%AF%AF%E6%8C%87%E7%A4%BA%E6%B6%88%E6%81%AF">Netlink 提供的错误指示消息&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E7%BC%96%E7%A8%8B%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98">Netlink 编程需要注意的问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E7%9A%84%E5%9C%B0%E5%9D%80%E7%BB%93%E6%9E%84%E4%BD%93">Netlink 的地址结构体&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#%E5%AE%9E%E9%AA%8C">实验&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#%E7%AC%AC%E4%B8%80%E6%AD%A5">第一步&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#%E7%AC%AC%E4%BA%8C%E6%AD%A5">第二步&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#%E7%AC%AC%E4%B8%89%E6%AD%A5">第三步&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1#netlink%E5%A4%9A%E6%92%AD">Netlink 多播&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1">请尊重原创版权，转载注明出处。&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://e-mailky.github.io/2017-02-14-netlink-user-kernel1">https://e-mailky.github.io/2017-02-14-netlink-user-kernel1&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Alan Cox 在内核 1.3 版本的开发阶段最先引入了 Netlink，刚开始时 Netlink 是以 字符驱动接口的方式提供内核与用户空间的双向数据通信；随后，在 2.1 内核开发过程中，Alexey Kuznetsov 将 Netlink 改写成一个更加灵活、且易于扩展的基于消息通信接口，并将其应用到高级路由子系统的基础框架里。 自那时起，Netlink 就成了 Linux 内核子系统和用户态的应用程序通信的主要手段之一。
2001 年，ForCES IETF 委员会正式对 Netlink 进行了标准化的工作。Jamal Hadi Salim 提议将 Netlink 定义成一种用于网络设备的路由引擎组件和其控制管理组件之间通信的协议。不过他的建议 最终没有被采纳，取而代之的是我们今天所看到的格局：Netlink 被设计成一个新的协议域，domain。
Linux 之父托瓦斯曾说过“Linux is evolution, not intelligent design”。 什么意思？就是说，Netlink 也同样遵循了 Linux 的某些设计理念，即没有完整的规范文档，亦没有设计文档。 只有什么？你懂得—“Read the f**king source code”。
当然，本文不是分析 Netlink 在 Linux 上的实现机制，而是就“什么是 Netlink”以及 “如何用好 Netlink”的话题和大家做个分享，只有在遇到问题时才需要去阅读内核源码弄清个所以然。&lt;/p>
&lt;h2 id="什么是-netlink">什么是 Netlink&lt;/h2>
&lt;p>关于 Netlink 的理解，需要把握几个关键点：&lt;/p>
&lt;ol>
&lt;li>面向数据报的无连接消息子系统&lt;/li>
&lt;li>基于通用的 BSD Socket 架构而实现&lt;/li>
&lt;/ol>
&lt;p>关于第一点使我们很容易联想到 UDP 协议，能想到这一点就非常棒了。按着 UDP 协议 来理解 Netlink 不是不无道理，只要你能触类旁通，做到“活学”，善于总结归纳、联想，最后实现知识迁移这就是 学习的本质。Netlink 可以实现内核-&amp;gt;用户以及用户-&amp;gt;内核的双向、异步的数据通信，同时它还支持两个用户进程之间、 甚至两个内核子系统之间的数据通信。本文中，对后两者我们不予考虑，焦点集中在如何实现用户&amp;lt;-&amp;gt;内核之间的数据通信。
看到第二点脑海中是不是瞬间闪现了下面这张图片呢？ 如果是，则说明你确实有慧根；当然，不是也没关系，慧根可以慢慢长嘛，呵呵。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609972955-cbd73443-4468-439d-87a1-cc83f2daa893.jpeg" alt="">
在后面实战 Netlink 套接字编程时我们主要会用到 socket()，bind()，sendmsg() 和 recvmsg()等系统调用，当然还有 socket 提供的轮训(polling)机制。&lt;/p>
&lt;h2 id="netlink-通信类型">Netlink 通信类型&lt;/h2>
&lt;p>Netlink 支持两种类型的通信方式：单播和多播。
单播：经常用于一个用户进程和一个内核子系统之间 1:1 的数据通信。用户空间发送命令到内核，然后从内核接受命令的返回结果。
多播：经常用于一个内核进程和多个用户进程之间的 1:N 的数据通信。内核作为会话的发起者， 用户空间的应用程序是接收者。为了实现这个功能，内核空间的程序会创建一个多播组， 然后所有用户空间的对该内核进程发送的消息感兴趣的进程都加入到该组即可接收来自内核发送的消息了。如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609976015-6753cbdb-f771-4657-a4b6-d7c3735f8d40.jpeg" alt="">
其中进程 A 和子系统 1 之间是单播通信，进程 B、C 和子系统 2 是多播通信。 上图还向我们说明了一个信息。从用户空间传递到内核的数据是不需要排队的，即其操作是同步完成； 而从内核空间向用户空间传递数据时需要排队，是异步的。了解了这一点在开发基于 Netlink 的应用模块时 可以使我们少走很多弯路。假如，你向内核发送了一个消息需要获取内核中某些信息，比如路由表，或其他信息， 如果路由表过于庞大，那么内核在通过 Netlink 向你返回数据时，你可以好生琢磨一下如何接收这些数据的问题， 毕竟你已经看到了那个输出队列了，不能视而不见啊。&lt;/p>
&lt;h2 id="netlink-的消息格式">Netlink 的消息格式&lt;/h2>
&lt;p>Netlink 消息由两部分组成：消息头和有效数据载荷， 且整个 Netlink 消息是 4 字节对齐，一般按主机字节序进行传递。消息头为固定的 16 字节，消息体长度可变：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609971315-01096608-1381-4ee9-9d56-d0c80b0ffe78.jpeg" alt="">&lt;/p>
&lt;h2 id="netlink-的消息头">Netlink 的消息头&lt;/h2>
&lt;p>消息头定义在文件里，由结构体 nlmsghdr 表示：
struct nlmsghdr { __u32 nlmsg&lt;em>len; /&lt;/em> Length of message including header &lt;em>/ __u16 nlmsg_type; /&lt;/em> Message content &lt;em>/ __u16 nlmsg_flags; /&lt;/em> Additional flags &lt;em>/ __u32 nlmsg_seq; /&lt;/em> Sequence number &lt;em>/ __u32 nlmsg_pid; /&lt;/em> Sending process PID _/ };
消息头中各成员属性的解释及说明：
nlmsg_len：整个消息的长度，按字节计算。包括了 Netlink 消息头本身。
nlmsg_type：消息的类型，即是数据还是控制消息。目前(内核版本 2.6.21)Netlink 仅支持四种类型的控制消息，如下：
NLMSG_NOOP- 空消息，什么也不做； NLMSG_ERROR- 指明该消息中包含一个错误； NLMSG_DONE- 如果内核通过 Netlink 队列返回了多个消息，那么队列的最后一条消息的类型为 NLMSG_DONE， 其余所有消息的 nlmsg_flags 属性都被设置 NLM_F_MULTI 位有效。 NLMSG_OVERRUN-暂时没用到。
nlmsg_flags：附加在消息上的额外说明信息，如上面提到的 NLM_F_MULTI。摘录如下：
标记 | 作用及说明 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; NLM_F_REQUEST | 如果消息中有该标记位，说明这是一个请求消息。所有从用户空间到内核空间的消息都要设置该位，否则内核将向用户返回一个 EINVAL 无效参数的错误 NLM_F_MULTI | 消息从用户-&amp;gt;内核是同步的立刻完成，而从内核-&amp;gt;用户则需要排队。如果内核之前收到过来自用户的消息中有 NLM_F_DUMP 位为 1 的消息，那么内核就会向用户空间发送一个由多个 Netlink 消息组成的链表。除了最后个消息外，其余每条消息中都设置了该位有效。 NLM_F_ACK | 该消息是内核对来自用户空间的 NLM_F_REQUEST 消息的响应 NLM_F_ECHO | 如果从用户空间发给内核的消息中该标记为 1，则说明用户的应用进程要求内核将用户发给它的每条消息通过单播的形式再发送给用户进程。和我们通常说的“回显”功能类似。 &amp;hellip; | &amp;hellip;
大家只要知道 nlmsg_flags 有多种取值就可以，至于每种值的作用和意义， 通过谷歌和源代码一定可以找到答案，这里就不展开了。上一张 2.6.21 内核中所有的取值情况：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609971427-92b211e9-5d53-41f0-9d5d-9c784df8397a.jpeg" alt="">
nlmsg_seq：消息序列号。因为 Netlink 是面向数据报的，所以存在丢失数据的风险，但是 Netlink 提供了如何确保消息不丢失的机制， 让程序开发人员根据其实际需求而实现。消息序列号一般和 NLM_F_ACK 类型的消息联合使用， 如果用户的应用程序需要保证其发送的每条消息都成功被内核收到的话，那么它发送消息时 需要用户程序自己设置序号，内核收到该消息后对提取其中的序列号，然后在发送给用户程序 回应消息里设置同样的序列号。有点类似于 TCP 的响应和确认机制。
&lt;strong>注意：当内核主动向用户空间发送广播消息时，消息中的该字段总是为 0。&lt;/strong>
nlmsg_pid：当用户空间的进程和内核空间的某个子系统之间通过 Netlink 建立了数据交换的通道后， Netlink 会为每个这样的通道分配一个唯一的数字标识。其主要作用就是将来自用户空间的请求消息 和响应消息进行关联。说得直白一点，假如用户空间存在多个用户进程，内核空间同样存在多个进程， Netlink 必须提供一种机制用于确保每一对“用户-内核”空间通信的进程之间的数据交互不会发生紊乱。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609971166-de3027d7-ff6a-4459-bd99-766a18a23b6d.jpeg" alt="">
即，进程 A、B 通过 Netlink 向子系统 1 获取信息时，子系统 1 必须确保回送给进程 A 的 响应数据不会发到进程 B 那里。主要适用于用户空间的进程从内核空间获取数据的场景。通常情况下，用户空间的进程 在向内核发送消息时一般通过系统调用 getpid()将当前进程的进程号赋给该变量，即用户空间的进程希望得到 内核的响应时才会这么做。从内核主动发送到用户空间的消息该字段都被设置为 0。&lt;/p>
&lt;h2 id="netlink-的消息体">Netlink 的消息体&lt;/h2>
&lt;p>Netlink 的消息体采用 TLV(Type-Length-Value)格式：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609974851-b2f8574a-cbf8-4cc5-a9fe-c21246356b52.jpeg" alt="">
Netlink 每个属性都由文件里的 struct nlattr{}来表示：
struct nlattr { __u16 nla_len; __u16 nla_type; };&lt;/p>
&lt;h2 id="netlink-提供的错误指示消息">Netlink 提供的错误指示消息&lt;/h2>
&lt;p>当用户空间的应用程序和内核空间的进程之间通过 Netlink 通信时发生了错误， Netlink 必须向用户空间通报这种错误。Netlink 对错误消息进行了单独封装，：
struct nlmsgerr { int error; //标准的错误码，定义在 errno.h 头文件中。可以用 perror()来解释 struct nlmsghdr msg; //指明了哪条消息触发了结构体中 error 这个错误值 };&lt;/p>
&lt;h2 id="netlink-编程需要注意的问题">Netlink 编程需要注意的问题&lt;/h2>
&lt;p>基于 Netlink 的用户-内核通信，有两种情况可能会导致丢包：&lt;/p>
&lt;ol>
&lt;li>内存耗尽；&lt;/li>
&lt;li>用户空间接收进程的缓冲区溢出。导致缓冲区溢出的主要原因有可能是：用户空间的进程运行太慢；或者接收队列太短。&lt;/li>
&lt;/ol>
&lt;p>如果 Netlink 不能将消息正确传递到用户空间的接收进程，那么用户空间的接收进程在 调用 recvmsg()系统调用时就会返回一个内存不足(ENOBUFS)的错误，这一点需要注意。换句话说， 缓冲区溢出的情况是不会发送在从用户-&amp;gt;内核的 sendmsg()系统调用里，原因前面我们也说过了，请大家自己思考一下。
当然，如果使用的是阻塞型 socket 通信，也就不存在内存耗尽的隐患了，这又是为什么呢？ 赶紧去谷歌一下，查查什么是阻塞型 socket 吧。学而不思则罔，思而不学则殆嘛。&lt;/p>
&lt;h2 id="netlink-的地址结构体">Netlink 的地址结构体&lt;/h2>
&lt;p>在 TCP 博文中我们提到过在 Internet 编程过程中所用到的地址结构体和标准地址结构体， 它们和 Netlink 地址结构体的关系如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609973262-e91727eb-0e30-4423-92e3-47ff1b08ded4.jpeg" alt="">
struct sockaddr&lt;em>nl{}的详细定义和描述如下：
struct sockaddr_nl { sa_family_t nl_family; /&lt;em>该字段总是为 AF&lt;/em>NETLINK &lt;em>/ unsigned short nl&lt;/em>pad; /&lt;/em> 目前未用到，填充为 0*/ __u32 nl_pid; /* process pid &lt;em>/ __u32 nl_groups; /&lt;/em> multicast groups mask _/ };
nl_pid：该属性为发送或接收消息的进程 ID，前面我们也说过，Netlink 不仅可以实现用户-内核空间的通信还可 使现实用户空间两个进程之间，或内核空间两个进程之间的通信。该属性为 0 时一般适用于如下两种情况：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>我们要发送的目的地是内核，即从用户空间发往内核空间时，我们构造的 Netlink 地址结构体中 nl_pid 通常 情况下都置为 0。这里有一点需要跟大家交代一下，在 Netlink 规范里，PID 全称是 Port-ID(32bits)， 其主要作用是用于唯一的标识一个基于 netlink 的 socket 通道。通常情况下 nl_pid 都设置为当前进程的进程号。 然而，对于一个进程的多个线程同时使用 netlink socket 的情况，nl_pid 的设置一般采用如下这个样子来实现：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>pthread_self() « 16&lt;/th>
&lt;th>getpid();&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;/li>
&lt;li>
&lt;p>从内核发出的多播报文到用户空间时，如果用户空间的进程处在该多播组中，那么其地址结构体中 nl_pid 也设置为 0，同时还要结合下面介绍到的另一个属性。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nl_groups：如果用户空间的进程希望加入某个多播组，则必须执行 bind()系统调用。 该字段指明了调用者希望加入的多播组号的掩码(注意不是组号，后面我们会详细讲解这个字段)。 如果该字段为 0 则表示调用者不希望加入任何多播组。对于每个隶属于 Netlink 协议域的协议， 最多可支持 32 个多播组(因为 nl_groups 的长度为 32 比特)，每个多播组用一个比特来表示。&lt;/p>
&lt;h2 id="实验">实验&lt;/h2>
&lt;p>在文件里包含了 Netlink 协议簇已经定义好的一些预定义协议：
#define NETLINK&lt;em>ROUTE 0 /&lt;/em> Routing/device hook &lt;em>/ #define NETLINK_UNUSED 1 /&lt;/em> Unused number &lt;em>/ #define NETLINK_USERSOCK 2 /&lt;/em> Reserved for user mode socket protocols &lt;em>/ #define NETLINK_FIREWALL 3 /&lt;/em> Firewalling hook &lt;em>/ #define NETLINK_INET_DIAG 4 /&lt;/em> INET socket monitoring &lt;em>/ #define NETLINK_NFLOG 5 /&lt;/em> netfilter/iptables ULOG &lt;em>/ #define NETLINK_XFRM 6 /&lt;/em> ipsec &lt;em>/ #define NETLINK_SELINUX 7 /&lt;/em> SELinux event notifications &lt;em>/ #define NETLINK_ISCSI 8 /&lt;/em> Open-iSCSI &lt;em>/ #define NETLINK_AUDIT 9 /&lt;/em> auditing &lt;em>/ #define NETLINK_FIB_LOOKUP 10 #define NETLINK_CONNECTOR 11 #define NETLINK_NETFILTER 12 /&lt;/em> netfilter subsystem &lt;em>/ #define NETLINK_IP6_FW 13 #define NETLINK_DNRTMSG 14 /&lt;/em> DECnet routing messages &lt;em>/ #define NETLINK_KOBJECT_UEVENT 15 /&lt;/em> Kernel messages to userspace &lt;em>/ #define NETLINK_GENERIC 16 /&lt;/em> leave room for NETLINK&lt;em>DM (DM Events) &lt;em>/ #define NETLINK&lt;/em>SCSITRANSPORT 18 /&lt;/em> SCSI Transports &lt;em>/ #define NETLINK_ECRYPTFS 19 #define NETLINK_TEST 20 /&lt;/em> 用户添加的自定义协议 _/
如果我们在 Netlink 协议簇里开发一个新的协议，只要在该文件中定义协议号即可， 例如我们定义一种基于 Netlink 协议簇的、协议号是 20 的自定义协议，如上所示。同时记得，将内核头文件目录中 的 netlink.h 也做对应的修改，在我的系统中它的路径是：/usr/src/linux-2.6.21/include/linux/netlink.h
接下来我们在用户空间以及内核空间模块的开发过程中就可以使用这种协议了，一共分为三个阶段。&lt;/p>
&lt;h3 id="第一步">第一步&lt;/h3>
&lt;p>我们首先实现的功能是用户-&amp;gt;内核的单向数据通信，即用户空间发送一个消息给内核，然后内核将其打印输出， 就这么简单。用户空间的示例代码如下【mynlusr.c】
#include &amp;lt;sys/stat.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;sys/socket.h&amp;gt; #include &amp;lt;sys/types.h&amp;gt; #include &amp;lt;string.h&amp;gt; #include &amp;lt;asm/types.h&amp;gt; #include &amp;lt;linux/netlink.h&amp;gt; #include &amp;lt;linux/socket.h&amp;gt; #define MAX&lt;em>PAYLOAD 1024 /&lt;em>消息最大负载为 1024 字节&lt;/em>/ int main(int argc, char&lt;/em> argv[]) { struct sockaddr&lt;em>nl dest_addr; struct nlmsghdr &lt;em>nlh = NULL; struct iovec iov; int sock_fd=-1; struct msghdr msg; if(-1 == (sock_fd=socket(PF_NETLINK, SOCK_RAW,NETLINK_TEST))){ //创建套接字 perror(&amp;ldquo;can&amp;rsquo;t create netlink socket!&amp;rdquo;); return 1; } memset(&amp;amp;dest_addr, 0, sizeof(dest_addr)); dest_addr.nl_family = AF_NETLINK; dest_addr.nl_pid = 0; /&lt;em>我们的消息是发给内核的&lt;/em>/ dest_addr.nl_groups = 0; /&lt;em>在本示例中不存在使用该值的情况&lt;/em>/ //将套接字和 Netlink 地址结构体进行绑定 if(-1 == bind(sock_fd, (struct sockaddr&lt;/em>)&amp;amp;dest_addr, sizeof(dest_addr))){ perror(&amp;ldquo;can&amp;rsquo;t bind sockfd with sockaddr_nl!&amp;rdquo;); return 1; } if(NULL == (nlh=(struct nlmsghdr &lt;em>)malloc(NLMSG&lt;/em>SPACE(MAX_PAYLOAD)))){ perror(&amp;ldquo;alloc mem failed!&amp;rdquo;); return 1; } memset(nlh,0,MAX_PAYLOAD); /&lt;/em> 填充 Netlink 消息头部 &lt;em>/ nlh-&amp;gt;nlmsg_len = NLMSG_SPACE(MAX_PAYLOAD); nlh-&amp;gt;nlmsg_pid = 0; nlh-&amp;gt;nlmsg_type = NLMSG_NOOP; //指明我们的 Netlink 是消息负载是一条空消息 nlh-&amp;gt;nlmsg_flags = 0; /&lt;em>设置 Netlink 的消息内容，来自我们命令行输入的第一个参数&lt;/em>/ strcpy(NLMSG_DATA(nlh), argv[1]); /&lt;em>这个是模板，暂时不用纠结为什么要这样用。有时间详细讲解 socket 时再说&lt;/em>/ memset(&amp;amp;iov, 0, sizeof(iov)); iov.iov_base = (void &lt;em>)nlh; iov.iov&lt;/em>len = nlh-&amp;gt;nlmsg_len; memset(&amp;amp;msg, 0, sizeof(msg)); msg.msg_iov = &amp;amp;iov; msg.msg_iovlen = 1; sendmsg(sock_fd, &amp;amp;msg, 0); //通过 Netlink socket 向内核发送消息 /&lt;/em> 关闭 netlink 套接字 */ close(sock_fd); free(nlh); return 0; }
上面的代码逻辑已经非常清晰了，都是 socket 编程的 API，唯一不同的是我们这次编程 是针对 Netlink 协议簇的。这里我们提前引入了 BSD 层的消息结构体 struct msghdr{}，定义在文件里， 以及其数据块 struct iovec{}定义在头文件里。这里就不展开了，大家先记住这个用法就行。以后有时间再深入 到 socket 的骨子里去转悠一番。
另外，需要格外注意的就是 Netlink 的地址结构体和其消息头结构中 pid 字段为 0 的情况，很容易让人产生混淆，再总结一下：
nl_pid | 0 ————————-|—– netlink 地址结构体.nl_pid | 1、内核发出的多播报文 2、消息的接收方是内核，即从用户空间发往内核的消息 netlink 消息头体 nlmsg_pid | 来自内核主动发出的消息
这个例子仅是从用户空间到内核空间的单向数据通信，所以 Netlink 地址结构体中 我们设置了 dest_addr.nl_pid = 0，说明我们的报文的目的地是内核空间；在填充 Netlink 消息头部时， 我们做了 nlh-&amp;gt;nlmsg_pid = 0 这样的设置。
需要注意几个宏的使用：
NLMSG_SPACE(MAX_PAYLOAD): 该宏用于返回不小于 MAX_PAYLOAD 且 4 字节对齐的最小长度值，一般用于向内存系统申请空间是指定所申请的内存字节数
NLMSG_LENGTH(len): 所不同的是，前者所申请的空间里不包含 Netlink 消息头部所占的字节数，后者是消息负载和消息头加起来的总长度
NLMSG_DATA(nlh): 该宏用于返回 Netlink 消息中数据部分的首地址，在写入和读取消息数据部分时会用到它。
它们之间的关系如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609977751-61452e76-5d04-4003-8e2e-80e3e173fd59.jpeg" alt="">
内核空间的示例代码如下【mynlkern.c】
#include &amp;lt;linux/kernel.h&amp;gt; #include &amp;lt;linux/module.h&amp;gt; #include &amp;lt;linux/skbuff.h&amp;gt; #include &amp;lt;linux/init.h&amp;gt; #include &amp;lt;linux/ip.h&amp;gt; #include &amp;lt;linux/types.h&amp;gt; #include &amp;lt;linux/sched.h&amp;gt; #include &amp;lt;net/sock.h&amp;gt; #include &amp;lt;linux/netlink.h&amp;gt; MODULE_LICENSE(&amp;ldquo;GPL&amp;rdquo;); MODULE_AUTHOR(&amp;ldquo;Koorey King&amp;rdquo;); struct sock &lt;em>nl&lt;/em>sk = NULL; static void nl_data_ready (struct sock &lt;em>sk, int len) { struct sk&lt;/em>buff _skb; struct nlmsghdr _nlh = NULL; while((skb = skb_dequeue(&amp;amp;sk-&amp;gt;sk_receive_queue)) != NULL) { nlh = (struct nlmsghdr &lt;em>)skb-&amp;gt;data; printk(&amp;quot;%s: received netlink message payload: %s \n&amp;quot;, &lt;strong>FUNCTION&lt;/strong>, (char&lt;/em>)NLMSG_DATA(nlh)); kfree_skb(skb); } printk(&amp;ldquo;recvied finished!\n&amp;rdquo;); } static int __init myinit_module() { printk(&amp;ldquo;my netlink in\n&amp;rdquo;); nl_sk = netlink_kernel_create(NETLINK_TEST,0,nl_data_ready,THIS_MODULE); return 0; } static void __exit mycleanup_module() { printk(&amp;ldquo;my netlink out!\n&amp;rdquo;); sock_release(nl_sk-&amp;gt;sk_socket); } module_init(myinit_module); module_exit(mycleanup_module);
在内核模块的初始化函数里我们用
nl_sk = netlink_kernel_create(NETLINK_TEST,0,nl_data_ready,THIS_MODULE);
创建了一个内核态的 socket，第一个参数我们扩展的协议号；第二个参数为多播组号，目前我们用不上，将其置为 0； 第三个参数是个回调函数，即当内核的 Netlink socket 套接字收到数据时的处理函数；第四个参数就不多说了。
在回调函数 nl_data_ready()中，我们不断的从 socket 的接收队列去取数据， 一旦拿到数据就将其打印输出。在协议栈的 INET 层，用于存储数据的是大名鼎鼎的 sk_buff 结构，所以我们通过 nlh = (struct nlmsghdr *)skb-&amp;gt;data；可以拿到 netlink 的消息体，然后通过 NLMSG_DATA(nlh)定位到 netlink 的消息负载。
将上述代码编译后测试结果如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609975598-daa41911-732a-416a-993e-1b07150c3775.jpeg" alt="">&lt;/p>
&lt;h3 id="第二步">第二步&lt;/h3>
&lt;p>我们将上面的代码稍加改造就可以实现用户&amp;lt;-&amp;gt;内核的双向数据通信。
首先是改造用户空间的代码：
#include &amp;lt;sys/stat.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;sys/socket.h&amp;gt; #include &amp;lt;sys/types.h&amp;gt; #include &amp;lt;string.h&amp;gt; #include &amp;lt;asm/types.h&amp;gt; #include &amp;lt;linux/netlink.h&amp;gt; #include &amp;lt;linux/socket.h&amp;gt; #define MAX&lt;em>PAYLOAD 1024 /&lt;em>消息最大负载为 1024 字节&lt;/em>/ int main(int argc, char&lt;/em> argv[]) { struct sockaddr&lt;em>nl dest_addr; struct nlmsghdr &lt;em>nlh = NULL; struct iovec iov; int sock&lt;/em>fd=-1; struct msghdr msg; if(-1 == (sock_fd=socket(PF_NETLINK, SOCK_RAW,NETLINK_TEST))){ perror(&amp;ldquo;can&amp;rsquo;t create netlink socket!&amp;rdquo;); return 1; } memset(&amp;amp;dest_addr, 0, sizeof(dest_addr)); dest_addr.nl_family = AF_NETLINK; dest_addr.nl_pid = 0; /&lt;em>我们的消息是发给内核的&lt;/em>/ dest_addr.nl_groups = 0; /&lt;em>在本示例中不存在使用该值的情况&lt;/em>/ if(-1 == bind(sock_fd, (struct sockaddr&lt;/em>)&amp;amp;dest_addr, sizeof(dest_addr))){ perror(&amp;ldquo;can&amp;rsquo;t bind sockfd with sockaddr_nl!&amp;rdquo;); return 1; } if(NULL == (nlh=(struct nlmsghdr &lt;em>)malloc(NLMSG_SPACE(MAX_PAYLOAD)))){ perror(&amp;ldquo;alloc mem failed!&amp;rdquo;); return 1; } memset(nlh,0,MAX_PAYLOAD); /&lt;/em> 填充 Netlink 消息头部 &lt;em>/ nlh-&amp;gt;nlmsg_len = NLMSG_SPACE(MAX_PAYLOAD); &lt;code>nlh-&amp;gt;nlmsg_pid = getpid();//我们希望得到内核回应，所以得告诉内核我们ID号&lt;/code> nlh-&amp;gt;nlmsg_type = NLMSG_NOOP; //指明我们的 Netlink 是消息负载是一条空消息 nlh-&amp;gt;nlmsg_flags = 0; /&lt;em>设置 Netlink 的消息内容，来自我们命令行输入的第一个参数&lt;/em>/ strcpy(NLMSG_DATA(nlh), argv[1]); /&lt;em>这个是模板，暂时不用纠结为什么要这样用。&lt;/em>/ memset(&amp;amp;iov, 0, sizeof(iov)); iov.iov&lt;/em>base = (void &lt;em>)nlh; iov.iov&lt;/em>len = nlh-&amp;gt;nlmsg_len; memset(&amp;amp;msg, 0, sizeof(msg)); msg.msg_iov = &amp;amp;iov; msg.msg_iovlen = 1; sendmsg(sock_fd, &amp;amp;msg, 0); //通过 Netlink socket 向内核发送消息 //接收内核消息的消息 printf(&amp;ldquo;waiting message from kernel!\n&amp;rdquo;); memset((char*)NLMSG&lt;em>DATA(nlh),0,1024); recvmsg(sock_fd,&amp;amp;msg,0); printf(&amp;ldquo;Got response: %s\n&amp;rdquo;,NLMSG_DATA(nlh)); /&lt;/em> 关闭 netlink 套接字 &lt;em>/ close(sock_fd); free(nlh); return 0; }
内核空间的修改如下：
#include &amp;lt;linux/kernel.h&amp;gt; #include &amp;lt;linux/module.h&amp;gt; #include &amp;lt;linux/skbuff.h&amp;gt; #include &amp;lt;linux/init.h&amp;gt; #include &amp;lt;linux/ip.h&amp;gt; #include &amp;lt;linux/types.h&amp;gt; #include &amp;lt;linux/sched.h&amp;gt; #include &amp;lt;net/sock.h&amp;gt; #include &amp;lt;net/netlink.h&amp;gt; /&lt;em>该文头文件里包含了 linux/netlink.h，因为我们要用到 net/netlink.h 中的某些 API 函数，nlmsg&lt;/em>put()&lt;/em>/ MODULE*LICENSE(&amp;ldquo;GPL&amp;rdquo;); MODULE_AUTHOR(&amp;ldquo;Koorey King&amp;rdquo;); struct sock *nl_sk = NULL; //向用户空间发送消息的接口 void sendnlmsg(char *message,int dstPID) { struct sk_buff *skb; struct nlmsghdr *nlh; int len = NLMSG_SPACE(MAX_MSGSIZE); int slen = 0; if(!message || !nl_sk){ return; } // 为新的 sk_buffer 申请空间 skb = alloc_skb(len, GFP_KERNEL); if(!skb){ printk(KERN_ERR &amp;ldquo;my_net_link: alloc_skb Error./n&amp;rdquo;); return; } slen = strlen(message)+1; //用 nlmsg_put()来设置 netlink 消息头部 nlh = nlmsg_put(skb, 0, 0, 0, MAX_MSGSIZE, 0); // 设置 Netlink 的控制块 NETLINK_CB(skb).pid = 0; // 消息发送者的 id 标识，如果是内核发的则置 0 NETLINK_CB(skb).dst_group = 0; //如果目的组为内核或某一进程，该字段也置 0 message[slen] = &amp;lsquo;\0&amp;rsquo;; memcpy(NLMSG_DATA(nlh), message, slen+1); //通过 netlink_unicast()将消息发送用户空间由 dstPID 所指定了进程号的进程 netlink_unicast(nl_sk,skb,dstPID,0); printk(&amp;ldquo;send OK!\n&amp;rdquo;); return; } static void nl_data_ready (struct sock *sk, int len) { struct sk_buff _skb; struct nlmsghdr _nlh = NULL; while((skb = skb_dequeue(&amp;amp;sk-&amp;gt;sk_receive_queue)) != NULL) { nlh = (struct nlmsghdr *)skb-&amp;gt;data; printk(&amp;quot;%s: received netlink message payload: %s \n&amp;quot;, &lt;strong>FUNCTION&lt;/strong>, (char*)NLMSG_DATA(nlh)); sendnlmsg(&amp;ldquo;I see you&amp;rdquo;,nlh-&amp;gt;nlmsg_pid); //发送者的进程 ID 我们已经将其存储在了 netlink 消息头部里的 nlmsg_pid 字段里，所以这里可以拿来用。 kfree_skb(skb); } printk(&amp;ldquo;recvied finished!\n&amp;rdquo;); } static int __init myinit_module() { printk(&amp;ldquo;my netlink in\n&amp;rdquo;); nl_sk = netlink_kernel_create(NETLINK_TEST,0,nl_data_ready,THIS_MODULE); return 0; } static void __exit mycleanup_module() { printk(&amp;ldquo;my netlink out!\n&amp;rdquo;); sock_release(nl_sk-&amp;gt;sk_socket); } module_init(myinit_module); module_exit(mycleanup_module);
重新编译后，测试结果如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609976403-7d22f39a-4e7c-4c7c-aec3-376db6379b04.jpeg" alt="">&lt;/p>
&lt;h3 id="第三步">第三步&lt;/h3>
&lt;p>前面我们提到过，如果用户进程希望加入某个多播组时才需要调用 bind()函数。 前面的示例中我们没有这个需求，可还是调了 bind()，心头有些不爽。在前几篇博文里有关于 socket 编程时 几个常见 API 的详细解释和说明，不明白的童鞋可以回头去复习一下。
因为 Netlink 是面向无连接的数据报的套接字，所以我们还可以用 sendto()和 recvfrom() 来实现数据的收发，这次我们不再调用 bind()。将 Stage 2 的例子稍加改造一下，用户空间的修改如下：
#include &amp;lt;sys/stat.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;sys/socket.h&amp;gt; #include &amp;lt;sys/types.h&amp;gt; #include &amp;lt;string.h&amp;gt; #include &amp;lt;asm/types.h&amp;gt; #include &amp;lt;linux/netlink.h&amp;gt; #include &amp;lt;linux/socket.h&amp;gt; #define MAX&lt;em>PAYLOAD 1024 /&lt;em>消息最大负载为 1024 字节&lt;/em>/ int main(int argc, char&lt;/em> argv[]) { struct sockaddr&lt;em>nl dest_addr; struct nlmsghdr &lt;em>nlh = NULL; //struct iovec iov; int sock&lt;/em>fd=-1; //struct msghdr msg; if(-1 == (sock_fd=socket(PF_NETLINK, SOCK_RAW,NETLINK_TEST))){ perror(&amp;ldquo;can&amp;rsquo;t create netlink socket!&amp;rdquo;); return 1; } memset(&amp;amp;dest_addr, 0, sizeof(dest_addr)); dest_addr.nl_family = AF_NETLINK; dest_addr.nl_pid = 0; /&lt;em>我们的消息是发给内核的&lt;/em>/ dest_addr.nl_groups = 0; /&lt;em>在本示例中不存在使用该值的情况&lt;/em>/ /&lt;em>不再调用 bind()函数了 if(-1 == bind(sock&lt;/em>fd, (struct sockaddr&lt;/em>)&amp;amp;dest&lt;em>addr, sizeof(dest_addr))){ perror(&amp;ldquo;can&amp;rsquo;t bind sockfd with sockaddr_nl!&amp;rdquo;); return 1; }&lt;/em>/ if(NULL == (nlh=(struct nlmsghdr &lt;em>)malloc(NLMSG&lt;/em>SPACE(MAX_PAYLOAD)))){ perror(&amp;ldquo;alloc mem failed!&amp;rdquo;); return 1; } memset(nlh,0,MAX_PAYLOAD); /* 填充 Netlink 消息头部 &lt;em>/ nlh-&amp;gt;nlmsg_len = NLMSG_SPACE(MAX_PAYLOAD); nlh-&amp;gt;nlmsg_pid = getpid();//我们希望得到内核回应，所以得告诉内核我们 ID 号 nlh-&amp;gt;nlmsg_type = NLMSG_NOOP; //指明我们的 Netlink 是消息负载是一条空消息 nlh-&amp;gt;nlmsg_flags = 0; /&lt;em>设置 Netlink 的消息内容，来自我们命令行输入的第一个参数&lt;/em>/ strcpy(NLMSG_DATA(nlh), argv[1]); /&lt;em>这个模板就用不上了。&lt;/em>/ /_ memset(&amp;amp;iov, 0, sizeof(iov)); iov.iov&lt;/em>base = (void &lt;em>)nlh; iov.iov&lt;/em>len = nlh-&amp;gt;nlmsg_len; memset(&amp;amp;msg, 0, sizeof(msg)); msg.msg_iov = &amp;amp;iov; msg.msg_iovlen = 1; &lt;em>/ //sendmsg(sock&lt;/em>fd, &amp;amp;msg, 0); //不再用这种方式发消息到内核 sendto(sock_fd,nlh,NLMSG_LENGTH(MAX_PAYLOAD),0,(struct sockaddr*)(&amp;amp;dest&lt;em>addr),sizeof(dest_addr)); //接收内核消息的消息 printf(&amp;ldquo;waiting message from kernel!\n&amp;rdquo;); //memset((char&lt;/em>)NLMSG_DATA(nlh),0,1024); memset(nlh,0,MAX_PAYLOAD); //清空整个 Netlink 消息头包括消息头和负载 //recvmsg(sock_fd,&amp;amp;msg,0); recvfrom(sock_fd,nlh,NLMSG_LENGTH(MAX_PAYLOAD),0,(struct sockaddr*)(&amp;amp;dest_addr),NULL); printf(&amp;ldquo;Got response: %s\n&amp;rdquo;,NLMSG_DATA(nlh)); /* 关闭 netlink 套接字 _/ close(sock_fd); free(nlh); return 0; }
内核空间的代码完全不用修改，我们仍然用 netlink_unicast()从内核空间发送消息到用户空间。
重新编译后，测试结果如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609976660-e321adca-1224-41cf-8fda-74048e4f6523.jpeg" alt="">
和 Stage 2 中代码运行效果完全一样。也就是说，在开发 Netlink 程序过程中， 如果没牵扯到多播机制，那么用户空间的 socket 代码其实是不用执行 bind()系统调用的， 但此时就需要用 sendto()和 recvfrom()完成数据的发送和接收的任务；如果执行了 bind()系统调用， 当然也可以继续用 sendto()和 recvfrom()，但给它们传递的参数就有所区别。这时候一般使用 sendmsg()和 recvmsg() 来完成数据的发送和接收。大家根据自己的实际情况灵活选择。&lt;/p>
&lt;h2 id="netlink-多播">Netlink 多播&lt;/h2>
&lt;p>在上面我们所遇到的情况都是用户空间作为消息进程的发起者， Netlink 还支持内核作为消息的发送方的情况。这一般用于内核主动向用户空间报告一些内核状态， 例如我们在用户空间看到的 USB 的热插拔事件的通告就是这样的应用。
先说一下我们的目标，内核线程每个一秒钟往一个多播组里发送一条消息， 然后用户空间所以加入了该组的进程都会收到这样的消息，并将消息内容打印出来。
Netlink 地址结构体中的 nl&lt;em>groups 是 32 位，也就是说每种 Netlink 协议最多支持 32 个多播组。 如何理解这里所说的每种 Netlink 协议？在里预定义的如下协议都是 Netlink 协议簇的具体协议， 还有我们添加的 NETLINK_TEST 也是一种 Netlink 协议。
#define NETLINK_ROUTE 0 /&lt;/em> Routing/device hook &lt;em>/ #define NETLINK_UNUSED 1 /&lt;/em> Unused number &lt;em>/ #define NETLINK_USERSOCK 2 /&lt;/em> Reserved for user mode socket protocols &lt;em>/ #define NETLINK_FIREWALL 3 /&lt;/em> Firewalling hook &lt;em>/ #define NETLINK_INET_DIAG 4 /&lt;/em> INET socket monitoring &lt;em>/ #define NETLINK_NFLOG 5 /&lt;/em> netfilter/iptables ULOG &lt;em>/ #define NETLINK_XFRM 6 /&lt;/em> ipsec &lt;em>/ #define NETLINK_SELINUX 7 /&lt;/em> SELinux event notifications &lt;em>/ #define NETLINK_ISCSI 8 /&lt;/em> Open-iSCSI &lt;em>/ #define NETLINK_AUDIT 9 /&lt;/em> auditing &lt;em>/ #define NETLINK_FIB_LOOKUP 10 #define NETLINK_CONNECTOR 11 #define NETLINK_NETFILTER 12 /&lt;/em> netfilter subsystem &lt;em>/ #define NETLINK_IP6_FW 13 #define NETLINK_DNRTMSG 14 /&lt;/em> DECnet routing messages &lt;em>/ #define NETLINK_KOBJECT_UEVENT 15 /&lt;/em> Kernel messages to userspace &lt;em>/ #define NETLINK_GENERIC 16 /&lt;/em> leave room for NETLINK&lt;em>DM (DM Events) &lt;em>/ #define NETLINK&lt;/em>SCSITRANSPORT 18 /&lt;/em> SCSI Transports &lt;em>/ #define NETLINK_ECRYPTFS 19 #define NETLINK_TEST 20 /&lt;/em> 用户添加的自定义协议 &lt;em>/
在我们自己添加的 NETLINK_TEST 协议里，同样地，最多允许我们设置 32 个多播组， 每个多播组用 1 个比特表示，所以不同的多播组不可能出现重复。你可以根据自己的实际需求，决定哪个多播组是用来做什么的。 用户空间的进程如果对某个多播组感兴趣，那么它就加入到该组中，当内核空间的进程往该组发送多播消息时， 所有已经加入到该多播组的用户进程都会收到该消息。
再回到我们 Netlink 地址结构体里的 nl_groups 成员，它是多播组的地址掩码， 注意是掩码不是多播组的组号。如何根据多播组号取得多播组号的掩码呢？在 af_netlink.c 中有个函数：
static u32 netlink_group_mask(u32 group) { return group ? 1 &amp;laquo; (group - 1) : 0; }
也就是说，在用户空间的代码里，如果我们要加入到多播组 1，需要设置 nl_groups 设置为 1； 多播组 2 的掩码为 2；多播组 3 的掩码为 4，依次类推。为 0 表示我们不希望加入任何多播组。理解这一点很重要。 所以我们可以在用户空间也定义一个类似于 netlink_group_mask()的功能函数，完成从多播组号到多播组掩码的转换。 最终用户空间的代码如下：
#include &amp;lt;sys/stat.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;sys/socket.h&amp;gt; #include &amp;lt;sys/types.h&amp;gt; #include &amp;lt;string.h&amp;gt; #include &amp;lt;asm/types.h&amp;gt; #include &amp;lt;linux/netlink.h&amp;gt; #include &amp;lt;linux/socket.h&amp;gt; #include &amp;lt;errno.h&amp;gt; #define MAX_PAYLOAD 1024 // Netlink 消息的最大载荷的长度 unsigned int netlink_group_mask(unsigned int group) { return group ? 1 &amp;laquo; (group - 1) : 0; } int main(int argc, char* argv[]) { struct sockaddr_nl src_addr; struct nlmsghdr &lt;em>nlh = NULL; struct iovec iov; struct msghdr msg; int sock_fd, retval; // 创建 Socket sock_fd = socket(PF_NETLINK, SOCK_RAW, NETLINK_TEST); if(sock_fd == -1){ printf(&amp;ldquo;error getting socket: %s&amp;rdquo;, strerror(errno)); return -1; } memset(&amp;amp;src_addr, 0, sizeof(src_addr)); src_addr.nl_family = PF_NETLINK; src_addr.nl_pid = 0; // 表示我们要从内核接收多播消息。注意：该字段为 0 有双重意义，另一个意义是表示我们发送的数据的目的地址是内核。 src_addr.nl_groups = netlink_group_mask(atoi(argv[1])); // 多播组的掩码，组号来自我们执行程序时输入的第一个参数 // 因为我们要加入到一个多播组，所以必须调用 bind()。 retval = bind(sock_fd, (struct sockaddr&lt;/em>)&amp;amp;src_addr, sizeof(src_addr)); if(retval &amp;lt; 0){ printf(&amp;ldquo;bind failed: %s&amp;rdquo;, strerror(errno)); close(sock_fd); return -1; } // 为接收 Netlink 消息申请存储空间 nlh = (struct nlmsghdr *)malloc(NLMSG_SPACE(MAX_PAYLOAD)); if(!nlh){ printf(&amp;ldquo;malloc nlmsghdr error!\n&amp;rdquo;); close(sock_fd); return -1; } memset(nlh, 0, NLMSG_SPACE(MAX_PAYLOAD)); iov.iov_base = (void *)nlh; iov.iov_len = NLMSG_SPACE(MAX_PAYLOAD); memset(&amp;amp;msg, 0, sizeof(msg)); msg.msg_iov = &amp;amp;iov; msg.msg_iovlen = 1; // 从内核接收消息 printf(&amp;ldquo;waitinf for&amp;hellip;\n&amp;rdquo;); recvmsg(sock_fd, &amp;amp;msg, 0); printf(&amp;ldquo;Received message: %s \n&amp;rdquo;, NLMSG_DATA(nlh)); close(sock_fd); return 0; }
可以看到，用户空间的程序基本没什么变化，唯一需要格外注意的就是 Netlink 地址结构体 中的 nl_groups 的设置。由于对它的解释很少，加之没有有效的文档，所以我也是一边看源码，一边在网上搜集资料。 有分析不当之处，还请大家帮我指出。
内核空间我们添加了内核线程和内核线程同步方法 completion 的使用。内核空间修改后的代码如下：
#include &amp;lt;linux/kernel.h&amp;gt; #include &amp;lt;linux/module.h&amp;gt; #include &amp;lt;linux/skbuff.h&amp;gt; #include &amp;lt;linux/init.h&amp;gt; #include &amp;lt;linux/ip.h&amp;gt; #include &amp;lt;linux/types.h&amp;gt; #include &amp;lt;linux/sched.h&amp;gt; #include &amp;lt;net/sock.h&amp;gt; #include &amp;lt;net/netlink.h&amp;gt; MODULE_LICENSE(&amp;ldquo;GPL&amp;rdquo;); MODULE_AUTHOR(&amp;ldquo;Koorey King&amp;rdquo;); struct sock *nl_sk = NULL; static struct task_struct *mythread = NULL; //内核线程对象 //向用户空间发送消息的接口 void sendnlmsg(char *message/&lt;/em>,int dstPID*/) { struct sk_buff &lt;em>skb; struct nlmsghdr _nlh; int len = NLMSG_SPACE(MAX_MSGSIZE); int slen = 0; if(!message || !nl_sk){ return; } // 为新的 sk_buffer 申请空间 skb = alloc_skb(len, GFP_KERNEL); if(!skb){ printk(KERN_ERR &amp;ldquo;my_net_link: alloc_skb Error./n&amp;rdquo;); return; } slen = strlen(message)+1; //用 nlmsg_put()来设置 netlink 消息头部 nlh = nlmsg_put(skb, 0, 0, 0, MAX_MSGSIZE, 0); // 设置 Netlink 的控制块里的相关信息 NETLINK_CB(skb).pid = 0; // 消息发送者的 id 标识，如果是内核发的则置 0 NETLINK_CB(skb).dst_group = 5; //多播组号为 5，但置成 0 好像也可以。 message[slen] = &amp;lsquo;\0&amp;rsquo;; memcpy(NLMSG_DATA(nlh), message, slen+1); //通过 netlink_unicast()将消息发送用户空间由 dstPID 所指定了进程号的进程 //netlink_unicast(nl_sk,skb,dstPID,0); netlink_broadcast(nl_sk, skb, 0,5, GFP_KERNEL); //发送多播消息到多播组 5，这里我故意没有用 1 之类的“常见”值，目的就是为了证明我们上面提到的多播组号和多播组号掩码之间的对应关系 printk(&amp;ldquo;send OK!\n&amp;rdquo;); return; } //每隔 1 秒钟发送一条“I am from kernel!”消息，共发 10 个报文 static int sending_thread(void _data) { int i = 10; struct completion cmpl; while(i&amp;ndash;){ init_completion(&amp;amp;cmpl); wait_for_completion_timeout(&amp;amp;cmpl, 1 _ HZ); sendnlmsg(&amp;ldquo;I am from kernel!&amp;rdquo;); } printk(&amp;ldquo;sending thread exited!&amp;rdquo;); return 0; } static int __init myinit&lt;/em>module() { printk(&amp;ldquo;my netlink in\n&amp;rdquo;); nl_sk = netlink_kernel_create(NETLINK_TEST,0,NULL,THIS_MODULE); if(!nl_sk){ printk(KERN_ERR &amp;ldquo;my_net_link: create netlink socket error.\n&amp;rdquo;); return 1; } printk(&amp;ldquo;my netlink: create netlink socket ok.\n&amp;rdquo;); mythread = kthread_run(sending_thread,NULL,&amp;ldquo;thread_sender&amp;rdquo;); return 0; } static void __exit mycleanup_module() { if(nl_sk != NULL){ sock_release(nl_sk-&amp;gt;sk_socket); } printk(&amp;ldquo;my netlink out!\n&amp;rdquo;); } module_init(myinit_module); module_exit(mycleanup_module);
关于内核中 netlink_kernel_create(int unit, unsigned int groups,…)函数里的 第二个参数指的是我们内核进程最多能处理的多播组的个数，如果该值小于 32，则默认按 32 处理， 所以在调用 netlink_kernel_create()函数时可以不用纠结第二个参数，一般将其置为 0 就可以了。
在 skbuff{}结构体中，有个成员叫做”控制块”，源码对它的解释如下：
struct sk_buff { /* These two members must be first. _/ struct sk_buff *next; struct sk_buff &lt;em>prev; … … /* _ This is the control buffer. It is free to use for every _ layer. Please put your private variables there. If you _ want to keep them across layers you have to do a skb&lt;/em>clone() * first. This is owned by whoever has the skb queued ATM. &lt;em>/ char cb[48]; … … }
当内核态的 Netlink 发送数据到用户空间时一般需要填充 skbuff 的控制块，填充的方式是通过强制类型转换， 将其转换成 struct netlink_skb_parms{}之后进行填充赋值的：
struct netlink_skb_parms { struct ucred creds; /&lt;/em> Skb credentials &lt;em>/ __u32 pid; __u32 dst_group; kernel_cap_t eff_cap; __u32 loginuid; /&lt;/em> Login (audit) uid &lt;em>/ __u32 sid; /&lt;/em> SELinux security id */ };
填充时的模板代码如下：
NETLINK_CB(skb).pid=xx; NETLINK_CB(skb).dst_group=xx;
这里要注意的是在 Netlink 协议簇里提到的 skbuff 的 cb 控制块里保存的是属于 Netlink 的私有信息。 怎么讲，就是 Netlink 会用该控制块里的信息来完成它所提供的一些功能，只是完成 Netlink 功能所必需的一些私有数据。 打个比方，以开车为例，开车的时候我们要做的就是打火、控制方向盘、适当地控制油门和刹车，车就开动了， 这就是汽车提供给我们的“功能”。汽车的发动机，轮胎，传动轴，以及所用到的螺丝螺栓等都属于它的“私有”数据 cb。 汽车要运行起来这些东西是不可或缺的，但它们之间的协作和交互对用户来说又是透明的。就好比我们 Netlink 的 私有控制结构 struct netlink_skb_parms{}一样。
目前我们的例子中，将 NETLINK_CB(skb).dst_group 设置为相应的多播组号和 0 效果都是一样，用户空间都可以收到该多播消息， 原因还不是很清楚，还请 Netlink 的大虾们帮我点拨点拨。
编译后重新运行，最后的测试结果如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oextqn/1621609979369-87b7a52d-2caa-4bd9-be17-1069e4aee0ad.jpeg" alt="">
注意，这里一定要先执行 insmod 加载内核模块，然后再运行用户空间的程序。如果没有加载 mynlkern.ko 而直接执行./test 5 在 bind()系统调用时会报如下的错误：
bind failed: No such file or directory
因为网上有写文章在讲老版本 Netlink 的多播时用法时先执行了用户空间的程序， 然后才加载内核模块，现在(2.6.21)已经行不通了，这一点请大家注意。
小结：通过这三篇博文我们对 Netlink 有了初步的认识，并且也可以开发基于 Netlink 的基本应用程序。 但这只是冰山一角，要想写出高质量、高效率的软件模块还有些差距，特别是对 Netlink 本质的理解还需要提高一个层次， 当然这其中牵扯到内核编程的很多基本功，如临界资源的互斥、线程安全性保护、用 Netlink 传递大数据时的处理等等 都是开发人员需要考虑的问题。&lt;/p></description></item></channel></rss>