<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 2.Kernel(内核)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/</link><description>Recent content in 2.Kernel(内核) on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 1.Linux Kernel</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/1.linux-kernel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/1.linux-kernel/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/torvalds">Linus Torvalds&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/torvalds/linux">GitHub,Linux 内核项目&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/">官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/">官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/man-pages/index.html">官方 Manual(手册)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Kernel_(operating_system)">Wiki,Kernel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki//boot/">Wiki,/boot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Vmlinux">Wiki,vmlinux&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Initial_ramdisk">Wiki,Initial ramdisk&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/System.map">Wiki,System.map&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/raspberrypi/linux">树莓派 Linux&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/index">RedHat 官方文档,8-管理、监控和更新内核&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.linfo.org/vmlinuz.html">http://www.linfo.org/vmlinuz.html&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.zhihu.com/question/22045825">知乎,initrd 和 initramfs 的区别&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**Kernel(内核) **是一个作为操作系统核心的计算机程序，对系统中的一切具有完全控制权。它负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。&lt;/p>
&lt;p>Kernel 是计算器启动时首先加载程序之一(在 &lt;a href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1.Bootloader/1.Bootloader.md">Bootloader&lt;/a>并处理硬件和软件之间的交互。并且处理启动过程的其余部分、以及内存、外设、和来自软件的输入/输出请求，将他们转换为 CPU 的数据处理指令。&lt;/p>
&lt;h2 id="kernel-组成-及-系统调用">Kernel 组成 及 系统调用&lt;/h2>
&lt;p>Linux 内核由如下几部分组成：内存管理、进程管理、设备驱动程序管理、文件系统管理、网络管理等。如图：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/fkp6xi/1616168349819-c21dd43c-79b7-4ec2-abd4-c8bb0e3c7686.jpeg" alt="">
**System Call Interface(系统调用接口，简称 SCI) **层提供了某些机制执行从用户空间到内核的函数调用。这个接口依赖于体系结构，甚至在相同的处理器家族内也是如此。SCI 实际上是一个非常有用的函数调用多路复用和多路分解服务。&lt;/p>
&lt;p>系统调用介绍详见[ system call(系统调用)](✏IT 学习笔记/📄1.操作系统/2.Kernel(内核)/3.System%20Call(系统调用).md Call(系统调用).md) 章节&lt;/p>
&lt;h2 id="linux-man-手册使用说明">Linux man 手册使用说明&lt;/h2>
&lt;p>在 Linux Kernel 的官方 man 手册中，记录了用户空间程序使用 Linux 内核 和 C 库的接口。对于 C 库，主要聚焦于 GUN C(glibc)，尽管在已知的情况下，还包括可用于 Linux 的其他 C 库中的变体文档。在这个 man 手册中，分为如下几部分&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_1.html">User commands&lt;/a>(用户命令)&lt;/strong> # 介绍一些用户空间的应用程序。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_2.html">System calls&lt;/a>(系统调用)&lt;/strong> # Linux Kernel 可以提供的所有 System Calls(系统调用)&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_3.html">Library functions&lt;/a>(库函数)&lt;/strong> # C 标准库可以提供的函数。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_4.html">Devices&lt;/a>(设备)&lt;/strong> # 各种设备的详细信息，这些设备大多都在 /dev 目录中。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_5.html">Files&lt;/a>(文件)&lt;/strong> # 各种文件格式和文件系统&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_7.html">Overviews, conventions, and miscellaneous&lt;/a>(概述、约定 和 其他)&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://man7.org/linux/man-pages/dir_section_8.html">Superuser and system administration commands&lt;/a>(超级用户和系统管理员命令)&lt;/strong> # 介绍一些 GUN C 库提供的程序。&lt;/li>
&lt;/ul>
&lt;p>在 Linux man 手册中，可以找到 Linux 系统中的一切使用说明。Linux 操作系统围绕 Linux Kernel 构建了一套高效、健壮的应用程序运行环境&lt;/p>
&lt;h1 id="intirdimgvmlinuzsystemmap-文件">intird.img、vmlinuz、System.map 文件&lt;/h1>
&lt;p>Kernel 会被安装到 /boot 目录中，并生成 &lt;strong>config、initrd.img、System.map、vmlinuz&lt;/strong> 这几个文件&lt;/p>
&lt;h2 id="vmlinuz">vmlinuz&lt;/h2>
&lt;p>_vmlinuz _是 &lt;a href="http://www.linfo.org/linuxdef.html">Linux&lt;/a> &lt;a href="http://www.linfo.org/kernel.html">&lt;em>内核&lt;/em>&lt;/a> _可执行文件_的名称。&lt;/p>
&lt;p>vmlinuz 是一个压缩的 Linux 内核，它是_可引导的_。可引导意味着它能够将操作系统加载到内存中，以便计算机变得可用并且可以运行应用程序。&lt;/p>
&lt;p>vmlinuz 不应与_vmlinux_混淆，后者是非压缩和不可引导形式的内核。vmlinux 通常只是生成 vmlinuz 的中间步骤。&lt;/p>
&lt;p>vmlinuz 位于_/boot_目录中，该目录包含开始引导系统所需的文件。名为_vmlinuz_的文件可能是实际的内核可执行文件本身，也可能是内核可执行文件的链接，该链接可能带有诸如_/boot/vmlinuz-2.4.18-19.8.0_之类的名称（即特定内核的名称）内核版本）。这可以通过使用_ls_ &lt;a href="http://www.linfo.org/command.html">命令&lt;/a>（其目的是列出指定目录的内容）及其_-l_选项（它告诉 ls 提供有关指定目录中每个对象的详细信息）来轻松确定，如下所示：&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>ls -l /boot&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>如果 vmlinuz 是一个普通文件（包括可执行文件），则第一列中有关它的信息将以连字符开头。如果是链接，它将以字母_l_开头。
通过发出以下命令   来_编译_Linux 内核：&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>make bzImage&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>这会在 _/usr/src/linux/arch/i386/linux/boot/ _等目录中创建名为_bzImage_的文件。&lt;/p>
&lt;p>编译是将内核的&lt;a href="http://www.linfo.org/source_code.html">&lt;em>源代码&lt;/em>&lt;/a>（即内核由人类编写的原始形式）转换为_目标代码_（计算机处理器可以直接理解）。它由称为&lt;a href="http://www.linfo.org/compiler.html">&lt;em>编译器&lt;/em>&lt;/a>的专门程序执行，通常是&lt;a href="http://www.linfo.org/gcc.html">&lt;em>GCC&lt;/em>&lt;/a>（&lt;a href="http://www.linfo.org/gnu.html">GNU&lt;/a>编译器集合）中的一个。&lt;/p>
&lt;p>然后使用 _cp &lt;em>命令将 bzImage 复制到 /boot 目录，同时使用诸如以下命令   重命名_vmlinuz&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>cp /usr/src/linux/arch/i386/linux/boot/bzImage /boot/vmlinuz&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>vmlinuz 不仅仅是一个压缩图像。它还内置了_gzip_解压缩器代码。gzip 是&lt;a href="http://www.linfo.org/unix-like.html">类 Unix&lt;/a>操作系统上最流行的压缩实用程序之一。&lt;/p>
&lt;p>一个名为_zImage_文件的编译内核是在一些较旧的系统上创建的，并保留在较新的系统上以实现向后兼容性。zImage 和 bzImage 都是用 gzip 压缩的。区别在于 zImage 解压到_低内存_（即前 640kB），bzImage 解压到_高内存_（1MB 以上）。有一个常见的误解，认为 bzImage 是使用_bzip2_实用程序压缩的。实际上，&lt;em>b_只代表_big&lt;/em>。&lt;/p>
&lt;p>&lt;em>vmlinuz&lt;/em> 这个名字很大程度上是历史的偶然。在贝尔实验室开发的原始 UNIX 上的内核二进制文件称为_unix_。当后来在加州大学伯克利分校 (UCB) 编写包含支持&lt;a href="http://www.linfo.org/virtual_memory.html">&lt;em>虚拟内存&lt;/em>&lt;/a>的新内核时，内核二进制文件更名为_vmunix_。&lt;/p>
&lt;p>虚拟内存是使用硬盘驱动器 (HDD) 上的空间来模拟额外的 RAM（随机存取内存）容量。与当时使用的其他一些流行操作系统（例如&lt;a href="http://www.linfo.org/ms-dos.html">MS-DOS）&lt;/a>相比，Linux 内核几乎从 Linux 一开始就支持它。&lt;/p>
&lt;p>因此，Linux 内核很自然地被称为_vmlinux_。由于 Linux 内核可执行文件被制作成压缩文件，并且压缩文件在类 Unix 系统上通常具有_z_或_gz_扩展名，因此压缩内核可执行文件的名称变为_vmlinuz_。&lt;/p>
&lt;h2 id="initrd">initrd&lt;/h2>
&lt;p>&lt;strong>Initial RAM Disk(初始内存磁盘，简称 initrd)&lt;/strong> 是一种将临时根文件系统加载到内存中的方案，可以作为 Linux 启动过程的一部分。有两种方法来实现这种方案：&lt;/p>
&lt;ul>
&lt;li>**initrd # Initial RAM Disk。**就是把一块内存（ram）当做磁盘（disk）去挂载，然后找到 ram 里的 init 执行。&lt;/li>
&lt;li>**initramfs # Initial RAM Filesystem。**直接在 ram 上挂载文件系统，执行文件系统中的 init。&lt;/li>
&lt;/ul>
&lt;p>这两者通常用于在挂载真正的根文件系统之前执行一些准备工作。&lt;/p>
&lt;blockquote>
&lt;p>不要被文件名迷惑，kernel 2.6 以来都是 initramfs 了，只是很多还沿袭传统使用 initrd 的名字
initramfs 的工作方式更加简单直接一些，启动的时候加载内核和 initramfs 到内存执行，内核初始化之后，切换到用户态执行 initramfs 的程序/脚本，加载需要的驱动模块、必要配置等，然后加载 rootfs 切换到真正的 rootfs 上去执行后续的 init 过程。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>initrd 是 2.4 及更早的用法（现在你能见到的 initrd 文件实际差不多都是 initramfs 了），运行过程大概是内核启动，执行一些 initrd 的内容，加载模块啥的，然后交回控制权给内核，最后再切到用户态去运行用户态的启动流程。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>从格式看，老的 initrd 是一个压缩的内存文件系统，具体是啥忘了，年月太久了。现在的 initramfs 是一个 gzip 压缩的 cpio 文件系统打包，如果遇到什么紧急情况需要处理的时候，你可以建立一个临时目录，把 initramfs 解压之后，直接 cpio -idv 解压出来，改之后再用 cpio 和 gzip 封上即可。虽然大家都喜欢用 tar 打包，但掌握点 cpio 在关键时刻还是可以救命的。&lt;/p>
&lt;/blockquote>
&lt;p>在早期的 Linux 系统中，一般就只有软盘或者硬盘被用来作为 Linux 的根文件系统，因此很容易把这些设备的驱动程序集成到内核中。但是现在根文件系统可能保存在各种存储设备上，包括 SCSI、SATA、U 盘等等。总不能每出一个，就要重新编译一遍内核吧？~这样不但麻烦，也不实用，所以后来 Linux 就提供了一个灵活的方法来解决这些问题。就是 &lt;strong>initrd&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>可以把 initrd 当做 WinPE。当使用 WinPE 启动后会发现你的计算机就算没有硬盘也能在正常运行，其中有个文件系统 B:/ 分区，这个分区就是内存模拟的磁盘。&lt;/p>
&lt;/blockquote>
&lt;p>initrd.img 文件就是个 ram disk 的映像文件。ramdisk 是用一部分内存模拟成磁盘，让操作系统访问。ram disk 是标准内核文件认识的设备(/dev/ram0)文件系统也是标准内核认识的文件系统。内核加载这个 ram disk 作为根文件系统并开始执行其中的&amp;quot;某个文件&amp;quot;（2.6 内核是 init 文件）来加载各种模块，服务等。经过一些配置和运行后，就可以去物理磁盘加载真正的 root 分区了，然后又是一些配置等，最后启动成功。&lt;/p>
&lt;p>也就是你只需要定制适合自己的 initrd.img 文件就可以了。这要比重编内核简单多了，省时省事低风险。&lt;/p>
&lt;h3 id="查看-initrd-文件">查看 initrd 文件&lt;/h3>
&lt;p>我们可以通过如下方式，解压出 initrd.img 文件，下面分别以 Ubuntu 20.04 TLS 系统和 CentOS Stream 8 系统为例：&lt;/p>
&lt;blockquote>
&lt;p>解压方法来源：&lt;a href="https://unix.stackexchange.com/questions/163346/why-is-it-that-my-initrd-only-has-one-directory-namely-kernel">https://unix.stackexchange.com/questions/163346/why-is-it-that-my-initrd-only-has-one-directory-namely-kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Ubuntu 20.04 TLS&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@lichenhao:~/test_dir# mkdir -p /root/test_dir/root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@lichenhao:~/test_dir# cp /boot/initrd.img /root/test_dir/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@lichenhao:~/test_dir/root# &lt;span style="color:#f92672">(&lt;/span>cpio -id; cpio -i; unlz4 | cpio -id&lt;span style="color:#f92672">)&lt;/span> &amp;lt; ../initrd.img
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">62&lt;/span> blocks
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">9004&lt;/span> blocks
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">450060&lt;/span> blocks
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@lichenhao:~/test_dir/root# ls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bin conf cryptroot etc init kernel lib lib32 lib64 libx32 run sbin scripts usr var
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>CentOS Stream 8&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-2 root&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># mkdir -p /root/test_dir/root&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-2 root&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cp /boot/initramfs-4.18.0-294.el8.x86_64.img /root/test_dir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-2 root&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cd /root/test_dir/root&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-2 root&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># (cpio -id; zcat | cpio -id) &amp;lt; ../initramfs-4.18.0-294.el8.x86_64.img&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-2 root&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bin dev early_cpio etc init kernel lib lib64 proc root run sbin shutdown sys sysroot tmp usr var
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以看到，initrd.img 中包含了一个系统最基本的目录结构&lt;/p>
&lt;h1 id="kernel-关联文件">Kernel 关联文件&lt;/h1>
&lt;p>**/boot/* **#&lt;/p>
&lt;ul>
&lt;li>**./config-$(uname -r) **# Kernel 的扩展配置文件。Kernel 文档中，将该文件称为 &lt;strong>Boot Configuration&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>./initrd.img&lt;/strong> # 在内核挂载真正的根文件系统前使用的临时文件系统&lt;/li>
&lt;li>&lt;strong>./vmlinuz&lt;/strong> # Linux 内核&lt;/li>
&lt;/ul>
&lt;p>**/etc/sysctl.conf **# 系统启动时读取的内核参数文件&lt;/p>
&lt;ul>
&lt;li>**/etc/sysctl.d/* **# 系统启动时时读取的内核参数目录&lt;/li>
&lt;/ul>
&lt;p>**/usr/lib/sysctl.d/* **#
**/proc/sys/* **# 内核参数(也称为内核变量)所在路径。该目录(从 1.3.57 版本开始)包含许多与内核变量相对应的文件和子目录。 可以使用 &lt;a href="https://www.yuque.com/go/doc/33222789">proc 文件系统&lt;/a> 以及 sysctl(2) 系统读取或加载这些变量，有时可以对其进行修改。&lt;/p></description></item><item><title>Docs: 3.System Call(系统调用)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/3.system-call%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/3.system-call%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man2/syscalls.2.html">Manual(手册)，syscalls(2)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/System_call">Wiki,System_call&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/system-call-definitive-guide-zh/">http://arthurchiao.art/blog/system-call-definitive-guide-zh/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>System Call(系统调用，简称 syscall)&lt;/strong> 是 Application(应用程序) 和 Linux Kernel(内核) 之间的基本接口。是操作内核的唯一入口。其实，所谓 syscall 就是各种编程语言中的 &lt;strong>Function(函数)&lt;/strong> 概念。一个 syscall 也有名称、参数、返回值。syscall 即可以是名词，用来描述一个具体的 syscall；也可以是动词，用来表示某物调用了某个 syscall。当用户进程需要发生系统调用时，CPU 通过软中断切换到内核态开始执行内核系统调用函数。&lt;/p>
&lt;blockquote>
&lt;p>syscall 还有另一种意思，是一种编程方式，比如我们常说的 API，就是 syscall 的一种实现。&lt;/p>
&lt;/blockquote>
&lt;p>在 &lt;a href="https://man7.org/linux/man-pages/man2/syscalls.2.html#DESCRIPTION">syscalls(2) 手册中的 System call list 章节&lt;/a>可以看到 Linux 可用的完整的 syscall 列表。也就是说所有 Kernel 暴露出来的可供用户调用的 Function。&lt;/p>
&lt;h2 id="用户程序内核和-cpu-特权级别">用户程序、内核和 CPU 特权级别&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bbar3l/1616168230254-e3c38b73-8092-41bd-a17d-d3c4768de743.jpeg" alt="">&lt;/p>
&lt;p>用户程序（例如编辑器、终端、ssh daemon 等）需要和 Linux 内核交互，内核代替它们完 成一些它们自身无法完成的操作。&lt;/p>
&lt;p>例如，如果用户程序需要做 IO 操作（open、read、write 等），或者需要修改它的 内存地址（mmpa、sbrk 等），那它必须触发内核替它完成。&lt;/p>
&lt;p>为什么禁止用户程序做这些操作呢？&lt;/p>
&lt;p>因为 x86-64 CPU 有一个特权级别 （privilege levels）的概念。这个概念很复杂，完全可以单独写一篇博客。 出于本文讨论目的，我们将其（大大地）简化为如下：&lt;/p>
&lt;ol>
&lt;li>特权级别是权限控制的一种方式。当前的特权级别决定了允许执行哪些 CPU 指令和操作&lt;/li>
&lt;li>内核运行在最高级别，称为 “Ring 0”；用户程序运行在稍低的一个级别，一般称作 “Ring 3”&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>内核空间（Ring 0）具有最高权限，可以直接访问所有资源；&lt;/li>
&lt;li>用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。&lt;/li>
&lt;/ul>
&lt;p>用户程序要进行特权操作必须触发一次特权级别切换（从 “Ring 3” 到 “Ring 0”）， 由内核（替它）执行。触发特权级别切换有多种方式，我们先从最常见的方式开始：中断。&lt;/p>
&lt;h1 id="interrupts中断">Interrupts(中断)&lt;/h1>
&lt;p>详见：[Interrupts(中断) 概念详解](✏IT 学习笔记/📄1.操作系统/2.Kernel(内核)/4.CPU%20 管理/Interrupts(中断).md 管理/Interrupts(中断).md)&lt;/p>
&lt;h1 id="syscall-的方式">syscall 的方式&lt;/h1>
&lt;p>通过 glibc 提供的库函数&lt;/p>
&lt;p>glibc 是 Linux 下使用的开源的标准 C 库，它是 GNU 发布的 libc 库，即运行时库。glibc 为程序员提供丰富的 API，除了例如字符串处理、数学运算等用户态服务之外，最重要的是封装了操作系统提供的系统服务，即系统调用的封装。那么 glibc 提供的系统调用 API 与内核特定的系统调用之间的关系是什么呢？&lt;/p>
&lt;ul>
&lt;li>通常情况，每个特定的系统调用对应了至少一个 glibc 封装的库函数，如系统提供的打开文件系统调用 sys_open 对应的是 glibc 中的 open 函数；&lt;/li>
&lt;li>其次，glibc 一个单独的 API 可能调用多个系统调用，如 glibc 提供的 printf 函数就会调用如 sys_open、sys_mmap、sys_write、sys_close 等等系统调用；&lt;/li>
&lt;li>另外，多个 API 也可能只对应同一个系统调用，如 glibc 下实现的 malloc、calloc、free 等函数用来分配和释放内存，都利用了内核的 sys_brk 的系统调用。&lt;/li>
&lt;/ul>
&lt;p>举例来说，我们通过 glibc 提供的 chmod 函数来改变文件 etc/passwd 的属性为 444&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;sys/types.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;sys/stat.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;errno.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;stdio.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> rc;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">chmod&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;/etc/passwd&amp;#34;&lt;/span>, &lt;span style="color:#ae81ff">0444&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (rc &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">fprintf&lt;/span>(stderr, &lt;span style="color:#e6db74">&amp;#34;chmod failed, errno = %d&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>, errno);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">printf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;chmod success!&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在普通用户下编译运用，输出结果为：&lt;/p>
&lt;pre>&lt;code>chmod failed, errno = 1
&lt;/code>&lt;/pre>
&lt;p>上面系统调用返回的值为-1，说明系统调用失败，错误码为 1，在 /usr/include/asm-generic/errno-base.h 文件中有如下错误代码说明：&lt;/p>
&lt;pre>&lt;code>#define EPERM 1 /* Operation not permitted */
&lt;/code>&lt;/pre>
&lt;p>即无权限进行该操作，我们以普通用户权限是无法修改 /etc/passwd 文件的属性的，结果正确。&lt;/p>
&lt;h2 id="使用指定的-syscallname-直接调用">使用指定的 SyscallName 直接调用&lt;/h2>
&lt;p>使用上面的方法有很多好处，首先你无须知道更多的细节，如 chmod 系统调用号，你只需了解 glibc 提供的 API 的原型；其次，该方法具有更好的移植性，你可以很轻松将该程序移植到其他平台，或者将 glibc 库换成其它库，程序只需做少量改动。&lt;/p>
&lt;p>但有点不足是，如果 glibc 没有封装某个内核提供的系统调用时，我就没办法通过上面的方法来调用该系统调用。如我自己通过编译内核增加了一个系统调用，这时 glibc 不可能有你新增系统调用的封装 API，此时我们可以利用 glibc 提供的 syscall 函数直接调用。该函数定义在 unistd.h 头文件中，函数原型如下：&lt;/p>
&lt;p>long int syscall (long int sysno, &amp;hellip;)&lt;/p>
&lt;ul>
&lt;li>sysno 是系统调用号，每个系统调用都有唯一的系统调用号来标识。在 sys/syscall.h 中有所有可能的系统调用号的宏定义。&lt;/li>
&lt;li>&amp;hellip; 为剩余可变长的参数，为系统调用所带的参数，根据系统调用的不同，可带 0~5 个不等的参数，如果超过特定系统调用能带的参数，多余的参数被忽略。&lt;/li>
&lt;li>返回值 该函数返回值为特定系统调用的返回值，在系统调用成功之后你可以将该返回值转化为特定的类型，如果系统调用失败则返回 -1，错误代码存放在 errno 中。&lt;/li>
&lt;/ul>
&lt;p>还以上面修改 /etc/passwd 文件的属性为例，这次使用 syscall 直接调用：&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/syscall.h&amp;gt;
#include &amp;lt;errno.h&amp;gt;
int main()
{
int rc;
rc = syscall(SYS_chmod, &amp;quot;/etc/passwd&amp;quot;, 0444);
if (rc == -1)
fprintf(stderr, &amp;quot;chmod failed, errno = %d\n&amp;quot;, errno);
else
printf(&amp;quot;chmod succeess!\n&amp;quot;);
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>在普通用户下编译执行，输出的结果与上例相同。&lt;/p>
&lt;h2 id="通过-syscall-间接调用">通过 syscall() 间接调用&lt;/h2>
&lt;h2 id="通过-int-指令陷入">通过 int 指令陷入&lt;/h2>
&lt;p>如果我们知道系统调用的整个过程的话，应该就能知道用户态程序通过软中断指令 int 0x80 来陷入内核态（在 Intel Pentium II 又引入了 sysenter 指令），参数的传递是通过寄存器，eax 传递的是系统调用号，ebx、ecx、edx、esi 和 edi 来依次传递最多五个参数，当系统调用返回时，返回值存放在 eax 中。&lt;/p>
&lt;p>仍然以上面的修改文件属性为例，将调用系统调用那段写成内联汇编代码：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;stdio.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;sys/types.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;sys/syscall.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;errno.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">long&lt;/span> rc;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">char&lt;/span> &lt;span style="color:#f92672">*&lt;/span>file_name &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/etc/passwd&amp;#34;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">short&lt;/span> mode &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0444&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">asm&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;int $0x80&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;=a&amp;#34;&lt;/span> (rc)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;0&amp;#34;&lt;/span> (SYS_chmod), &lt;span style="color:#e6db74">&amp;#34;b&amp;#34;&lt;/span> ((&lt;span style="color:#66d9ef">long&lt;/span>)file_name), &lt;span style="color:#e6db74">&amp;#34;c&amp;#34;&lt;/span> ((&lt;span style="color:#66d9ef">long&lt;/span>)mode)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> ((&lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">long&lt;/span>)rc &lt;span style="color:#f92672">&amp;gt;=&lt;/span> (&lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">long&lt;/span>)&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">132&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> errno &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>rc;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rc &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> (rc &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">fprintf&lt;/span>(stderr, &lt;span style="color:#e6db74">&amp;#34;chmode failed, errno = %d&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>, errno);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">printf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;success!&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果 eax 寄存器存放的返回值（存放在变量 rc 中）在 -1~-132 之间，就必须要解释为出错码（在/usr/include/asm-generic/errno.h 文件中定义的最大出错码为 132），这时，将错误码写入 errno 中，置系统调用返回值为 -1；否则返回的是 eax 中的值。&lt;/p>
&lt;p>上面程序在 32 位 Linux 下以普通用户权限编译运行结果与前面两个相同！&lt;/p></description></item><item><title>Docs: 4.CPU 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://time.geekbang.org/column/article/69859">极客时间，Linux 性能优化实战-03 基础篇：经常说的 CPU 上下文切换是什么意思&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://linuxperf.com/?p=209">LinuxPerformance 博客，进程切换：自愿与强制&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 Linux 中，CPU 的管理，绝大部分时间都是在进行任务的调度，所以很多时候也称为&lt;strong>调度管理&lt;/strong>。&lt;/p>
&lt;h2 id="cpu-多线程并发并行-概念">&lt;strong>CPU 多线程、并发、并行 概念&lt;/strong>&lt;/h2>
&lt;p>Node：在这里时间片只是一种描述，理解 CPU 的并行与并发概念就好&lt;/p>
&lt;p>1、CPU 时间分片、多线程？
如果线程数不多于 CPU 核心数，会把各个线程都分配一个核心，不需分片，而当线程数多于 CPU 核心数时才会分片。&lt;/p>
&lt;p>2、并发和并行的区别&lt;/p>
&lt;ul>
&lt;li>并发：当有多个线程在操作时,如果系统只有一个 CPU,把 CPU 运行时间划分成若干个时间片,分配给各个线程执行，在一个时间段的线程代码运行时，其它线程处于挂起状态。这种方式我们称之为_ _&lt;strong>Concurrent(并发)&lt;/strong>。并发=间隔发生&lt;/li>
&lt;li>并行：当系统有一个以上 CPU 时,则线程的操作有可能非并发。当一个 CPU 执行一个线程时，另一个 CPU 可以执行另一个线程，两个线程互不抢占 CPU 资源，可以同时进行，这种方式我们称之为 &lt;strong>Parallel(并行)&lt;/strong>。 并行=同时进行&lt;/li>
&lt;/ul>
&lt;p>区别：并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。&lt;/p>
&lt;p>并行是同时做多件事情。&lt;/p>
&lt;p>并发表示同时发生了多件事情，通过时间片切换，哪怕只有单一的核心，也可以实现“同时做多件事情”这个效果。&lt;/p>
&lt;p>根据底层是否有多处理器，并发与并行是可以等效的，这并不是两个互斥的概念。&lt;/p>
&lt;p>举个我们开发中会遇到的例子，我们说资源请求并发数达到了 1 万。这里的意思是有 1 万个请求同时过来了。但是这里很明显不可能真正的同时去处理这 1 万个请求的吧！&lt;/p>
&lt;p>如果这台机器的处理器有 4 个核心，不考虑超线程，那么我们认为同时会有 4 个线程在跑。也就是说，并发访问数是 1 万，而底层真实的并行处理的请求数是 4。如果并发数小一些只有 4 的话，又或者你的机器牛逼有 1 万个核心，那并发在这里和并行一个效果。也就是说，并发可以是虚拟的同时执行，也可以是真的同时执行。而并行的意思是真的同时执行。&lt;/p>
&lt;p>结论是：并行是我们物理时空观下的同时执行，而并发则是操作系统用线程这个模型抽象之后站在线程的视角上看到的“同时”执行。&lt;/p>
&lt;h3 id="time-slice时间片-概念">time slice(时间片) 概念&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://en.wikipedia.org/wiki/Preemption_(computing)#Time_slice">https://en.wikipedia.org/wiki/Preemption_(computing)#Time_slice&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>The period of time for which a process is allowed to run in a preemptive multitasking system is generally called the &lt;em>time slice&lt;/em> or &lt;em>quantum&lt;/em>.&lt;/p>
&lt;p>&lt;strong>time slice(时间片)&lt;/strong> 是一个程序运行在&lt;a href="https://en.wikipedia.org/wiki/Preemption_(computing)">抢占式多任务系统&lt;/a>中的一段时间。也可以称为 quantum(量子)。&lt;/p>
&lt;h2 id="cpu-使用率概念">CPU 使用率概念&lt;/h2>
&lt;p>CPU 不像硬盘、内存，并不具备逻辑上数量、大小、空间之类的概念。只要使用 CPU，就是使用了这个 CPU 的全部，也就无法通过大小之类的概念来衡量一个 CPU，所以我们日常所说的 CPU 的使用率 ，实际上是指的在一段时间范围内，CPU 执行 &lt;strong>Tasks(任务)&lt;/strong> 花费时间的百分比。比如 60 分钟内，一颗 CPU 执行各种任务花费了 6 分钟，则 CPU 在这一小时时间内的使用率为 10%。&lt;/p>
&lt;blockquote>
&lt;p>上文说的 &lt;strong>Tasks(任务)&lt;/strong>，即会指系统中的进程、线程，也代表各种硬件去请求 CPU 执行的各种事情，比如网卡接收到数据，就会告诉 CPU 需要处理(i.e.中断)。&lt;/p>
&lt;/blockquote>
&lt;p>在 Linux 系统中，CPU 的使用率一般可分为 4 大类：&lt;/p>
&lt;ol>
&lt;li>User Time(用户进程运行时间)&lt;/li>
&lt;li>System Time(系统内核运行时间)&lt;/li>
&lt;li>Idle Time(空闲时间)&lt;/li>
&lt;li>Steal Time(被抢占时间)&lt;/li>
&lt;/ol>
&lt;p>除了 Idle Time 外，CPU 在其余时间都处于工作运行状态。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/srucoz/1616168021555-68fba1de-f5d5-462d-bef6-a78b476521ad.png" alt="">&lt;/p>
&lt;p>通常而言，我们泛指的整体 CPU 使用率为 User Time 和 Systime 占比之和(例如 tsar 中 CPU util)，即：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/srucoz/1616168021559-394ecaa6-59db-453a-b5b1-c5ab88193f49.png" alt="">&lt;/p>
&lt;p>为了便于定位问题，大多数性能统计工具都将这 4 类时间片进一步扩展成了 8 类，如下图，是在 top 命令的 man 手册中对 CPU 使用率的分类。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/srucoz/1616168021546-ebe53556-f50b-49f2-8477-c10cf2b8f2f5.png" alt="">&lt;/p>
&lt;ul>
&lt;li>us：用户进程空间中未改变过优先级的进程占用 CPU 百分比&lt;/li>
&lt;li>sy：内核空间占用 CPU 百分比&lt;/li>
&lt;li>ni：用户进程空间内改变过优先级的进程占用 CPU 百分比&lt;/li>
&lt;li>id：空闲时间百分比&lt;/li>
&lt;li>wa：等待 I/O 的时间百分比&lt;/li>
&lt;li>hi：硬中断时间百分比&lt;/li>
&lt;li>si：软中断时间百分比&lt;/li>
&lt;li>st：虚拟化时被其余 VM 窃取时间百分比&lt;/li>
&lt;/ul>
&lt;p>这 8 类分片中，除 wa 和 id 外，其余分片 CPU 都处于工作态。&lt;/p>
&lt;h1 id="调度算法">调度算法&lt;/h1>
&lt;blockquote>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33222924">CPU 调度算法&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>首先明确一个概念：&lt;strong>Task(任务)&lt;/strong>，一个进程从处理到结束就算一个任务，处理网卡收到的数据包也算一个任务。一般来说，CPU 就是在处理一个个的 &lt;strong>Task(任务)&lt;/strong>，并度过其一生。&lt;/p>
&lt;p>在 Linux 内核中，进程和线程都是用 tark_struct 结构体表示的，区别在于线程的 tark_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 Linux 中的线程也被称为轻量级进程，因为线程的 tark_struct 相比进程的 tark_struct 承载的 资源比较少，因此以「轻」得名。&lt;/p>
&lt;p>一般来说，没有创建线程的进程，是只有单个执行流，它被称为是主线程。如果想让进程处理更多的事情，可以创建多个线程分别去处理，但不管怎么样，它们对应到内核里都是 tark_struct。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/srucoz/1616168021545-596ecf70-ac19-4620-8845-bfe72ef7bdce.jpeg" alt="">&lt;/p>
&lt;p>所以，Linux 内核里的调度器，调度的对象就是 tark_struct，接下来我们就把这个数据结构统称为任务。&lt;/p>
&lt;p>在 Linux 系统中，根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越小，优先级越高：&lt;/p>
&lt;ul>
&lt;li>实时任务，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在 0~99 范围内的就算实时任务；&lt;/li>
&lt;li>普通任务，响应时间没有很高的要求，优先级在 100~139 范围内都是普通任务级别；&lt;/li>
&lt;/ul>
&lt;p>也就是说，在 LInux 内核中，实时任务总是比普通任务的优先级要高。&lt;/p></description></item><item><title>Docs: 5.Memory 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/5.memory-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/5.memory-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/HJB_ATQFNqG82YBCRr97CA">公众号,小林 coding-真棒！ 20 张图揭开内存管理的迷雾，瞬间豁然开朗&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/0g3sS63yM2qbBja-blw5Dw">公众号，码农的荒岛求生-神秘！申请内存时底层发生了什么？&lt;/a>(malloc 简介)&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>对任何一台计算机而言，其内存以及其它资源都是有限的。为了让有限的&lt;strong>物理内存&lt;/strong>满足应用程序对内存的大需求量，Linux 采用了称为 **虚拟内存 **的内存管理方式。Linux 将内存划分为容易处理的“内存页”（对于大部分体系结构来说都是 4KB）。Linux 包括了管理可用内存的方式，以及物理和虚拟映射所使用的硬件机制。&lt;/p>
&lt;p>不过内存管理要管理的可不止 4KB 缓冲区。Linux 提供了对 4KB 缓冲区的抽象，例如 slab 分配器。这种内存管理模式使用 4KB 缓冲区为基数，然后从中分配结构，并跟踪内存页使用情况，比如哪些内存页是满的，哪些页面没有完全使用，哪些页面为空。这样就允许该模式根据系统需要来动态调整内存使用。&lt;/p>
&lt;p>为了支持多个用户使用内存，有时会出现可用内存被消耗光的情况。由于这个原因，页面可以移出内存并放入磁盘中。这个过程称为交换，因为页面会被从内存交换到硬盘上。内存管理的源代码可以在 ./linux/mm 中找到。&lt;/p>
&lt;h1 id="虚拟内存">虚拟内存&lt;/h1>
&lt;p>如果你是电子相关专业的，肯定在大学里捣鼓过单片机。&lt;/p>
&lt;p>单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。&lt;/p>
&lt;p>另外，单片机的 CPU 是直接操作内存的「物理地址」。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919093-db60b152-2475-49e7-8a9d-813007e27b8d.jpeg" alt="">&lt;/p>
&lt;p>在这种情况下，要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。&lt;/p>
&lt;p>操作系统是如何解决这个问题呢？&lt;/p>
&lt;p>这里关键的问题是这两个程序都引用了绝对物理地址，而这正是我们最需要避免的。&lt;/p>
&lt;p>我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「虚拟地址」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919100-24627511-d5f5-4663-9a2c-76b4b3b75664.jpeg" alt="">&lt;/p>
&lt;p>进程的中间层&lt;/p>
&lt;p>操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。&lt;/p>
&lt;p>如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。&lt;/p>
&lt;p>于是，这里就引出了两种地址的概念：&lt;/p>
&lt;ul>
&lt;li>我们程序所使用的内存地址叫做虚拟内存地址（Virtual Memory Address）&lt;/li>
&lt;li>实际存在硬件里面的空间地址叫物理内存地址（Physical Memory Address）。&lt;/li>
&lt;/ul>
&lt;p>操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919111-7b5c26a3-f885-4ae8-bb6b-2bfab9cef4c9.jpeg" alt="">&lt;/p>
&lt;p>操作系统是如何管理虚拟地址与物理地址之间的关系？&lt;/p>
&lt;p>主要有两种方式，分别是内存分段和内存分页，分段是比较早提出的，我们先来看看内存分段。&lt;/p>
&lt;h2 id="内存分段">内存分段&lt;/h2>
&lt;p>程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。&lt;/p>
&lt;p>分段机制下，虚拟地址和物理地址是如何映射的？&lt;/p>
&lt;p>分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919128-c7568177-0bee-4534-84c6-694d268dd85d.jpeg" alt="">&lt;/p>
&lt;p>内存分段-寻址的方式&lt;/p>
&lt;ul>
&lt;li>段选择子就保存在段寄存器里面。段选择子里面最重要的是段号，用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。&lt;/li>
&lt;li>虚拟地址中的段内偏移量应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。&lt;/li>
&lt;/ul>
&lt;p>在上面了，知道了虚拟地址是通过段表与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919115-2be6810a-fa1d-41f1-92f8-b6390209f15a.jpeg" alt="">&lt;/p>
&lt;p>内存分段-虚拟地址与物理地址&lt;/p>
&lt;p>如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。&lt;/p>
&lt;p>分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：&lt;/p>
&lt;ul>
&lt;li>第一个就是内存碎片的问题。&lt;/li>
&lt;li>第二个就是内存交换的效率低的问题。&lt;/li>
&lt;/ul>
&lt;p>接下来，说说为什么会有这两个问题。&lt;/p>
&lt;p>我们先来看看，分段为什么会产生内存碎片的问题？&lt;/p>
&lt;p>我们来看看这样一个例子。假设有 1G 的物理内存，用户执行了多个程序，其中：&lt;/p>
&lt;ul>
&lt;li>游戏占用了 512MB 内存&lt;/li>
&lt;li>浏览器占用了 128MB 内存&lt;/li>
&lt;li>音乐占用了 256 MB 内存。&lt;/li>
&lt;/ul>
&lt;p>这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。&lt;/p>
&lt;p>如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919115-056b2c69-14f1-48d7-8162-7915c80cd090.jpeg" alt="">&lt;/p>
&lt;p>内存碎片的问题&lt;/p>
&lt;p>这里的内存碎片的问题共有两处地方：&lt;/p>
&lt;ul>
&lt;li>外部内存碎片，也就是产生了多个不连续的小物理内存，导致新的程序无法被装载；&lt;/li>
&lt;li>内部内存碎片，程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使用，这也会导致内存的浪费；&lt;/li>
&lt;/ul>
&lt;p>针对上面两种内存碎片的问题，解决的方式会有所不同。&lt;/p>
&lt;p>解决外部内存碎片的问题就是内存交换。&lt;/p>
&lt;p>可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。&lt;/p>
&lt;p>这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。&lt;/p>
&lt;p>再来看看，分段为什么会导致内存交换效率低的问题？&lt;/p>
&lt;p>对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，产生了内存碎片，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。&lt;/p>
&lt;p>因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。&lt;/p>
&lt;p>所以，如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。&lt;/p>
&lt;p>为了解决内存分段的内存碎片和内存交换效率低的问题，就出现了内存分页。&lt;/p>
&lt;h2 id="内存分页">内存分页&lt;/h2>
&lt;p>分段的好处就是能产生连续的内存空间，但是会出现内存碎片和内存交换的空间太大的问题。&lt;/p>
&lt;p>要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是内存分页（Paging）。&lt;/p>
&lt;p>分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。在 Linux 下，每一页的大小为 4KB。&lt;/p>
&lt;p>虚拟地址与物理地址之间通过页表来映射，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919131-ca0a6877-5b8c-49fc-8982-ff9bd4f247a2.jpeg" alt="">&lt;/p>
&lt;p>内存映射&lt;/p>
&lt;p>页表实际上存储在 CPU 的内存管理单元 （MMU） 中，于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。&lt;/p>
&lt;p>而当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。&lt;/p>
&lt;p>分页是怎么解决分段的内存碎片、内存交换效率低的问题？&lt;/p>
&lt;p>由于内存空间都是预先划分好的，也就不会像分段会产生间隙非常小的内存，这正是分段会产生内存碎片的原因。而采用了分页，那么释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。&lt;/p>
&lt;p>如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为换出（Swap Out）。一旦需要的时候，再加载进来，称为换入（Swap In）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，内存交换的效率就相对比较高。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919157-7b5cfce7-6d02-4a53-8193-49982f06399f.jpeg" alt="">&lt;/p>
&lt;p>换入换出&lt;/p>
&lt;p>更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。&lt;/p>
&lt;p>分页机制下，虚拟地址和物理地址是如何映射的？&lt;/p>
&lt;p>在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919140-c0d128d7-0238-450c-981a-8477221071a4.jpeg" alt="">&lt;/p>
&lt;p>内存分页寻址&lt;/p>
&lt;p>总结一下，对于一个内存地址转换，其实就是这样三个步骤：&lt;/p>
&lt;ul>
&lt;li>把虚拟内存地址，切分成页号和偏移量；&lt;/li>
&lt;li>根据页号，从页表里面，查询对应的物理页号；&lt;/li>
&lt;li>直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。&lt;/li>
&lt;/ul>
&lt;p>下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919128-24ebe4f8-b4c8-407b-8dc5-98da804e5e0c.jpeg" alt="">&lt;/p>
&lt;p>虚拟页与物理页的映射&lt;/p>
&lt;p>这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。&lt;/p>
&lt;p>简单的分页有什么缺陷吗？&lt;/p>
&lt;p>有空间上的缺陷。&lt;/p>
&lt;p>因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。&lt;/p>
&lt;p>在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表。&lt;/p>
&lt;p>这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。&lt;/p>
&lt;p>那么，100 个进程的话，就需要 400MB 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。&lt;/p>
&lt;h3 id="多级页表">多级页表&lt;/h3>
&lt;p>要解决上面的问题，就需要采用的是一种叫作多级页表（Multi-Level Page Table）的解决方案。&lt;/p>
&lt;p>在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 4KB 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。&lt;/p>
&lt;p>我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 1024 个页表（二级页表），每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。如下图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919185-cc42b53c-b247-413b-b8ea-9545012c17a3.jpeg" alt="">&lt;/p>
&lt;p>二级分页&lt;/p>
&lt;p>你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？&lt;/p>
&lt;p>当然如果 4GB 的虚拟地址全部都映射到了物理内上的，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。&lt;/p>
&lt;p>其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的局部性原理么？&lt;/p>
&lt;p>每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。&lt;/p>
&lt;p>如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= 0.804MB&lt;/p>
&lt;p>，这对比单级页表的 4MB 是不是一个巨大的节约？&lt;/p>
&lt;p>那么为什么不分级的页表就做不到这样节约内存呢？我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。&lt;/p>
&lt;p>我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。&lt;/p>
&lt;p>对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：&lt;/p>
&lt;ul>
&lt;li>全局页目录项 PGD（Page Global Directory）；&lt;/li>
&lt;li>上层页目录项 PUD（Page Upper Directory）；&lt;/li>
&lt;li>中间页目录项 PMD（Page Middle Directory）；&lt;/li>
&lt;li>页表项 PTE（Page Table Entry）；&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919165-47fbe1d8-e5a2-421b-9215-5152f9f6d457.jpeg" alt="">&lt;/p>
&lt;p>四级目录&lt;/p>
&lt;h3 id="tlb">TLB&lt;/h3>
&lt;p>多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。&lt;/p>
&lt;p>程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919150-0d7ed5fb-19de-4398-84ca-e77a4c67ab46.jpeg" alt="">&lt;/p>
&lt;p>程序的局部性&lt;/p>
&lt;p>我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 Translation Lookaside Buffer(转译后备缓冲器，简称 TLB、缓存、快表)等。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919151-684032a9-151a-4c86-be9a-7535abfc6444.jpeg" alt="">&lt;/p>
&lt;p>地址转换&lt;/p>
&lt;p>在 CPU 芯片里面，封装了内存管理单元（Memory Management Unit）芯片，它用来完成地址转换和 TLB 的访问与交互。&lt;/p>
&lt;p>有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。&lt;/p>
&lt;p>TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。&lt;/p>
&lt;h2 id="段页式内存管理">段页式内存管理&lt;/h2>
&lt;p>内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为段页式内存管理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919147-0a7f2b14-9364-4a8e-ba60-c48e8cdf65d0.jpeg" alt="">&lt;/p>
&lt;p>段页式地址空间&lt;/p>
&lt;p>段页式内存管理实现的方式：&lt;/p>
&lt;ul>
&lt;li>先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；&lt;/li>
&lt;li>接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；&lt;/li>
&lt;/ul>
&lt;p>这样，地址结构就由段号、段内页号和页内位移三部分组成。&lt;/p>
&lt;p>用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919172-07b25f4c-02c8-47f2-a784-d89ba249e13e.jpeg" alt="">&lt;/p>
&lt;p>段页式管理中的段表、页表与内存的关系&lt;/p>
&lt;p>段页式地址变换中要得到物理地址须经过三次内存访问：&lt;/p>
&lt;ul>
&lt;li>第一次访问段表，得到页表起始地址；&lt;/li>
&lt;li>第二次访问页表，得到物理页号；&lt;/li>
&lt;li>第三次将物理页号与页内位移组合，得到物理地址。&lt;/li>
&lt;/ul>
&lt;p>可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。&lt;/p>
&lt;h1 id="linux-内存管理">Linux 内存管理&lt;/h1>
&lt;p>那么，Linux 操作系统采用了哪种方式来管理内存呢？&lt;/p>
&lt;p>在回答这个问题前，我们得先看看 Intel 处理器的发展历史。&lt;/p>
&lt;p>早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了对页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。&lt;/p>
&lt;p>但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，页式内存管理的作用是在由段式内存管理所映射而成的的地址上再加上一层地址映射。&lt;/p>
&lt;p>由于此时段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919178-17fb261b-55fa-44ac-a553-ed8a842e9080.jpeg" alt="">&lt;/p>
&lt;p>Intel X86 逻辑地址解析过程&lt;/p>
&lt;p>这里说明下逻辑地址和线性地址：&lt;/p>
&lt;ul>
&lt;li>程序所使用的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；&lt;/li>
&lt;li>通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址；&lt;/li>
&lt;/ul>
&lt;p>逻辑地址是「段式内存管理」转换前的地址，线性地址则是「页式内存管理」转换前的地址。&lt;/p>
&lt;p>了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采用了什么方式管理内存？&lt;/p>
&lt;p>Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制。&lt;/p>
&lt;p>这主要是上面 Intel 处理器发展历史导致的，因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。&lt;/p>
&lt;p>但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作用。也就是说，“上有政策，下有对策”，若惹不起就躲着走。&lt;/p>
&lt;p>Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。&lt;/p>
&lt;p>我们再来瞧一瞧，Linux 的虚拟地址空间是如何分布的？&lt;/p>
&lt;p>在 Linux 操作系统中，虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919191-4099f869-9dbe-462c-90f6-ff02cec5a9a6.jpeg" alt="">&lt;/p>
&lt;p>用户空间与内存空间&lt;/p>
&lt;p>通过这里可以看出：&lt;/p>
&lt;ul>
&lt;li>32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间；&lt;/li>
&lt;li>64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。&lt;/li>
&lt;/ul>
&lt;p>再来说说，内核空间与用户空间的区别：&lt;/p>
&lt;ul>
&lt;li>进程在用户态时，只能访问用户空间内存；&lt;/li>
&lt;li>只有进入内核态后，才可以访问内核空间的内存；&lt;/li>
&lt;/ul>
&lt;p>虽然每个进程都各自有独立的虚拟内存，但是每个虚拟内存中的内核地址，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919223-81c46952-4cf1-4b83-a9a5-2bdd118168b6.jpeg" alt="">&lt;/p>
&lt;p>每个进程的内核空间都是一致的&lt;/p>
&lt;p>接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。&lt;/p>
&lt;p>我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/odexpg/1616167919192-30c3ec2b-724a-4146-9bc4-3c630ea69e13.jpeg" alt="">&lt;/p>
&lt;p>虚拟内存空间划分&lt;/p>
&lt;p>通过这张图你可以看到，用户空间内存，从低到高分别是 7 种不同的内存段：&lt;/p>
&lt;ul>
&lt;li>程序文件段，包括二进制可执行代码；&lt;/li>
&lt;li>已初始化数据段，包括静态常量；&lt;/li>
&lt;li>未初始化数据段，包括未初始化的静态变量；&lt;/li>
&lt;li>堆段，包括动态分配的内存，从低地址开始向上增长；&lt;/li>
&lt;li>文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关）&lt;/li>
&lt;li>栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。当然系统也提供了参数，以便我们自定义大小；&lt;/li>
&lt;/ul>
&lt;p>在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。&lt;/p>
&lt;h1 id="总结">总结&lt;/h1>
&lt;p>为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套的虚拟地址空间，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。作为程序，也不用关心物理地址的事情。&lt;/p>
&lt;p>每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过内存交换技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。&lt;/p>
&lt;p>那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。&lt;/p>
&lt;p>那么对于虚拟地址与物理地址的映射关系，可以有分段和分页的方式，同时两者结合都是可以的。&lt;/p>
&lt;p>内存分段是根据程序的逻辑角度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是一块连续的空间。但是每个段的大小都不是统一的，这就会导致内存碎片和内存交换效率低的问题。&lt;/p>
&lt;p>于是，就出现了内存分页，把虚拟空间和物理空间分成大小固定的页，如在 Linux 系统中，每一页的大小为 4KB。由于分了页后，就不会产生细小的内存碎片。同时在内存交换的时候，写入硬盘也就一个页或几个页，这就大大提高了内存交换的效率。&lt;/p>
&lt;p>再来，为了解决简单分页产生的页表过大的问题，就有了多级页表，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加大了时间上的开销。于是根据程序的局部性原理，在 CPU 芯片中加入了 TLB，负责缓存最近常被访问的页表项，大大提高了地址的转换速度。&lt;/p>
&lt;p>Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理。于是 Linux 就把所有段的基地址设为 0，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。&lt;/p>
&lt;p>另外，Linxu 系统中虚拟空间分布可分为用户态和内核态两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。&lt;/p></description></item><item><title>Docs: 6.File System 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/6.file-system-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/6.file-system-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw">小林公众号&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://time.geekbang.org/column/article/76876">Linux 性能优化实践-文件系统&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Category:Computer_file_systems">Wiki-Category,Computer file systemd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw">公众号，小林 coding-一口气搞懂「文件系统」，就靠这 25 张图了&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805545-2b948cff-7e56-4eb8-8c12-3851fd6c2e36.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>图片来源：&lt;a href="https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram">https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram&lt;/a>
从上面的结构可以看到，文件系统的作用就是用来接收用户的操作，并将数据保存到物理硬盘的。可以想见，如果没有文件系统帮助用户操作，那么人们又怎么能将数据保存到存储设备上呢~&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>File System(文件系统，简称 FS)&lt;/strong> 是一种对存储设备上的数据，进行组织管理的机制。组织方式的不同，就会形成不同的文件系统
如果没有文件系统，放置在存储介质中的数据将是一个庞大的数据主体，无法分辨一个数据在哪里停止以及下一个数据在哪里开始。通过将数据分成多个部分并给每个部分命名，可以轻松地隔离和识别数据。每组数据称为 &lt;strong>File(文件)&lt;/strong>。所以，用于管理这些文件及其名称的&lt;strong>结构和逻辑规则&lt;/strong>，称为 &lt;strong>File System(文件系统)&lt;/strong>。&lt;/p>
&lt;h2 id="什么是-file文件">什么是 File(文件)&lt;/h2>
&lt;p>详见《&lt;a href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel(%E5%86%85%E6%A0%B8)/6.File%20System%20%E7%AE%A1%E7%90%86/%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/_index.md">文件管理&lt;/a>》章节&lt;/p>
&lt;h1 id="文件组织结构">文件组织结构&lt;/h1>
&lt;blockquote>
&lt;p>文件管理详解见&lt;a href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.Kernel(%E5%86%85%E6%A0%B8)/6.File%20System%20%E7%AE%A1%E7%90%86/%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/_index.md">单独章节&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>为了方便管理，Linux 的文件系统为每个文件都分配了两个数据结构。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>index node(索引节点，简称 inode)&lt;/strong> # 记录文件的元数据。inode 编号、文件大小、访问权限、修改日期、数据的位置等。
&lt;ul>
&lt;li>inode 和文件一一对应，它跟文件内容一样，都会被持久化到存储的磁盘中。所以** inode 同样占用磁盘空间**。&lt;/li>
&lt;li>inode 包含文件的元数据，具体来说有以下内容：
&lt;ul>
&lt;li>文件的字节数&lt;/li>
&lt;li>文件拥有者的 User ID&lt;/li>
&lt;li>文件的 Group ID&lt;/li>
&lt;li>文件的读、写、执行权限&lt;/li>
&lt;li>文件的时间戳，共有三个：ctime 指 inode 上一次变动的时间，mtime 指文件内容上一次变动的时间，atime 指文件上一次打开的时间。&lt;/li>
&lt;li>链接数，即有多少文件名指向这个 inode&lt;/li>
&lt;li>文件数据 block 的位置&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>directory entry(目录项，简称 dentry)&lt;/strong> # 记录文件的名字、inode 指针、与其他目录项的关联关系。
&lt;ul>
&lt;li>多个关联的目录项，就构成了文件系统的目录结构(&lt;strong>一个层次化的树形结构&lt;/strong>)。不过，不同于 inode，目录项是由内核维护的一个内存数据结构，所以通常也被叫做 &lt;strong>dentries(目录项缓存)。&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这个层次化的树形结构就像下图一样：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1617088781476-3d7a9ccc-e8df-4680-acc5-26f4f82aa8b5.png" alt="image.png">
&lt;strong>注意：目录项缓存记录在 slab 中，当我们使用 find 命令时，slab 中的 dentry 缓存就会增大；打开文件过多，slab 中的 dentry 缓存也会增大。&lt;/strong>
inode 是每个文件的唯一标志，而 dentry 维护的正是文件系统的树状结构。dentry 与 inode 的关系是多对一(可以简单理解为一个文件可以有多个别名)
下面用一个形象点的白话来描述这些概念，假如现在系统中有如下目录结构&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># tree --inodes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── &lt;span style="color:#f92672">[&lt;/span> 2218&lt;span style="color:#f92672">]&lt;/span> dir_1 &lt;span style="color:#75715e"># 这是目录类型的文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── &lt;span style="color:#f92672">[&lt;/span> 2235&lt;span style="color:#f92672">]&lt;/span> file_1 &lt;span style="color:#75715e"># 这是普通类型的文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └── &lt;span style="color:#f92672">[&lt;/span> 2236&lt;span style="color:#f92672">]&lt;/span> file_2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── &lt;span style="color:#f92672">[&lt;/span>269167463&lt;span style="color:#f92672">]&lt;/span> dir_2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └── &lt;span style="color:#f92672">[&lt;/span>269167464&lt;span style="color:#f92672">]&lt;/span> file_3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── &lt;span style="color:#f92672">[&lt;/span>537384536&lt;span style="color:#f92672">]&lt;/span> dir_3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── &lt;span style="color:#f92672">[&lt;/span> 2235&lt;span style="color:#f92672">]&lt;/span> fie_1_ln
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── &lt;span style="color:#f92672">[&lt;/span>537384537&lt;span style="color:#f92672">]&lt;/span> file_4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">3&lt;/span> directories, &lt;span style="color:#ae81ff">5&lt;/span> files
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以这么描述上述看到的内容：dir_1、file_1、dir_2 这些名称都是 dentry 中的文件名，&lt;code>[]&lt;/code> 中的数字是 inode 号，每个 dentry 都会与 inode 关联。其中 file_1 和 file_1_ln 的 inode 相同，但是 dentry 不同，这就对应了 dentry 与 inode 是多对一的关系。而哪些文件在哪个目录中，则是由每个文件的 dentry 中的关联关系来决定。比如 dir_1 目录中，包含了 file_1 和 file_2 文件。&lt;/p>
&lt;blockquote>
&lt;p>索引节点和目录项记录了文件的元数据，以及文件间的目录关系，那么具体来说，文件数据到底是怎么存储的呢？是不是直接写到磁盘中就好了呢？
实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805558-180916c2-cc19-40a0-b8f4-3ff805929883.png" alt="">
注意：&lt;/p>
&lt;ul>
&lt;li>dentry 本身只是一个存储在内存中的缓存，而 inode 则是存储在磁盘中的数据。由于内存的 Buffer 和 Cache 原理，所以 inode 也会缓存到内存中，以便加速文件的访问。&lt;/li>
&lt;li>磁盘在执行文件系统格式化时，会被分成三个存储区域，超级快、索引节点区、数据区块
&lt;ul>
&lt;li>超级块 # 存储整个文件系统的状态&lt;/li>
&lt;li>索引节点区 # 存储 inode&lt;/li>
&lt;li>数据区块 # 存储文件数据&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：
&lt;ul>
&lt;li>超级块 # 当文件系统挂载时进入内存；&lt;/li>
&lt;li>索引节点区 # 当文件被访问时进入内存&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>dentry、inode、逻辑块以及超级块构成了 Linux 文件系统的四大基本要素。&lt;/p>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>通过上面的描述，文件在文件系统中，也就可以归纳为两个部分&lt;/p>
&lt;ul>
&lt;li>指针部分 # 指针位于文件系统的元数据中，在将数据删除后，这个指针就从元数据中清除了(元数据其实就是上文的 inode 与 dentry)。&lt;/li>
&lt;li>数据部分 # 文件的具体内容，存储在磁盘中。&lt;/li>
&lt;/ul>
&lt;p>平时我们在删除数据时，其实仅仅从元数据中删除了数据对应的指针。当指针被删除时，其原本占用的空间就可以被覆盖并写入新内容。&lt;/p>
&lt;h2 id="常见问题">常见问题&lt;/h2>
&lt;ul>
&lt;li>这也是为什么我们可以恢复数据的原因，只要旧数据还没被覆盖，就依然可以获取到。&lt;/li>
&lt;li>有时候在删除文件时，会发现并没有释放空间，也是同样的道理，当某个进程持续写入内容时，如果强制删除了文件，由于进程锁定文件对应的指针部分并不会从元数据中清除，而由于指针并未删除，系统内核就默认文件并未删除，因此查询文件系统空间时，显示空间并未释放。可以通过 lsof 命令筛选 deleted 查找这些有问题的文件。&lt;/li>
&lt;/ul>
&lt;h1 id="virtual-file-system虚拟文件系统">Virtual File System(虚拟文件系统)&lt;/h1>
&lt;blockquote>
&lt;p>参考： - &lt;a href="https://en.wikipedia.org/wiki/Virtual_file_system">Wiki,Virtual file system&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/69289429">知乎&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://time.geekbang.org/column/article/76876">极客-Linux 性能优化实践&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Virtual File System(虚拟文件系统，简称 VFS)&lt;/strong>。是 Linux 为了支持多种多样的文件系统，在用户空间进程和文件系统中间，引入的一个抽象层。VFS 的目的是运行客户端应用程序以统一的方式访问不同类型的文件系统。VFS &lt;strong>定义了&lt;/strong>一组所有文件系统都支持的&lt;strong>数据结构和标准 API&lt;/strong>。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互即可，而不需要关系底层各种文件系统的实现细节。&lt;/p>
&lt;blockquote>
&lt;p>比如不同文件系统的调用函数不一样，如果没有 VFS ，那么在使用的时候，就需要为特定的文件系统，编写不同的调用方式，非常繁琐复杂。
比如 VFS 可以用来弥合 Windows，经典 Mac OS / macOS 和 Unix 文件系统中的差异，以便应用程序可以访问那些类型的本地文件系统上的文件，而不必知道它们正在访问哪种文件系统。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805621-09dbf293-4f9a-4892-8e30-8d33f32031c4.png" alt="">
举个例子，Linux 用户程序可以通过&lt;code>read()&lt;/code> 来读取&lt;code>ext4&lt;/code>、&lt;code>NFS&lt;/code>、&lt;code>XFS&lt;/code>等文件系统的文件，也可以读取存储在&lt;code>SSD&lt;/code>、&lt;code>HDD&lt;/code>等不同存储介质的文件，无须考虑不同文件系统或者不同存储介质的差异。
通过 VFS 系统，Linux 提供了通用的系统调用，可以跨越不同文件系统和介质之间执行，极大简化了用户访问不同文件系统的过程。另一方面，新的文件系统、新类型的存储介质，可以无须编译的情况下，动态加载到 Linux 中。
&amp;ldquo;一切皆文件&amp;quot;是 Linux 的基本哲学之一，不仅是普通的文件，包括目录、字符设备、块设备、套接字等，都可以以文件的方式被对待。实现这一行为的基础，正是 Linux 的虚拟文件系统机制。
VFS 之所以能够衔接各种各样的文件系统，是因为它抽象了一个通用的文件系统模型，定义了通用文件系统都支持的、概念上的接口。新的文件系统只要支持并实现这些接口，并注册到 Linux 内核中，即可安装和使用。
再举个例子，比如 Linux 写一个文件：&lt;/p>
&lt;pre>&lt;code>int ret = write(fd, buf, len);
&lt;/code>&lt;/pre>
&lt;p>调用了&lt;code>write()&lt;/code>系统调用，它的过程简要如下：&lt;/p>
&lt;ul>
&lt;li>首先，勾起 VFS 通用系统调用&lt;code>sys_write()&lt;/code>处理。&lt;/li>
&lt;li>接着，&lt;code>sys_write()&lt;/code>根据&lt;code>fd&lt;/code>找到所在的文件系统提供的写操作函数，比如&lt;code>op_write()&lt;/code>。&lt;/li>
&lt;li>最后，调用&lt;code>op_write()&lt;/code>实际的把数据写入到文件中。&lt;/li>
&lt;/ul>
&lt;p>操作示意图如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805551-1b23e389-6142-4e11-8ef1-b1b1c1722cbe.jpeg" alt="">&lt;/p>
&lt;h1 id="文件系统类型">文件系统类型&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/File_system#Types_of_file_systems">Wiki,File system-Types_of_file_systems&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>可以通过** /proc/filesystems **文件查看当前内核所支持的文件系统类型&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/filesystems&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nodev sysfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nodev proc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nodev cgroup2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nodev tmpfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nodev devtmpfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nodev configfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ext4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>第一列说明文件系统是否需要挂载在一个块设备上
&lt;ul>
&lt;li>nodev 表明本行的文件系统类型不需要挂接在块设备上。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>第二列是内核支持的文件系统类型。&lt;/li>
&lt;/ul>
&lt;p>当系统中安装了某个文件系统的驱动，则该文件内容也会有增加，比如我安装了 nfs-utils 包，则该文件还会增加 nfs 行。&lt;/p>
&lt;h2 id="按照存储位置的不同这些文件系统可以大体分为如下几类">按照存储位置的不同，这些文件系统可以大体分为如下几类&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Disk file systems(磁盘文件系统)&lt;/strong>
&lt;ul>
&lt;li>基于磁盘的文件系统，也就是把数据直接存储 计算机本地挂载磁盘中。常见的** ext4、xfs、overlayFS** 等，都是这类文件系统&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Network File Systems(网络文件系统)&lt;/strong>
&lt;ul>
&lt;li>网络文件系统是充当远程文件访问协议的客户端的文件系统，提供对服务器上文件的访问。 使用本地接口的程序可以透明地创建，管理和访问远程网络连接计算机中的分层目录和文件。 网络文件系统的示例包括 NFS，AFS，SMB 协议的客户端，以及 FTP 和 WebDAV 的类似于文件系统的客户端。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Distributed File System(分布式文件系统)&lt;/strong> # 使用网络协议的分布式文件系统也属于网络文件系统的一种。&lt;/li>
&lt;li>&lt;strong>Special-purpose File Systems(特殊目的文件系统)&lt;/strong> # 特殊的文件系统将操作系统的非文件元素显示为文件，以便可以使用文件系统 API 对其进行操作。 这种文件系统一般都是基于内存的，不需要任何磁盘为其分配存储空间，但会占用内存。
&lt;ul>
&lt;li>&lt;strong>device file system(设备文件系统)&lt;/strong> # 简称 devfs，设备文件系统将 I/O 设备和伪设备表示为文件，称为设备文件。 默认挂载到&lt;code>/dev&lt;/code>目录下。&lt;/li>
&lt;li>&lt;strong>Proc File System(进程文件系统)&lt;/strong> # 简称_ _procfs，将进程以及 Linux 上的其他操作系统结构映射到文件空间。默认挂载到&lt;code>/proc&lt;/code>目录下。&lt;/li>
&lt;li>&lt;strong>configfs&lt;/strong> 和 &lt;strong>sysfs&lt;/strong> 提供了可用于向内核查询信息并在内核中配置实体的文件。&lt;/li>
&lt;li>等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="文件系统的使用">文件系统的使用&lt;/h1>
&lt;blockquote>
&lt;p>和 DOS 等操作系统不同，Linux 操作系统中文件系统并不是由驱动器号或驱动器名称（如 A: 或 C: 等）来标识的。Linux 操作系统将独立的文件系统组合成了一个层次化的树形结构，并且由一个单独的实体代表这一文件系统。&lt;/p>
&lt;/blockquote>
&lt;p>Linux 将新的文件系统通过一个称为 **Mount(挂载) **的操作将其挂载到某个目录上，从而让不同的文件系统结合成为一个整体。
这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。拿第一类，也就是基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录&lt;code>/&lt;/code>，在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。&lt;/p>
&lt;h2 id="文件系统-io">文件系统 I/O&lt;/h2>
&lt;p>把文件系统挂载到挂载点后，就可以通过挂载点访问它管理的文件了。 VFS 提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用。
就比如 cat 命令，首先调用 &lt;code>openat()&lt;/code> 打开一个文件，然后调用 &lt;code>read()&lt;/code> 读取文件内容，最后调用 &lt;code>write()&lt;/code> 将内容输出到控制台的标准输出中&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@lichenhao ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># strace -e openat,read,write cat /root/test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openat&lt;span style="color:#f92672">(&lt;/span>AT_FDCWD, &lt;span style="color:#e6db74">&amp;#34;/root/test&amp;#34;&lt;/span>, O_RDONLY&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>read&lt;span style="color:#f92672">(&lt;/span>3, &lt;span style="color:#e6db74">&amp;#34;Test I/O for File System&amp;#34;&lt;/span>, 131072&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">24&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>write&lt;span style="color:#f92672">(&lt;/span>1, &lt;span style="color:#e6db74">&amp;#34;Test I/O for File System&amp;#34;&lt;/span>, 24Test I/O &lt;span style="color:#66d9ef">for&lt;/span> File System&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">24&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 代码中的方法如下：open() 与 openat() 这两个调用效果一样。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>int open&lt;span style="color:#f92672">(&lt;/span>const char *pathname, int flags, mode_t mode&lt;span style="color:#f92672">)&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ssize_t read&lt;span style="color:#f92672">(&lt;/span>int fd, void *buf, size_t count&lt;span style="color:#f92672">)&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ssize_t write&lt;span style="color:#f92672">(&lt;/span>int fd, const void *buf, size_t count&lt;span style="color:#f92672">)&lt;/span>;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>文件读写方式的各种差异，导致 I/O 的分类多种多样。最常见的有，缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等。&lt;/p>
&lt;h3 id="缓冲与非缓冲-io">缓冲与非缓冲 I/O&lt;/h3>
&lt;p>根据是否利用标准库缓存&lt;/p>
&lt;ol>
&lt;li>缓冲 I/O # 利用标准库缓存来加速文件的访问，而标准库内部再通过系统调用访问文件&lt;/li>
&lt;li>非缓冲 I/O # 直接通过系统调用来访问文件，不再经过标准库缓存。&lt;/li>
&lt;/ol>
&lt;p>注意，这里所说的“缓冲”，是指标准库内部实现的缓存。比方说，你可能见到过，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来。
无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。而根据上一节内容，我们知道，系统调用后，还会通过页缓存，来减少磁盘的 I/O 操作。&lt;/p>
&lt;h3 id="直接与非直接-io">直接与非直接 I/O&lt;/h3>
&lt;p>根据是否利用操作系统的页缓存&lt;/p>
&lt;ol>
&lt;li>直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。&lt;/li>
&lt;li>非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。&lt;/li>
&lt;/ol>
&lt;p>想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。
不过要注意，直接 I/O、非直接 I/O，本质上还是和文件系统交互。如果是在数据库等场景中，你还会看到，跳过文件系统读写磁盘的情况，也就是我们通常所说的裸 I/O。&lt;/p>
&lt;h3 id="阻塞与非阻塞-io">阻塞与非阻塞 I/O&lt;/h3>
&lt;p>根据应用程序是否阻塞自身运行&lt;/p>
&lt;ol>
&lt;li>所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。&lt;/li>
&lt;li>所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。&lt;/li>
&lt;/ol>
&lt;p>比方说，访问管道或者网络套接字时，设置 O_NONBLOCK 标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问。&lt;/p>
&lt;h3 id="同步与异步-io">同步与异步 I/O&lt;/h3>
&lt;p>根据是否等待响应结果&lt;/p>
&lt;ol>
&lt;li>所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。&lt;/li>
&lt;li>所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。&lt;/li>
&lt;/ol>
&lt;p>举个例子，在操作文件时，如果你设置了 O_SYNC 或者 O_DSYNC 标志，就代表同步 I/O。如果设置了 O_DSYNC，就要等文件数据写入磁盘后，才能返回；而 O_SYNC，则是在 O_DSYNC 基础上，要求文件元数据也要写入磁盘后，才能返回。
再比如，在访问管道或者网络套接字时，设置了 O_ASYNC 选项后，相应的 I/O 就是异步 I/O。这样，内核会再通过 SIGIO 或者 SIGPOLL，来通知进程文件是否可读写。
你可能发现了，这里的好多概念也经常出现在网络编程中。比如非阻塞 I/O，通常会跟 select/poll 配合，用在网络套接字的 I/O 中。
你也应该可以理解，“Linux 一切皆文件”的深刻含义。无论是普通文件和块设备、还是网络套接字和管道等，它们都通过统一的 VFS 接口来访问。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677139-656d34bf-0195-4576-919f-2eedc4f4ba5a.png" alt="">
在前面我们知道了，I/O 是分为两个过程的：&lt;/p>
&lt;ol>
&lt;li>数据准备的过程&lt;/li>
&lt;li>数据从内核空间拷贝到用户进程缓冲区的过程&lt;/li>
&lt;/ol>
&lt;p>阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。
异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。&lt;/p>
&lt;h2 id="用故事去理解这几种-io-模型">用故事去理解这几种 I/O 模型&lt;/h2>
&lt;p>举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。
阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。
非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。
基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（&lt;code>select&lt;/code> 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。
异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。&lt;/p>
&lt;h1 id="文件的存储">文件的存储&lt;/h1>
&lt;p>文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：&lt;/p>
&lt;ul>
&lt;li>连续空间存放方式&lt;/li>
&lt;li>非连续空间存放方式&lt;/li>
&lt;/ul>
&lt;p>其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。
不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。&lt;/p>
&lt;h2 id="连续空间存放方式">连续空间存放方式&lt;/h2>
&lt;blockquote>
&lt;p>注意：这里只针对机械硬盘，固态硬盘并没有磁道等概念&lt;/p>
&lt;/blockquote>
&lt;p>连续空间存放方式顾名思义，&lt;strong>文件存放在磁盘「连续的」物理空间中&lt;/strong>。这种模式下，文件的数据都是紧密相连，&lt;strong>读写效率很高&lt;/strong>，因为一次磁盘寻道就可以读出整个文件。
使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。
所以，&lt;strong>文件头里需要指定「起始块的位置」和「长度」&lt;/strong>，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。
注意，此处说的文件头，就类似于 Linux 的 inode。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677049-26c7ae42-9e37-426a-99f9-6e59df62e691.png" alt="">
连续空间存放的方式虽然读写效率高，&lt;strong>但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。&lt;/strong>
如下图，如果文件 B 被删除，磁盘上就留下一块空缺，这时，如果新来的文件小于其中的一个空缺，我们就可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但却小于空缺大小之和，则虽然磁盘上有足够的空缺，但该文件还是不能存放。当然了，我们可以通过将现有文件进行挪动来腾出空间以容纳新的文件，但是这个在磁盘挪动文件是非常耗时，所以这种方式不太现实。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677046-4fd1191e-4e74-4653-8ef4-24ba781c4f57.png" alt="">
另外一个缺陷是文件长度扩展不方便，例如上图中的文件 A 要想扩大一下，需要更多的磁盘空间，唯一的办法就只能是挪动的方式，前面也说了，这种方式效率是非常低的。
那么有没有更好的方式来解决上面的问题呢？答案当然有，既然连续空间存放的方式不太行，那么我们就改变存放的方式，使用非连续空间存放方式来解决这些缺陷。&lt;/p>
&lt;h2 id="非连续空间存放方式">非连续空间存放方式&lt;/h2>
&lt;p>非连续空间存放方式分为「链表方式」和「索引方式」。&lt;/p>
&lt;blockquote>
&lt;p>我们先来看看链表的方式。&lt;/p>
&lt;/blockquote>
&lt;p>链表的方式存放是&lt;strong>离散的，不用连续的&lt;/strong>，于是就可以&lt;strong>消除磁盘碎片&lt;/strong>，可大大提高磁盘空间的利用率，同时&lt;strong>文件的长度可以动态扩展&lt;/strong>。根据实现的方式的不同，链表可分为「&lt;strong>隐式链表&lt;/strong>」和「&lt;strong>显式链接&lt;/strong>」两种形式。
文件要以「&lt;strong>隐式链表&lt;/strong>」的方式存放的话，&lt;strong>实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置&lt;/strong>，这样一个数据块连着一个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677039-c71df217-651d-42ac-8cf6-444dedae8c1c.png" alt="">
隐式链表的存放方式的&lt;strong>缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间&lt;/strong>。隐式链接分配的&lt;strong>稳定性较差&lt;/strong>，系统在运行过程中由于软件或者硬件错误&lt;strong>导致链表中的指针丢失或损坏，会导致文件数据的丢失。&lt;/strong>
如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「&lt;strong>显式链接&lt;/strong>」，它指&lt;strong>把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中&lt;/strong>，该表在整个磁盘仅设置一张，&lt;strong>每个表项中存放链接指针，指向下一个数据块号&lt;/strong>。
对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为&lt;strong>文件分配表（***&lt;strong>File Allocation Table，FAT&lt;/strong>*&lt;/strong>）&lt;strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677063-b7bb2a32-dde6-4c17-9119-096413902ae8.png" alt="">
由于查找记录的过程是在内存中进行的，因而不仅显著地&lt;/strong>提高了检索速度**，而且&lt;strong>大大减少了访问磁盘的次数&lt;/strong>。但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适用于大磁盘**。
比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。&lt;/p>
&lt;blockquote>
&lt;p>接下来，我们来看看索引的方式。&lt;/p>
&lt;/blockquote>
&lt;p>链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT 除外），索引的方式可以解决这个问题。
索引的实现是为每个文件创建一个「&lt;strong>索引数据块&lt;/strong>」，里面存放的是&lt;strong>指向文件数据块的指针列表&lt;/strong>，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。
另外，&lt;strong>文件头需要包含指向「索引数据块」的指针&lt;/strong>，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。
创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677054-825db739-3c6c-4fec-bb21-1dd52effb539.png" alt="">
索引的方式优点在于：&lt;/p>
&lt;ul>
&lt;li>文件的创建、增大、缩小很方便；&lt;/li>
&lt;li>不会有碎片的问题；&lt;/li>
&lt;li>支持顺序读写和随机读写；&lt;/li>
&lt;/ul>
&lt;p>由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。&lt;/p>
&lt;p>如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。&lt;/p>
&lt;p>先来看看链表 + 索引的组合，这种组合称为「&lt;strong>链式索引块&lt;/strong>」，它的实现方式是&lt;strong>在索引数据块留出一个存放下一个索引数据块的指针&lt;/strong>，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677072-417dcaa4-348d-4a45-b9b0-aa170d0f3bd7.png" alt="">
还有另外一种组合方式是索引 + 索引的方式，这种组合称为「&lt;strong>多级索引块&lt;/strong>」，实现方式是&lt;strong>通过一个索引块来存放多个索引数据块&lt;/strong>，一层套一层索引，像极了俄罗斯套娃是吧。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677056-ecb3b996-c14c-455d-82aa-7ca16aeb7feb.png" alt="">&lt;/p>
&lt;h2 id="unix-文件的实现方式">Unix 文件的实现方式&lt;/h2>
&lt;p>我们先把前面提到的文件实现方式，做个比较：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1671074085764-374b9218-86e8-412d-85c7-9d051b9340b2.png" alt="image.png">
那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677095-e05fe455-c302-43bb-a9d8-dd43cd4551e1.png" alt="">
它是根据文件的大小，存放的方式会有所变化：&lt;/p>
&lt;ul>
&lt;li>如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；&lt;/li>
&lt;li>如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；&lt;/li>
&lt;li>如果前面两种方式都不够存放大文件，则采用二级间接索引方式；&lt;/li>
&lt;li>如果二级间接索引也不够存放大文件，这采用三级间接索引方式；&lt;/li>
&lt;/ul>
&lt;p>那么，文件头（&lt;em>Inode&lt;/em>）就需要包含 13 个指针：&lt;/p>
&lt;ul>
&lt;li>10 个指向数据块的指针；&lt;/li>
&lt;li>第 11 个指向索引块的指针；&lt;/li>
&lt;li>第 12 个指向二级索引块的指针；&lt;/li>
&lt;li>第 13 个指向三级索引块的指针；&lt;/li>
&lt;/ul>
&lt;p>所以，这种方式能很灵活地支持小文件和大文件的存放：&lt;/p>
&lt;ul>
&lt;li>对于小文件使用直接查找的方式可减少索引数据块的开销；&lt;/li>
&lt;li>对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；&lt;/li>
&lt;/ul>
&lt;p>这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。&lt;/p>
&lt;p>为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。&lt;/p>
&lt;h1 id="空闲空间管理">空闲空间管理&lt;/h1>
&lt;p>前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？
那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：&lt;/p>
&lt;ul>
&lt;li>空闲表法&lt;/li>
&lt;li>空闲链表法&lt;/li>
&lt;li>位图法&lt;/li>
&lt;/ul>
&lt;h2 id="空闲表法">空闲表法&lt;/h2>
&lt;p>空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677079-9e2989d6-2cda-4460-80ca-f3c0f28783f1.png" alt="">
当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。
这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。&lt;/p>
&lt;h2 id="空闲链表法">空闲链表法&lt;/h2>
&lt;p>我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677070-be98384a-ad4f-4a20-b5da-1d969816b938.png" alt="">
当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。&lt;/p>
&lt;p>这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。&lt;/p>
&lt;p>空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。&lt;/p>
&lt;h2 id="位图法">位图法&lt;/h2>
&lt;p>位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。&lt;/p>
&lt;p>当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：&lt;/p>
&lt;pre>&lt;code>1111110011111110001110110111111100111 ...
&lt;/code>&lt;/pre>
&lt;p>在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。&lt;/p>
&lt;h1 id="文件系统的结构">文件系统的结构&lt;/h1>
&lt;p>前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。&lt;/p>
&lt;p>数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 &lt;code>4 * 1024 * 8 = 2^15&lt;/code> 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 &lt;code>2^15 * 4 * 1024 = 2^27&lt;/code> 个 byte，也就是 128M。&lt;/p>
&lt;p>也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。&lt;/p>
&lt;p>在 Linux 文件系统，把这个结构称为一个&lt;strong>块组&lt;/strong>，那么有 N 多的块组，就能够表示 N 大的文件。&lt;/p>
&lt;p>下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677099-7ffa20c4-57d9-4c49-9e02-24afef066cfb.png" alt="">
最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：&lt;/p>
&lt;ul>
&lt;li>&lt;em>超级块&lt;/em>，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。&lt;/li>
&lt;li>&lt;em>块组描述符&lt;/em>，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。&lt;/li>
&lt;li>&lt;em>数据位图和 inode 位图&lt;/em>， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。&lt;/li>
&lt;li>&lt;em>inode 列表&lt;/em>，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。&lt;/li>
&lt;li>&lt;em>数据块&lt;/em>，包含文件的有用数据。&lt;/li>
&lt;/ul>
&lt;p>你可以会发现每个块组里有很多重复的信息，比如&lt;strong>超级块和块组描述符表，这两个都是全局信息，而且非常的重要&lt;/strong>，这么做是有两个原因：&lt;/p>
&lt;ul>
&lt;li>如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。&lt;/li>
&lt;li>通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。&lt;/li>
&lt;/ul>
&lt;p>不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。&lt;/p></description></item><item><title>Docs: 7.Process 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/7.process-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/7.process-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.csdn.net/ljianhui/article/details/46718835">原文连接&lt;/a>，本文为 IBM RedBook 的 &lt;a href="http://users.polytech.unice.fr/~bilavarn/fichier/elec5_linux/linux_perf_and_tuning_IBM.pdf">Linux Performanceand Tuning Guidelines&lt;/a> 的 1.1 节的翻译&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/fzLcAkYwKhj-9hgoVkTzaw">阿里技术，CPU 飙高，系统性能问题如何排查？&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>进程管理是操作系统的最重要的功能之一。有效率的进程管理能保证一个程序平稳而高效地运行。它包括进程调度、中断处理、信号、进程优先级、上下文切换、进程状态、进度内存等。&lt;/p>
&lt;p>&lt;strong>Process(进程)&lt;/strong> 实际是运行在 CPU 中的一个** Program(应用程序) 的实体**。在 Linux 系统中，能够同时运行多个进程，Linux 通过在短的时间间隔内轮流运行这些进程而实现“多任务”。这一短的时间间隔称为“时间片”，让进程轮流运行的方法称为“进程调度” ，完成调度的程序称为调度程序。&lt;/p>
&lt;p>进程调度控制进程对 CPU 的访问。当需要选择下一个进程运行时，由调度程序选择最值得运行的进程。可运行进程实际上是仅等待 CPU 资源的进程，如果某个进程在等待其它资源，则该进程是不可运行进程。Linux 使用了比较简单的基于优先级的进程调度算法选择新的进程。&lt;/p>
&lt;p>通过多任务机制，每个进程可认为只有自己独占计算机，从而简化程序的编写。每个进程有自己单独的地址空间，并且只能由这一进程访问，这样，操作系统避免了进程之间的互相干扰以及“坏”程序对系统可能造成的危害。 为了完成某特定任务，有时需要综合两个程序的功能，例如一个程序输出文本，而另一个程序对文本进行排序。为此，操作系统还提供进程间的通讯机制来帮助完成这样的任务。Linux 中常见的进程间通讯机制有信号、管道、共享内存、信号量和套接字等。&lt;/p>
&lt;p>内核通过 SCI 提供了一个 API 来创建一个新进程(fork、exec 或 Portable Operating System Interface [POSⅨ] 函数)、停止进程(kill、exit)、并在它们之间进行通信和同步(signal 或者 POSⅨ 机制)。&lt;/p>
&lt;p>计算机实际上可以做的事情实质上非常简单，比如计算两个数的和，再比如在内存中寻找到某个地址等等。这些最基础的计算机动作被称为指令(instruction)。所谓的程序(program)，就是这样一系列指令的所构成的集合。通过程序，我们可以让计算机完成复杂的操作。程序大多数时候被存储为可执行的文件。这样一个可执行文件就像是一个菜谱，计算机可以按照菜谱作出可口的饭菜。&lt;/p>
&lt;p>Program(程序) 和 Process(进程) 的区别是什么呢?&lt;/p>
&lt;ol>
&lt;li>在很久很久以前，计算机刚出现的时候，是没有操作系统的，那时候一台机器只是运行一个程序，得出数据，后来人们为了同时运行多个程序从而研究出了操作系统，在操作系统之上可以运行多个程序&lt;/li>
&lt;li>进程是程序的一个具体实现。类似于按照食谱，真正去做菜的过程。同一个程序可以执行多次，每次都可以在内存中开辟独立的空间来装载，从而产生多个进程。不同的进程还可以拥有各自独立的 IO 接口。&lt;/li>
&lt;/ol>
&lt;p>操作系统的一个重要功能就是为进程提供方便，比如说为进程分配内存空间，管理进程的相关信息等等，就好像是为我们准备好了一个精美的厨房。&lt;/p>
&lt;h2 id="进程的生命周期">进程的生命周期&lt;/h2>
&lt;p>每一个进程都有其生命周期，例如创建、运行、终止和消除。这些阶段会在系统启动和运行中重复无数次。因此，进程的生命周期对于其性能的分析是非常重要的。下图展示了经典的进程生命周期。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507353-2f676d82-88da-483c-a939-399f284d6425.jpeg" alt="">
不会关闭的常驻进程可以称为 &lt;strong>Daemon Process(守护进程，简称 Daemon)&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>一般 daemon 的名称都会在进程名后加一个字母 d 作为 daemon 的 process，比如 vsftp 的 daemon 就是 vsftpd。&lt;/p>
&lt;/blockquote>
&lt;p>当一个进程创建一个新的进程，创建进程(父进程)的进程调用 一个 fork() 系统调用。当 fork() 系统调用被调用，它得到该新创建进程（子进程）的进程描述并调用一个新的进程 id。它复制该值到父进程进程描述到子进程中。此时整个的父进程的地址空间是没有被复制的；父子进程共享相同的地址空间。&lt;/p>
&lt;p>exec() 系统调用复制新的程序到子进程的地址空间。因为父子进程共享地址空间，写入一个新的程序的数据会引起一个分页错误。在这种情况下，内存会分配新的物理内存页给子进程。&lt;/p>
&lt;p>这个推迟的操作叫作写时复制。子进程通常运行他们自己的程序而不是与父进程运行相同的程序。这个操作避免了不必要的开销，因为复制整个地址空间是一个非常缓慢和效率低下的操作，它需要使用大量的处理器时间和资源。&lt;/p>
&lt;p>当程序已经执行完成，子进程通过调用 exit()系统调用终止。exit()系统调用释放进程大部分的数据并通过发送一个信号通知其父进程。此时，子进程是一个被叫作僵尸进程的进程（参阅 page 7 的“Zombie processes”）。&lt;/p>
&lt;p>子进程不会被完全移除直到其父进程知道其子进程的调用 wait()系统调用而终止。当父进程被通知子进程终止，它移除子进程的所有数据结构并释放它的进程描述。&lt;/p>
&lt;h2 id="父进程与子进程">父进程与子进程&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507409-d531245f-abbe-4a2a-b575-d2ae72c6949f.jpeg" alt="">&lt;/p>
&lt;ul>
&lt;li>用颜色的线标示的两列，左侧的为进程号(PID)右侧的为父进程号(PPID)&lt;/li>
&lt;li>子进程与父进程的环境变量相同&lt;/li>
&lt;li>老进程成为新进程的父进程(parent process)，而相应的，新进程就是老的进程的子进程(child process)。一个进程除了有一个 PID 之外，还会有一个 PPID(parent PID)来存储的父进程 PID。如果我们循着 PPID 不断向上追溯的话，总会发现其源头是 init 进程。所以说，所有的进程也构成一个以 init 为根的树状结构。&lt;/li>
&lt;li>如上图所示，我们查询当前 shell 下的进程：
&lt;ul>
&lt;li>我们可以看到，第二个进程 ps 是第一个进程 bash 的子进程。&lt;/li>
&lt;li>还可以用 &lt;code>pstree&lt;/code> 命令来显示整个进程树。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>fork() 通常作为一个函数被调用。这个函数会有两次返回，将子进程的 PID 返回给父进程，0 返回给子进程。实际上，子进程总可以查询自己的 PPID 来知道自己的父进程是谁，这样，一对父进程和子进程就可以随时查询对方。&lt;/li>
&lt;li>通常在调用 fork 函数之后，程序会设计一个 if 选择结构。当 PID 等于 0 时，说明该进程为子进程，那么让它执行某些指令,比如说使用 exec 库函数(library function)读取另一个程序文件，并在当前的进程空间执行 (这实际上是我们使用 fork 的一大目的: 为某一程序创建进程)；而当 PID 为一个正整数时，说明为父进程，则执行另外一些指令。由此，就可以在子进程建立之后，让它执行与父进程不同的功能。&lt;/li>
&lt;/ul>
&lt;h3 id="子进程的-termination终结">子进程的 termination(终结)&lt;/h3>
&lt;p>当子进程终结时，它会通知父进程，并清空自己所占据的内存，并在内核里留下自己的退出信息(exit code，如果顺利运行，为 0；如果有错误或异常状况，为&amp;gt;0 的整数)。在这个信息里，会解释该进程为什么退出。父进程在得知子进程终结时，有责任对该子进程使用 wait 系统调用。这个 wait 函数能从内核中取出子进程的退出信息，并清空该信息在内核中所占据的空间。但是，如果父进程早于子进程终结，子进程就会成为一个孤儿(orphand)进程。孤儿进程会被过继给 init 进程，init 进程也就成了该进程的父进程。init 进程负责该子进程终结时调用 wait 函数。&lt;/p>
&lt;p>当然，一个糟糕的程序也完全可能造成子进程的退出信息滞留在内核中的状况（父进程不对子进程调用 wait 函数），这样的情况下，子进程成为僵尸(zombie)进程。当大量僵尸进程积累时，内存空间会被挤占。&lt;/p>
&lt;h2 id="thread线程">Thread(线程)&lt;/h2>
&lt;p>一个线程是一个单独的进程生成的一个执行单元。它与其他的线程并行地运行在同一个进程中。各个线程可以共享进程的资源，例如内存、地址空间、打开的文件等等。它们能访问相同的程序数据集。线程也被叫作轻量级的进程（Light Weight Process，LWP）。因为它们共享资源，所以每个线程不应该在同一时间改变它们共享的资源。互斥的实现、锁、序列化等是用户程序的责任。&lt;/p>
&lt;p>从性能的角度来说，创建线程的开销比创建进程少，因数创建一个线程时不需要复制资源。另一方面，进程和线程拥在调度算法上有相似的特性。&lt;strong>内核以相似的方式处理它们&lt;/strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507380-b6ae3b1e-b47c-454c-b3c7-9942dde4f480.jpeg" alt="">
所以，一个进程创建的线程，也是可以运行在多个 CPU 上的。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616645843002-c07df4a7-3d7a-4969-8203-4bc20169721a.png" alt="image.png">
在现在的 Linux 实现中，线程支持 UNIX 的可移植操作系统接口（POSIX）标准库。在 Linux 操作系统中有几种可用的线程实现。以下是广泛使用的线程库：&lt;/p>
&lt;p>Linux Threads 自从 Linux 内核 2.0 起就已经被作为默认的线程实现。Linux Threads 的一些实现并不符合 POSIX 标准。Native POSIX Thread Library（NPTL）正在取代 Linux Threads。Linux Threads 在将来的 Linux 企业发行版中将不被支持。&lt;/p>
&lt;p>Native POSIX Thread Libary（NPTL）&lt;/p>
&lt;p>NPTL 最初是由红帽公司开发的。NPTL 与 POSIX 更加兼容。通过 Linux 内核 2.6 的高级特性，例如，新的 clone()系统调用、信号处理的实现等等，它具有比 LinuxThreads 更高的性能和伸缩性。&lt;/p>
&lt;p>NPTL 与 LinuxThreads 有一些不兼容。一个依赖于 LinuxThreads 的应用可能不能在 NPTL 实现中工作。&lt;/p>
&lt;p>Next Generation POSIX Thread（NGPT）&lt;/p>
&lt;p>NGPT 是一个 IBM 开发的 POSIX 线程库。现在处于维护阶段并且在未来也没有开发计划。&lt;/p>
&lt;p>使用 LD_ASSUME_KERNEL 环境变量，你可以选择在应用中使用哪一个线程库。&lt;/p>
&lt;h2 id="linux-内核代码中的-process">Linux 内核代码中的 Process&lt;/h2>
&lt;p>在 Linux 中，&lt;strong>Process(进程) 属于&lt;/strong> &lt;strong>Task(任务)&lt;/strong> 的一种类型，都被 task_struct 结构管理，该结构同时被叫作进程描述。一个进程描述包含一个运行进程所有的必要信息，例如进程标识、进程属性和构建进程的资源。如果你了解该进程构造，你就能理解对于进程的运行和性能来说，什么是重要的。&lt;/p>
&lt;p>v5.14 代码：&lt;a href="https://github.com/torvalds/linux/blob/v5.14/include/linux/sched.h#L661">include/linux/sched.h&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">struct&lt;/span> task_struct {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#ifdef CONFIG_THREAD_INFO_IN_TASK
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">/*
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> * For reasons of header soup (see current_thread_info()), this
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> * must be the first element of task_struct.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> thread_info thread_info;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#endif
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 进程状态
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> __state;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 进程唯一标识符
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">pid_t&lt;/span> pid;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pid_t&lt;/span> tgid;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 进程名称，上限 16 字符
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">char&lt;/span> comm[TASK_COMM_LEN];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 打开的文件
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> files_struct &lt;span style="color:#f92672">*&lt;/span>files;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>下图展示了进程结构相关的进程信息概述。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507336-aaeec645-b9df-41c3-99ab-6bf39aed4f42.jpeg" alt="">
其实从这里能看出来，从某种角度来看，**对于内核来说并没有线程这个概念。Linux 把所有的线程都当做进程来实现，内核也没有特别的调度算法来处理线程。**线程仅仅被视为一个与其他进程共享某些资源的进程，和进程一样，每个线程也都是有自己的 &lt;code>task_struct&lt;/code>，所以在内核中，线程看起来就是一个普通的进程。线程也被称作轻量级进程，一个进程可以有多个线程，线程拥有自己独立的栈，切换也由操作系统调度。在 Linux 上可以通过 &lt;code>pthread_create()&lt;/code> 方法或者 &lt;code>clone()&lt;/code> 系统调用创建；&lt;/p>
&lt;h1 id="进程优先级和-nice-值">进程优先级和 nice 值&lt;/h1>
&lt;p>进程优先级是一个数值，它通过动态的优先级和静态的优先级来决定进程被 CPU 处理的顺序。一个拥有更高进程优先级的进程拥有更大的机率得到处理器的处理。&lt;/p>
&lt;p>内核根据进程的行为和特性使用试探算法，动态地调整调高或调低动态优先级。一个用户进程可以通过使用进程的 nice 值间接改变静态优先级。一个拥有更高静态优先级的进程将会拥有更长的时间片（进程能在处理上运行多长时间）。&lt;/p>
&lt;p>Linux 支持从 19（最低优先级）到-20（最高优先级）的 nice 值。默认值为 0。把程序的 nice 值修改为负数（使进程的优先级更高），需要以 root 身份登陆或使用 su 命令以 root 身份执行。&lt;/p>
&lt;h1 id="上下文切换">上下文切换&lt;/h1>
&lt;p>在进程运行过程中，进程的运行信息被保存于处理器的寄存器和它的缓存中。正在执行的进程加载到寄存器中的数据集被称为上下文。为了切换进程，运行中进程的上下文将会被保存，接下来的运行进程的上下文将被被恢复到寄存器中。进程描述和内核模式堆栈的区域将会用来保存上下文。这个切换被称为上下文切换。过多的上下文切换是不受欢迎的，因为处理器每次都必须清空刷新寄存器和缓存，为新的进程制造空间。它可能会引起性能问题。&lt;/p>
&lt;p>下图说明了上下文切换如何工作。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507475-6f5a9385-f033-4c00-8344-2953197b973c.jpeg" alt="">&lt;/p>
&lt;h1 id="中断处理">中断处理&lt;/h1>
&lt;p>中断处理是优先级最高的任务之一。中断通常由 I/O 设备产生，例如网络接口卡、键盘、磁盘控制器、串行适配器等等。中断处理器通过一个事件通知内核（例如，键盘输入、以太网帧到达等等）。它让内核中断进程的执行，并尽可能快地执行中断处理，因为一些设备需要快速的响应。它是系统稳定的关键。当一个中断信号到达内核，内核必须切换当前的进程到一个新的中断处理进程。这意味着中断引起了上下文切换，因此大量的中断将会引起性能的下降。&lt;/p>
&lt;p>在 Linux 的实现中，有两种类型的中断。硬中断是由请求响应的设备发出的（磁盘 I/O 中断、网络适配器中断、键盘中断、鼠标中断）。软中断被用于处理可以延迟的任务（TCP/IP 操作，SCSI 协议操作等等）。你可以在 &lt;code>/proc/interrupts&lt;/code> 文件中查看硬中断的相关信息。&lt;/p>
&lt;p>在多处理器的环境中，中断被每一个处理器处理。绑定中断到单个的物理处理中能提高系统的性能。更多的细节，请参阅 4.4.2，“CPU 的中断处理亲和力”。&lt;/p>
&lt;h1 id="进程的状态">进程的状态&lt;/h1>
&lt;p>每一个进程拥有自己的状态，状态表示了进程当前在发生什么。LINUX 2.6 以后的内核中，在进程的执行期间进程的状态会发生改变，进程一般存在 7 种基础状态：D-不可中断睡眠、R-可执行、S-可中断睡眠、T-暂停态、t-跟踪态、X-死亡态、Z-僵尸态，这几种状态在 ps 命令的 man 手册中有对应解释。&lt;/p>
&lt;ul>
&lt;li>**D **＃不间断的睡眠（通常是 IO）&lt;/li>
&lt;li>&lt;strong>R&lt;/strong> ＃正在运行或可运行（在运行队列上）&lt;/li>
&lt;li>&lt;strong>S&lt;/strong> ＃可中断的睡眠（等待事件完成）&lt;/li>
&lt;li>&lt;strong>T&lt;/strong> ＃被作业控制信号停止&lt;/li>
&lt;li>**t **＃在跟踪过程中被调试器停止&lt;/li>
&lt;li>&lt;strong>X&lt;/strong> ＃已死（永远都不会出现）&lt;/li>
&lt;li>&lt;strong>Z&lt;/strong> ＃已终止运行（“僵尸”）的进程，已终止但未由其父进程获得&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507456-ca89ed8d-d8a1-4cd6-96ab-c78372840f4a.jpeg" alt="">&lt;/p>
&lt;h2 id="d-task_uninterruptible不可中断睡眠态">D (TASK_UNINTERRUPTIBLE)，不可中断睡眠态。&lt;/h2>
&lt;p>顾名思义，位于这种状态的进程处于睡眠中，并且不允许被其他进程或中断(异步信号)打断。因此这种状态的进程，是无法使用 kill -9 杀死的(kill 也是一种信号)，除非重启系统(没错，就是这么头硬)。不过这种状态一般由 I/O 等待(比如磁盘 I/O、网络 I/O、外设 I/O 等)引起，出现时间非常短暂，大多很难被 PS 或者 TOP 命令捕获(除非 I/O HANG 死)。SLEEP 态进程不会占用任何 CPU 资源。&lt;/p>
&lt;h2 id="r-task_running可执行态">R (TASK_RUNNING)，可执行态。&lt;/h2>
&lt;p>这种状态的进程都位于 CPU 的可执行队列中，正在运行或者正在等待运行，即不是在上班就是在上班的路上。&lt;/p>
&lt;p>在此状态下，表示进程正在 CPU 中运行或在队列中等待运行（运行队列）。&lt;/p>
&lt;h2 id="s-task_interruptible可中断睡眠态">S (TASK_INTERRUPTIBLE)，可中断睡眠态。&lt;/h2>
&lt;p>不同于 D，这种状态的进程虽然也处于睡眠中，但是是允许被中断的。这种进程一般在等待某事件的发生（比如 socket 连接、信号量等），而被挂起。一旦这些时间完成，进程将被唤醒转为 R 态。如果不在高负载时期，系统中大部分进程都处于 S 态。SLEEP 态进程不会占用任何 CPU 资源。&lt;/p>
&lt;p>在此状态下，进程被暂停并等待一个某些条件状态的到达。如果一个进程处于 TASK_INTERRUPTIBLE 状态并接收到一个停止的信号，进程的状态将会被改变并中断操作。一个典型的 TASK_INTERRUPTIBLE 状态的进程的例子是一个进程等待键盘中断。&lt;/p>
&lt;h2 id="t--t-task_stopped--task_traced暂停-or-跟踪态">T &amp;amp; t (TASK_STOPPED &amp;amp; TASK_TRACED)，暂停 or 跟踪态。&lt;/h2>
&lt;p>这种两种状态的进程都处于运行停止的状态。不同之处是暂停态一般由于收到 SIGSTOP、SIGTSTP、SIGTTIN、SIGTTOUT 四种信号被停止，而跟踪态是由于进程被另一个进程跟踪引起(比如 gdb 断点）。暂停态进程会释放所有占用资源。&lt;/p>
&lt;p>TASK_STOPPED 在此状态下的进程被某些信号（如 SIGINT，SIGSTOP）暂停。进程正在等待通过一个信号恢复运行，例如 SIGCONT。&lt;/p>
&lt;h2 id="z-exit_zombietask_zombie-僵尸态">Z (EXIT_ZOMBIE/TASK_ZOMBIE), 僵尸态。&lt;/h2>
&lt;p>这种状态的进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。僵尸态进程会释放除进程入口之外的所有资源。&lt;/p>
&lt;p>当一个进程调用 exit()系统调用退出后，它的父进程应该知道该进程的终止。处于 TASK_ZOMBIE 状态的进程会等待其父进程通知其释放所有的数据结构。&lt;/p>
&lt;p>当一个进程接收到一个信号而终止，它在结束自己之前，通常需要一些时间来结束所有的任务（例如关闭打开的文件）。在这个通常非常短暂的时间内，该进程就是一个僵尸进程。&lt;/p>
&lt;p>进程已经完成所有的关闭任务后，它会向父进程报告其即将终止。有些时候，一个僵尸进程不能把自己终止，这将会引导它的状态显示为 z（zombie）。&lt;/p>
&lt;p>使用 kill 命令来关闭这样的一个进程是不可能的，因为该进程已经被认为已经死掉了。如果你不能清除僵尸进程，你可以结束其父进程，然后僵尸进程也随之消失。但是，如果父进程为 init 进程，你不能结束它。init 进程是一个非常重要的进程，因此可能需要重启系统来清除僵尸进程。&lt;/p>
&lt;h2 id="x-exit_dead-死亡态">X (EXIT_DEAD), 死亡态。&lt;/h2>
&lt;p>进程的真正结束态，这种状态一般在正常系统中捕获不到。&lt;/p>
&lt;h1 id="进程内存段">进程内存段&lt;/h1>
&lt;p>进程使用其自身的内存区域来执行工作。工作的变化根据情况和进程的使用而决定。进程可以拥有不同的工作量特性和不同的数据大小需求。进程必须处理各种数据大小。为了满足需求，Linux 内核为每个进程使用动态申请内存的机制。进程内存分配的数据结构如图 1-7 所示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507458-2bbc9553-910c-4d66-9ad1-8f45893277da.jpeg" alt="">&lt;/p>
&lt;p>图 1-7 进程地址空间&lt;/p>
&lt;p>进程内存区由以下几部分组成：&lt;/p>
&lt;p>Text 段&lt;/p>
&lt;p>该区域用于存储运行代码。&lt;/p>
&lt;p>Data 段&lt;/p>
&lt;p>数据段包括三个区域。&lt;/p>
&lt;p>– Data：该区域存储已被初始化的数据，如静态变量。&lt;/p>
&lt;p>– BSS：该区域存储初始化为 0 的数据。数据被初始化为 0。&lt;/p>
&lt;p>– Heap：该区域用于根据需求使用 malloc()动态申请的内存。堆向高地址方向增长。&lt;/p>
&lt;p>Stack 段&lt;/p>
&lt;p>该区域用于存储局部变量、函数参数和返回函数的地址。栈向低地址方向增长。&lt;/p>
&lt;p>用户进程的地址空间内存分布可以使用 pmap 命令来查看。你可以使用 ps 命令来查看内存段的大小。可以参阅 2.3.10 的“pmap”，“ps 和 pstree”。&lt;/p>
&lt;h1 id="进程的-exit-code退出码">进程的 exit code(退出码)&lt;/h1>
&lt;p>在 Linux 系统中，程序可以在执行终止后传递值给其父进程，这个值被称为 &lt;strong>exit code(退出码)&lt;/strong> 或 **exit status(退出状态)**或 &lt;strong>reture status(返回码)&lt;/strong>。在 POSIX 系统中，惯例做法是当程序成功执行时 **exit code 为 0 **，当程序执行失败时 **exit code 非 0 **。&lt;/p>
&lt;p>传递状态码为何重要？如果你在命令行脚本上下文中查看状态码，答案显而易见。任何有用的脚本，它将不可避免地要么被其他脚本所使用，要么被 bash 单行脚本包裹所使用。特别是脚本被用来与自动化工具 SaltStack 或者监测工具 Nagios 配合使用。这些工具会执行脚本并检查它的状态，来确定脚本是否执行成功。&lt;/p>
&lt;p>其中最重要的原因是，即使你不定义状态码，它仍然存在于你的脚本中。如果你不定义恰当的退出码，执行失败的脚本可能会返回成功的状态，这样会导致问题，问题大小取决于你的脚本做了什么。&lt;/p>
&lt;p>Linux 提供了一个专门的变量$?来保存上个已执行命令的退出状态码。&lt;/p>
&lt;p>对于需要进行检查的命令，必须在其运行完毕后立刻查看或使用$?变量，它的值会变成由 shell 所执行的最后一条命令的退出状态码。&lt;/p>
&lt;p>一个成功结束的命令的退出状态码是 0，如果一个命令结束时有错误，退出状态码就是一个正数值（1-255）。&lt;/p>
&lt;p>Linux 上执行 exit 可使 shell 以指定的状态值退出。若不设置状态值参数，则 shell 以预设值退出。状态值 0 代表执行成功，其他值代表执行失败。exit 也可用在 script，离开正在执行的 script，回到 shell。&lt;/p>
&lt;p>Linux 错误退出状态码没有什么标准可循，但有一些可用的参考。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ld23ik/1616167507500-9f1aab01-171b-4ece-a6fa-9f576852a403.webp" alt="">&lt;/p>
&lt;p>关于具体的服务，相应的退出码，由开发者代码决定。&lt;/p>
&lt;p>&lt;strong>Linux 进程退出码&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://jin-yang.github.io/post/linux-process-exit-code-introduce.html">https://jin-yang.github.io/post/linux-process-exit-code-introduce.html&lt;/a>&lt;/p>
&lt;p>&lt;strong>Linux 退出状态码及 exit 命令&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://www.cnblogs.com/01-single/p/7206664.html">https://www.cnblogs.com/01-single/p/7206664.html&lt;/a>&lt;/p>
&lt;p>&lt;strong>理解 Exit Code 并学会如何在 Bash 脚本中使用&lt;/strong>&lt;/p>
&lt;p>&lt;a href="http://blog.jayxhj.com/2016/02/understanding-exit-codes-and-how-to-use-them-in-bash-scripts">http://blog.jayxhj.com/2016/02/understanding-exit-codes-and-how-to-use-them-in-bash-scripts&lt;/a>&lt;/p>
&lt;p>&lt;strong>Appendix E. Exit Codes With Special Meanings&lt;/strong>&lt;/p>
&lt;p>&lt;a href="http://www.tldp.org/LDP/abs/html/exitcodes.html">http://www.tldp.org/LDP/abs/html/exitcodes.html&lt;/a>&lt;/p>
&lt;p>&lt;strong>What is the authoritative list of Docker Run exit codes?&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://stackoverflow.com/questions/31297616/what-is-the-authoritative-list-of-docker-run-exit-codes">https://stackoverflow.com/questions/31297616/what-is-the-authoritative-list-of-docker-run-exit-codes&lt;/a>&lt;/p>
&lt;p>&lt;strong>Identifying Exit Codes and their meanings&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://support.circleci.com/hc/en-us/articles/360002341673-Identifying-Exit-Codes-and-their-meanings">https://support.circleci.com/hc/en-us/articles/360002341673-Identifying-Exit-Codes-and-their-meanings&lt;/a>&lt;/p>
&lt;p>&lt;strong>OpenShift Exit Status Codes&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://access.redhat.com/documentation/en-US/OpenShift_Online/2.0/html/Cartridge_Specification_Guide/Exit_Status_Codes.html">https://access.redhat.com/documentation/en-US/OpenShift_Online/2.0/html/Cartridge_Specification_Guide/Exit_Status_Codes.html&lt;/a>&lt;/p></description></item><item><title>Docs: 8.Network 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/8.network-%E7%AE%A1%E7%90%86/</guid><description/></item><item><title>Docs: BPF</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/bpf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/bpf/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Berkeley_Packet_Filter">Wiki-BPF&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.infradead.org/~mchehab/kernel_docs/bpf/index.html">Kernel 官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md">GitHub 项目,bcc-BPF 特性与 LInux 内核版本对照表&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>《Linux 内核观测技术 BPF》&lt;/p>
&lt;p>&lt;strong>Berkeley Packet Filter(伯克利包过滤器，简称 BPF)&lt;/strong>，是类 Unix 系统上数据链路层的一种原始接口，提供原始链路层封包的收发。在 Kernel 官方文档中，BPF 也称为 **Linux Socket Filtering(LInux 套接字过滤，简称 LSF)。**BPF 有时也只表示 &lt;strong>filtering mechanism(过滤机制)&lt;/strong>，而不是整个接口。&lt;/p>
&lt;p>&lt;strong>注意：不管是后面描述的 eBPF 还是 BPF，这个名字或缩写，其本身所表达的含义，其实已经没有太大的意义了，因为这个项目的发展远远超出了它最初的构想。&lt;/strong>&lt;/p>
&lt;p>在 BPF 之前，如果想做数据包过滤，则必须将所有数据包复制到用户空间中，然后在那里过滤它们，这种方式意味着必须将所有数据包复制到用户空间中，复制数据的开销很大。当然可以通过将过滤逻辑转移到内核中解决开销问题，我们来看 BPF 做了什么工作。&lt;/p>
&lt;p>实际上，BPF 最早称为 &lt;strong>BSD Packet Filter&lt;/strong>，是很早就有的 Unix 内核特性，最早可追溯到 1992 年发表在 USENIX Conference 上的一篇论文&lt;a href="http://www.tcpdump.org/papers/bpf-usenix93.pdf">《BSD 数据包过滤：一种新的用户级包捕获架构》&lt;/a>，这篇文章作者描述了他们如何在 Unix 内核实现网络数据包过滤，这种技术比当时最先进的数据包过滤技术快了 20 倍。这篇文章描述的 BPF 在数据包过滤上引入了两大革新：&lt;/p>
&lt;ul>
&lt;li>一个新的虚拟机设计，可以有效得工作在基于寄存器结构的 CPU 之上。&lt;/li>
&lt;li>应用程序使用缓存只复制与过滤数据包相关的数据，不会复制数据包的宿友信息。这样可以最大程度得减少 BPF 处理的数据。&lt;/li>
&lt;/ul>
&lt;p>随后，得益于如此强大的性能优势，所有 Unix 系统都将 BPF 作为网络包过滤的首选技术，抛弃了消耗更多内存和性能更差的原有技术实现。后来由于 BPF 的理念逐渐成为主流，为各大操作系统所接受，这样早期 &amp;ldquo;B&amp;rdquo; 所代表的 BSD 便渐渐淡去，最终演化成了今天我们眼中的 BPF(Berkeley Packet Filter)。&lt;/p>
&lt;p>比如我们熟知的 Tcpdump 程序，其底层就是依赖 BPF 实现的包过滤。我们可以在命令后面增加 ”-d“ 来查看 tcpdump 过滤条件的底层汇编指令。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># tcpdump -d &amp;#39;ip and tcp port 8080&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>000&lt;span style="color:#f92672">)&lt;/span> ldh &lt;span style="color:#f92672">[&lt;/span>12&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>001&lt;span style="color:#f92672">)&lt;/span> jeq &lt;span style="color:#75715e">#0x800 jt 2 jf 12&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>002&lt;span style="color:#f92672">)&lt;/span> ldb &lt;span style="color:#f92672">[&lt;/span>23&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>003&lt;span style="color:#f92672">)&lt;/span> jeq &lt;span style="color:#75715e">#0x6 jt 4 jf 12&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>004&lt;span style="color:#f92672">)&lt;/span> ldh &lt;span style="color:#f92672">[&lt;/span>20&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>005&lt;span style="color:#f92672">)&lt;/span> jset &lt;span style="color:#75715e">#0x1fff jt 12 jf 6&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>006&lt;span style="color:#f92672">)&lt;/span> ldxb 4*&lt;span style="color:#f92672">([&lt;/span>14&lt;span style="color:#f92672">]&lt;/span>&amp;amp;0xf&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>007&lt;span style="color:#f92672">)&lt;/span> ldh &lt;span style="color:#f92672">[&lt;/span>x + 14&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>008&lt;span style="color:#f92672">)&lt;/span> jeq &lt;span style="color:#75715e">#0x1f90 jt 11 jf 9&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>009&lt;span style="color:#f92672">)&lt;/span> ldh &lt;span style="color:#f92672">[&lt;/span>x + 16&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>010&lt;span style="color:#f92672">)&lt;/span> jeq &lt;span style="color:#75715e">#0x1f90 jt 11 jf 12&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>011&lt;span style="color:#f92672">)&lt;/span> ret &lt;span style="color:#75715e">#262144&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>012&lt;span style="color:#f92672">)&lt;/span> ret &lt;span style="color:#75715e">#0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>-dd 可以打印字节码&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># tcpdump -dd &amp;#39;ip and tcp port 8080&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x28, 0, 0, 0x0000000c &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x15, 0, 10, 0x00000800 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x30, 0, 0, 0x00000017 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x15, 0, 8, 0x00000006 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x28, 0, 0, 0x00000014 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x45, 6, 0, 0x00001fff &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0xb1, 0, 0, 0x0000000e &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x48, 0, 0, 0x0000000e &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x15, 2, 0, 0x00001f90 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x48, 0, 0, 0x00000010 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x15, 0, 1, 0x00001f90 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x6, 0, 0, 0x00040000 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">{&lt;/span> 0x6, 0, 0, 0x00000000 &lt;span style="color:#f92672">}&lt;/span>,
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="bpf-的进化">BPF 的进化&lt;/h2>
&lt;p>得益于 BPF 在包过滤上的良好表现，Alexei Starovoitov 对 BPF 进行彻底的改造，并增加了新的功能，改善了它的性能，这个新版本被命名为 &lt;strong>extended BPF(扩展的 BPF，简称 eBPF)&lt;/strong>，新版本的 BPF 全面兼容并扩充了原有 BPF 的功能。因此，将传统的 BPF 重命名为 &lt;strong>classical BPF(传统的 BPF，简称 cBPF)&lt;/strong>，相对应的，新版本的 BPF 则命名为 eBPF 或直接称为 BPF(&lt;strong>所以，我们现在所说的 BPF，大部分情况下就是指 eBPF&lt;/strong>)。Linux Kernel 3.18 版本开始实现对 eBPF 的支持。&lt;/p>
&lt;h1 id="ebpf-概述">eBPF 概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ebpf.io/">eBPF 官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://kerneltravel.net/categories/ebpf/">某网站系列文章&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**extended Berkeley Packet Filter(扩展的 BPF，简称 eBPF) **起源于 BPF，是对 BPF 的扩展。eBPF 针对现代硬件进行了优化和全新的设计，使其生成的指令集比 cBPF 解释器生成的机器码更快。这个扩展版本还将 cBPF VM 中的寄存器数量从两个 32 位寄存器增加到 10 个 64 位寄存器。寄存器数量和寄存器宽度的增加为编写更复杂的程序提供了可能性，开发人员可以自由的使用函数参数交换更多的信息。这些改进使得 eBPF 比原来的 cBPF 快四倍。这些改进，主要还是对网络过滤器内部处理的 eBPF 指令集进行优化，仍然被限制在内核空间中，只有少数用户空间中的程序可以编写 BPF 过滤器供内核处理，比如 Tcpdump 和 Seccomp。&lt;/p>
&lt;p>除了上述的优化之外，eBPF 最让人兴奋的改进，是其向用户空间的开放。开发者可以在用户空间，编写 eBPF 程序，并将其加在到内核空间执行。虽然 eBPF 程序看起来更像内核模块，但与内核模块不同的是，eBPF 程序不需要开发者重新编译内核，而且保证了在内核不崩溃的情况下完成加载操作，着重强调了安全性和稳定性。BPF 代码的主要贡献单位主要包括 Cilium、Facebook、Red Hat 以及 Netronome 等。&lt;/p>
&lt;p>Linux Kernel 一直是实现 可观察性、网络、安全性 的理想场所。不幸的是，想要自定义这些实现通常是不切实际的，因为它需要更改内核源代码或加载内核模块，并导致彼此堆叠的抽象层。而 eBPF 的出现，让这一切成为可能，&lt;strong>eBPF 可以在 Linux 内核中运行沙盒程序，而无需更改内核源代码或加载内核模块&lt;/strong>。通过&lt;strong>使 Linux Kernel 可编程&lt;/strong>，基础架构软件可以利用现有的层，从而使它们更加智能和功能丰富，而无需继续为系统增加额外的复杂性层。&lt;/p>
&lt;p>也正由于此，eBPF 不再局限于网络的过滤，而且 eBPF 就相当于内核本身的代码，想象空间无限，并且热加载到内核，换句话说，一旦加载到内核，内核的行为就变了。所以，eBPF 带动了 安全性、应用程序配置/跟踪、性能故障排除 等等领域的新一代工具的开发，这些工具不再依赖现有的内核功能，而是在不影响执行效率或安全性的情况下主动重新编程运行时行为：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Networking(网络)&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Observability(可观测性)&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Monitoring(监控)&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Tracing(跟踪)&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Profiling(分析)&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Security(安全)&lt;/strong>&lt;/li>
&lt;li>&lt;strong>等等，随着发展，eBPF 还可以实现更多!&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>可以这么说，BPF 的种种能力，实现了 &lt;strong>Software Define Kernel(软件定义内核)&lt;/strong>。&lt;/p>
&lt;h2 id="ebpf-与-内核模块-的对比">eBPF 与 内核模块 的对比&lt;/h2>
&lt;p>在 Linux 观测方面，eBPF 总是会拿来与 kernel 模块方式进行对比，eBPF 在安全性、入门门槛上比内核模块都有优势，这两点在观测场景下对于用户来讲尤其重要。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>维度&lt;/th>
&lt;th>Linux 内核模块&lt;/th>
&lt;th>eBPF&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kprobes/tracepoints&lt;/td>
&lt;td>支持&lt;/td>
&lt;td>支持&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>安全性&lt;/td>
&lt;td>可能引入安全漏洞或导致内核 Panic&lt;/td>
&lt;td>通过验证器进行检查，可以保障内核安全&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>内核函数&lt;/td>
&lt;td>可以调用内核函数&lt;/td>
&lt;td>只能通过 BPF Helper 函数调用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>编译性&lt;/td>
&lt;td>需要编译内核&lt;/td>
&lt;td>不需要编译内核，引入头文件即可&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>运行&lt;/td>
&lt;td>基于相同内核运行&lt;/td>
&lt;td>基于稳定 ABI 的 BPF 程序可以编译一次，各处运行&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>与应用程序交互&lt;/td>
&lt;td>打印日志或文件&lt;/td>
&lt;td>通过 perf_event 或 map 结构&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>数据结构丰富性&lt;/td>
&lt;td>一般&lt;/td>
&lt;td>丰富&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>入门门槛&lt;/td>
&lt;td>高&lt;/td>
&lt;td>低&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>升级&lt;/td>
&lt;td>需要卸载和加载，可能导致处理流程中断&lt;/td>
&lt;td>原子替换升级，不会造成处理流程中断&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>内核内置&lt;/td>
&lt;td>视情况而定&lt;/td>
&lt;td>内核内置支持&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="ebpf-发展史">eBPF 发展史&lt;/h1>
&lt;p>eBPF 是如何诞生的呢？我最初开始讲起。这里“最初”我指的是 2013 年之前。&lt;/p>
&lt;h2 id="2013">2013&lt;/h2>
&lt;h3 id="传统的流量控制工具和系统">传统的流量控制工具和系统&lt;/h3>
&lt;p>回顾一下当时的 “SDN” 蓝图。&lt;/p>
&lt;ol>
&lt;li>当时有 OpenvSwitch（OVS）、&lt;code>tc&lt;/code>（Traffic control），以及内核中的 Netfilter 子系 统（包括 &lt;code>iptables&lt;/code>、&lt;code>ipvs&lt;/code>、&lt;code>nftalbes&lt;/code> 工具），可以用这些工具对 datapath 进行“ 编程”：。&lt;/li>
&lt;li>BPF 当时用于 &lt;code>tcpdump&lt;/code>，&lt;strong>在内核中尽量前面的位置抓包&lt;/strong>，它不会 crash 内核；此 外，它还用于 seccomp，&lt;strong>对系统调用进行过滤&lt;/strong>（system call filtering），但当时 使用的非常受限，远不是今天我们已经在用的样子。&lt;/li>
&lt;li>此外就是前面提到的 feature creeping 问题，以及 &lt;strong>tc 和 netfilter 的代码重复问题，因为这两个子系统是竞争关系&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>OVS 当时被认为是内核中最先进的数据平面&lt;/strong>，但它最大的问题是：与内核中其他网 络模块的集成不好【译者注 1】。此外，很多核心的内核开发者也比较抵触 OVS，觉得它很怪。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>【译者注 1】
例如，OVS 的 internal port、patch port 用 tcpdump 都是 &lt;a href="http://arthurchiao.art/blog/ovs-deep-dive-4-patch-port/">抓不到包的&lt;/a>，排障非常不方便。&lt;/p>
&lt;/blockquote>
&lt;h3 id="ebpf-与-传统流量控制-的区别">eBPF 与 传统流量控制 的区别&lt;/h3>
&lt;p>对比 eBPF 和这些已经存在很多年的工具：&lt;/p>
&lt;ol>
&lt;li>tc、OVS、netfilter 可以对 datapath 进行“编程”：但前提是 datapath 知道你想做什 么（but only if the datapath knows what you want to do）。
&lt;ul>
&lt;li>只能利用这些工具或模块提供的既有功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>eBPF 能够让你&lt;strong>创建新的 datapath&lt;/strong>（eBPF lets you create the datapath instead）。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;ul>
&lt;li>eBPF 就是内核本身的代码，想象空间无限，并且热加载到内核；换句话说，一旦加 载到内核，内核的行为就变了。&lt;/li>
&lt;li>在 eBPF 之前，改变内核行为这件事情，只能通过修改内核再重新编译，或者开发内 核模块才能实现。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>译者注&lt;/p>
&lt;h3 id="ebpf第一个巨型patch">eBPF：第一个（巨型）patch&lt;/h3>
&lt;ul>
&lt;li>描述 eBPF 的 RFC 引起了广泛讨论，但普遍认为侵入性太强了（改动太大）。&lt;/li>
&lt;li>另外，当时 nftables (inspired by BPF) 正在上升期，它是一个与 eBPF 有点类似的 BPF 解释器，大家不想同时维护两个解释器。&lt;/li>
&lt;/ul>
&lt;p>最终这个 patch 被拒绝了。
被拒的另外一个原因是前面提到的，没有遵循“大改动小提交”原则，全部代码放到了一个 patch。Linus 会疯的。&lt;/p>
&lt;h2 id="2014">2014&lt;/h2>
&lt;h3 id="第一个-ebpf-patch-合并到内核">第一个 eBPF patch 合并到内核&lt;/h3>
&lt;ul>
&lt;li>用一个&lt;strong>扩展（extended）指令集&lt;/strong>逐步、全面替换原来老的 BPF 解释器。&lt;/li>
&lt;li>&lt;strong>自动新老 BPF 转换&lt;/strong>：in-kernel translation。&lt;/li>
&lt;li>后续 patch 将 eBPF 暴露给 UAPI，并添加了 verifier 代码和 JIT 代码。&lt;/li>
&lt;li>更多后续 patch，从核心代码中移除老的 BPF。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ksq56w/1617847999424-2c5e5133-2862-4574-bffb-0776c6b0aa4b.png" alt="">
我们也从那时开始，顺理成章地成为了 eBPF 的 maintainer。&lt;/p>
&lt;h3 id="kubernetes-提交第一个-commit">Kubernetes 提交第一个 commit&lt;/h3>
&lt;p>巧合的是，&lt;strong>对后来影响深远的 Kubernetes，也在这一年提交了第一个 commit&lt;/strong>：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ksq56w/1617847999414-121919ac-67f3-4841-b2a0-c0bd1668c7bd.png" alt="">&lt;/p>
&lt;h2 id="2015">2015&lt;/h2>
&lt;h3 id="ebpf-分成两个方向networking--tracing">eBPF 分成两个方向：networking &amp;amp; tracing&lt;/h3>
&lt;p>到了 2015 年，eBPF 开发分成了两个方向：&lt;/p>
&lt;ul>
&lt;li>networking&lt;/li>
&lt;li>tracing&lt;/li>
&lt;/ul>
&lt;h3 id="ebpf-backend-合并到-llvm-37">eBPF backend 合并到 LLVM 3.7&lt;/h3>
&lt;p>这一年的一个重要里程碑是 eBPF backend 合并到了 upstream LLVM 编译器套件，因此你 现在才能用 clang 编译 eBPF 代码。&lt;/p>
&lt;h3 id="支持将-ebpf-attach-到-kprobes">支持将 eBPF attach 到 kprobes&lt;/h3>
&lt;p>这是 tracing 的第一个使用案例。
Alexei 主要负责 tracing 部分，他添加了一个 patch，支持加载 eBPF 用来做 tracing， 能获取系统的观测数据。&lt;/p>
&lt;h3 id="通过-cls_bpftc-变得完全可编程">通过 cls_bpf，tc 变得完全可编程&lt;/h3>
&lt;p>我主要负责 networking 部分，使 tc 子系统可编程，这样我们就能用 eBPF 来灵活的对 datapath 进行编程，获得一个高性能 datapath。&lt;/p>
&lt;h3 id="为-tc-添加了一个-lockless-ingress--egress-hook-点">为 tc 添加了一个 lockless ingress &amp;amp; egress hook 点&lt;/h3>
&lt;blockquote>
&lt;p>译注：可参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://arthurchiao.art/blog/understanding-tc-da-mode-zh/">深入理解 tc ebpf 的 direct-action (da) 模式（2020）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://arthurchiao.art/blog/advanced-bpf-kernel-features-for-container-age-zh/">为容器时代设计的高级 eBPF 内核特性（FOSDEM, 2021）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h3 id="添加了很多-verifer-和-ebpf-辅助代码helper">添加了很多 verifer 和 eBPF 辅助代码（helper）&lt;/h3>
&lt;p>使用更方便。&lt;/p>
&lt;h3 id="bcc-项目发布">bcc 项目发布&lt;/h3>
&lt;p>作为 tracing frontend for eBPF。&lt;/p>
&lt;h2 id="2016">2016&lt;/h2>
&lt;h3 id="ebpf-添加了一个新-fast-pathxdp">eBPF 添加了一个新 fast path：XDP&lt;/h3>
&lt;ul>
&lt;li>XDP 合并到内核，支持在驱动的 ingress 层 attach BPF 程序。&lt;/li>
&lt;li>nfp 最为第一家网卡及驱动，支持将 eBPF 程序 offload 到 cls_bpf &amp;amp; XDP hook 点。&lt;/li>
&lt;/ul>
&lt;h3 id="cilium-项目发布">Cilium 项目发布&lt;/h3>
&lt;p>Cilium 最开始的目标是 &lt;strong>docker 网络解决方案&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>通过 eBPF 实现高效的 label-based policy、NAT64、tunnel mesh、容器连通性。&lt;/li>
&lt;li>整个 datapath &amp;amp; forwarding 逻辑全用 eBPF 实现，不再需要 Docker 或 OVS 桥接设备。&lt;/li>
&lt;/ul>
&lt;h2 id="2017">2017&lt;/h2>
&lt;h3 id="ebpf-开始大规模应用于生产环境">eBPF 开始大规模应用于生产环境&lt;/h3>
&lt;p>2016 ~ 2017 年，eBPF 开始应用于生产环境：&lt;/p>
&lt;ol>
&lt;li>Netflix on eBPF for tracing: ‘Linux BPF superpowers’&lt;/li>
&lt;li>Facebook 公布了生产环境 XDP+eBPF 使用案例（DDoS &amp;amp; LB）
&lt;ul>
&lt;li>用 XDP/eBPF 重写了原来基于 IPVS 的 L4LB，性能 &lt;code>10x&lt;/code>。&lt;/li>
&lt;li>&lt;strong>eBPF 经受住了严苛的考验&lt;/strong>：从 2017 开始，每个进入 facebook.com 的包，都是经过了 XDP &amp;amp; eBPF 处理的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cloudflare 将 XDP+BPF 集成到了它们的 DDoS mitigation 产品。
&lt;ul>
&lt;li>成功将其组件从基于 Netfilter 迁移到基于 eBPF。&lt;/li>
&lt;li>到 2018 年，它们的 XDP L4LB 完全接管生产环境。&lt;/li>
&lt;li>扩展阅读：&lt;a href="http://arthurchiao.art/blog/cloudflare-arch-and-bpf-zh/">(译) Cloudflare 边缘网络架构：无处不在的 BPF（2019）&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>译者注：基于 XDP/eBPF 的 L4LB 原理都是类似的，简单来说，&lt;/p>
&lt;ol>
&lt;li>通过 BGP 宣告 VIP&lt;/li>
&lt;li>通过 ECMP 做物理链路高可用&lt;/li>
&lt;li>通过 XDP/eBPF 代码做重定向，将请求转发到后端（VIP -&amp;gt; Backend）&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>对此感兴趣可参考入门级介绍：&lt;a href="http://arthurchiao.art/blog/k8s-l4lb/">L4LB for Kubernetes: Theory and Practice with Cilium+BGP+ECMP&lt;/a>&lt;/p>
&lt;h2 id="2017--2018">2017 ~ 2018&lt;/h2>
&lt;h3 id="ebpf-成为内核独立子系统">eBPF 成为内核独立子系统&lt;/h3>
&lt;p>随着 eBPF 社区的发展，feature 和 patch 越来越多，为了管理这些 patch，Alexei、我和 networking 的一位 maintainer David Miller 经过讨论，决定将 eBPF 作为独立的内核子 系统。&lt;/p>
&lt;ul>
&lt;li>eBPF patch 合并到 &lt;code>bpf&lt;/code> &amp;amp; &lt;code>bpf-next&lt;/code> kernel trees on git.kernel.org&lt;/li>
&lt;li>拆分 eBPF 邮件列表：&lt;code>bpf@vger.kernel.org&lt;/code> (archive at: &lt;code>lore.kernel.org/bpf/&lt;/code>)&lt;/li>
&lt;li>eBPF PR 经内核网络部分的 maintainer David S. Miller 提交给 Linus Torvalds&lt;/li>
&lt;/ul>
&lt;h3 id="ktls--ebpf">kTLS &amp;amp; eBPF&lt;/h3>
&lt;blockquote>
&lt;p>kTLS &amp;amp; eBPF for introspection and ability for in-kernel TLS policy enforcement&lt;/p>
&lt;/blockquote>
&lt;p>kTLS 是&lt;strong>将 TLS 处理 offload 到内核&lt;/strong>，例如，将加解密过程从 openssl 下放到内核进 行，以&lt;strong>使得内核具备更强的可观测性&lt;/strong>（gain visibility）。
有了 kTLS，就可以用 eBPF 查看数据和状态，在内核应用安全策略。 &lt;strong>目前 openssl 已经完全原生支持这个功能&lt;/strong>。&lt;/p>
&lt;h3 id="bpftool--libbpf">bpftool &amp;amp; libbpf&lt;/h3>
&lt;p>为了检查内核内 eBPF 的状态（introspection）、查看内核加载了哪些 BPF 程序等， 我们添加了一个新工具 bpftool。现在这个工具已经功能非常强大了。
同样，为了方便用户空间应用使用 eBPF，我们提供了&lt;strong>用户空间 API&lt;/strong>（user space API for applications） &lt;code>libbpf&lt;/code>。这是一个 C 库，接管了所有加载工作，这样用户就不需要 自己处理复杂的加载过程了。&lt;/p>
&lt;h3 id="bpf-to-bpf-function-calls">BPF to BPF function calls&lt;/h3>
&lt;p>增加了一个 BPF 函数调用另一个 BPF 函数的支持，使得 BPF 程序的编写更加灵活。&lt;/p>
&lt;h2 id="2018">2018&lt;/h2>
&lt;h3 id="cilium-10-发布">Cilium 1.0 发布&lt;/h3>
&lt;p>这标志着 &lt;strong>BPF 革命之火燃烧到了 Kubernetes networking &amp;amp; security 领域&lt;/strong>。
Cilium 此时支持的功能：&lt;/p>
&lt;ul>
&lt;li>K8s CNI&lt;/li>
&lt;li>Identity-based L3-L7 policy&lt;/li>
&lt;li>ClusterIP Services&lt;/li>
&lt;/ul>
&lt;h3 id="btfbyte-type-format">BTF（Byte Type Format）&lt;/h3>
&lt;p>内核添加了一个称为 BTF 的组件。这是一种元数据格式，和 DWARF 这样的 debugging data 类似。但 BTF 的 size 要小的多，而更重要的是，有史以来，&lt;strong>内核第一次变得可自 描述了&lt;/strong>（self-descriptive）。什么意思？
想象一下当前正在运行中的内核，它&lt;strong>内置了自己的数据格式&lt;/strong>（its own data format） 和&lt;strong>内部数据结构&lt;/strong>（internal structures），你能用工具来查看这些东西（you can introspect them）。还是不太懂？这么说吧，&lt;strong>BTF 是后来的 “一次编译、到处运行”、 热补丁（live-patching）、BPF global data 处理等等所有这些 BPF 特性的基础&lt;/strong>。
新的特性不断加入，它们都依赖 BTF 提供富元数据（rich metadata）这个基础。&lt;/p>
&lt;blockquote>
&lt;p>更多 BTF 内容，可参考 &lt;a href="http://arthurchiao.art/blog/cilium-bpf-xdp-reference-guide-zh/">(译) Cilium：BPF 和 XDP 参考指南（2019）&lt;/a>
译者注&lt;/p>
&lt;/blockquote>
&lt;h3 id="linux-plumbers-会议开辟-bpfxdp-主题">Linux Plumbers 会议开辟 BPF/XDP 主题&lt;/h3>
&lt;p>这一年，Linux Plumbers 会议第一次开辟了专门讨论 BPF/XDP 的微型分会，我们 一起组织这场会议。其中，Networking Track 一半以上的议题都涉及 BPF 和 XDP 主题，因为这是一个非常振奋人心的特性，越来越多的人用它来解决实际问题。&lt;/p>
&lt;h3 id="新-socket-类型af_xdp">新 socket 类型：AF_XDP&lt;/h3>
&lt;p>内核添加了一个**新 socket 类型 **&lt;code>**AF_XDP**&lt;/code>。它提供的能力是：&lt;strong>在零拷贝（ zero-copy）的前提下将包从网卡驱动送到用户空间&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>回忆前面的内容，数据包到达网卡后，先经过 XDP，然后才为这个包分配内存。 因此在 XDP 层直接将包送到用户态是无需拷贝的。
译者注&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>AF_XDP&lt;/code> 提供的能力与 DPDK 有点类似，不过&lt;/p>
&lt;ul>
&lt;li>DPDK 需要&lt;strong>重写网卡驱动&lt;/strong>，需要额外维护&lt;strong>用户空间的驱动代码&lt;/strong>。&lt;/li>
&lt;li>&lt;code>AF_XDP&lt;/code> 在&lt;strong>复用内核网卡驱动&lt;/strong>的情况下，能达到与 DPDK 一样的性能。&lt;/li>
&lt;/ul>
&lt;p>而且由于&lt;strong>复用了内核基础设施，所有的网络管理工具还都是可以用的&lt;/strong>，因此非常方便， 而 DPDK 这种 bypass 内核的方案导致绝大大部分现有工具都用不了了。
由于所有这些操作都是发生在 XDP 层的，因此它称为 &lt;code>AF_XDP&lt;/code>。插入到这里的 BPF 代码 能直接将包送到 socket。&lt;/p>
&lt;h3 id="bpffilter">bpffilter&lt;/h3>
&lt;p>开始了 bpffilter prototype，作用是通过用户空间驱动（userspace driver），&lt;strong>将 iptables 规则转换成 eBPF 代码&lt;/strong>。
这是将 iptables 转换成 eBPF 的第一次尝试，整个过程对用户都是无感知的，其中的某些 组件现在还在用，用于在其他方面扩展内核的功能。&lt;/p>
&lt;h2 id="2018--2019">2018 ~ 2019&lt;/h2>
&lt;h3 id="bpftrace">bpftrace&lt;/h3>
&lt;p>Brendan 发布了 bpftrace 工具，作为 DTrace 2.0 for Linux。&lt;/p>
&lt;h3 id="bpf-专著bpf-performance-tools">BPF 专著《BPF Performance Tools》&lt;/h3>
&lt;p>Berendan 写了一本 800 多页的 BPF 书。&lt;/p>
&lt;h3 id="cilium-16-发布">Cilium 1.6 发布&lt;/h3>
&lt;p>第一次支持完全干掉基于 iptables 的 kube-proxy，全部功能基于 eBPF。&lt;/p>
&lt;blockquote>
&lt;p>这个版本其实是有问题的，例如 1.6 发布之后我们发现 externalIPs 的实现是有问题 ，社区在后面的版本修复了这个问题。在修复之前，还是得用 kube-proxy： &lt;a href="https://github.com/cilium/cilium/issues/9285">https://github.com/cilium/cilium/issues/9285&lt;/a>
译者注&lt;/p>
&lt;/blockquote>
&lt;h3 id="bpf-live-patching">BPF live-patching&lt;/h3>
&lt;p>添加了一些内核新特性，例如尾调用（tail call），这使得 &lt;strong>eBPF 核心基础 设施第一次实现了热加载&lt;/strong>。这个功能帮我们极大地优化了 datapath。
另一个重要功能是 BPF trampolines，这里就不展开了，感兴趣的可以搜索相关资料，我只 能说这是另一个振奋人心的技术。&lt;/p>
&lt;h3 id="第一次-bpfconf受邀请才能参加的-bpf-内核专家会议">第一次 bpfconf：受邀请才能参加的 BPF 内核专家会议&lt;/h3>
&lt;p>如题，这是 BPF 内核专家交换想法和讨论问题的会议。与 Linux Plumbers 会议互补。&lt;/p>
&lt;h3 id="bpf-backend-合并到-gcc">BPF backend 合并到 GCC&lt;/h3>
&lt;p>前面提到，BPF backend 很早就合并到 LLVM/Clang，现在，它终于合并到 GCC 了。 至此，&lt;strong>GCC 和 LLVM 这两个最主要的编译器套件都支持了 BPF backend&lt;/strong>。
此外，BPF 开始支持有限循环（bounded loops），在此之前，是不支持循环的，以防止程 序无限执行。&lt;/p>
&lt;h2 id="2019--2020">2019 ~ 2020&lt;/h2>
&lt;h3 id="不知疲倦的增长和-ebpf-的第三个方向linux-security-modules">不知疲倦的增长和 eBPF 的第三个方向：Linux security modules&lt;/h3>
&lt;ul>
&lt;li>Google 贡献了 &lt;a href="https://www.kernel.org/doc/html/latest/bpf/bpf_lsm.html">BPF LSM&lt;/a>（安全），部署在了他们的数据中心服务器上。&lt;/li>
&lt;li>BPF verifier 防护 Spectre 漏洞（2018 年轰动世界的 CPU bug）：even verifying safety on speculative program paths。&lt;/li>
&lt;li>&lt;strong>主流云厂商开始通过 SRIOV 支持 XDP&lt;/strong>：AWS (ena driver), Azure (hv_netvsc driver), …&lt;/li>
&lt;li>Cilium 1.8 支持基于 XDP 的 Service 负载均衡和 host network policies。&lt;/li>
&lt;li>Facebook 开发了基于 BPF 的 TCP 拥塞控制模块。&lt;/li>
&lt;li>Microsoft 基于 BPF 重写了将他们的 Windows monitoring 工具。&lt;/li>
&lt;/ul></description></item><item><title>Docs: 微内核</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/%E5%BE%AE%E5%86%85%E6%A0%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/%E5%BE%AE%E5%86%85%E6%A0%B8/</guid><description/></item></channel></rss>