<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 3.Pod 集群最小的工作单元</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/</link><description>Recent content in 3.Pod 集群最小的工作单元 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 3.Pod 集群最小的工作单元</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/">官方文档,概念-工作负载-Pods&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Pod 是 Kubernetes 的&lt;strong>最小工作单元&lt;/strong>，是一个逻辑概念。Kubernetes 真正处理的，还是通过 CRI 在 HostOS 上的 Namespace 和 Cgroups。所谓的 Pod 只是一组共享了某些资源的 Container，这一组 Container 共享同一个 NetworkNamespace 并且可以声明共享同一个 Volume。&lt;/p>
&lt;p>&lt;strong>Infrastructure(基础设施，简称 Infra) 容器&lt;/strong>：为了保证多个 Container 在共享的时候是对等关系(一般情况可以先启动 ContainerA，再启动 ContainerB 并共享 ContainerA 的资源，但是这样 A 与 B 不对等，A 是必须先启动才能启动 B)，需要一个中间 Container，即 &lt;strong>Infra 容器&lt;/strong>，Infra 容器 永远是第一个被创建的 Container，想要共享某些资源的 Container 则通过加入 NetworkNamespce 的方式，与 Infra 容器 关联在一起。效果如图
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iogldt/1616119861463-06b2877d-519d-43a9-a6e4-6fc743d6ee30.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>Infra 类型的 Container 使用一个名为 pause 的镜像，就像它的名字一样，永远处于&amp;quot;暂停&amp;quot;状态&lt;/li>
&lt;li>Kubernetes 为每个 Pod 都附属了 k8s.gcr.ip/pause，这个 Container 只接管 Pod 的网络信息，业务 Container 通过加入网络 Container 的网络来实现网络共享。此容器随着 pod 创建而创建，随着 Pod 删除而删除。该容器是对业务 pod 的命名空间的解析。Note：如果想要更改该容器，则需要在 kubelet 中使用&amp;ndash;pod-infra-container-image 参数进行配置&lt;/li>
&lt;li>与 Infra 关联的 Container 的所有 NetworkNamespace 必然是完全一样的。&lt;/li>
&lt;li>该链接有一种详细的解释&lt;/li>
&lt;li>Note：对于 kubelet 来说，这种容器称为 Sandbox。每次 kubelet 创建 pod 时，首先创建的也是 sandbox(i.e.pause)容器&lt;/li>
&lt;/ol>
&lt;p>一组 Container 共享 Infra 的 NetworkNamespace 意味着：&lt;/p>
&lt;ol>
&lt;li>它们可以直接使用 localhost 进行通信&lt;/li>
&lt;li>它们看到的网络设备跟 Infra 容器中看到的完全一样&lt;/li>
&lt;li>一个 Pod 只能有有一个 IP 地址，就是这个 Pod 的 NetworkNamespace 对应的 IP 地址&lt;/li>
&lt;li>Pod 的生命周期只跟 Infra 容器一致，同与 Infra 关联的所有 Container 无关&lt;/li>
&lt;li>Pod 中的所有 Container 的进出流量都是通过 Infra 容器完成的，所以网络插件不必关心除 Infra 以外的容器的启动与否，只需关注如何配置 Pod(也就是 Infra 容器的 NetworkNamespace)即可&lt;/li>
&lt;/ol>
&lt;p>每个 Pod 包含一个或多个容器。Pod 中的容器会作为一个整体被 Master 调度到一个 Node 上运行。&lt;/p>
&lt;p>如果把 Pod 想象成一台&amp;quot;服务器&amp;quot;，把 Container 想象成运行在这台服务器中的&amp;quot;用户程序&amp;quot;&lt;/p>
&lt;ol>
&lt;li>凡是调度、网络、存储、以及安全相关的字段，基本都是 Pod 级别的，比如：&lt;/li>
&lt;li>配置这台&amp;quot;服务器&amp;quot;的网卡(Pod 的网络)、配置这台“服务器”的磁盘(Pod 的存储，Volume)、配置这台”服务器“的防火墙(Pod 中的安全)、配置这台”服务器“运行在哪个机房(Pod 的调度)&lt;/li>
&lt;li>凡是资源配额、所要使用的 port、探测该进程是否存活或就绪、需要使用&amp;quot;服务器&amp;quot;上的哪块 Volume 等等字段，都是 Container 级别的&lt;/li>
&lt;/ol>
&lt;h2 id="kubernetes-引入-pod-主要基于下面两个目的">Kubernetes 引入 Pod 主要基于下面两个目的&lt;/h2>
&lt;ol>
&lt;li>可管理性。
&lt;ol>
&lt;li>有些 Container 天生就是需要紧密联系，一起工作。Pod 提供了比容器更高层次的抽象，将它们封装到一个部署单元中。Kubernetes 以 Pod 为最小单位进行调度、扩展、共享资源、管理生命周期。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>通信和资源共享。
&lt;ol>
&lt;li>Pod 中的所有 Container 使用同一个网络 namespace，即相同的 IP 地址和 Port 空间。它们可以直接用 localhost 通信。同样的，这些 Container 可以共享存储，当 Kubernetes 挂载 volume 到 Pod，本质上是将 volume 挂载到 Pod 中的每一个 Container。user,mnt,pnt。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>使用户从传统虚拟机环境向容器环境迁移更加平滑，可以把 Pod 想象成 VM，Pod 中的 Container 是 VM 中的进程，甚至可以启动一个 systemd 的 Container&lt;/li>
&lt;li>还可以把 Pod 理解为传统环境中的物理主机&lt;/li>
&lt;/ol>
&lt;h2 id="container-设计模式">Container 设计模式&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iogldt/1616119861478-cf678269-344a-4932-8448-c9eee14a8438.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>sidecar&lt;/strong> #(该英文的解释“跨斗”：一辆小而低的车辆，安装在摩托车旁边，用于载客，就像右图中的样子)，所以该模式就类似于这个，指可以再一个 Pod 中启动一个辅助 Container，来完成一些独立于主进程(主 Container)之外的工作。
&lt;ul>
&lt;li>比如 Container 的日志收集：现在有一个 APP，需要不断把日志文件输出到 Container 的/var/log 目录中。这时，把一个 Pod 里的 Volume 挂载到应用 Container 的/var/log 目录上，然后在 Pod 中同时运行一个 sidecar 的 Container 也声明挂载同一个 Volume 到自己的/var/log 目录上，然后 sidecar 只需要不断得从自己的/var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来即可。&lt;/li>
&lt;li>Istio 项目也是使用 sidecar 模式的 Container 完成微服务治理的原理。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-的类型">Pod 的类型&lt;/h2>
&lt;ul>
&lt;li>动态 Pod：由 k8s 管理，网络组件，监控，等等，这些在 使用 kubeadm 初始化集群后才创建的 Pod 为动态 Pod&lt;/li>
&lt;li>静态 Pod：由 kubelet 直接管理的，在 /etc/kubernetes/manifest/ 目录下的 yaml 文件&lt;/li>
&lt;/ul>
&lt;h1 id="pod-使用方式">Pod 使用方式&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">官方文档，概念-工作敷在-Pod-初始化容器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">官方文档，概念-工作负载-Pod-临时容器&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ul>
&lt;li>运行单一 Container。
&lt;ul>
&lt;li>one-container-per-Pod 是 Kubernetes 最常见的模型，这种情况下，只是将单个 Container 简单封装成 Pod。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>运行多个 Container。
&lt;ul>
&lt;li>这些 Container 联系必须非常紧密，而且需要直接共享资源的应该放到一个 Pod 中(注意：当使用多 Container 的时候，其中一个 Container 要加上 command 的参数，否则其中一个起不来。因为 container 如果不执行某些命令，则启动后会自动结束，详见 docker 说明 1.LXC 与 Docker 入门最佳实践.note 里《Dokcer 的工作模式》章节)&lt;/li>
&lt;li>比如：File Puller 会定期从外部的 Content Manager 中拉取最新的文件，将其存放在共享的 volume 中。Web Server 从 volume 读取文件，响应 Consumer 的请求。这两个容器是紧密协作的，它们一起为 Consumer 提供最新的数据；同时它们也通过 volume 共享数据。所以放到一个 Pod 是合适的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>在 Pod 中，可运行的容器分为三类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ephemeral_container(临时容器)&lt;/strong> # 与 1.23 版本进入 beta，用来调试集群&lt;/li>
&lt;li>&lt;strong>init_container(初始化容器)&lt;/strong> # 在应用容器启动前运行一次就结束的，常用来为容器运行初始化运行环境，比如设置权限等等&lt;/li>
&lt;li>&lt;strong>application_container(应用容器)&lt;/strong> # 真正运行业务的容器。&lt;/li>
&lt;/ul>
&lt;p>这三类容器，可以在 kubelet 代码中找到运行逻辑，详见 [《kubelet 源码解析-PodWorker 模块》](/docs/IT学习笔记/10.云原生/2.3.Kubernetes%20 容器编排系统/Kubernetes%20 开发/源码解析/Kubelet%20 源码/PodWorker%20 模块.md 开发/源码解析/Kubelet 源码/PodWorker 模块.md)&lt;/p>
&lt;h2 id="ephemeral_container临时容器">ephemeral_container(临时容器)&lt;/h2>
&lt;h2 id="init_container初始化容器">init_container(初始化容器)&lt;/h2>
&lt;p>Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。Init 容器在所有容器运行之前执行（run-to-completion），常用来初始化配置。&lt;/p>
&lt;p>如果为一个 Pod 指定了多个 Init 容器，那些容器会按顺序一次运行一个。 每个 Init 容器必须运行成功，下一个才能够运行。 当所有的 Init 容器运行完成时，Kubernetes 初始化 Pod 并像平常一样运行应用容器。&lt;/p>
&lt;p>下面是一个 Init 容器的示例：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">init-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workdir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/usr/share/nginx/html&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># These containers are run during pod initialization&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">initContainers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">install&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">busybox&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">wget&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;-O&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;/work-dir/index.html&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">http://kubernetes.io&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workdir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/work-dir&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dnsPolicy&lt;/span>: &lt;span style="color:#ae81ff">Default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workdir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">emptyDir&lt;/span>: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>因为 Init 容器具有与应用容器分离的单独镜像，使用 init 容器启动相关代码具有如下优势：&lt;/p>
&lt;ol>
&lt;li>它们可以包含并运行实用工具，出于安全考虑，是不建议在应用容器镜像中包含这些实用工具的。&lt;/li>
&lt;li>它们可以包含使用工具和定制化代码来安装，但是不能出现在应用镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。&lt;/li>
&lt;li>应用镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。&lt;/li>
&lt;li>它们使用 Linux Namespace，所以对应用容器具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用容器不能够访问。&lt;/li>
&lt;li>它们在应用容器启动之前运行完成，然而应用容器并行运行，所以 Init 容器提供了一种简单的方式来阻塞或延迟应用容器的启动，直到满足了一组先决条件。&lt;/li>
&lt;/ol>
&lt;p>Init 容器的资源计算，选择一下两者的较大值：&lt;/p>
&lt;ol>
&lt;li>所有 Init 容器中的资源使用的最大值&lt;/li>
&lt;li>Pod 中所有容器资源使用的总和&lt;/li>
&lt;/ol>
&lt;p>Init 容器的重启策略：&lt;/p>
&lt;ol>
&lt;li>如果 Init 容器执行失败，Pod 设置的 restartPolicy 为 Never，则 pod 将处于 fail 状态。否则 Pod 将一直重新执行每一个 Init 容器直到所有的 Init 容器都成功。&lt;/li>
&lt;li>如果 Pod 异常退出，重新拉取 Pod 后，Init 容器也会被重新执行。所以在 Init 容器中执行的任务，需要保证是幂等的。&lt;/li>
&lt;/ol>
&lt;h2 id="container容器--也称为-application_container应用容器">container(容器) # 也称为 application_container(应用容器)&lt;/h2>
&lt;h1 id="pod-名字的命名规范">Pod 名字的命名规范&lt;/h1>
&lt;p>一般情况都不会直接使用 Pod，而是通过 Controller 来创建。通过 Controller 创建一个 POD 的流程为，以及 POD 名字的命名方式每个对象的命名方式是：子对象的名字 = 父对象名字 + 随机字符串或数字。如图所示，Controller 详见：2.0.Controller：控制器.note
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iogldt/1616119861434-f2c4735b-c549-40a4-ab70-67217755ed3f.png" alt="">&lt;/p>
&lt;ol>
&lt;li>用户通过 kubectl 创建 Deployment。&lt;/li>
&lt;li>Deployment 创建 ReplicaSet。&lt;/li>
&lt;li>ReplicaSet 创建 Pod。&lt;/li>
&lt;/ol></description></item><item><title>Docs: CPU资源的调度和管理(CFS)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/cpu%E8%B5%84%E6%BA%90%E7%9A%84%E8%B0%83%E5%BA%A6%E5%92%8C%E7%AE%A1%E7%90%86cfs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/cpu%E8%B5%84%E6%BA%90%E7%9A%84%E8%B0%83%E5%BA%A6%E5%92%8C%E7%AE%A1%E7%90%86cfs/</guid><description>
&lt;h1 id="一前言">一、前言&lt;/h1>
&lt;p>在使用 Kubernetes 的过程中，我们看到过这样一个告警信息：&lt;/p>
&lt;p>[K8S]告警主题: CPUThrottlingHigh&lt;/p>
&lt;p>告警级别: warning&lt;/p>
&lt;p>告警类型: CPUThrottlingHigh&lt;/p>
&lt;p>故障实例:&lt;/p>
&lt;p>告警详情: 27% throttling of CPU in namespace kube-system for container kube-proxy in pod kube-proxy-9pj9j.&lt;/p>
&lt;p>触发时间: 2020-05-08 17:34:17&lt;/p>
&lt;p>这个告警信息说明 kube-proxy 容器被 throttling 了，然而查看该容器的资源使用历史信息，发现该容器以及容器所在的节点的 CPU 资源使用率都不高：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119568683-4d3d60e5-d34f-4e0c-914b-203298c7c642.png" alt="image.png">
告警期间容器所在节点 CPU 使用率&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119576905-621581d5-8e07-49cf-9588-11e12445e976.png" alt="image.png">
告警期间 kube-proxy 的资源使用率&lt;/p>
&lt;p>经过我们的分析，发现该告警实际上是和 Kubernetes 对于 CPU 资源的限制和管控机制有关。Kubernetes 依赖于容器的 runtime 进行 CPU 资源的调度，而容器 runtime 以 Docker 为例，是借助于 cgroup 和 CFS 调度机制进行资源管控。本文基于这个告警案例，首先分析了 CFS 的基本原理，然后对于 Kubernetes 借助 CFS 进行 CPU 资源的调度和管控方法进行了介绍，最后使用一个例子来分析 CFS 的一些调度特性来解释这个告警的 root cause 和解决方案。&lt;/p>
&lt;p>转载自&lt;a href="https://blog.csdn.net/cloudvtech">https://blog.csdn.net/cloudvtech&lt;/a>&lt;/p>
&lt;p>二、CFS 基本原理&lt;/p>
&lt;p>参考：CPU 管理&lt;/p>
&lt;p>2.3 运行和观察&lt;/p>
&lt;p>部署这样一个 yaml POD：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>apiVersion: v1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kind: Pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>metadata:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>name: busybox&lt;/p>
&lt;/li>
&lt;li>
&lt;p>labels:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>app: busybox&lt;/p>
&lt;/li>
&lt;li>
&lt;p>spec:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>containers:&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>image: busybox&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>resources:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>requests:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>memory: &amp;ldquo;64Mi&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cpu: &amp;ldquo;250m&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>limits:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>memory: &amp;ldquo;128Mi&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cpu: &amp;ldquo;500m&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>command:&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&amp;ldquo;/bin/sh&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&amp;ldquo;-c&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&amp;ldquo;while true; do sleep 10; done&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>imagePullPolicy: IfNotPresent&lt;/p>
&lt;/li>
&lt;li>
&lt;p>name: busybox&lt;/p>
&lt;/li>
&lt;li>
&lt;p>restartPolicy: Always&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>可以看到该容器内部的进程对应的 CPU 调度信息变化如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>[root@k8s-node-04 ~]# cat /proc/121133/sched&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sh (121133, #threads: 1)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>-&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.exec_start : 20229360324.308323&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.vruntime : 0.179610&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.sum_exec_runtime : 31.190620&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.nr_migrations : 12&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_switches : 79&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_voluntary_switches : 78&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_involuntary_switches : 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.load.weight : 1024&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prio : 120&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clock-delta : 26&lt;/p>
&lt;/li>
&lt;li>
&lt;p>mm-&amp;gt;numa_scan_seq : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_migrations, 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 1, 1, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;li>&lt;/li>
&lt;li>
&lt;p>[root@k8s-node-04 ~]# cat /proc/121133/sched&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sh (121133, #threads: 1)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>-&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.exec_start : 20229480327.896307&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.vruntime : 0.149504&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.sum_exec_runtime : 33.325310&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.nr_migrations : 17&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_switches : 91&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_voluntary_switches : 90&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_involuntary_switches : 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.load.weight : 1024&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prio : 120&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clock-delta : 31&lt;/p>
&lt;/li>
&lt;li>
&lt;p>mm-&amp;gt;numa_scan_seq : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_migrations, 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 0, 1, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;li>&lt;/li>
&lt;li>
&lt;p>[root@k8s-node-04 ~]# cat /proc/121133/sched&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sh (121133, #threads: 1)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>-&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.exec_start : 20229520328.862396&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.vruntime : 1.531536&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.sum_exec_runtime : 34.053116&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.nr_migrations : 18&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_switches : 95&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_voluntary_switches : 94&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_involuntary_switches : 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.load.weight : 1024&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prio : 120&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clock-delta : 34&lt;/p>
&lt;/li>
&lt;li>
&lt;p>mm-&amp;gt;numa_scan_seq : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_migrations, 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 1, 1, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>其中 sum_exec_runtime 表示实际运行的物理时间。&lt;/p>
&lt;p>转载自&lt;a href="https://blog.csdn.net/cloudvtech">https://blog.csdn.net/cloudvtech&lt;/a>&lt;/p>
&lt;p>三、Kubernetes 借助 CFS 进行 CPU 管理&lt;/p>
&lt;p>3.1 CFS 进行 CPU 资源限流(throtting)的原理&lt;/p>
&lt;p>根据文章《Kubernetes 生产实践系列之三十：Kubernetes 基础技术之集群计算资源管理》的描述，Kubernetes 的资源定义：&lt;/p>
&lt;ol>
&lt;li>resources:&lt;/li>
&lt;li>requests:&lt;/li>
&lt;li>memory: &amp;ldquo;64Mi&amp;rdquo;&lt;/li>
&lt;li>cpu: &amp;ldquo;250m&amp;rdquo;&lt;/li>
&lt;li>limits:&lt;/li>
&lt;li>memory: &amp;ldquo;128Mi&amp;rdquo;&lt;/li>
&lt;li>cpu: &amp;ldquo;500m&amp;rdquo;&lt;/li>
&lt;/ol>
&lt;p>比如里面的 CPU 需求，会被翻译成容器 runtime 的运行时参数，并最终变成 cgroups 和 CFS 的参数配置：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cat cpu.shares&lt;/p>
&lt;/li>
&lt;li>
&lt;p>256&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_quota_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>50000&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_period_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>100000&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这里有一个默认的参数：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cat /proc/sys/kernel/sched_latency_ns&lt;/p>
&lt;/li>
&lt;li>
&lt;p>24000000&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>所以在这个节点上，正常压力下，系统的 CFS 调度周期是 24ms，CFS 重分配周期是 100ms，而该 POD 在一个重分配周期最多占用 50ms 的时间，在有压力的情况下，POD 可以占据的 CPU share 比例是 256。&lt;/p>
&lt;p>下面一个例子可以说明不同资源需求的 POD 容器是如何在 CFS 的调度下占用 CPU 资源的：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119602382-f95c9175-9b27-4d95-9746-2c7725bca2ef.png" alt="image.png">CPU 资源配置和 CFS 调度&lt;/p>
&lt;p>在这个例子中，有如下系统配置情况：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>CFS 调度周期为 10ms，正常负载情况下，进程 ready 队列里面的进程在每 10ms 的间隔内都会保证被执行一次&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CFS 重分配周期为 100ms，用于保证一个进程的 limits 设置会被反映在每 100ms 的重分配周期内可以占用的 CPU 时间数，在多核系统中，limit 最大值可以是 CFS 重分配周期*CPU 核数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>该执行进程队列只有进程 A 和进程 B 两个进程&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进程 A 和 B 定义的 CPU share 占用都一样，所以在系统资源紧张的时候可以保证 A 和 B 进程都可以占用可用 CPU 资源的一半&lt;/p>
&lt;/li>
&lt;li>
&lt;p>定义的 CFS 重分配周期都是 100ms&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进程 A 在 100ms 内最多占用 50ms，进程 B 在 100ms 内最多占用 20ms&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>所以在一个 CFS 重分配周期(相当于 10 个 CFS 调度周期)内，进程队列的执行情况如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在前面的 4 个 CFS 调度周期内，进程 A 和 B 由于 share 值是一样的，所以每个 CFS 调度内(10ms)，进程 A 和 B 都会占用 5ms&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在第 4 个 CFS 调度周期结束的时候，在本 CFS 重分配周期内，进程 B 已经占用了 20ms，在剩下的 8 个 CFS 调度周期即 80ms 内，进程 B 都会被限流，一直到下一个 CFS 重分配周期内，进程 B 才可以继续占用 CPU&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在第 5-7 这 3 个 CFS 调度周期内，由于进程 B 被限流，所以进程 A 可以完全拥有这 3 个 CFS 调度的 CPU 资源，占用 30ms 的执行时间，这样在本 CFS 重分配周期内，进程 A 已经占用了 50ms 的 CPU 时间，在后面剩下的 3 个 CFS 调度周期即后面的 30ms 内，进程 A 也会被限流，一直到下一个 CFS 重分配周期内，进程 A 才可以继续占用 CPU&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>如果进程被限流了，可以在如下的路径看到：&lt;/p>
&lt;p>cat /sys/fs/cgroup/cpu/kubepods/pod5326d6f4-789d-11ea-b093-fa163e23cb69/69336c973f9f414c3f9fdfbd90200b7083b35f4d54ce302a4f5fc330f2889846/cpu.stat&lt;/p>
&lt;p>nr_periods 14001693&lt;/p>
&lt;p>nr_throttled 2160435&lt;/p>
&lt;p>throttled_time 570069950532853&lt;/p>
&lt;p>3.2 本文开头问题的原因分析&lt;/p>
&lt;p>根据 3.1 描述的原理，很容易理解本文开通的告警信息的出现，是由于在某些特定的 CFS 重分配周期内，kube-proxy 的 CPU 占用率超过了给它分配的 limits，而参看 kube-proxy daemonset 的配置，确实它的 limits 配置只有 200ms，这就意味着在默认的 100ms 的 CFS 重调度周期内，它只能占用 20ms，所以在特定繁忙场景会有问题：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119635719-1bd824ee-7535-423f-8316-293a4f5b4daf.png" alt="image.png">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cat cpu.shares&lt;/p>
&lt;/li>
&lt;li>
&lt;p>204&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_period_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>100000&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_quota_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>20000&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>注：这里 cpu.shares 的计算方法如下：200x1024/1000~=204&lt;/p>
&lt;p>而这个问题的解决方案就是将 CPU limits 提高。&lt;/p>
&lt;p>Zalando 公司有一个分享《Optimizing Kubernetes Resource Requests/Limits for Cost-Efficiency and Latency / Henning Jacobs》很好的讲述了 CPU 资源管理的问题，可以参考，这个演讲的 PPT 在这里可以找到。&lt;/p>
&lt;p>更具体问题分析和讨论还可以参考如下文章：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>CPUThrottlingHigh false positives #108&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CFS quotas can lead to unnecessary throttling #67577&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CFS Bandwidth Control&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Overly aggressive CFS&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>其中《Overly aggressive CFS》里面还有几个小实验可以帮助大家更好的认识到 CFS 进行 CPU 资源管控的特点：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119625964-b27b0e72-58fd-441f-81eb-c7d543aa9733.png" alt="image.png">&lt;/p>
&lt;p>转载自&lt;a href="https://blog.csdn.net/cloudvtech">https://blog.csdn.net/cloudvtech&lt;/a>&lt;/p></description></item><item><title>Docs: k8s 创建 Pod 时，背后到底发生了什么</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/k8s-%E5%88%9B%E5%BB%BA-pod-%E6%97%B6%E8%83%8C%E5%90%8E%E5%88%B0%E5%BA%95%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/k8s-%E5%88%9B%E5%BB%BA-pod-%E6%97%B6%E8%83%8C%E5%90%8E%E5%88%B0%E5%BA%95%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/HjoU_RKBQKPCQPEQZ_fBNA">公众号,万字长文：K8S 创建 Pod 时，背后到底发生了什么&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>典型的创建 Pod 的流程为
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zhow5n/1616119512783-67ed1273-0291-4462-8535-1ea845b176f1.png" alt="image.png">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>用户通过 REST API 创建一个 Pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>apiserver 将其写入 etcd&lt;/p>
&lt;/li>
&lt;li>
&lt;p>scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>各组件默认所占用端口号
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zhow5n/1616119523444-d2794850-3f4c-41c8-8f75-e168a8825177.png" alt="image.png">&lt;/p></description></item><item><title>Docs: Pod 的生命周期，Probe(探针)，Hook(钩子)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/pod-%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9Fprobe%E6%8E%A2%E9%92%88hook%E9%92%A9%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/pod-%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9Fprobe%E6%8E%A2%E9%92%88hook%E9%92%A9%E5%AD%90/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">官方文档,概念-工作负载-Pods-Pod 的生命周期&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">官方文档,任务-配置 Pods 与 容器-配置 Liveness、Readiness、Startup Probes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/jPkAj2C0ZNHbaSZRwTOk9g">公众号,YP 小站-怎么使用 Pod 的 liveness 和 readiness 与 startupProbe&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="pod-从开始到结束会有以下几个-phase阶段">Pod 从开始到结束，会有以下几个 phase(阶段)&lt;/h2>
&lt;ol>
&lt;li>Pending：调度尚未完成。Pod 已经在 apiserver 中创建，但还没有调度到 Node 上面&lt;/li>
&lt;li>Running：运行中。od 已经调度到 Node 上面，所有容器都已经创建，并且至少有一个容器还在运行或者正在启动&lt;/li>
&lt;li>Failed：失败。 Pod 调度到 Node 上面后至少有一个容器运行失败（即退出码不为 0 或者被系统终止）&lt;/li>
&lt;li>Succeeded：已经成功。Pod 调度到 Node 上面后成功运行结束，并且不会重启。使用 kubectl 命令看到的就是 Completed&lt;/li>
&lt;li>Unkown：得不到该 Pod 的信息。 状态未知，通常是由于 apiserver 无法与 kubelet 通信导致&lt;/li>
&lt;li>Completed：已完成。主要用于 Job 模式的 Pod，表示该 Job 正常执行结束&lt;/li>
&lt;/ol>
&lt;h1 id="容器生命周期钩子container-lifecycle-hooks">容器生命周期钩子(Container Lifecycle Hooks)&lt;/h1>
&lt;p>Pod 启动的时候，先运行多个 Container 的初始化程序，然后运行 Container 的主程序(主程序中可以在开始 postStart 和结尾 postStop 处执行一些用户自定义“钩子”，这个钩子类似于 awk 命令的 START 和 STOP 功能)。以下是两种钩子的描述。&lt;/p>
&lt;ol>
&lt;li>postStart： 容器创建后立即执行，注意由于是异步执行，它无法保证一定在 ENTRYPOINT 之前运行。如果失败，容器会被杀死，并根据 RestartPolicy 决定是否重启&lt;/li>
&lt;li>preStop：容器终止前执行，常用于资源清理。如果失败，容器同样也会被杀死&lt;/li>
&lt;/ol>
&lt;p>钩子的回调函数支持两种方式：&lt;/p>
&lt;ol>
&lt;li>exec：在容器内执行命令，如果命令的退出状态码是 0 表示执行成功，否则表示失败&lt;/li>
&lt;li>httpGet：向指定 URL 发起 GET 请求，如果返回的 HTTP 状态码在 [200, 400) 之间表示请求成功，否则表示失败&lt;/li>
&lt;/ol>
&lt;p>postStart 和 preStop 钩子示例&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">lifecycle-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">lifecycle-demo-container&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">lifecycle&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">postStart&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">httpGet&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preStop&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">exec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;/usr/sbin/nginx&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;-s&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;quit&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 Container 运行起来之后，Pod 会执行对容器的健康状态检查。i.e.探针，下文就会描述&lt;/p>
&lt;h2 id="pod-里-container-的-probe探针健康状态检查">Pod 里 Container 的 probe(探针)，健康状态检查&lt;/h2>
&lt;p>对于 Pod 的健康检查状态有以下几点说明：&lt;/p>
&lt;ol>
&lt;li>对于 Pod 中的 Container 有两种检查方式
&lt;ol>
&lt;li>存活性 liveness 探测：周期性探测 Container 的活性。如果探测失败那么 Container 将重新启动。无法更新&lt;/li>
&lt;li>就绪状态 readiness 检测：定期探测 Container 中服务的准备情况。如果探测失败的话 Container 将从服务的后端移除(即使用 kubectl get pod 命令中 READY 标签中左侧数字会减少，减少的就是该 Pod 中某个不在准备状态的 Container)。无法更新。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>检查方式的探针类型：
&lt;ol>
&lt;li>exec，发送命令进行检查。在容器中执行一个命令，如果命令的退出状态码为 0，则探针成功，否则失败&lt;/li>
&lt;li>tcpSocket：通过 TCP 协议来检查。对指定容器 IP 和 PORT 执行一个 TCP 检查，如果端口是开发的则探针成功，否则失败&lt;/li>
&lt;li>httpGet：通过 HTTP 返回的响应报文来检查。对指定容器的 IP、Port、Path 执行一个 http 的 get 请求，如果返回的状态码在 200 到 400 之间则表示成功，否则表示失败&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Pod 中容器失败时候(存活性)的重启策略，Always，OnFailure，Never：Always(一失败就重启)&lt;/li>
&lt;li>Pod 删除的时候：先发送 terminal 信号，有一个宽限期，宽限期一过发送终止信号&lt;/li>
&lt;/ol>
&lt;h2 id="pod-运行中的几种状态">Pod 运行中的几种状态&lt;/h2>
&lt;ol>
&lt;li>CrashLoopBackOff： 容器退出，kubelet 正在将它重启&lt;/li>
&lt;li>InvalidImageName： 无法解析镜像名称&lt;/li>
&lt;li>ImageInspectError： 无法校验镜像&lt;/li>
&lt;li>ErrImageNeverPull： 策略禁止拉取镜像&lt;/li>
&lt;li>ImagePullBackOff： 正在重试拉取&lt;/li>
&lt;li>RegistryUnavailable： 连接不到镜像中心&lt;/li>
&lt;li>ErrImagePull： 通用的拉取镜像出错&lt;/li>
&lt;li>CreateContainerConfigError： 不能创建 kubelet 使用的容器配置&lt;/li>
&lt;li>CreateContainerError： 创建容器失败&lt;/li>
&lt;li>m.internalLifecycle.PreStartContainer 执行 hook 报错&lt;/li>
&lt;li>RunContainerError： 启动容器失败&lt;/li>
&lt;li>PostStartHookError： 执行 hook 报错&lt;/li>
&lt;li>ContainersNotInitialized： 容器没有初始化完毕&lt;/li>
&lt;li>ContainersNotReady： 容器没有准备完毕&lt;/li>
&lt;li>ContainerCreating：容器创建中&lt;/li>
&lt;li>PodInitializing：pod 初始化中&lt;/li>
&lt;li>DockerDaemonNotReady：docker 还没有完全启动&lt;/li>
&lt;li>NetworkPluginNotReady： 网络插件还没有完全启动&lt;/li>
&lt;/ol>
&lt;h1 id="probe-的-yaml-样例">Probe 的 yaml 样例&lt;/h1>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
labels:
app: nginx
name: nginx
spec:
containers:
- image: nginx
imagePullPolicy: Always
name: http
livenessProbe:
httpGet:
path: /
port: 80
httpHeaders:
- name: X-Custom-Header
value: Awesome
initialDelaySeconds: 15
timeoutSeconds: 1
readinessProbe:
exec:
command:
- cat
- /usr/share/nginx/html/index.html
initialDelaySeconds: 5
timeoutSeconds: 1
- name: goproxy
image: gcr.io/google_containers/goproxy:0.1
ports:
- containerPort: 8080
readinessProbe:
tcpSocket:
port: 8080
initialDelaySeconds: 5
periodSeconds: 10
livenessProbe:
tcpSocket:
port: 8080
initialDelaySeconds: 15
periodSeconds: 20
&lt;/code>&lt;/pre></description></item><item><title>Docs: Security Context(安全环境)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/security-context%E5%AE%89%E5%85%A8%E7%8E%AF%E5%A2%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/security-context%E5%AE%89%E5%85%A8%E7%8E%AF%E5%A2%83/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">官方文档,任务-配置 Pod 和 Containers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/NFgQrvn_LyU0qQbhMZwDAQ">公众号-阳明&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Security Context(安全环境)&lt;/strong> 用来定义 Pod 或 Container 的特权与访问控制设置。 安全上下文包括但不限于：&lt;/p>
&lt;ul>
&lt;li>自主访问控制（Discretionary Access Control）：基于 &lt;a href="https://wiki.archlinux.org/index.php/users_and_groups">用户 ID（UID）和组 ID（GID）&lt;/a>. 来判定对对象（例如文件）的访问权限。&lt;/li>
&lt;li>&lt;a href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%85%A8%E5%A2%9E%E5%BC%BA%E5%BC%8FLinux">安全性增强的 Linux（SELinux）&lt;/a>： 为对象赋予安全性标签。&lt;/li>
&lt;li>以特权模式或者非特权模式运行。&lt;/li>
&lt;li>&lt;a href="https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/">Linux 权能&lt;/a>: 为进程赋予 root 用户的部分特权而非全部特权。&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/tutorials/clusters/apparmor/">AppArmor&lt;/a>：使用程序框架来限制个别程序的权能。&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp&lt;/a>：过滤进程的系统调用。&lt;/li>
&lt;li>AllowPrivilegeEscalation：控制进程是否可以获得超出其父进程的特权。 此布尔值直接控制是否为容器进程设置 &lt;code>[no_new_privs](https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt)&lt;/code>标志。 当容器以特权模式运行或者具有 &lt;code>CAP_SYS_ADMIN&lt;/code> 权能时，AllowPrivilegeEscalation 总是为 true。&lt;/li>
&lt;li>readOnlyRootFilesystem：以只读方式加载容器的根文件系统。&lt;/li>
&lt;/ul>
&lt;p>以上条目不是安全上下文设置的完整列表 &amp;ndash; 请参阅 &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#securitycontext-v1-core">SecurityContext&lt;/a> 了解其完整列表。
关于在 Linux 系统中的安全机制的更多信息，可参阅 &lt;a href="https://www.linux.com/learn/overview-linux-kernel-security-features">Linux 内核安全性能力概述&lt;/a>。&lt;/p>
&lt;p>特别注意：限制自由，会产生很多问题，比如：&lt;/p>
&lt;ul>
&lt;li>使用 hostPath 类型的 volume 时，如果容器不以 root 用户运行，则无法对 hostPath 所在目录执行操作，任何写操作将会提示权限不够。因为目录权限 755&lt;/li>
&lt;/ul>
&lt;h1 id="应该了解的-10-个-kubernetes-安全上下文配置">应该了解的 10 个 Kubernetes 安全上下文配置&lt;/h1>
&lt;p>在 Kubernetes 中安全地运行工作负载是很困难的，有很多配置都可能会影响到整个 Kubernetes API 的安全性，这需要我们有大量的知识积累来正确的实施。Kubernetes 在安全方面提供了一个强大的工具 securityContext，每个 Pod 和容器清单都可以使用这个属性。在本文中我们将了解各种 securityContext 的配置，探讨它们的含义，以及我们应该如何使用它们。
securityContext 设置在 PodSpec 和 ContainerSpec 规范中都有定义，这里我们分别用[P]和[C]来表示。需要注意的是，如果一个设置在两个作用域中都可以使用和配置，那么我们应该优先考虑设置容器级别的。&lt;/p>
&lt;h2 id="1runasnonroot-pc">1runAsNonRoot [P/C]&lt;/h2>
&lt;p>我们知道容器是使用 namespaces 和 cgroups 来限制其进程，但只要在部署的时候做了一次错误的配置，就可以让这些进程访问主机上的资源。如果该进程以 root 身份运行，它对这些资源的访问权限与主机 root 账户是相同的。此外，如果其他 pod 或容器设置被用来减少约束（比如 procMount 或 capabilities），拥有一个 root UID 就会提高风险，除非你有一个非常好的原因，否则你不应该以 root 身份运行一个容器。
那么，如果你有一个使用 root 的镜像需要部署，那应该怎么办呢？&lt;/p>
&lt;h3 id="11-使用基础镜像中提供的用户">1.1 使用基础镜像中提供的用户&lt;/h3>
&lt;p>通常情况下，基础镜像已经创建并提供了一个用户，例如，官方的 Node.js 镜像带有一个 UID 为 1000 的名为 node 的用户，我们就可以使用该身份来运行容器，但他们并没有在 Dockerfile 中明确地设置当前用户。我们可以在运行时用 runAsUser 设置来配置它，或者用自定义的 Dockerfile 来更改镜像中的当前用户。这里我们来看看使用自定义的 Dockerfile 来构建我们自己的镜像的例子。
在不深入了解镜像构建的情况下，让我们假设我们有一个预先构建好的 npm 应用程序。这里是一个最小的 Dockerfile 文件，用来构建一个基于 node:slim 的镜像，并以提供的 node 用户身份运行。
FROM node:slim
COPY &amp;ndash;chown=node . /home/node/app/   &lt;em># &amp;lt;&amp;mdash; Copy app into the home directory with right ownership&lt;/em>
USER 1000                             &lt;em># &amp;lt;&amp;mdash; Switch active user to “node” (by UID)&lt;/em>
WORKDIR /home/node/app                &lt;em># &amp;lt;&amp;mdash; Switch current directory to app&lt;/em>
ENTRYPOINT [&amp;ldquo;npm&amp;rdquo;, &amp;ldquo;start&amp;rdquo;]           &lt;em># &amp;lt;&amp;mdash; This will now exec as the “node” user instead of root&lt;/em>&lt;/p>
&lt;p>其中以 USER 开头的一行就是关键设置，这使得 node 成为从这个镜像启动的任何容器里面的默认用户。我们使用 UID 而不是用户的名字，因为 Kubernetes 无法在启动容器前将镜像的默认用户名映射到 UID 上，并且在部署时指定 runAsNotRoot: true，会返回有关错误。&lt;/p>
&lt;h3 id="12-基础镜像没有提供用户">1.2 基础镜像没有提供用户&lt;/h3>
&lt;p>如果我们使用的基础镜像没有提供一个可以使用的用户，那么我们又应该怎么做呢？对于大部分进程来说，我们只需在自定义的 Dockerfile 中创建一个用户并使用它即可。如下所示：
FROM node:slim
RUN useradd somebody -u 10001 &amp;ndash;create-home &amp;ndash;user-group  &lt;em># &amp;lt;&amp;mdash; Create a user&lt;/em>
COPY &amp;ndash;chown=somebody . /home/somebody/app/
USER 10001
WORKDIR /home/somebody/app
ENTRYPOINT [&amp;ldquo;npm&amp;rdquo;, &amp;ldquo;start&amp;rdquo;]&lt;/p>
&lt;p>这里我们增加了一行创建用户的 RUN 命令即可。不过需要注意的是这对于 node.js 和 npm 来说，这很好用，但是其他工具可能需要文件系统的不同元素进行所有权变更。如果遇到任何问题，需要查阅对应工具的文档。&lt;/p>
&lt;h2 id="2runasuserrunasgroup-pc">2runAsUser/runAsGroup [P/C]&lt;/h2>
&lt;p>容器镜像可能有一个特定的用户或组，我们可以用 runAsUser 和 runAsGroup 来进行覆盖。通常，这些设置与包含具有相同所有权 ID 的文件的卷挂载结合在一起。
&amp;hellip;.
spec:
  containers:
    - name: web
      image: mycorp/webapp:1.2.3
  securityContext:
    runAsNonRoot: true
    runAsUser: 10001
&amp;hellip;.&lt;/p>
&lt;p>不过使用这些配置也是有风险的，因为你为容器做出的运行时决定可能与原始镜像不兼容。例如，jenkins/jenkins 镜像以名为 jenkins:jenkins 的&lt;strong>组:用户&lt;/strong>身份运行，其应用文件全部由该用户拥有。如果我们配置一个不同的用户，它将无法启动，因为该用户不存在于镜像的/etc/passwd 文件中。即使它以某种方式存在，它也很可能在读写 jenkins:jenkins 拥有的文件时出现问题。我们可以用一个简单的 docker 运行命令来验证这个问题。
$ docker run &amp;ndash;rm -it -u eric:eric jenkins/jenkins
docker: Error response from daemon: unable to find user eric: no matching entries in passwd file.&lt;/p>
&lt;p>上面我们提到确保容器进程不以 root 用户身份运行是一个非常好的主意，但不要依赖 runAsUser 或 runAsGroup 设置来保证这一点，未来有人可能会删除这些配置，请确保同时将 runAsNonRoot 设置为 true。&lt;/p>
&lt;h2 id="3selinuxoptions-pc">3seLinuxOptions [P/C]&lt;/h2>
&lt;p>SELinux 是一个用于控制对 Linux 系统上的应用、进程和文件进行访问的策略驱动系统，它在 Linux 内核中实现了 Linux 安全模块框架。SELinux 是基于标签的策略，它将一些标签应用于系统中的所有元素，然后将元素进行分组。这些标签被称为&lt;strong>安全上下文&lt;/strong>（不要和 Kubernetes 中的 securityContext 混淆了）- 由用户、角色、类型和可选的一些其他属性组成，格式为：user:role:type:level。
然后，SELinux 使用策略来定义特定上下文中的哪些进程可以访问系统中其他被标记的对象。SELinux 可以是严格执行 enforced 模式，在这种情况下，访问将被拒绝，如果被配置为允许的 permissive 模式，那么安全策略没有被强制执行，当安全策略规则应该拒绝访问时，访问仍然被允许，然而，此时会向日志文件发送一条消息，表示该访问应该被拒绝。在容器中，SELinux 通常给容器进程和容器镜像打上标签，以限制该进程只能访问镜像中的文件。
默认的 SELinux 策略将在实例化容器时由容器运行时应用，securityContext 中的 seLinuxOptions 允许配置自定义的 SELinux 策略标签，请注意，改变容器的 SELinux 策略标签有可能允许容器进程摆脱容器镜像并访问主机文件系统。
当然只有当宿主机操作系统支持 SELinux 时，这个功能才会起作用。&lt;/p>
&lt;h2 id="4seccompprofile-pc">4seccompProfile [P/C]&lt;/h2>
&lt;p>Seccomp 表示一种安全计算模式，是 Linux 内核的一项功能，它可以限制一个特定进程从用户空间到内核的调用。seccomp 配置文件是使用一个 JSON 文件进行定义的，通常由一组系统调用和发生这些系统调用时的默认动作组成。如下配置所示：
{
    &amp;ldquo;defaultAction&amp;rdquo;: &amp;ldquo;SCMP_ACT_ERRNO&amp;rdquo;,
    &amp;ldquo;architectures&amp;rdquo;: [
        &amp;ldquo;SCMP_ARCH_X86_64&amp;rdquo;,
        &amp;ldquo;SCMP_ARCH_X86&amp;rdquo;,
        &amp;ldquo;SCMP_ARCH_X32&amp;rdquo;
    ],
    &amp;ldquo;syscalls&amp;rdquo;: [
        {
            &amp;ldquo;name&amp;rdquo;: &amp;ldquo;accept&amp;rdquo;,
            &amp;ldquo;action&amp;rdquo;: &amp;ldquo;SCMP_ACT_ALLOW&amp;rdquo;,
            &amp;ldquo;args&amp;rdquo;: []
        },
        {
            &amp;ldquo;name&amp;rdquo;: &amp;ldquo;accept4&amp;rdquo;,
            &amp;ldquo;action&amp;rdquo;: &amp;ldquo;SCMP_ACT_ALLOW&amp;rdquo;,
            &amp;ldquo;args&amp;rdquo;: []
        },
        &amp;hellip;
    ]
}&lt;/p>
&lt;p>Kubernetes 通过在 securityContext 中的 seccompProfile 属性来提供一个使用自定义配置文件的机制。
seccompProfile:
  type: Localhost
  localhostProfile: profiles/myprofile.json&lt;/p>
&lt;p>这里配置的 type 字段有三个可选的值：&lt;/p>
&lt;ul>
&lt;li>Localhost：其中 localhostProfile 配置为容器内的 seccomp 配置文件路径。&lt;/li>
&lt;li>Unconfined：其中没有配置文件。&lt;/li>
&lt;li>RuntimeDefault：其中使用容器运行时的默认值&amp;ndash;如果没有指定类型，就是默认值。&lt;/li>
&lt;/ul>
&lt;p>我们可以在 PodSecurityContext 或 securityContext 中使用这些配置，如果两者都配置了，就会使用容器级别中的配置。
此外与大多数安全相关的设置一样，&lt;strong>最小权限原则&lt;/strong>在此同样适用。只给你的容器访问它所需要的权限即可。首先创建一个配置文件，简单地记录哪些系统调用正在发生，然后测试你的应用程序，建立一套允许的系统调用规则。我们可以在 Kubernetes 教程(&lt;a href="https://kubernetes.io/docs/tutorials/clusters/seccomp">https://kubernetes.io/docs/tutorials/clusters/seccomp&lt;/a>)中找到关于Seccomp的更多信息。&lt;/p>
&lt;h2 id="5-避免使用特权容器-c">5 避免使用特权容器 [C]&lt;/h2>
&lt;p>给容器授予特权模式是非常危险的，一般会有一种更简单的方式来实现特定的权限，或者可以通过授予 Linux Capabilities 权限来控制。容器运行时控制器着特权模式的具体实现，但是它会授予容器所有的特权，并解除由 cgroup 控制器执行的限制，它还可以修改 Linux 安全模块的配置，并允许容器内的进程逃离容器。
容器在宿主机中提供了进程隔离，所以即使容器是使用 root 身份运行的，也有容器运行时不授予容器的 Capabilities。如果配置了特权模式，容器运行时就会授予系统 root 的所有能力，从安全角度来看，这是很危险的，因为它允许对底层宿主机系统的所有操作访问。
避免使用特权模式，如果你的容器确实需要额外的能力，只需通过添加 capabilities 来满足你的需求。除非你的容器需要控制主机内核中的系统级设置，如访问特定的硬件或重新配置网络，并且需要访问主机文件系统，那么它就不需要特权模式。&lt;/p>
&lt;h2 id="6linux-capabilities-c">6Linux Capabilities [C]&lt;/h2>
&lt;p>Capabilities 是一个内核级别的权限，它允许对内核调用权限进行更细粒度的控制，而不是简单地以 root 身份运行。Capabilities 包括更改文件权限、控制网络子系统和执行系统管理等功能。在 securityContext 中，Kubernetes 可以添加或删除 Capabilities，单个 Capabilities 或逗号分隔的列表可以作为一个字符串数组进行配置。另外，我们也可以使用 all 来添加或删除所有的配置。这种配置会被传递给容器运行时，在它创建容器的时候会配置上 Capabilities 集合，如果 securityContext 中没有配置，那么容器将会直接容器运行时提供的所有默认配置。
securityContext:
  capabilities:
    drop:
      - all
    add: [&amp;ldquo;MKNOD&amp;rdquo;]&lt;/p>
&lt;p>一般推荐的做法是先删除所有的配置，然后只添加你的应用程序实际需要的，在大部分情况下，应用程序在正常运行中实际上不需要任何 Capabilities，通过删除所有配置来测试，并通过监控审计日志来调试问题，看看哪些功能被阻止了。
请注意，当在 securityContext 中列出要放弃或添加的 Capabilities 时，你要删除内核在命名 Capabilities 时使用的 CAP_前缀。capsh 工具可以给我们一个比较友好的调试信息，可以来说明你的容器中到底启用了哪些 Capabilities，当然不要在生产容器中使用这个工具，因为这使得攻击者很容易弄清楚哪些 Capabilities 被启用了。&lt;/p>
&lt;h2 id="7-以只读文件系统运行-c">7 以只读文件系统运行 [C]&lt;/h2>
&lt;p>如果你的容器被入侵，而且它有一个可读写的文件系统，那么攻击者就可以随意地改变它的配置、安装软件，并有可能启动其他的漏洞。拥有一个只读的文件系统有助于防止这些类型的安全问题，因为它限制了攻击者可以执行的操作。一般来说，容器不应该要求对容器文件系统进行写入，如果你的应用程序是有状态数据，那么你应该使用外部持久化方法，如数据库、volume 或其他一些服务。另外，确保所有的日志都写到 stdout 或日志转发器上。&lt;/p>
&lt;h2 id="8procmount-c">8procMount [C]&lt;/h2>
&lt;p>默认情况下，为了防止潜在的安全问题，容器运行时会屏蔽容器内/proc 文件系统的某些部分文件。然而有时需要访问/proc 的这些文件，特别是在使用嵌套容器时，因为它经常被用作集群内构建过程的一部分。该配置只有两个有效的选项：&lt;/p>
&lt;ul>
&lt;li>Default：保持标准的容器运行时行为&lt;/li>
&lt;li>Unmasked：它删除/proc 文件系统的所有屏蔽行为&lt;/li>
&lt;/ul>
&lt;p>显然只有当我们知道在做什么的时候才应该使用这个配置，如果你是为了构建镜像而使用它，请检查构建工具的最新版本，因为许多工具不再需要这个设置了，最好升级下工具并设置为 Default 默认的 procMount。&lt;/p>
&lt;h2 id="9fsgroupfsgroupchangepolicy-p">9fsGroup/fsGroupChangePolicy [P]&lt;/h2>
&lt;p>fsGroup 设置定义了一个组，当卷被 pod 挂载时，Kubernetes 将把卷中所有文件的权限改为该组。这里的行为也由 fsGroupChangePolicy 控制，它可以被设置为 onRootMismatch 或 Always。如果设置为 onRootMismatch 则只有当权限与容器 root 的权限不匹配时才会被改变。
不过在使用 fsGroup 时也要慎重，改变整个 volume 卷的组所有权会导致&lt;strong>变慢&lt;/strong>，如果是大型文件系统&lt;strong>启动也会延迟&lt;/strong>。如果共享同一卷的其他进程没有对新的 GID 的访问权限，它也会对这些进程造成损害。由于这个原因，一些共享文件系统如 NFS，没有实现这个功能。这些设置也不影响临时的 ephemeral 卷。&lt;/p>
&lt;h2 id="10sysctls-p">10sysctls [P]&lt;/h2>
&lt;p>Sysctls 是 Linux 内核的一个功能，它允许管理员修改内核配置。在一个完整的 Linux 操作系统中，这些是通过使用/etc/sysctl.conf 定义的，也可以使用 sysctl 工具进行修改。
securityContext 中的 sysctls 配置允许在容器中修改特定的 sysctls。只有一小部分的 sysctls 可以在每个容器的基础上进行修改，它们都在内核中被命名的。在这个可以配置的子集中，有些被认为是安全的，而更多的则被认为是不安全的，这取决于对其他 pod 的潜在影响。在集群中，不安全的 sysctls 通常是被禁用，需要由集群管理员专门开启。
鉴于有可能破坏底层操作系统的稳定，除非你有非常特殊的要求，否则应该避免通过 sysctls 修改内核参数。&lt;/p>
&lt;h2 id="11-总结">11 总结&lt;/h2>
&lt;p>在用 securityContext 加固你的应用时，有很多事情需要注意。如果使用得当，它们是一种非常有效的工具，我们希望这个列表能帮助你的团队为你的工作负载和环境进行正确的安全配置。&lt;/p></description></item><item><title>Docs: 从外部访问pod的方式</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/%E4%BB%8E%E5%A4%96%E9%83%A8%E8%AE%BF%E9%97%AEpod%E7%9A%84%E6%96%B9%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/%E4%BB%8E%E5%A4%96%E9%83%A8%E8%AE%BF%E9%97%AEpod%E7%9A%84%E6%96%B9%E5%BC%8F/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://jimmysong.io/kubernetes-handbook/guide/accessing-kubernetes-pods-from-outside-of-the-cluster.html">原文，jimmysong.io kubernetes 手书，指南-从集群外部访问 Pod &lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>本文主要讲解访问 kubenretes 中的 Pod 和 Serivce 的集中方式，包括如下几种：&lt;/p>
&lt;ul>
&lt;li>hostNetwork&lt;/li>
&lt;li>hostPort&lt;/li>
&lt;li>NodePort&lt;/li>
&lt;li>LoadBalancer&lt;/li>
&lt;li>Ingress&lt;/li>
&lt;/ul>
&lt;p>说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的 backend。&lt;/p>
&lt;h1 id="hostnetwork-true">hostNetwork: true&lt;/h1>
&lt;p>这是一种直接定义 Pod 网络的方式。&lt;/p>
&lt;p>如果在 Pod 中使用 hostNetwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: influxdb
spec:
hostNetwork: true
containers:
-name: influxdb
image: influxdb
&lt;/code>&lt;/pre>
&lt;p>部署该 Pod：&lt;/p>
&lt;pre>&lt;code>$ kubectl create -f influxdb-hostnetwork.yml
&lt;/code>&lt;/pre>
&lt;p>访问该 pod 所在主机的 8086 端口：&lt;/p>
&lt;pre>&lt;code>curl -v http://$POD_IP:8086/ping
&lt;/code>&lt;/pre>
&lt;p>将看到 204 No Content 的 204 返回码，说明可以正常访问。&lt;/p>
&lt;p>注意每次启动这个 Pod 的时候都可能被调度到不同的节点上，所有外部访问 Pod 的 IP 也是变化的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 hostNetwork: true 的方式。&lt;/p>
&lt;p>这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。&lt;/p>
&lt;p>这种网络方式可以用来做 nginx Ingress controller。外部流量都需要通过 kubenretes node 节点的 80 和 443 端口。&lt;/p>
&lt;h1 id="hostport">hostPort&lt;/h1>
&lt;p>这是一种直接定义 Pod 网络的方式。&lt;/p>
&lt;p>hostPort 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了，如:。&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: influxdb
spec:
containers:
-name: influxdb
image: influxdb
ports:
- containerPort:8086
hostPort:8086
&lt;/code>&lt;/pre>
&lt;p>这样做有个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系&lt;/p>
&lt;h1 id="nodeport">NodePort&lt;/h1>
&lt;p>NodePort 在 kubenretes 里是一个广泛应用的服务暴露方式。Kubernetes 中的 service 默认情况下都是使用的 ClusterIP 这种类型，这样的 service 会产生一个 ClusterIP，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service，需要将 service type 修改为 nodePort。&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: influxdb
labels:
name: influxdb
spec:
containers:
- name: influxdb
image: influxdb
ports:
- containerPort:8086
&lt;/code>&lt;/pre>
&lt;p>同时还可以给 service 指定一个 nodePort 值，范围是 30000-32767，这个值在 API server 的配置文件中，用&amp;ndash;service-node-port-range 定义。&lt;/p>
&lt;pre>&lt;code>kind: Service
apiVersion: v1
metadata:
name: influxdb
spec:
type: NodePort
ports:
- port:8086
nodePort:30000
selector:
name: influxdb
&lt;/code>&lt;/pre>
&lt;p>集群外就可以使用 kubernetes 任意一个节点的 IP 加上 30000 端口访问该服务了。kube-proxy 会自动将流量以 round-robin 的方式转发给该 service 的每一个 pod。&lt;/p>
&lt;p>这种服务暴露方式，无法让你指定自己想要的应用常用端口，不过可以在集群上再部署一个反向代理作为流量入口。&lt;/p>
&lt;h1 id="loadbalancer">LoadBalancer&lt;/h1>
&lt;p>LoadBalancer 只能在 service 上定义。这是公有云提供的负载均衡器，如 AWS、Azure、CloudStack、GCE 等。&lt;/p>
&lt;pre>&lt;code>kind: Service
apiVersion: v1
metadata:
name: influxdb
spec:
type: LoadBalancer
ports:
- port:8086
selector:
name: influxdb
&lt;/code>&lt;/pre>
&lt;p>查看服务：&lt;/p>
&lt;pre>&lt;code>$ kubectl get svc influxdb
NAME CLUSTER-IP EXTERNAL-IP PORT(S)
AGEinfluxdb 10.97.121.42 10.13.242.236 8086:30051/TCP 39s
&lt;/code>&lt;/pre>
&lt;p>内部可以使用 ClusterIP 加端口来访问服务，如 19.97.121.42:8086。&lt;/p>
&lt;p>外部可以用以下两种方式访问该服务：&lt;/p>
&lt;ul>
&lt;li>使用任一节点的 IP 加 30051 端口访问该服务&lt;/li>
&lt;li>使用 EXTERNAL-IP 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如 10.13.242.236:8086。&lt;/li>
&lt;/ul>
&lt;h1 id="ingress">Ingress&lt;/h1>
&lt;p>Ingress 是自 kubernetes1.1 版本后引入的资源类型。必须要部署 Ingress controller 才能创建 Ingress 资源，Ingress controller 是以一种插件的形式提供。Ingress controller 是部署在 Kubernetes 之上的 Docker 容器。它的 Docker 镜像包含一个像 nginx 或 HAProxy 的负载均衡器和一个控制器守护进程。控制器守护程序从 Kubernetes 接收所需的 Ingress 配置。它会生成一个 nginx 或 HAProxy 配置文件，并重新启动负载平衡器进程以使更改生效。换句话说，Ingress controller 是由 Kubernetes 管理的负载均衡器。&lt;/p>
&lt;p>Kubernetes Ingress 提供了负载平衡器的典型特性：HTTP 路由，粘性会话，SSL 终止，SSL 直通，TCP 和 UDP 负载平衡等。目前并不是所有的 Ingress controller 都实现了这些功能，需要查看具体的 Ingress controller 文档。&lt;/p>
&lt;p>外部访问 URL &lt;a href="http://influxdb.kube.example.com/ping">http://influxdb.kube.example.com/ping&lt;/a> 访问该服务，入口就是 80 端口，然后 Ingress controller 直接将流量转发给后端 Pod，不需再经过 kube-proxy 的转发，比 LoadBalancer 方式更高效。&lt;/p>
&lt;h1 id="总结">总结&lt;/h1>
&lt;p>总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括 Nginx、HAProxy、Traefik，还有各种 Service Mesh，而其它服务暴露方式可以更适用于服务调试、特殊应用的部署。&lt;/p>
&lt;h1 id="参考">参考&lt;/h1>
&lt;p>&lt;a href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/">Accessing Kubernetes Pods from Outside of the Cluster - alesnosek.com&lt;/a>&lt;/p></description></item><item><title>Docs: 为 Pod 注入数据</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/%E4%B8%BA-pod-%E6%B3%A8%E5%85%A5%E6%95%B0%E6%8D%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/%E4%B8%BA-pod-%E6%B3%A8%E5%85%A5%E6%95%B0%E6%8D%AE/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/">官方文档,任务-给应用注入数据&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="将-pod-的-manifests-信息映射到容器中的环境变量上">将 Pod 的 Manifests 信息映射到容器中的环境变量上&lt;/h1>
&lt;h2 id="用-pod-字段作为环境变量的值">用 Pod 字段作为环境变量的值&lt;/h2>
&lt;p>在这个练习中，你将创建一个包含一个容器的 Pod。这是该 Pod 的配置文件：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">dapi-envars-fieldref&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">test-container&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">k8s.gcr.io/busybox&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;sh&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;-c&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">args&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">while true; do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">echo -en &amp;#39;\n&amp;#39;;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">sleep 10;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">done;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">MY_NODE_NAME&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">valueFrom&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldPath&lt;/span>: &lt;span style="color:#ae81ff">spec.nodeName&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">MY_POD_NAME&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">valueFrom&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldPath&lt;/span>: &lt;span style="color:#ae81ff">metadata.name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">MY_POD_NAMESPACE&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">valueFrom&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldPath&lt;/span>: &lt;span style="color:#ae81ff">metadata.namespace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">MY_POD_IP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">valueFrom&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldPath&lt;/span>: &lt;span style="color:#ae81ff">status.podIP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">MY_POD_SERVICE_ACCOUNT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">valueFrom&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fieldPath&lt;/span>: &lt;span style="color:#ae81ff">spec.serviceAccountName&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">restartPolicy&lt;/span>: &lt;span style="color:#ae81ff">Never&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这个配置文件中，你可以看到五个环境变量。&lt;code>env&lt;/code> 字段是一个 &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#envvar-v1-core">EnvVars&lt;/a>. 对象的数组。 数组中第一个元素指定 &lt;code>MY_NODE_NAME&lt;/code> 这个环境变量从 Pod 的 &lt;code>spec.nodeName&lt;/code> 字段获取变量值。 同样，其它环境变量也是从 Pod 的字段获取它们的变量值。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>说明：&lt;/strong> 本示例中的字段是 Pod 字段，不是 Pod 中 Container 的字段。&lt;/p>
&lt;/blockquote>
&lt;p>创建 Pod：&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-pod.yaml
&lt;/code>&lt;/pre>
&lt;p>验证 Pod 中的容器运行正常：&lt;/p>
&lt;pre>&lt;code>kubectl get pods
&lt;/code>&lt;/pre>
&lt;p>查看容器日志：&lt;/p>
&lt;pre>&lt;code>kubectl logs dapi-envars-fieldref
&lt;/code>&lt;/pre>
&lt;p>输出信息显示了所选择的环境变量的值：&lt;/p>
&lt;pre>&lt;code>minikube
dapi-envars-fieldref
default
172.17.0.4
default
&lt;/code>&lt;/pre>
&lt;p>要了解为什么这些值在日志中，请查看配置文件中的&lt;code>command&lt;/code> 和 &lt;code>args&lt;/code>字段。 当容器启动时，它将五个环境变量的值写入 stdout。每十秒重复执行一次。
接下来，通过打开一个 Shell 进入 Pod 中运行的容器：&lt;/p>
&lt;pre>&lt;code>kubectl exec -it dapi-envars-fieldref -- sh
&lt;/code>&lt;/pre>
&lt;p>在 Shell 中，查看环境变量：&lt;/p>
&lt;pre>&lt;code>/# printenv
&lt;/code>&lt;/pre>
&lt;p>输出信息显示环境变量已经设置为 Pod 字段的值。&lt;/p>
&lt;pre>&lt;code>MY_POD_SERVICE_ACCOUNT=default
...
MY_POD_NAMESPACE=default
MY_POD_IP=172.17.0.4
...
MY_NODE_NAME=minikube
...
MY_POD_NAME=dapi-envars-fieldref
&lt;/code>&lt;/pre>
&lt;h2 id="用-container-字段作为环境变量的值">用 Container 字段作为环境变量的值&lt;/h2>
&lt;p>前面的练习中，你将 Pod 字段作为环境变量的值。 接下来这个练习中，你将用 Container 字段作为环境变量的值。这里是包含一个容器的 Pod 的配置文件：
&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/ooyi9u/1621520643090-327ae7d2-cb76-4240-b963-9070371cdaca.svg">&lt;code>pods/inject/dapi-envars-container.yaml&lt;/code>&lt;/a>&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: dapi-envars-resourcefieldref
spec:
containers:
- name: test-container
image: k8s.gcr.io/busybox:1.24
command: [ &amp;quot;sh&amp;quot;, &amp;quot;-c&amp;quot;]
args:
- while true; do
echo -en '\n';
printenv MY_CPU_REQUEST MY_CPU_LIMIT;
printenv MY_MEM_REQUEST MY_MEM_LIMIT;
sleep 10;
done;
resources:
requests:
memory: &amp;quot;32Mi&amp;quot;
cpu: &amp;quot;125m&amp;quot;
limits:
memory: &amp;quot;64Mi&amp;quot;
cpu: &amp;quot;250m&amp;quot;
env:
- name: MY_CPU_REQUEST
valueFrom:
resourceFieldRef:
containerName: test-container
resource: requests.cpu
- name: MY_CPU_LIMIT
valueFrom:
resourceFieldRef:
containerName: test-container
resource: limits.cpu
- name: MY_MEM_REQUEST
valueFrom:
resourceFieldRef:
containerName: test-container
resource: requests.memory
- name: MY_MEM_LIMIT
valueFrom:
resourceFieldRef:
containerName: test-container
resource: limits.memory
restartPolicy: Never
&lt;/code>&lt;/pre>
&lt;p>这个配置文件中，你可以看到四个环境变量。&lt;code>env&lt;/code> 字段是一个 &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#envvar-v1-core">EnvVars&lt;/a>. 对象的数组。数组中第一个元素指定 &lt;code>MY_CPU_REQUEST&lt;/code> 这个环境变量从 Container 的 &lt;code>requests.cpu&lt;/code> 字段获取变量值。同样，其它环境变量也是从 Container 的字段获取它们的变量值。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>说明：&lt;/strong> 本例中使用的是 Container 的字段而不是 Pod 的字段。&lt;/p>
&lt;/blockquote>
&lt;p>创建 Pod：&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-container.yaml
&lt;/code>&lt;/pre>
&lt;p>验证 Pod 中的容器运行正常：&lt;/p>
&lt;pre>&lt;code>kubectl get pods
&lt;/code>&lt;/pre>
&lt;p>查看容器日志：&lt;/p>
&lt;pre>&lt;code>kubectl logs dapi-envars-resourcefieldref
&lt;/code>&lt;/pre>
&lt;p>输出信息显示了所选择的环境变量的值：&lt;/p>
&lt;pre>&lt;code>1
1
33554432
67108864
&lt;/code>&lt;/pre></description></item><item><title>Docs: 为Pod分配物理资源与限制物理资源的使用</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/%E4%B8%BApod%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E9%99%90%E5%88%B6%E7%89%A9%E7%90%86%E8%B5%84%E6%BA%90%E7%9A%84%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/%E4%B8%BApod%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E9%99%90%E5%88%B6%E7%89%A9%E7%90%86%E8%B5%84%E6%BA%90%E7%9A%84%E4%BD%BF%E7%94%A8/</guid><description>
&lt;h1 id="pod-中-container-的资源需求与资源限制">Pod 中 Container 的资源需求与资源限制&lt;/h1>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;p>可以在 Pod 的 yaml 中定义该 Pod 中各个 Container 对内存与 CPU 的最低需求量和最大使用量&lt;/p>
&lt;ol>
&lt;li>
&lt;p>requests：资源需求，最低保障，资源最少需要多少&lt;/p>
&lt;/li>
&lt;li>
&lt;p>limits：限制，硬限制，限额，资源最大不能超过多少&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>当对 Container 进行资源制定后，会出现 QoS(服务质量)的属性，下列 3 个属性从上往下优先级下降；当节点资源不够时，优先级越高，越会保证其正常运行，其余不够提供资源的 Container 则不再运行&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Guarateed：有保证的，Pod 中每个 Container 同时设置 CPU 和内存的 requests 和 limits，且 request 和 limits 的值相同&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Burstable：超频，Pod 中至少有一个 Container 设置了 CPU 或内存资源的 requests 属性&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BestEffort：尽力努力(尽力而为)没有任何一个 Container 设置了 requests 和 limits 属性&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/xu9mbw/1617283210163-cdf33748-8346-40d3-96b1-c3da2cf90df5.jpeg" alt="image.jpeg">&lt;/p>
&lt;p>关于在 yaml 中如何写资源限制中数值的说明：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>kubernetes 中的一个 CPU 是一个逻辑 CPU，1CPU 的核心数=1000millicores 毫核心(也就是说 500m 相当于 0.5 个 CPU)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>限制 cpu 可以用整数写，1 就是 1 个 cpu、0.5 就是 0.5 个 cpu。也可以带单位，1000m 就是 1 个 cpu，500m 就是 0.5 个 cpu。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>限制内存需要使用单位(IEC 标准、公制标准都可以)，即 Mi、Gi 或者 M、G 等。例如 1024Mi、1Gi 等&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Note：polinux/stress 这是一个非常好用的压测容器，可以对容器指定其所使用的内存和 cpu 等资源的大小。当创建完资源配合等资源限制的对象后，可以通过该容器来测试资源限制是否生效。&lt;/p>
&lt;p>分配内存资源给 容器 和 Pods&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">memory-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">mem-example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">memory-demo-ctr&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">polinux/stress&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;200Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;100Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;stress&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">args&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;--vm&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;--vm-bytes&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;150M&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;--vm-hang&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>分配 CPU 资源给 容器 和 Pods&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">cpu-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">cpu-example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">cpu-demo-ctr&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">vish/stress&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0.5&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">args&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - -&lt;span style="color:#ae81ff">cpus&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>