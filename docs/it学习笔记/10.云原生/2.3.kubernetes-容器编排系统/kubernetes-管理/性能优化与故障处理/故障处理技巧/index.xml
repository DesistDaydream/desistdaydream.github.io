<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 故障处理技巧</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</link><description>Recent content in 故障处理技巧 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: /etc/kubernetes 目录误删恢复</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/etc_kubernetes-%E7%9B%AE%E5%BD%95%E8%AF%AF%E5%88%A0%E6%81%A2%E5%A4%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/etc_kubernetes-%E7%9B%AE%E5%BD%95%E8%AF%AF%E5%88%A0%E6%81%A2%E5%A4%8D/</guid><description>
&lt;h1 id="故障现象">故障现象&lt;/h1>
&lt;p>参考：&lt;a href="https://mp.weixin.qq.com/s/O3fJF5aZuxPOKa7lIjrHnQ">阳明公众号原文&lt;/a>&lt;/p>
&lt;p>Kubernetes 是一个很牛很牛的平台，Kubernetes 的架构可以让你轻松应对各种故障，今天我们将来破坏我们的集群、删除证书，然后再想办法恢复我们的集群，进行这些危险的操作而不会对已经运行的服务造成宕机。&lt;/p>
&lt;blockquote>
&lt;p>如果你真的想要执行接下来的操作，还是建议别在生产环境去折腾，虽然理论上不会造成服务宕机，但是如果出现了问题，&lt;strong>可千万别骂我~~~&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>我们知道 Kubernetes 的控制平面是由几个组件组成的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>etcd：作为整个集群的数据库使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-apiserver：集群的 API 服务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-controller-manager：整个集群资源的控制操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-scheduler：核心调度器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubelet：是运行在节点上用来真正管理容器的组件&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这些组件都由一套针对客户端和服务端的 TLS 证书保护，用于组件之间的认证和授权，大部分情况下它们并不是直接存储在 Kubernetes 的数据库中的，而是以普通文件的形式存在。&lt;/p>
&lt;pre>&lt;code># tree /etc/kubernetes/pki/
/etc/kubernetes/pki/
├── apiserver.crt
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
├── apiserver.key
├── apiserver-kubelet-client.crt
├── apiserver-kubelet-client.key
├── ca.crt
├── ca.key
├── CTNCA.pem
├── etcd
│ ├── ca.crt
│ ├── ca.key
│ ├── healthcheck-client.crt
│ ├── healthcheck-client.key
│ ├── peer.crt
│ ├── peer.key
│ ├── server.crt
│ └── server.key
├── front-proxy-ca.crt
├── front-proxy-ca.key
├── front-proxy-client.crt
├── front-proxy-client.key
├── sa.key
└── sa.pub
&lt;/code>&lt;/pre>
&lt;p>控制面板的组件以静态 Pod (我这里用 kubeadm 搭建的集群)的形式运行在 master 节点上，默认资源清单位于 &lt;code>/etc/kubernetes/manifests&lt;/code> 目录下。通常来说这些组件之间会进行互相通信，基本流程如下所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ghm4g3/1616115588241-7c7f556a-1526-43e4-847a-d78a70821f6b.png" alt="">&lt;/p>
&lt;p>组件之间为了通信，他们需要使用到 TLS 证书。假设我们已经有了一个部署好的集群，接下来让我们开始我们的破坏行为。&lt;/p>
&lt;pre>&lt;code>rm -rf /etc/kubernetes/
&lt;/code>&lt;/pre>
&lt;p>在 master 节点上，这个目录包含：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>etcd 的一组证书和 CA（在 &lt;code>/etc/kubernetes/pki/etcd&lt;/code> 目录下）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一组 kubernetes 的证书和 CA（在 &lt;code>/etc/kubernetes/pki&lt;/code> 目录下）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>还有 kube-controller-manager、kube-scheduler、cluster-admin 以及 kubelet 这些使用的 kubeconfig 文件&lt;/p>
&lt;/li>
&lt;li>
&lt;p>etcd、kube-apiserver、kube-scheduler 和 kube-controller-manager 的静态 Pod 资源清单文件（位于 &lt;code>/etc/kubernetes/manifests&lt;/code> 目录）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>现在我们就上面这些全都删除了，如果是在生产环境做了这样的操作，可能你现在正瑟瑟发抖吧~&lt;/p>
&lt;p>修复控制平面&lt;/p>
&lt;p>首先我也确保下我们的所有控制平面 Pod 已经停止了。&lt;/p>
&lt;pre>&lt;code># 如果你用 docker 也是可以的
crictl rm `crictl ps -aq`
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>注意：kubeadm 默认不会覆盖现有的证书和 kubeconfigs，为了重新颁发证书，你必须先手动删除旧的证书。&lt;/p>
&lt;/blockquote>
&lt;p>接下来我们首先恢复 etcd，在**一个 master **节点上执行下面的命令生成 etcd 集群的证书：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs etcd-ca --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令将为我们的 etcd 集群生成一个新的 CA，由于所有其他证书都必须由它来签署，我们也将把它和私钥复制到其他 master 节点(如果你是多 master)。&lt;/p>
&lt;pre>&lt;code>/etc/kubernetes/pki/etcd/ca.{key,crt}
&lt;/code>&lt;/pre>
&lt;p>接下来让我们在&lt;strong>所有 master&lt;/strong> 节点上为它重新生成其余的 etcd 证书和静态资源清单。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs etcd-healthcheck-client --config=kubeadm-config.yaml
kubeadm init phase certs etcd-peer --config=kubeadm-config.yaml
kubeadm init phase certs etcd-server --config=kubeadm-config.yaml
kubeadm init phase etcd local --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令执行后，你应该已经有了一个正常工作的 etcd 集群了。&lt;/p>
&lt;pre>&lt;code># crictl ps
CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID
ac82b4ed5d83a 0369cf4303ffd 2 seconds ago Running etcd 0 bc8b4d568751b
&lt;/code>&lt;/pre>
&lt;p>接下来我们对 Kubernetes 服务做同样的操作，在其中&lt;strong>一个 master&lt;/strong> 节点上执行如下的命令：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs all --config=kubeadm-config.yaml
kubeadm init phase kubeconfig all --config=kubeadm-config.yaml
kubeadm init phase control-plane all --config=kubeadm-config.yaml
rm -rf /root/.kube/*
cp -f /etc/kubernetes/admin.conf ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>上面的命令将生成 Kubernetes 的所有 SSL 证书，以及 Kubernetes 服务的静态 Pods 清单和 kubeconfigs 文件。&lt;/p>
&lt;p>如果你使用 kubeadm 加入 kubelet，你还需要更新 &lt;code>kube-public&lt;/code> 命名空间中的 cluster-info 配置，因为它仍然包含你的旧 CA 的哈希值。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase bootstrap-token --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>由于其他 master 节点上的所有证书也必须由单一 CA 签署，所以我们将其复制到其他控制面节点，并在每个节点上重复上述命令。&lt;/p>
&lt;pre>&lt;code>/etc/kubernetes/pki/{ca,front-proxy-ca}.{key,crt}
/etc/kubernetes/pki/sa.{key,pub}
&lt;/code>&lt;/pre>
&lt;p>顺便说一下，作为手动复制证书的替代方法，你也可以使用 Kubernetes API，如下所示的命令：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase upload-certs --upload-certs --config=kubeadm-config.yaml
# 上一条命令输出的 certificate-key 替换 ${MasterJoinKey}
kubeadm token create --ttl=2h --certificate-key=${MasterJoinKey} --print-join-command
&lt;/code>&lt;/pre>
&lt;p>该命令将加密并上传证书到 Kubernetes，时间为 2 小时，所以你可以按以下方式注册 master 节点：&lt;/p>
&lt;pre>&lt;code># 注意替换上面命令输出的 join 命令的内容
kubeadm join phase control-plane-prepare all kubernetes-apiserver:6443 --control-plane --token cs0etm.ua7fbmwuf1jz946l --discovery-token-ca-cert-hash sha256:555f6ececd4721fed0269d27a5c7f1c6d7ef4614157a18e56ed9a1fd031a3ab8 --certificate-key 385655ee0ab98d2441ba8038b4e8d03184df1806733eac131511891d1096be73
kubeadm join phase control-plane-join all
&lt;/code>&lt;/pre>
&lt;p>需要注意的是，Kubernetes API 还有一个配置，它为 &lt;code>front-proxy&lt;/code> 客户端持有 CA 证书，它用于验证从 apiserver 到 webhooks 和聚合层服务的请求。不过 kube-apiserver 会自动更新它。到在这个阶段，我们已经有了一个完整的控制平面了。&lt;/p>
&lt;h2 id="修复-kubelet">修复 kubelet&lt;/h2>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/
kubeadm init phase kubeconfig kubelet --config=kubeadm-config.yaml
kubeadm init phase kubelet-start --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;h2 id="修复工作节点">修复工作节点&lt;/h2>
&lt;p>现在我们可以使用下面的命令列出集群的所有节点：&lt;/p>
&lt;pre>&lt;code>kubectl get nodes
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>若报错：Unable to connect to the server: x509: certificate signed by unknown authority
删除 /root/.kube/config 文件，并重新拷贝一遍&lt;/p>
&lt;/blockquote>
&lt;p>当然正常现在所有节点的状态都是 NotReady，这是因为他们仍然还使用的是旧的证书，为了解决这个问题，我们将使用 kubeadm 来执行重新加入集群节点。&lt;/p>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/ /etc/kubernetes/kubelet.conf
kubeadm init phase kubeconfig kubelet --config=kubeadm-config.yaml
kubeadm init phase kubelet-start --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>但要加入工作节点，我们必须生成一个新的 token。&lt;/p>
&lt;pre>&lt;code>kubeadm token create --print-join-command
&lt;/code>&lt;/pre>
&lt;p>然后在工作节点分别执行下面的命令：&lt;/p>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/ /etc/kubernetes/pki/ /etc/kubernetes/kubelet.conf
kubeadm join phase kubelet-start kubernetes-apiserver:6443 --token cs0etm.ua7fbmwuf1jz946l --discovery-token-ca-cert-hash sha256:555f6ececd4721fed0269d27a5c7f1c6d7ef4614157a18e56ed9a1fd031a3ab8
&lt;/code>&lt;/pre>
&lt;p>上面的操作会把你所有的 kubelet 重新加入到集群中，它并不会影响任何已经运行在上面的容器，但是，如果集群中有多个节点并且不同时进行，则可能会遇到一种情况，即 kube-controller-mananger 开始从 NotReady 节点重新创建容器，并尝试在活动节点上重新调度它们。&lt;/p>
&lt;p>为了防止这种情况，我们可以暂时停掉 master 节点上的 controller-manager。&lt;/p>
&lt;pre>&lt;code>rm /etc/kubernetes/manifests/kube-controller-manager.yaml
crictl rmp `crictl ps --name kube-controller-manager -q`
&lt;/code>&lt;/pre>
&lt;p>一旦集群中的所有节点都被加入，你就可以为 controller-manager 生成一个静态资源清单，在所有 master 节点上运行下面的命令。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase control-plane controller-manager
&lt;/code>&lt;/pre>
&lt;p>如果 kubelet 被配置为请求由你的 CA 签署的证书(选项 serverTLSBootstrap: true)，你还需要批准来自 kubelet 的 CSR：&lt;/p>
&lt;pre>&lt;code>kubectl get csrkubectl certificate approve &amp;lt;csr&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="修复-serviceaccounts">修复 ServiceAccounts&lt;/h2>
&lt;p>因为我们丢失了 &lt;code>/etc/kubernetes/pki/sa.key&lt;/code> ，这个 key 用于为集群中所有 &lt;code>ServiceAccounts&lt;/code> 签署 &lt;code>jwt tokens&lt;/code>，因此，我们必须为每个 sa 重新创建 tokens。这可以通过类型为 &lt;code>kubernetes.io/service-account-token&lt;/code> 的 Secret 中删除 token 字段来完成。&lt;/p>
&lt;pre>&lt;code>kubectl get secret --all-namespaces | awk '/kubernetes.io\/service-account-token/ { print &amp;quot;kubectl patch secret -n &amp;quot; $1 &amp;quot; &amp;quot; $2 &amp;quot; -p {\\\&amp;quot;data\\\&amp;quot;:{\\\&amp;quot;token\\\&amp;quot;:null}}&amp;quot;}' | sh -x
&lt;/code>&lt;/pre>
&lt;p>删除之后，kube-controller-manager 会自动生成用新密钥签名的新令牌。不过需要注意的是并非所有的微服务都能即时更新 tokens，因此很可能需要手动重新启动使用 tokens 的容器。&lt;/p>
&lt;pre>&lt;code>kubectl get pod --field-selector 'spec.serviceAccountName!=default' --no-headers --all-namespaces | awk '{print &amp;quot;kubectl delete pod -n &amp;quot; $1 &amp;quot; &amp;quot; $2 &amp;quot; --wait=false --grace-period=0&amp;quot;}'
&lt;/code>&lt;/pre>
&lt;p>例如，这个命令会生成一个命令列表，会将所有使用非默认的 serviceAccount 的 Pod 删除，我建议从 kube-system 命名空间执行，因为 kube-proxy 和 CNI 插件都安装在这个命名空间中，它们对于处理你的微服务之间的通信至关重要。&lt;/p>
&lt;p>到这里我们的集群就恢复完成了。&lt;/p>
&lt;blockquote>
&lt;p>参考链接：&lt;a href="https://itnext.io/breaking-down-and-fixing-kubernetes-4df2f22f87c3">https://itnext.io/breaking-down-and-fixing-kubernetes-4df2f22f87c3&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Kubernetes排障图谱</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/kubernetes%E6%8E%92%E9%9A%9C%E5%9B%BE%E8%B0%B1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/kubernetes%E6%8E%92%E9%9A%9C%E5%9B%BE%E8%B0%B1/</guid><description>
&lt;p>自从用上这张图解指南后， Kubernetes 故障排除不再难！&lt;/p>
&lt;p>TonyBai&lt;/p>
&lt;p>与技术博客 tonybai.com 同源。近期关注 Kubernetes、Docker、Golang、儿童编程、DevOps、云计算平台和机器学习。&lt;/p>
&lt;p>下面是一个示意图，可帮助你调试 Kubernetes Deployment。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901226-fb732c1c-ed7a-4e18-8a64-e80a34b81dae.jpeg" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901221-7573c3fb-9ef4-4ad4-a6a8-d81a8e9917c7.jpeg" alt="">&lt;/p>
&lt;p>（如需此图解中文版或 PDF 版 ，请在公众号对话框回复关键字：「K8s 排障图解」，进行获取。）&lt;/p>
&lt;p>当你希望在 Kubernetes 中部署应用程序时，你通常会定义三个组件：&lt;/p>
&lt;p>•一个 Deployment - 这是一份用于创建你的应用程序的 Pod 副本的&amp;quot;食谱&amp;quot;；•一个 Service - 一个内部负载均衡器，用于将流量路由到内部的 Pod 上；•一个 Ingress - 描述如何流量应该如何从集群外部流入到集群内部的你的服务上。&lt;/p>
&lt;p>下面让我们用示意图快速总结一下要点。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901235-8630b724-d28b-48c5-95bc-27d79cff185a.jpeg" alt="">&lt;/p>
&lt;p>在 Kubernetes 中，你的应用程序通过两层负载均衡器暴露服务：内部的和外部的&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901221-5d8c7b6c-ca6d-436b-a06b-f742a930eea1.jpeg" alt="">&lt;/p>
&lt;p>内部的负载均衡器称为 Service，而外部的负载均衡器称为 Ingress&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901238-2e7a95cc-bf0f-4497-9809-f5597c97ad14.jpeg" alt="">&lt;/p>
&lt;p>Pod 不会直接部署。Deployment 会负责创建 Pod 并管理它们&lt;/p>
&lt;p>假设你要部署一个简单的 &amp;ldquo;HelloWorld&amp;rdquo; 应用，该应用的 YAML 文件的内容应该类似下面这样：&lt;/p>
&lt;p>// hello-world.yaml&lt;/p>
&lt;p>apiVersion: apps/v1kind: Deploymentmetadata: name: my-deployment labels: track: canaryspec: selector: matchLabels: any-name: my-app template: metadata: labels: any-name: my-app spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080&amp;mdash;apiVersion: v1kind: Servicemetadata: name: my-servicespec: ports: - port: 80 targetPort: 8080 selector: name: app&amp;mdash;apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: my-ingressspec: rules: - http: paths: - backend: serviceName: app servicePort: 80 path: /&lt;/p>
&lt;p>这个定义很长，组件之间的相互关系并不容易看出来。&lt;/p>
&lt;p>例如：&lt;/p>
&lt;p>•什么时候应使用端口 80，又是何时应使用端口 8080？•你是否应该为每个服务创建一个新端口以免它们相互冲突？•标签( label )名重要吗？它们是否在每一处都应该是一样的？&lt;/p>
&lt;p>在进行调试之前，让我们回顾一下这三个组件是如何相互关联的。&lt;/p>
&lt;p>让我们从 Deployment 和 Service 开始。&lt;/p>
&lt;p>连接 Deployment 和 Service&lt;/p>
&lt;p>令人惊讶的消息是，Service 和 Deployment 之间根本没有连接。&lt;/p>
&lt;p>事实是：Service 直接指向 Pod，并完全跳过了 Deployment。&lt;/p>
&lt;p>因此，你应该注意的是 Pod 和 Service 之间的相互关系。&lt;/p>
&lt;p>你应该记住三件事：&lt;/p>
&lt;p>•Service selector 应至少与 Pod 的一个标签匹配；•Service 的 targetPort 应与 Pod 中容器的 containerPort 匹配；•Service 的 port 可以是任何数字。多个 Service 可以使用同一端口号，因为它们被分配了不同的 IP 地址。&lt;/p>
&lt;p>下面的图总结了如何连接端口：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901239-06faa388-61a9-488a-9619-bcfc53411330.jpeg" alt="">&lt;/p>
&lt;p>考虑上面被一个服务暴露的 Pod&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901301-a142530b-b355-4633-bc12-048d70b3cac5.jpeg" alt="">&lt;/p>
&lt;p>创建 Pod 时，应为 Pod 中的每个容器定义 containerPort 端口&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901395-afa14958-45e9-4504-83d7-a5d8900afda0.jpeg" alt="">&lt;/p>
&lt;p>当创建一个 Service 时，你可以定义 port 和 targetPort，但是哪个用来连接容器呢？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901261-b1d2694e-e419-4414-8a8f-b9d0fb2422b5.jpeg" alt="">&lt;/p>
&lt;p>targetPort 和 containerPort 应该始终保持匹配&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901248-74768780-b6dc-4ed2-8826-0a435b4292b2.jpeg" alt="">&lt;/p>
&lt;p>如果容器暴露 3000 端口(containerPort)，那么 targetPort 应该匹配这一个端口号&lt;/p>
&lt;p>再来看看 YAML，标签和 ports/targetPort 应该匹配：&lt;/p>
&lt;p>// hello-world.yaml&lt;/p>
&lt;p>apiVersion: apps/v1kind: Deploymentmetadata: name: my-deployment labels: track: canaryspec: selector: matchLabels: any-name: my-app template: metadata: labels: any-name: my-app spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080&amp;mdash;apiVersion: v1kind: Servicemetadata: name: my-servicespec: ports: - port: 80 targetPort: 8080 selector: any-name: my-app&lt;/p>
&lt;p>那 deployment 顶部的 track: canary 标签呢?&lt;/p>
&lt;p>它也应该匹配吗？&lt;/p>
&lt;p>该标签属于 deployment，service 的选择器未使用它来路由流量。&lt;/p>
&lt;p>换句话说，你可以安全地删除它或为其分配其他值。&lt;/p>
&lt;p>那 matchLabels 选择器呢？&lt;/p>
&lt;p>它必须始终与 Pod 的标签匹配，并且被 Deployment 用来跟踪 Pod。&lt;/p>
&lt;p>假设你已经进行了所有正确的设置，该如何测试它呢？&lt;/p>
&lt;p>你可以使用以下命令检查 Pod 是否具有正确的标签：&lt;/p>
&lt;p>$ kubectl get pods &amp;ndash;show-labels&lt;/p>
&lt;p>或者，如果你拥有属于多个应用程序的 Pod：&lt;/p>
&lt;p>$ kubectl get pods &amp;ndash;selector any-name=my-app &amp;ndash;show-labels&lt;/p>
&lt;p>any-name=my-app 就是标签：any-name: my-app。&lt;/p>
&lt;p>还有问题吗？&lt;/p>
&lt;p>你也可以连接到 Pod！&lt;/p>
&lt;p>你可以使用 kubectl 中的 port-forward 命令连接到 service 并测试连接。&lt;/p>
&lt;p>$ kubectl port-forward service/&lt;!-- raw HTML omitted --> 3000:80&lt;/p>
&lt;p>•service/ 是服务的名称- 在上面的 YAML 中是 “my-service”•3000 是你希望在计算机上打开的端口•80 是 service 通过 port 字段暴露的端口&lt;/p>
&lt;p>如果可以连接，则说明设置正确。&lt;/p>
&lt;p>如果不行，则很可能是你填写了错误的标签或端口不匹配。&lt;/p>
&lt;p>连接 Service 和 Ingress&lt;/p>
&lt;p>接下来是配置 Ingress 以将你的应用暴露到集群外部。&lt;/p>
&lt;p>Ingress 必须知道如何检索服务，然后检索 Pod 并将流量路由给它们。&lt;/p>
&lt;p>Ingress 按名字和暴露的端口检索正确的服务。&lt;/p>
&lt;p>在 Ingress 和 Service 中应该匹配两件事：&lt;/p>
&lt;p>•Ingress 的 servicePort 应该匹配 service 的 port；•Ingress 的 serviceName 应该匹配服务的 name。&lt;/p>
&lt;p>下面的图总结了如何连接端口：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901290-fcc7652d-8d5b-4f06-b43f-98b877c56e19.jpeg" alt="">&lt;/p>
&lt;p>你已经知道 servive 暴露一个 port&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901253-35bc52fa-6d7a-44cb-963b-d77f475e1779.jpeg" alt="">&lt;/p>
&lt;p>Ingress 有一个字段叫 servicePort&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901314-eefaa4c1-0bfb-4ca1-92ee-5c9c85331a0e.jpeg" alt="">&lt;/p>
&lt;p>service 的 port 和 Ingress 的 service 应该始终保持匹配&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901257-a63037d3-458c-4a60-ad0a-237312494c98.jpeg" alt="">&lt;/p>
&lt;p>如果你为 service 指定的 port 是 80，那么你也应该将 ingress 的 servicePort 改为 80&lt;/p>
&lt;p>实践中，你应该查看以下几行(下面代码中的 my-service 和 80)：&lt;/p>
&lt;p>// hello-world.yaml&lt;/p>
&lt;p>apiVersion: v1kind: Servicemetadata: name: my-service &amp;mdash; 需关注 spec: ports: - port: 80 &amp;mdash; 需关注 targetPort: 8080 selector: any-name: my-app&amp;mdash;apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: my-ingressspec: rules: - http: paths: - backend: serviceName: my-service &amp;mdash; 需关注 servicePort: 80 &amp;mdash; 需关注 path: /&lt;/p>
&lt;p>你如何测试 Ingress 是否正常工作呢？&lt;/p>
&lt;p>你可以使用与以前相同的策略 kubectl port-forward，但是这次你应该连接到 Ingress 控制器，而不是连接到 Service。&lt;/p>
&lt;p>首先，使用以下命令检索 Ingress 控制器的 Pod 名称：&lt;/p>
&lt;p>$ kubectl get pods &amp;ndash;all-namespacesNAMESPACE NAME READY STATUSkube-system coredns-5644d7b6d9-jn7cq 1/1 Runningkube-system etcd-minikube 1/1 Runningkube-system kube-apiserver-minikube 1/1 Runningkube-system kube-controller-manager-minikube 1/1 Runningkube-system kube-proxy-zvf2h 1/1 Runningkube-system kube-scheduler-minikube 1/1 Runningkube-system nginx-ingress-controller-6fc5bcc 1/1 Running&lt;/p>
&lt;p>标识 Ingress Pod（可能在其他命名空间中）并描述它以检索端口：&lt;/p>
&lt;p>$ kubectl describe pod nginx-ingress-controller-6fc5bcc \ &amp;ndash;namespace kube-system \ | grep PortsPorts: 80/TCP, 443/TCP, 18080/TCP&lt;/p>
&lt;p>最后，连接到 Pod：&lt;/p>
&lt;p>$ kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 &amp;ndash;namespace kube-system&lt;/p>
&lt;p>此时，每次你访问计算机上的端口 3000 时，请求都会转发到 Ingress 控制器 Pod 上的端口 80。&lt;/p>
&lt;p>如果访问 http://localhost:3000，则应找到提供网页服务的应用程序。&lt;/p>
&lt;p>回顾 Port&lt;/p>
&lt;p>快速回顾一下哪些端口和标签应该匹配：&lt;/p>
&lt;p>•service selector 应与 Pod 的标签匹配•service 的 targetPort 应与 Pod 中容器的 containerPort 匹配•service 的端口可以是任何数字。多个服务可以使用同一端口，因为它们分配了不同的 IP 地址。•ingress 的 servicePort 应该匹配 service 的 port•serivce 的名称应与 ingress 中的 serviceName 字段匹配&lt;/p>
&lt;p>知道如何构造 YAML 定义只是故事的一部分。&lt;/p>
&lt;p>出了问题后该怎么办？&lt;/p>
&lt;p>Pod 可能无法启动，或者正在崩溃。&lt;/p>
&lt;p>Kubernetes Deployment 故障排除的 3 个步骤&lt;/p>
&lt;p>在深入研究失败的 Deployment 之前，我们必须对 Kubernetes 的工作原理有一个明确定义的思维模型。&lt;/p>
&lt;p>由于每个 Deployment 中都有三个组件，因此你应该自下而上依次调试所有组件。&lt;/p>
&lt;p>•你应该先确保 Pods 正在运行•然后，专注于让 service 将流量路由到到正确的 Pod•然后，检查是否正确配置了 Ingress&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901281-fa8c53a0-9a07-47d6-a373-a52ceb1950f3.jpeg" alt="">&lt;/p>
&lt;p>你应该从底部开始对 deployment 进行故障排除。首先，检查 Pod 是否已就绪并正在运行。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901283-cadfbf2b-a459-4ef4-a5b3-c514192a38b0.jpeg" alt="">&lt;/p>
&lt;p>如果 Pod 已就绪，则应调查 service 是否可以将流量分配给 Pod。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dliags/1616114901291-972691dc-8e6d-49d0-b28a-a527e43036c6.jpeg" alt="">&lt;/p>
&lt;p>最后，你应该检查 service 与 ingress 之间的连接。&lt;/p>
&lt;ol>
&lt;li>Pod 故障排除&lt;/li>
&lt;/ol>
&lt;p>在大多数情况下，问题出在 Pod 本身。&lt;/p>
&lt;p>你应该确保 Pod 正在运行并准备就绪。&lt;/p>
&lt;p>该如何检查呢？&lt;/p>
&lt;p>$ kubectl get podsNAME READY STATUS RESTARTS AGEapp1 0/1 ImagePullBackOff 0 47happ2 0/1 Error 0 47happ3-76f9fcd46b-xbv4k 1/1 Running 1 47h&lt;/p>
&lt;p>在上述会话中，最后一个 Pod 处于就绪并正常运行的状态；但是，前两个 Pod 既不处于 Running 也不是 Ready。&lt;/p>
&lt;p>你如何调查出了什么问题？&lt;/p>
&lt;p>有四个有用的命令可以对 Pod 进行故障排除：&lt;/p>
&lt;p>•kubectl logs 有助于检索 Pod 容器的日志•kubectl describe pod 检索与 Pod 相关的事件列表很有用•kubectl get pod 用于提取存储在 Kubernetes 中的 Pod 的 YAML 定义•kubectl exec -ti bash 在 Pod 的一个容器中运行交互式命令很有用&lt;/p>
&lt;p>应该使用哪一个呢？&lt;/p>
&lt;p>没有一种万能的。&lt;/p>
&lt;p>相反，我们应该结合着使用它们。&lt;/p>
&lt;p>常见 Pod 错误&lt;/p>
&lt;p>Pod 可能会出现启动和运行时错误。&lt;/p>
&lt;p>启动错误包括：&lt;/p>
&lt;p>•ImagePullBackoff•ImageInspectError•ErrImagePull•ErrImageNeverPull•RegistryUnavailable•InvalidImageName&lt;/p>
&lt;p>运行时错误包括：&lt;/p>
&lt;p>•CrashLoopBackOff•RunContainerError•KillContainerError•VerifyNonRootError•RunInitContainerError•CreatePodSandboxError•ConfigPodSandboxError•KillPodSandboxError•SetupNetworkError•TeardownNetworkError&lt;/p>
&lt;p>有些错误比其他错误更常见。&lt;/p>
&lt;p>以下是最常见的错误列表以及如何修复它们的方法。&lt;/p>
&lt;p>ImagePullBackOff&lt;/p>
&lt;p>当 Kubernetes 无法获取到 Pod 中某个容器的镜像时，将出现此错误。&lt;/p>
&lt;p>共有三个可能的原因：&lt;/p>
&lt;p>•镜像名称无效-例如，你拼错了名称，或者 image 不存在•你为 image 指定了不存在的标签•你尝试检索的 image 属于一个私有 registry，而 Kubernetes 没有凭据可以访问它&lt;/p>
&lt;p>前两种情况可以通过更正 image 名称和标记来解决。&lt;/p>
&lt;p>针对第三种情况，你应该将私有 registry 的访问凭证通过 Secret 添加到 k8s 中并在 Pod 中引用它。&lt;/p>
&lt;p>官方文档中有一个有关如何实现此目标的示例。&lt;/p>
&lt;p>CrashLoopBackOff&lt;/p>
&lt;p>如果容器无法启动，则 Kubernetes 将显示错误状态为：CrashLoopBackOff。&lt;/p>
&lt;p>通常，在以下情况下容器无法启动：&lt;/p>
&lt;p>•应用程序中存在错误，导致无法启动•你未正确配置容器•Liveness 探针失败太多次&lt;/p>
&lt;p>你应该尝试从该容器中检索日志以调查其失败的原因。&lt;/p>
&lt;p>如果由于容器重新启动太快而看不到日志，则可以使用以下命令：&lt;/p>
&lt;p>$ kubectl logs &lt;!-- raw HTML omitted --> &amp;ndash;previous&lt;/p>
&lt;p>这个命令打印前一个容器的错误消息。&lt;/p>
&lt;p>RunContainerError&lt;/p>
&lt;p>当容器无法启动时，出现此错误。&lt;/p>
&lt;p>甚至在容器内的应用程序启动之前。&lt;/p>
&lt;p>该问题通常是由于配置错误，例如：&lt;/p>
&lt;p>•挂载不存在的卷，例如 ConfigMap 或 Secrets•将只读卷安装为可读写&lt;/p>
&lt;p>你应该使用 kubectl describe pod 命令收集和分析错误。&lt;/p>
&lt;p>处于 Pending 状态的 Pod&lt;/p>
&lt;p>当创建 Pod 时，该 Pod 保持 Pending 状态。&lt;/p>
&lt;p>为什么？&lt;/p>
&lt;p>假设你的调度程序组件运行良好，可能的原因如下：&lt;/p>
&lt;p>•集群没有足够的资源（例如 CPU 和内存）来运行 Pod•当前的命名空间具有 ResourceQuota 对象，创建 Pod 将使命名空间超过配额•该 Pod 绑定到一个处于 pending 状态的 PersistentVolumeClaim&lt;/p>
&lt;p>最好的选择是检查 kubectl describe 命令输出的“事件”部分内容：&lt;/p>
&lt;p>$ kubectl describe pod &lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>对于因 ResourceQuotas 而导致的错误，可以使用以下方法检查集群的日志：&lt;/p>
&lt;p>$ kubectl get events &amp;ndash;sort-by=.metadata.creationTimestamp&lt;/p>
&lt;p>处于未就绪状态的 Pod&lt;/p>
&lt;p>如果 Pod 正在运行但未就绪(not ready)，则表示 readiness 就绪探针失败。&lt;/p>
&lt;p>当“就绪”探针失败时，Pod 未连接到服务，并且没有流量转发到该实例。&lt;/p>
&lt;p>就绪探针失败是应用程序的特定错误，因此你应检查 kubectl describe 中的“ 事件”部分以识别错误。&lt;/p>
&lt;ol start="2">
&lt;li>服务的故障排除&lt;/li>
&lt;/ol>
&lt;p>如果你的 Pod 正在运行并处于就绪状态，但仍无法收到应用程序的响应，则应检查服务的配置是否正确。&lt;/p>
&lt;p>service 旨在根据流量的标签将流量路由到 Pod。&lt;/p>
&lt;p>因此，你应该检查的第一件事是服务关联了多少个 Pod。&lt;/p>
&lt;p>你可以通过检查服务中的端点( endpoint )来做到这一点：&lt;/p>
&lt;p>$ kubectl describe service &lt;!-- raw HTML omitted --> | grep Endpoints&lt;/p>
&lt;p>端点是一对，并且在服务（至少）以 Pod 为目标时，应该至少有一个端点。&lt;/p>
&lt;p>如果“端点”部分为空，则有两种解释：&lt;/p>
&lt;p>•你没有运行带有正确标签的 Pod（提示：你应检查自己是否在正确的命名空间中）•service 的 selector 标签上有错字&lt;/p>
&lt;p>如果你看到端点列表，但仍然无法访问你的应用程序，则 targetPort 可能是你服务中的罪魁祸首。&lt;/p>
&lt;p>你如何测试服务？&lt;/p>
&lt;p>无论服务类型如何，你都可以使用 kubectl port-forward 来连接它：&lt;/p>
&lt;p>$kubectl port-forward service/&lt;!-- raw HTML omitted --> 3000:80&lt;/p>
&lt;p>这里：&lt;/p>
&lt;p>• 是服务的名称•3000 是你希望在计算机上打开的端口•80 是服务公开的端口&lt;/p>
&lt;p>3.Ingress 的故障排除&lt;/p>
&lt;p>如果你已到达本节，则：&lt;/p>
&lt;p>•Pod 正在运行并准备就绪•服务会将流量分配到 Pod&lt;/p>
&lt;p>但是你仍然看不到应用程序的响应。&lt;/p>
&lt;p>这意味着最有可能是 Ingress 配置错误。&lt;/p>
&lt;p>由于正在使用的 Ingress 控制器是集群中的第三方组件，因此有不同的调试技术，具体取决于 Ingress 控制器的类型。&lt;/p>
&lt;p>但是在深入研究 Ingress 专用工具之前，你可以用一些简单的方法进行检查。&lt;/p>
&lt;p>Ingress 使用 serviceName 和 servicePort 连接到服务。&lt;/p>
&lt;p>你应该检查这些配置是否正确。&lt;/p>
&lt;p>你可以通过下面命令检查 Ingress 配置是否正确：&lt;/p>
&lt;p>$kubectl describe ingress &lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>如果 backend 一列为空，则配置中必然有一个错误。&lt;/p>
&lt;p>如果你可以在 “backend” 列中看到端点，但是仍然无法访问该应用程序，则可能是以下问题：&lt;/p>
&lt;p>•你如何将 Ingress 暴露于公共互联网•你如何将集群暴露于公共互联网&lt;/p>
&lt;p>你可以通过直接连接到 Ingress Pod 来将基础结构问题与 Ingress 隔离开。&lt;/p>
&lt;p>首先，获取你的 Ingress 控制器 Pod（可以位于其他名称空间中）：&lt;/p>
&lt;p>$ kubectl get pods &amp;ndash;all-namespacesNAMESPACE NAME READY STATUSkube-system coredns-5644d7b6d9-jn7cq 1/1 Runningkube-system etcd-minikube 1/1 Runningkube-system kube-apiserver-minikube 1/1 Runningkube-system kube-controller-manager-minikube 1/1 Runningkube-system kube-proxy-zvf2h 1/1 Runningkube-system kube-scheduler-minikube 1/1 Runningkube-system nginx-ingress-controller-6fc5bcc 1/1 Running&lt;/p>
&lt;p>描述它以检索端口：&lt;/p>
&lt;h1 id="kubectl-describe-pod-nginx-ingress-controller-6fc5bcc---namespace-kube-system---grep-ports">kubectl describe pod nginx-ingress-controller-6fc5bcc &amp;ndash;namespace kube-system \ | grep Ports&lt;/h1>
&lt;p>最后，连接到 Pod：&lt;/p>
&lt;p>$ kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 &amp;ndash;namespace kube-system&lt;/p>
&lt;p>此时，每次你访问计算机上的端口 3000 时，请求都会转发到 Pod 上的端口 80。&lt;/p>
&lt;p>现在可以用吗？&lt;/p>
&lt;p>•如果可行，则问题出在基础架构中。你应该调查流量如何路由到你的集群。•如果不起作用，则问题出在 Ingress 控制器中。你应该调试 Ingress。&lt;/p>
&lt;p>如果仍然无法使 Ingress 控制器正常工作，则应开始对其进行调试。&lt;/p>
&lt;p>目前有许多不同版本的 Ingress 控制器。&lt;/p>
&lt;p>热门选项包括 Nginx，HAProxy，Traefik 等。&lt;/p>
&lt;p>你应该查阅 Ingress 控制器的文档以查找故障排除指南。&lt;/p>
&lt;p>由于 Ingress Nginx 是最受欢迎的 Ingress 控制器，因此在下一部分中我们将介绍一些有关调试 ingress-nginx 的技巧。&lt;/p>
&lt;p>调试 Ingress Nginx&lt;/p>
&lt;p>Ingress-nginx 项目有一个 Kubectl 的官方插件。&lt;/p>
&lt;p>你可以用 kubectl ingress-nginx 来：&lt;/p>
&lt;p>•检查日志，后端，证书等。•连接到 ingress•检查当前配置&lt;/p>
&lt;p>你应该尝试的三个命令是：&lt;/p>
&lt;p>•kubectl ingress-nginx lint，它会检查 nginx.conf•kubectl ingress-nginx backend，以检查后端（类似于 kubectl describe ingress ）•kubectl ingress-nginx logs，查看日志&lt;/p>
&lt;p>请注意，你可能需要为 Ingress 控制器指定正确的名称空间 &amp;ndash;namespace 。&lt;/p>
&lt;p>总结&lt;/p>
&lt;p>如果你不知道从哪里开始，那么在 Kubernetes 中进行故障排除可能是一项艰巨的任务。&lt;/p>
&lt;p>你应该始终牢记从下至上解决问题：从 Pod 开始，然后通过 Service 和 Ingress 向上移动堆栈。&lt;/p>
&lt;p>你在本文中了解到的调试技术也可以应用于其他对象，例如：&lt;/p>
&lt;p>•failing Job 和 CronJob•StatefulSets 和 DaemonSets&lt;/p>
&lt;p>本文翻译自 learnk8s 上的文章 A visual guide on troubleshooting Kubernetes deployments 。&lt;/p>
&lt;p>如需 「Kubernetes 故障排除图解」中文版或 PDF 版本 ，请在公众号对话框回复关键字：「K8s 排障图解」，进行获取。&lt;/p>
&lt;p>本文转载自：「TonyBai」，原文：&lt;a href="https://url.cn/5GQdvKB">https://url.cn/5GQdvKB&lt;/a>，版权归原作者所有。欢迎投稿，投稿邮箱: &lt;a href="mailto:editor@hi-linux.com">editor@hi-linux.com&lt;/a> 。&lt;/p></description></item><item><title>Docs: 故障处理技巧</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">官方文档,监控、日志和调试-调试运行中的 Pods&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubernetes 作为分布式容器调度系统荣，难免出现问题。&lt;/p>
&lt;p>Kubernetes 让运维管理设备的方式发生了根本的转变，从一台一台设备登录，变为统一管理。在 1.19 版本官方文档的 &lt;a href="https://v1-19.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#node-shell-session">debug 章节&lt;/a>中，提到了这么一句话：&lt;/p>
&lt;blockquote>
&lt;p>If none of these approaches work, you can find the host machine that the pod is running on and SSH into that host, but this should generally not be necessary given tools in the Kubernetes API. Therefore, if you find yourself needing to ssh into a machine, please file a feature request on GitHub describing your use case and why these tools are insufficient.&lt;/p>
&lt;/blockquote>
&lt;p>Kubernetes 集群会积极推进让维护工作不再通过登录每一台设备才能进行调试。&lt;/p>
&lt;h1 id="pod-无法启动时让其强制启动">Pod 无法启动时，让其强制启动&lt;/h1>
&lt;p>当我们发现 Pod 无法启动时，除了日常通过 kubectl 命令，查看日志等常规手段以外，还有可能需要让 Pod 强制启动，以便更深入排障&lt;/p>
&lt;p>首先，编辑 Pod 的控制器，删除各种探针&lt;/p>
&lt;p>然后利用 pod.spec.containers.tty 和 pod.spec.containers.command 两个字段，为 Pod 分配一个终端，并保持 Pod 运行状态&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">myapp-bj-test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">command&lt;/span>: [&lt;span style="color:#ae81ff">sh]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tty&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这时，我们就可以通过 kubectl exec 命令进入这个容器中，进行各种调试了。&lt;/p>
&lt;h1 id="kubernetes-debug">Kubernetes Debug&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://mp.weixin.qq.com/s/PrmR-7vub9oVz-EFEZTGaQ">https://mp.weixin.qq.com/s/PrmR-7vub9oVz-EFEZTGaQ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/iPXKS36GKzfd404oT39vrQ">https://mp.weixin.qq.com/s/iPXKS36GKzfd404oT39vrQ&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>曾几何时，我们将自己的应用运行在 Kubernetes 上，每当出现容器异常崩溃时，我们往往都是一边重启容器，一边面对崩溃的容器无从下手。通常在业务研发自己 build 的镜像内包含了 shell，我们还能通过在 command 中嵌入一个[&amp;ldquo;sleep&amp;rdquo;, &amp;ldquo;3600&amp;rdquo;]命令来阻塞容器内服务启动，不过也有时候会出现不知道从哪里冒出来一个 distroless 镜像，这时可能最先崩溃的就是运维了。那是一种运维这个职业自诞生以来，第一次感受到手足无措并脱离掌控的无助感。于是在 k8s 环境下无法 debug 容器的梗开始在坊间广为吐槽。
第一个打破魔咒的是 kubectl-debug，它包含了&lt;strong>agent&lt;/strong>和&lt;strong>debug-tools&lt;/strong>两个部分。也是目前全网内搜到文档最全的解决方案。不过目前它的开发似乎已经停止，上一次提交还是在 8 个月之前，而最近一次 Release 版本也停留在两年前。更难以接受的是，当前它无法被集成在容器运行时为 Containerd 的 k8s 集群。
尽管 kubectl-debug 曾经确实是一款非常好用的容器调试工具，但如今 Kubernetes 已经有了更好的容器调试解决方案，Ephemeral Containers&lt;/p>
&lt;h2 id="ephemeral-containers">Ephemeral Containers&lt;/h2>
&lt;p>Ephemeral Containers 字如其名，它就是一个临时容器。这是一个自 Kubernetes v1.16 中作为 alpha 引入的新功能，虽然当前它还没有 GA，不过自从在 Kubernetes v1.18 之后，在 kubectl 内已经集成了 debug 客户端，我们几乎可以完整的使用并体验它的新特性。
临时容器的目标是为 Kubernetes 用户提供一个故障诊断工具，同时具备满足以下需求：&lt;/p>
&lt;ul>
&lt;li>作为一个开箱即用的平台化工具&lt;/li>
&lt;li>不依赖于已经包含在容器镜像中的工具&lt;/li>
&lt;li>不需要直接登陆计算节点(可以通过 Kubernetes API 的管理访问 Node)&lt;/li>
&lt;/ul>
&lt;p>不过也有东西是临时容器不计划支持的，比如对 windows 上启用临时容器就不太友好。&lt;/p>
&lt;p>启用临时容器的特性也非常简单，在 kubernetes v1.16 之后的版本中将启动参数&amp;ndash;feature-gates=EphemeralContainers=true 配置到 kube-api 和 kubelet 服务上重启即可。&lt;/p>
&lt;p>在 1.20 之前，kubectl debug 工具被放在 alpha 中，注意不同版本的命令操作差别 这里推荐使用客户端为 1.20+的版本体验会更好&lt;/p>
&lt;p>那么我们有了 Ephemeral Containers 能做哪些事情呢？&lt;/p>
&lt;h3 id="1-pod-troubleshooting">1. POD Troubleshooting&lt;/h3>
&lt;p>如上文所说，我们可以直接通过 kubectl debug 命令进行容器调试。最直接简单的对一个 pod 进行调试命令如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug mypod -it --image&lt;span style="color:#f92672">=&lt;/span>busybox
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>默认情况下用户不指定临时容器名称的话，debug 容器名称就由 kubectl 自动生成一个唯一 id 的名称。如果用户需要自己指定容器名称则使用&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug mypod -c debugger --image&lt;span style="color:#f92672">=&lt;/span>busybox
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>有了临时容器除了日常 debug 功能外，我们可以扩展出很多新花样的玩法。比如批量跑某个命名空间下的安全扫描的脚本而不用干扰原容器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> pod in &lt;span style="color:#66d9ef">$(&lt;/span>kubectl get -o name pod&lt;span style="color:#66d9ef">)&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl debug --image security/pod_scanner -p $pod /sanner.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="2-pod-troubleshooting-by-copy">2. POD Troubleshooting by Copy&lt;/h4>
&lt;p>对于没有开启 Ephemeral Containers 特性的集群，我们就只能通过复制模式来调试容器。它的原理是复制一个指定 pod 的新容器，并将 debug 作为 sidecar 跟随新容器一起启动。通过这种方式也能达到曲线救国的目的。此种方式的几个参数还是挺有意思：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>--copy-to 指定新pod的名称
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--replace&lt;span style="color:#f92672">=&lt;/span>true 是否删除原容器
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--same-node&lt;span style="color:#f92672">=&lt;/span>true 是否调度到和原容器一样的node上
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--share-processes&lt;span style="color:#f92672">=&lt;/span>true 是否共享容器pid空间
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>例如我们就可以启动一个跟需要调试 pod 一样配置的 debug 容器如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug mypod -it &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--container&lt;span style="color:#f92672">=&lt;/span>debug &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--image&lt;span style="color:#f92672">=&lt;/span>busybox &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--copy-to&lt;span style="color:#f92672">=&lt;/span>my-debugger &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--same-node&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--share-processes&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3-node-troubleshooting">3. Node Troubleshooting&lt;/h4>
&lt;p>对！你没看错！利用 Ephemeral Containers 还能对 Worker 节点进行调试。当以节点为目标调用时，kubectl debug 将创建一个带有 node 名称的 pod，并且调度到该节点。同时该容器还具备了 hostIPC、hostNetwork 和 hostPID 这些特权模式。不可思议的是 Worker 节点的根文件系统还被 mount 到了 debug 容器下的/host 目录下。
直接执行这个命令就能 debug 主机。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug node/mynode -it --image&lt;span style="color:#f92672">=&lt;/span>busybox
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="debug-镜像">Debug 镜像&lt;/h2>
&lt;p>工欲善其事，必先利其器。不管怎样我们都需要一套工具完善的 debug 镜像，在处理问题时能够得心应手。虽然网上也有不少 debug 镜像，不过都还是不如自己构建来的畅快。
这里小白分享一个 Debug 镜像的 Dockerfile，大家可以根据自己条件修改即可。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>FROM golang:alpine as grpcurl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ENV XXX 添加 go 代理
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RUN apk update &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> apk add --virtual build-dependencies git &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> apk add bash curl jq &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> go get -u github.com/fullstorydev/grpcurl &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>FROM alpine:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RUN sed -i &lt;span style="color:#e6db74">&amp;#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g&amp;#39;&lt;/span> /etc/apk/repositories &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> apk update &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> apk add --no-cache vim bash tcpdump curl wget strace mysql-client iproute2 redis jq iftop tzdata tar nmap bind-tools htop &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RUN wget -O /usr/bin/httpstat https://github.com/davecheney/httpstat/releases/download/v1.0.0/httpstat-linux-amd64-v1.0.0 &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> chmod +x /usr/bin/httpstat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>COPY --from&lt;span style="color:#f92672">=&lt;/span>grpcurl /go/bin/grpcurl /usr/bin/grpcurl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ENV TZ&lt;span style="color:#f92672">=&lt;/span>Asia/Shanghai LC_ALL&lt;span style="color:#f92672">=&lt;/span>C.UTF-8 LANG&lt;span style="color:#f92672">=&lt;/span>C.UTF-8 LANGUAGE&lt;span style="color:#f92672">=&lt;/span>C.UTF-8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ENTRYPOINT &lt;span style="color:#f92672">[&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/bin/bash&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker build -t lchdzh/k8s-debug:v1 .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug -n ingress-controller nginx-hw-cloud-ingress-nginx-controller-85m49 -it &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--container&lt;span style="color:#f92672">=&lt;/span>debug &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--image&lt;span style="color:#f92672">=&lt;/span>lchdzh/k8s-debug:v1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--copy-to&lt;span style="color:#f92672">=&lt;/span>my-debugger &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--same-node&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--share-processes&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>``debug 镜像内支持的工具包如下图
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ib9dxg/1627884711514-0a090b8c-a82b-481f-ac33-960e41a91080.png" alt="image.png">&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文主要讲述了 kubernetes 在 v1.18 版本之后被提上 alpha 的 Ephemeral Containers 特性，通过临时容器我们可以 debug 容器，甚至还可以 debug 主机。它确实是一个非常方便和足以替代 kubectl-debug 的解决方案。不过，目前临时容器对于用户权限这块并没有特别的说明，特别是用特权模式调试主机的时候，希望后面能够借助 PSP（Pod Security Policy）做一个额外的补充。&lt;/p>
&lt;h1 id="heading">&lt;/h1></description></item></channel></rss>