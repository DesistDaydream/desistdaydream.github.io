<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Kubernetes 管理案例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/</link><description>Recent content in Kubernetes 管理案例 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Etcd 备份与恢复</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/etcd-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/etcd-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</guid><description>
&lt;p>系统环境：&lt;/p>
&lt;ul>
&lt;li>Etcd 版本：3.4.3&lt;/li>
&lt;li>Kubernetes 版本：1.18.8&lt;/li>
&lt;li>Kubernetes 安装方式：Kubeadm&lt;/li>
&lt;/ul>
&lt;h1 id="备份-etcd-数据">备份 Etcd 数据&lt;/h1>
&lt;p>本人采用的是 Kubeadm 安装的 Kubernetes 集群，采用镜像方式部署的 Etcd，所以操作 Etcd 需要使用 Etcd 镜像提供的 Etcdctl 工具。如果是非镜像方式部署 Etcd，可以直接使用 Etcdctl 命令备份数据。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 备份现有 Etcd 数据和manifests&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p /root/backup/kubernetes/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp -r /var/lib/etcd/member /root/backup/kubernetes/member-&lt;span style="color:#66d9ef">$(&lt;/span>date +%F&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp -r /etc/kubernetes/manifests /root/backup/kubernetes/manifests-&lt;span style="color:#66d9ef">$(&lt;/span>date +%F&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 通过运行 Etcd 镜像，并且使用镜像内部的 etcdctl 工具连接 etcd 集群，执行数据快照备份：&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run --rm --name etcdctl &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /root/backup/kubernetes:/backup &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd:ro &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --env ETCDCTL_API&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> registry.aliyuncs.com/k8sxio/etcd:3.4.13-0 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /bin/sh -c &lt;span style="color:#e6db74">&amp;#34;etcdctl --endpoints=https://172.38.40.212:2379 \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --cacert=/etc/kubernetes/pki/etcd/ca.crt \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> snapshot save /backup/etcd.db-&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>date +%F&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="恢复-etcd-数据">恢复 Etcd 数据&lt;/h1>
&lt;p>注意：恢复数据前先停止 Kubernetes 相关组件！！防止恢复数据过程还会持续写入数据导致问题。然后进入 Etcd 镜像使用 etcdctl 工具执行恢复操作。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 停止所有 master 节点上 k8s 系统组件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 移除且备份 /etc/kubernetes/manifests 目录&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ mv /etc/kubernetes/manifests /root/backup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 查看 kube-apiserver、etcd 镜像是否停止&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ docker ps|grep etcd &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> docker ps|grep kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理已经损坏的 etcd 数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rm -rf /var/lib/etcd/member
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>恢复 Etcd 数据
运行 Etcd 镜像，然后执行数据恢复，默认会恢复到 /default.etcd/member/ 目录下，这里使用 mv 命令在移动到挂载目录 /var/lib/etcd/ 下。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 下面命令的 TIME 变量改成想要恢复的数据哪个时间的&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 在其中一个节点上恢复数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker run --rm &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-v /root/backup:/backup &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-v /var/lib/etcd:/var/lib/etcd &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--env ETCDCTL_API&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>registry.aliyuncs.com/k8sxio/etcd:3.4.3-0 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>/bin/sh -c &lt;span style="color:#e6db74">&amp;#34;etcdctl snapshot restore /backup/etcd.db-&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>TIME&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">; mv /default.etcd/member/ /var/lib/etcd/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 在其他 master 节点上，将数据拷贝过来&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>scp -r 172.38.40.212:/var/lib/etcd/member /var/lib/etcd/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>恢复 Kube-Apiserver 与 Etcd 镜像&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将 /etc/kubernetes/manifests 目录恢复，使 Kubernetes 重启 Kube-Apiserver 与 Etcd 镜像：&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp -r /root/backup/manifests/ /etc/kubernetes/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>恢复完成！！！&lt;/p>
&lt;h1 id="使用-k8s-的-cronjob-定时备份-etcd">使用 k8s 的 CronJob 定时备份 etcd&lt;/h1>
&lt;p>注意：正式使用时修改 .spec.schedule 的值来改变备份周期(示例为每月 1 号执行备份)&lt;/p>
&lt;p>基于 kubernetes-v1.23.2 编写&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">batch/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">CronJob&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd-backup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">etcd-backup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">schedule&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0 0 1 * *&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">jobTemplate&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd-backup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeSelectorTerms&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">node-role.kubernetes.io/master&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">Exists&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">sh&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - -&lt;span style="color:#ae81ff">c&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --key=/etc/kubernetes/pki/etcd/healthcheck-client.key --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> snapshot save /backup/etcd.db-$(printf &amp;#39;%(%Y-%m-%d-%H:%M:%S)T&amp;#39;) &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">ETCDCTL_API&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;3&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">registry.aliyuncs.com/dd_k8s/etcd:3.5.1-0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/backup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd-backup-db&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/etc/localtime&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">host-time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">readOnly&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/etc/kubernetes/pki/etcd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd-certs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">readOnly&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hostNetwork&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">restartPolicy&lt;/span>: &lt;span style="color:#ae81ff">Never&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tolerations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">effect&lt;/span>: &lt;span style="color:#ae81ff">NoSchedule&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">node-role.kubernetes.io/master&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">Exists&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd-backup-db&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">persistentVolumeClaim&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">claimName&lt;/span>: &lt;span style="color:#ae81ff">etcd-backup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">hostPath&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/etc/localtime&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">host-time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">hostPath&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/etc/kubernetes/pki/etcd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">etcd-certs&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Kubernetes 管理案例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/</guid><description>
&lt;h1 id="资源删除场景">资源删除场景&lt;/h1>
&lt;h2 id="处于-terminating-状态的对象处理">处于 Terminating 状态的对象处理&lt;/h2>
&lt;p>使用 &lt;code>kubectl edit&lt;/code> 命令来编辑该对象的配置，删除其中 finalizers 字段及其附属字段，即可.&lt;/p>
&lt;p>也可以使用 patch 命令来删除 finalizers 字段&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl patch -n NS Resource ResourceName -p &lt;span style="color:#e6db74">&amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;finalizers&amp;#34;:null}}&amp;#39;&lt;/span> -n log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>或&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl patch -n test configmap mymap &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --type json &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --patch&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;[ { &amp;#34;op&amp;#34;: &amp;#34;remove&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/metadata/finalizers&amp;#34; } ]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="资源无法删除">资源无法删除&lt;/h2>
&lt;p>首先使用命令找到该 ns 还有哪些对象，最后的 NAMESPACE 改为自己想要查找的 ns 名&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export NAMESPACE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;test&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl api-resources &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --verbs&lt;span style="color:#f92672">=&lt;/span>list --namespaced -o name | xargs -n &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> kubectl get --show-kind --ignore-not-found -n NAMESPACE
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>找到对象后，删除，如果删不掉，使用处理 Terminationg 状态对象的方法进行处理&lt;/p></description></item><item><title>Docs: 更新 APIServer 证书</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/%E6%9B%B4%E6%96%B0-apiserver-%E8%AF%81%E4%B9%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/%E6%9B%B4%E6%96%B0-apiserver-%E8%AF%81%E4%B9%A6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://mp.weixin.qq.com/s/bs0urFxOG71nq9K34H1b6Q">https://mp.weixin.qq.com/s/bs0urFxOG71nq9K34H1b6Q&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>本文我们将了解如何将一个新的 DNS 名称或者 IP 地址添加到 Kubernetes APIServer 使用的 TLS 证书中。在某些情况下默认的证书包含的名称可能不能满足我们的要求，又或者是 APIServer 地址有所变化，都需要重新更新证书。&lt;/p>
&lt;p>我们这里的集群是使用 kubeadm 搭建的单 master 集群，使用的也是 kubeadm 在启动集群时创建的默认证书授权 CA，对于其他环境的集群不保证本文也同样适用。&lt;/p>
&lt;p>Kubernetes APIServer 使用数字证书来加密 APIServer 的相关流量以及验证到 APIServer 的连接。所以如果我们想使用命令行客户端（比如 kubectl）连接到 APIServer，并且使用的主机名或者 IP 地址不包括在证书的 subject 的备选名称（SAN）列表中的话，访问的时候可能会出错，会提示对指定的 IP 地址或者主机名访问证书无效。要解决这个问题就需要更新证书，使 SAN 列表中包含所有你将用来访问 APIServer 的 IP 地址或者主机名。&lt;/p>
&lt;h1 id="步骤">步骤&lt;/h1>
&lt;h2 id="生成-kubeadm-配置文件">生成 kubeadm 配置文件&lt;/h2>
&lt;p>因为集群是使用 kubeadm 搭建的，所以我们可以直接使用 kubeadm 来更新 APIServer 的证书，来保证在 SAN 列表中包含额外的名称。&lt;/p>
&lt;p>首页我们一个 kubeadm 的配置文件，如果一开始安装集群的时候你就是使用的配置文件，那么我们可以直接更新这个配置文件，但是如果你没有使用配置文件，直接使用的 kubeadm init 来安装的集群，那么我们可以从集群中获取 kubeadm 的配置信息来创建一个配置文件，因为 kubeadm 会将其配置写入到 kube-system 命名空间下面一个名为 kubeadm-config 的 ConfigMap 中。可以直接执行如下所示的命令将该配置导出：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' &amp;gt; kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令会导出一个名为 kubeadm.yaml 的配置文件，内容如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiServer&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">extraArgs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">authorization-mode&lt;/span>: &lt;span style="color:#ae81ff">Node,RBAC&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeoutForControlPlane&lt;/span>: &lt;span style="color:#ae81ff">4m0s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kubeadm.k8s.io/v1beta2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">certificatesDir&lt;/span>: &lt;span style="color:#ae81ff">/etc/kubernetes/pki&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">clusterName&lt;/span>: &lt;span style="color:#ae81ff">kubernetes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">controllerManager&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">dns&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">CoreDNS&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">etcd&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">local&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dataDir&lt;/span>: &lt;span style="color:#ae81ff">/var/lib/etcd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">imageRepository&lt;/span>: &lt;span style="color:#ae81ff">registry.aliyuncs.com/k8sxio&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterConfiguration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kubernetesVersion&lt;/span>: &lt;span style="color:#ae81ff">v1.17.11&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">networking&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dnsDomain&lt;/span>: &lt;span style="color:#ae81ff">cluster.local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podSubnet&lt;/span>: &lt;span style="color:#ae81ff">10.244.0.0&lt;/span>&lt;span style="color:#ae81ff">/16&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">serviceSubnet&lt;/span>: &lt;span style="color:#ae81ff">10.96.0.0&lt;/span>&lt;span style="color:#ae81ff">/12&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">scheduler&lt;/span>: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="添加-certsans">添加 certSANs&lt;/h2>
&lt;p>上面的配置中并没有列出额外的 SAN 信息，我们要添加一个新的数据，需要在 apiServer 属性下面添加一个 certsSANs 的列表。如果你在启动集群的使用就使用的了 kubeadm 的配置文件，可能里面就已经包含 certSANs 列表了，如果没有我们就需要添加它，比如我们这里要添加一个新的域名 api.k8s.local 以及 ydzs-master2 和 ydzs-master3 这两个主机名和 10.151.30.70、10.151.30.71 这两个新的 IP 地址，那么我们需要在 apiServer 下面添加如下所示的数据：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiServer&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">certSANs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">api.k8s.local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ydzs-master2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ydzs-master3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">10.151.30.11&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">10.151.30.70&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">10.151.30.71&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面我只列出了 apiServer 下面新增的 certSANs 信息，这些信息是包括在标准的 SAN 列表之外的，所以不用担心这里没有添加 kubernetes、kubernetes.default 等等这些信息，因为这些都是标准的 SAN 列表中的。&lt;/p>
&lt;h2 id="备份老证书">备份老证书&lt;/h2>
&lt;p>更新完 kubeadm 配置文件后我们就可以更新证书了，首先我们移动现有的 APIServer 的证书和密钥，因为 kubeadm 检测到他们已经存在于指定的位置，它就不会创建新的了。&lt;/p>
&lt;pre>&lt;code>$ mv /etc/kubernetes/pki/apiserver.{crt,key} ~
&lt;/code>&lt;/pre>
&lt;h2 id="生成新证书">生成新证书&lt;/h2>
&lt;p>然后直接使用 kubeadm 命令生成一个新的证书：&lt;/p>
&lt;pre>&lt;code>$ kubeadm init phase certs apiserver --config kubeadm-config.yaml
W0902 10:05:28.006627 832 validation.go:28] Cannot validate kubelet config - no validator is available
W0902 10:05:28.006754 832 validation.go:28] Cannot validate kube-proxy config - no validator is available
[certs] Generating &amp;quot;apiserver&amp;quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [ydzs-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local api.k8s.local ydzs-master2 ydzs-master3] and IPs [10.96.0.1 123.59.188.12 10.151.30.11 10.151.30.70 10.151.30.71]
&lt;/code>&lt;/pre>
&lt;p>通过上面的命令可以查看到 APIServer 签名的 DNS 和 IP 地址信息，一定要和自己的目标签名信息进行对比，如果缺失了数据就需要在上面的 certSANs 中补齐，重新生成证书。&lt;/p>
&lt;p>该命令会使用上面指定的 kubeadm 配置文件为 APIServer 生成一个新的证书和密钥，由于指定的配置文件中包含了 certSANs 列表，那么 kubeadm 会在创建新证书的时候自动添加这些 SANs。&lt;/p>
&lt;h2 id="重启-kube-apiserver">重启 kube-apiserver&lt;/h2>
&lt;p>最后一步是重启 APIServer 来接收新的证书，最简单的方法是直接杀死 APIServer 的容器：&lt;/p>
&lt;pre>&lt;code>$ docker ps | grep kube-apiserver | grep -v pause
7fe227a5dd3c aa63290ccd50 &amp;quot;kube-apiserver --ad…&amp;quot; 14 hours ago Up 14 hours k8s_kube-apiserver_kube-apiserver-ydzs-master_kube-system_6aa38ee2d66b7d9b6660a88700d00581_0
$ docker kill 7fe227a5dd3c
7fe227a5dd3c
&lt;/code>&lt;/pre>
&lt;p>容器被杀掉后，kubelet 会自动重启容器，然后容器将接收新的证书，一旦 APIServer 重启后，我们就可以使用新添加的 IP 地址或者主机名来连接它了，比如我们新添加的 api.k8s.local。&lt;/p>
&lt;h2 id="验证">验证&lt;/h2>
&lt;p>要验证证书是否更新我们可以直接去编辑 kubeconfig 文件中的 APIServer 地址，将其更换为新添加的 IP 地址或者主机名，然后去使用 kubectl 操作集群，查看是否可以正常工作。&lt;/p>
&lt;p>当然我们可以使用 openssl 命令去查看生成的证书信息是否包含我们新添加的 SAN 列表数据：&lt;/p>
&lt;pre>&lt;code>$ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text
Certificate:
......
Subject: CN=kube-apiserver
......
X509v3 Subject Alternative Name:
DNS:ydzs-master, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:api.k8s.local, DNS:ydzs-master2, DNS:ydzs-master3, IP Address:10.96.0.1, IP Address:123.59.188.12, IP Address:10.151.30.11, IP Address:10.151.30.70, IP Address:10.151.30.71
......
&lt;/code>&lt;/pre>
&lt;h2 id="更新集群配置">更新集群配置&lt;/h2>
&lt;p>如果上面的操作都一切顺利，最后一步是将上面的集群配置信息保存到集群的 kubeadm-config 这个 ConfigMap 中去，这一点非常重要，这样以后当我们使用 kubeadm 来操作集群的时候，相关的数据不会丢失，比如升级的时候还是会带上 certSANs 中的数据进行签名的。&lt;/p>
&lt;pre>&lt;code>$ kubeadm config upload from-file --config kubeadm.yaml
&lt;/code>&lt;/pre>
&lt;p>使用上面的命令保存配置后，我们同样可以用下面的命令来验证是否保存成功了：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n kube-system get configmap kubeadm-config -o yaml
&lt;/code>&lt;/pre>
&lt;p>更新 APIServer 证书的名称在很多场景下都会使用到，比如在控制平面前面添加一个负载均衡器，或者添加新的 DNS 名称或 IP 地址来使用控制平面的端点，所以掌握更新集群证书的方法也是非常有必要的。&lt;/p></description></item><item><title>Docs: 华为云 CCE 动态扩容有状态应用的 EVS</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/%E5%8D%8E%E4%B8%BA%E4%BA%91-cce-%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%9A%84-evs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/%E5%8D%8E%E4%B8%BA%E4%BA%91-cce-%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%9A%84-evs/</guid><description>
&lt;p>想要为华为云 CCE 集群中的 Statefulset 的 PVC 扩容&lt;/p>
&lt;p>只修改 &lt;code>statefulset.spec.volumeClaimTemplates.spec.resources.requests&lt;/code> 字段下的内容是无法真正扩容成功的，仅仅在下面的页面中容量显示会变化，但是真实硬盘并没有变化。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vcyh4v/1648526233603-107d40e6-e618-4e00-a2da-9c1952ce28f4.png" alt="image.png">&lt;/p>
&lt;p>同时还需要手动对 PVC 进行扩容，只有对 PVC 进行了扩容操作，华为云的 CSI 才会检测到变化并执行扩容操作&lt;/p>
&lt;ul>
&lt;li>直接修改 PVC 中的 &lt;code>pvc.spec.resources.requests&lt;/code> 字段中存储的容量&lt;/li>
&lt;li>在华为云 web 控制台修改存储容量，如下图
&lt;ul>
&lt;li>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vcyh4v/1648526376264-fae48b42-637d-4300-8539-0483d17e5a06.png" alt="image.png">&lt;/li>
&lt;li>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vcyh4v/1648526400481-12a024c0-e4d9-46eb-ac8f-dc2580ca823b.png" alt="image.png">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>PVC 扩容完成后，真实的硬盘容量将会正常扩容&lt;/p></description></item><item><title>Docs: 将单 master 升级为多 master 集群</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/%E5%B0%86%E5%8D%95-master-%E5%8D%87%E7%BA%A7%E4%B8%BA%E5%A4%9A-master-%E9%9B%86%E7%BE%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/%E5%B0%86%E5%8D%95-master-%E5%8D%87%E7%BA%A7%E4%B8%BA%E5%A4%9A-master-%E9%9B%86%E7%BE%A4/</guid><description>
&lt;h1 id="如何将单-master-升级为多-master-集群">如何将单 master 升级为多 master 集群&lt;/h1>
&lt;p>前面我们课程中的集群是单 master 的集群，对于生产环境风险太大了，非常有必要做一个高可用的集群(&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/&lt;/a>)，这里的高可用主要是针对控制面板来说的，比如 kube-apiserver、etcd、kube-controller-manager、kube-scheduler 这几个组件，其中 kube-controller-manager 于 kube-scheduler 组件是 Kubernetes 集群自己去实现的高可用，当有多个组件存在的时候，会自动选择一个作为 Leader 提供服务，所以不需要我们手动去实现高可用，apiserver 和 etcd 就需要手动去搭建高可用的集群的。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ugaorz/1622597035172-9d7c9e8c-0d7b-4f40-99e4-499d30b44b91.png" alt="">
高可用的架构有很多，比如典型的 haproxy + keepalived 架构，或者使用 nginx 来做代理实现。我们这里为了说明如何将单 master 升级为高可用的集群，采用相对更简单的 nginx 模式，当然这种模式也有一些缺点，但是足以说明高可用的实现方式了。架构如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ugaorz/1622597058497-7d7328b4-15e4-4a83-abd3-4a468530dc51.png" alt="">
从上面架构图上可以看出来，我们需要在所有的节点上安装一个 nginx 来代理 apiserver，这里我们准备 3 个节点作为控制平面节点：ydzs-master、ydzs-master2、ydzs-master3，这里我们默认所有节点都已经正常安装配置好了 Docker：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ugaorz/1622597067673-3e90edd5-c272-47c8-b141-e39b03f7e255.png" alt="">
在开始下面的操作之前，在所有节点 hosts 中配置如下所示的信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ cat /etc/hosts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1 api.k8s.local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.70 ydzs-master2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.71 ydzs-master3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.11 ydzs-master
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.57 ydzs-node3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.59 ydzs-node4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.60 ydzs-node5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.62 ydzs-node6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.22 ydzs-node1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.151.30.23 ydzs-node2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>免责声明：本文操作已验证成功，但并不保证对集群没有任何影响，在操作之前一定做好备份，由此对集群产生的任何影响本人概不负责~&lt;/p>
&lt;h1 id="更新证书">更新证书&lt;/h1>
&lt;p>由于我们要将集群替换成高可用的集群，那么势必会想到我们会用一个负载均衡器来代理 APIServer，也就是这个负载均衡器访问 APIServer 的时候要能正常访问，所以默认安装的 APIServer 证书就需要更新，因为里面没有包含我们需要的地址，需要保证在 SAN 列表中包含一些额外的名称。&lt;/p>
&lt;p>首页我们一个 kubeadm 的配置文件，如果一开始安装集群的时候你就是使用的配置文件，那么我们可以直接更新这个配置文件，但是如果你没有使用配置文件，直接使用的 kubeadm init 来安装的集群，那么我们可以从集群中获取 kubeadm 的配置信息来创建一个配置文件，因为 kubeadm 会将其配置写入到 kube-system 命名空间下面一个名为 kubeadm-config 的 ConfigMap 中。可以直接执行如下所示的命令将该配置导出：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' &amp;gt; kubeadm.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令会导出一个名为 kubeadm.yaml 的配置文件，内容如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiServer&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">extraArgs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">authorization-mode&lt;/span>: &lt;span style="color:#ae81ff">Node,RBAC&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeoutForControlPlane&lt;/span>: &lt;span style="color:#ae81ff">4m0s&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kubeadm.k8s.io/v1beta2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">certificatesDir&lt;/span>: &lt;span style="color:#ae81ff">/etc/kubernetes/pki&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">clusterName&lt;/span>: &lt;span style="color:#ae81ff">kubernetes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">controllerManager&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">dns&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">CoreDNS&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">etcd&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">local&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dataDir&lt;/span>: &lt;span style="color:#ae81ff">/var/lib/etcd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">imageRepository&lt;/span>: &lt;span style="color:#ae81ff">registry.aliyuncs.com/k8sxio&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterConfiguration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kubernetesVersion&lt;/span>: &lt;span style="color:#ae81ff">v1.17.11&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">networking&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dnsDomain&lt;/span>: &lt;span style="color:#ae81ff">cluster.local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podSubnet&lt;/span>: &lt;span style="color:#ae81ff">10.244.0.0&lt;/span>&lt;span style="color:#ae81ff">/16&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">serviceSubnet&lt;/span>: &lt;span style="color:#ae81ff">10.96.0.0&lt;/span>&lt;span style="color:#ae81ff">/12&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">scheduler&lt;/span>: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的配置中并没有列出额外的 SAN 信息，我们要添加一个新的数据，需要在 apiServer 属性下面添加一个 certsSANs 的列表。如果你在启动集群的使用就使用的了 kubeadm 的配置文件，可能里面就已经包含 certSANs 列表了，如果没有我们就需要添加它，比如我们这里要添加一个新的域名 api.k8s.local 以及 ydzs-master2 和 ydzs-master3 这两个主机名和 10.151.30.70、10.151.30.71 这两个新的 IP 地址，那么我们需要在 apiServer 下面添加如下所示的数据：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiServer&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">certSANs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">api.k8s.local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ydzs-master&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ydzs-master2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ydzs-master3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">10.151.30.11&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">10.151.30.70&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">10.151.30.71&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">extraArgs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">authorization-mode&lt;/span>: &lt;span style="color:#ae81ff">Node,RBAC&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeoutForControlPlane&lt;/span>: &lt;span style="color:#ae81ff">4m0s&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面我只列出了 apiServer 下面新增的 certSANs 信息，这些信息是包括在标准的 SAN 列表之外的，所以不用担心这里没有添加 kubernetes、kubernetes.default 等等这些信息，因为这些都是标准的 SAN 列表中的。&lt;/p>
&lt;p>更新完 kubeadm 配置文件后我们就可以更新证书了，首先我们移动现有的 APIServer 的证书和密钥，因为 kubeadm 检测到他们已经存在于指定的位置，它就不会创建新的了。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ mv /etc/kubernetes/pki/apiserver.&lt;span style="color:#f92672">{&lt;/span>crt,key&lt;span style="color:#f92672">}&lt;/span> ~
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后直接使用 kubeadm 命令生成一个新的证书：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubeadm init phase certs apiserver --config kubeadm.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>W0902 10:05:28.006627 &lt;span style="color:#ae81ff">832&lt;/span> validation.go:28&lt;span style="color:#f92672">]&lt;/span> Cannot validate kubelet config - no validator is available
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>W0902 10:05:28.006754 &lt;span style="color:#ae81ff">832&lt;/span> validation.go:28&lt;span style="color:#f92672">]&lt;/span> Cannot validate kube-proxy config - no validator is available
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>certs&lt;span style="color:#f92672">]&lt;/span> Generating &lt;span style="color:#e6db74">&amp;#34;apiserver&amp;#34;&lt;/span> certificate and key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>certs&lt;span style="color:#f92672">]&lt;/span> apiserver serving cert is signed &lt;span style="color:#66d9ef">for&lt;/span> DNS names &lt;span style="color:#f92672">[&lt;/span>ydzs-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local api.k8s.local ydzs-master2 ydzs-master3&lt;span style="color:#f92672">]&lt;/span> and IPs &lt;span style="color:#f92672">[&lt;/span>10.96.0.1 123.59.188.12 10.151.30.11 10.151.30.70 10.151.30.71&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>通过上面的命令可以查看到 APIServer 签名的 DNS 和 IP 地址信息，一定要和自己的目标签名信息进行对比，如果缺失了数据就需要在上面的 certSANs 中补齐，重新生成证书。&lt;/p>
&lt;p>该命令会使用上面指定的 kubeadm 配置文件为 APIServer 生成一个新的证书和密钥，由于指定的配置文件中包含了 certSANs 列表，那么 kubeadm 会在创建新证书的时候自动添加这些 SANs。&lt;/p>
&lt;p>最后一步是重启 APIServer 来接收新的证书，最简单的方法是直接杀死 APIServer 的容器：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ docker ps | grep kube-apiserver | grep -v pause
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>7fe227a5dd3c aa63290ccd50 &lt;span style="color:#e6db74">&amp;#34;kube-apiserver --ad…&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">14&lt;/span> hours ago Up &lt;span style="color:#ae81ff">14&lt;/span> hours k8s_kube-apiserver_kube-apiserver-ydzs-master_kube-system_6aa38ee2d66b7d9b6660a88700d00581_0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ docker kill 7fe227a5dd3c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>7fe227a5dd3c
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>容器被杀掉后，kubelet 会自动重启容器，然后容器将接收新的证书，一旦 APIServer 重启后，我们就可以使用新添加的 IP 地址或者主机名来连接它了，比如我们新添加的 api.k8s.local。&lt;/p>
&lt;h2 id="验证证书">验证证书&lt;/h2>
&lt;p>要验证证书是否更新我们可以直接去编辑 kubeconfig 文件中的 APIServer 地址，将其更换为新添加的 IP 地址或者主机名，然后去使用 kubectl 操作集群，查看是否可以正常工作。&lt;/p>
&lt;p>当然我们可以使用 openssl 命令去查看生成的证书信息是否包含我们新添加的 SAN 列表数据：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Certificate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Subject: CN&lt;span style="color:#f92672">=&lt;/span>kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> X509v3 Subject Alternative Name:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DNS:ydzs-master, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:api.k8s.local, DNS:ydzs-master2, DNS:ydzs-master3, IP Address:10.96.0.1, IP Address:123.59.188.12, IP Address:10.151.30.11, IP Address:10.151.30.70, IP Address:10.151.30.71
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果上面的操作都一切顺利，最后一步是将上面的集群配置信息保存到集群的 kubeadm-config 这个 ConfigMap 中去，这一点非常重要，这样以后当我们使用 kubeadm 来操作集群的时候，相关的数据不会丢失，比如升级的时候还是会带上  certSANs 中的数据进行签名的。&lt;/p>
&lt;pre>&lt;code>$ kubeadm config upload from-file --config kubeadm.yaml
&lt;/code>&lt;/pre>
&lt;p>使用上面的命令保存配置后，我们同样可以用下面的命令来验证是否保存成功了：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n kube-system get configmap kubeadm-config -o yaml
&lt;/code>&lt;/pre>
&lt;p>更新 APIServer 证书的名称在很多场景下都会使用到，比如在控制平面前面添加一个负载均衡器，或者添加新的 DNS 名称或 IP 地址来使用控制平面的端点，所以掌握更新集群证书的方法也是非常有必要的。&lt;/p>
&lt;h1 id="为控制平面创建负载均衡器">为控制平面创建负载均衡器&lt;/h1>
&lt;p>接下来我们为控制平面创建一个负载平衡器。如何设置和配置负载均衡器的具体细节因解决方案不同，但是一般的方案都需要包括下面的功能：&lt;/p>
&lt;ul>
&lt;li>使用 4 层负载平衡器（TCP 而不是 HTTP / HTTPS）&lt;/li>
&lt;li>运行健康检查应配置为 SSL，而不是 TCP 运行状况检查&lt;/li>
&lt;/ul>
&lt;p>不管用哪一种方式，我们最好创建一个 DNS CNAME 条目以指向您的负载均衡器（强烈建议）。如果您需要更换或重新配置负载均衡解决方案，这将为您提供更多的灵活性，因为 DNS CNAME 保持不变，就不用再次去更新证书了。&lt;/p>
&lt;p>我们这里采用的方案是在节点上使用 nginx 来作为一个负载均衡器，下面的操作需要在所有节点上操作：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ mkdir -p /etc/kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cat &amp;gt; /etc/kubernetes/nginx.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">error_log stderr notice;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">worker_processes 2;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">worker_rlimit_nofile 130048;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">worker_shutdown_timeout 10s;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">events {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> multi_accept on;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> use epoll;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> worker_connections 16384;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">stream {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> upstream kube_apiserver {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> least_conn;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> server ydzs-master:6443;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> server ydzs-master2:6443;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> server ydzs-master3:6443;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> server {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> listen 8443;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> proxy_pass kube_apiserver;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> proxy_timeout 10m;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> proxy_connect_timeout 1s;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">http {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> aio threads;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> aio_write on;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> tcp_nopush on;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> tcp_nodelay on;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> keepalive_timeout 5m;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> keepalive_requests 100;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> reset_timedout_connection on;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> server_tokens off;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> autoindex off;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> server {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> listen 8081;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> location /stub_status {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> stub_status on;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> access_log off;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>使用上面的配置启动一个 nginx 容器：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ docker run --restart&lt;span style="color:#f92672">=&lt;/span>always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -v /etc/kubernetes/nginx.conf:/etc/nginx/nginx.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -v /etc/localtime:/etc/localtime:ro
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --name k8s-ha
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --net host
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>启动成功后 apiserver 的负载均衡地址就成了 &lt;a href="https://api.k8s.local:8443">https://api.k8s.local:8443&lt;/a>。然后我们将 kubeconfig 文件中的 apiserver 地址替换成负载均衡器的地址。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 修改 kubelet 配置&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ vi /etc/kubernetes/kubelet.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://api.k8s.local:8443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ systemctl restart kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 修改 controller-manager&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ vi /etc/kubernetes/controller-manager.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://api.k8s.local:8443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 重启&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ docker kill &lt;span style="color:#66d9ef">$(&lt;/span>docker ps | grep kube-controller-manager |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>grep -v pause | cut -d&lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span> -f1&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 修改 scheduler&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ vi /etc/kubernetes/scheduler.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://api.k8s.local:8443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 重启&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ docker kill &lt;span style="color:#66d9ef">$(&lt;/span>docker ps | grep kube-scheduler | grep -v pause |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cut -d&lt;span style="color:#e6db74">&amp;#39; &amp;#39;&lt;/span> -f1&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="然后更新-kube-proxy">然后更新 kube-proxy&lt;/h1>
&lt;pre>&lt;code>$ kubectl -n kube-system edit cm kube-proxy
......
kubeconfig.conf: |-
apiVersion: v1
kind: Config
clusters:
- cluster:
certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
server: https://api.k8s.local:8443
name: default
......
&lt;/code>&lt;/pre>
&lt;p>当然还有 kubectl 访问集群的 ~/.kube/config 文件也需要修改。&lt;/p>
&lt;h1 id="更新控制面板">更新控制面板&lt;/h1>
&lt;p>由于我们现在已经在控制平面的前面添加了一个负载平衡器，因此我们需要使用正确的信息更新此 ConfigMap。&lt;/p>
&lt;p>首先，使用以下命令从 ConfigMap 中获取当前配置：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl -n kube-system get configmap kubeadm-config -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{.data.ClusterConfiguration}&amp;#39;&lt;/span> &amp;gt; kubeadm.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后在当前配置文件里面里面添加 controlPlaneEndpoint 属性，用于指定控制面板的负载均衡器的地址。&lt;/p>
&lt;pre>&lt;code>$ vi kubeadm.yaml
controlPlaneEndpoint: api.k8s.local:8443 # 添加改配置
apiServer:
certSANs:
- api.k8s.local
- ydzs-master # 添加3个master节点的hostname和ip
- ydzs-master2
- ydzs-master3
- 10.151.30.11
- 10.151.30.70
- 10.151.30.71
......
&lt;/code>&lt;/pre>
&lt;p>编辑完文件后，使用以下命令将其上传回集群：&lt;/p>
&lt;pre>&lt;code>$ kubeadm config upload from-file --config kubeadm.yaml
&lt;/code>&lt;/pre>
&lt;p>然后需要在 kube-public 命名空间中更新 cluster-info 这个 ConfigMap，该命名空间包含一个 Kubeconfig 文件，该文件的 server: 一行指向单个控制平面节点。只需使用 kubectl -n kube-public edit cm cluster-info 更新该 server: 行以指向控制平面的负载均衡器即可。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl -n kube-public edit cm cluster-info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://api.k8s.local:8443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl cluster-info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Kubernetes master is running at https://api.k8s.local:8443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>KubeDNS is running at https://api.k8s.local:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>KubeDNSUpstream is running at https://api.k8s.local:8443/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Metrics-server is running at https://api.k8s.local:8443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>To further debug and diagnose cluster problems, use &lt;span style="color:#e6db74">&amp;#39;kubectl cluster-info dump&amp;#39;&lt;/span>.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>更新完成就可以看到 cluster-info 的信息变成了负载均衡器的地址了。&lt;/p>
&lt;h1 id="添加控制平面">添加控制平面&lt;/h1>
&lt;p>接下来我们来添加额外的控制平面节点，首先使用如下命令来将集群的证书上传到集群中，供其他控制节点使用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubeadm init phase upload-certs --upload-certs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0903 15:13:24.192467 &lt;span style="color:#ae81ff">20533&lt;/span> version.go:251&lt;span style="color:#f92672">]&lt;/span> remote version is much newer: v1.19.0; falling back to: stable-1.17
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>W0903 15:13:25.739892 &lt;span style="color:#ae81ff">20533&lt;/span> validation.go:28&lt;span style="color:#f92672">]&lt;/span> Cannot validate kube-proxy config - no validator is available
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>W0903 15:13:25.739966 &lt;span style="color:#ae81ff">20533&lt;/span> validation.go:28&lt;span style="color:#f92672">]&lt;/span> Cannot validate kubelet config - no validator is available
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>upload-certs&lt;span style="color:#f92672">]&lt;/span> Storing the certificates in Secret &lt;span style="color:#e6db74">&amp;#34;kubeadm-certs&amp;#34;&lt;/span> in the &lt;span style="color:#e6db74">&amp;#34;kube-system&amp;#34;&lt;/span> Namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>upload-certs&lt;span style="color:#f92672">]&lt;/span> Using certificate key:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>e71ef7ede98e49f5f094b150d604c7ad50f125279180a7320b1b14ef3ccc3a34
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的命令会生成一个新的证书密钥，但是只有 2 小时有效期。由于我们现有的集群已经运行一段时间了，所以之前的启动 Token 也已经失效了（Token 的默认生存期为 24 小时），所以我们也需要创建一个新的 Token 来添加新的控制平面节点：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubeadm token create --print-join-command --config kubeadm.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>W0903 15:29:10.958329 &lt;span style="color:#ae81ff">25049&lt;/span> validation.go:28&lt;span style="color:#f92672">]&lt;/span> Cannot validate kube-proxy config - no validator is available
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>W0903 15:29:10.958457 &lt;span style="color:#ae81ff">25049&lt;/span> validation.go:28&lt;span style="color:#f92672">]&lt;/span> Cannot validate kubelet config - no validator is available
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubeadm join api.k8s.local:8443 --token f27w7m.adelvl3waw9kqdhp --discovery-token-ca-cert-hash sha256:6917cbf7b0e73ecfef77217e9a27e76ef9270aa379c34af30201abd0f1088c34
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的命令最后给出的提示是添加 node 节点的命令，我们这里要添加控制平面节点就要使用如下所示的命令：&lt;/p>
&lt;pre>&lt;code>$ kubeadm join &amp;lt;DNS CNAME of load balancer&amp;gt;:&amp;lt;load balancer port&amp;gt;
--token &amp;lt;bootstrap-token&amp;gt;
--discovery-token-ca-cert-hash sha256:&amp;lt;CA certificate hash&amp;gt;
--control-plane --certificate-key &amp;lt;certificate-key&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>获得了上面的添加命令过后，登录到 ydzs-master2 节点进行相关的操作，在 ydzs-master2 节点上安装软件：&lt;/p>
&lt;pre>&lt;code>$ yum install -y kubeadm-1.17.11-0 kubelet-1.17.11-0 kubectl-1.17.11-0
&lt;/code>&lt;/pre>
&lt;p>要加入控制平面，我们可以先拉取相关镜像：&lt;/p>
&lt;pre>&lt;code>$ kubeadm config images pull --image-repository registry.aliyuncs.com/k8sxio
&lt;/code>&lt;/pre>
&lt;p>然后执行上面生成的 join 命令，将参数替换后如下所示：&lt;/p>
&lt;pre>&lt;code>$ kubeadm join api.k8s.local:8443
--token f27w7m.adelvl3waw9kqdhp
--discovery-token-ca-cert-hash sha256:6917cbf7b0e73ecfef77217e9a27e76ef9270aa379c34af30201abd0f1088c34
--control-plane --certificate-key e71ef7ede98e49f5f094b150d604c7ad50f125279180a7320b1b14ef3ccc3a34
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret &amp;quot;kubeadm-certs&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[certs] Using certificateDir folder &amp;quot;/etc/kubernetes/pki&amp;quot;
[certs] Generating &amp;quot;apiserver-kubelet-client&amp;quot; certificate and key
[certs] Generating &amp;quot;apiserver&amp;quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [ydzs-master2 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local api.k8s.local api.k8s.local ydzs-master2 ydzs-master3] and IPs [10.96.0.1 10.151.30.70 10.151.30.11 10.151.30.70 10.151.30.71]
[certs] Generating &amp;quot;front-proxy-client&amp;quot; certificate and key
[certs] Generating &amp;quot;etcd/healthcheck-client&amp;quot; certificate and key
[certs] Generating &amp;quot;etcd/server&amp;quot; certificate and key
[certs] etcd/server serving cert is signed for DNS names [ydzs-master2 localhost] and IPs [10.151.30.70 127.0.0.1 ::1]
[certs] Generating &amp;quot;etcd/peer&amp;quot; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [ydzs-master2 localhost] and IPs [10.151.30.70 127.0.0.1 ::1]
[certs] Generating &amp;quot;apiserver-etcd-client&amp;quot; certificate and key
[certs] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[certs] Using the existing &amp;quot;sa&amp;quot; key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder &amp;quot;/etc/kubernetes&amp;quot;
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing &amp;quot;admin.conf&amp;quot; kubeconfig file
[kubeconfig] Writing &amp;quot;controller-manager.conf&amp;quot; kubeconfig file
[kubeconfig] Writing &amp;quot;scheduler.conf&amp;quot; kubeconfig file
[control-plane] Using manifest folder &amp;quot;/etc/kubernetes/manifests&amp;quot;
[control-plane] Creating static Pod manifest for &amp;quot;kube-apiserver&amp;quot;
W0903 15:55:08.444989 4353 manifests.go:214] the default kube-apiserver authorization-mode is &amp;quot;Node,RBAC&amp;quot;; using &amp;quot;Node,RBAC&amp;quot;
[control-plane] Creating static Pod manifest for &amp;quot;kube-controller-manager&amp;quot;
W0903 15:55:08.457787 4353 manifests.go:214] the default kube-apiserver authorization-mode is &amp;quot;Node,RBAC&amp;quot;; using &amp;quot;Node,RBAC&amp;quot;
[control-plane] Creating static Pod manifest for &amp;quot;kube-scheduler&amp;quot;
W0903 15:55:08.459829 4353 manifests.go:214] the default kube-apiserver authorization-mode is &amp;quot;Node,RBAC&amp;quot;; using &amp;quot;Node,RBAC&amp;quot;
[check-etcd] Checking that the etcd cluster is healthy
......
This node has joined the cluster and a new control plane instance was created:
* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.
To start administering your cluster from this node, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Run 'kubectl get nodes' to see this node join the cluster.
&lt;/code>&lt;/pre>
&lt;p>到这里可以看到 ydzs-master2 节点就成功加入到了控制平面中，然后根据上面的提示配置 kubeconfig 文件。然后用同样的方式添加 ydzs-master3 节点，都添加成功后，在 ydzs-master3 节点上执行如下所示的命令来验证 etcd 集群使用正常：&lt;/p>
&lt;pre>&lt;code>$ docker run --rm -it
--net host
-v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/k8sxio/etcd:3.4.3-0 etcdctl
--cert /etc/kubernetes/pki/etcd/peer.crt
--key /etc/kubernetes/pki/etcd/peer.key
--cacert /etc/kubernetes/pki/etcd/ca.crt
--endpoints https://10.151.30.71:2379 endpoint health --cluster
https://10.151.30.70:2379 is healthy: successfully committed proposal: took = 19.410192ms
https://10.151.30.71:2379 is healthy: successfully committed proposal: took = 21.077275ms
https://10.151.30.11:2379 is healthy: successfully committed proposal: took = 31.282643ms
$ docker run --rm -it --net host -v /aliyuncs.com/k8sxio/etcd:3.4.3-0 etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --cacert /etc/kubernetes/pki/etcd/ca.crt --endpoints https://10.151.30.11:2379,https://10.151.30.70:2379,https://10.151.30.71:2379 endpoint status --write-out=table
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://10.151.30.11:2379 | 3d1fd8983aed809 | 3.4.3 | 52 MB | false | false | 113 | 119502257 | 119502257 | |
| https://10.151.30.70:2379 | af2e11ae8aa72bde | 3.4.3 | 52 MB | true | false | 113 | 119502259 | 119502259 | |
| https://10.151.30.71:2379 | e7a7b252880befdf | 3.4.3 | 52 MB | false | false | 113 | 119502259 | 119502259 | |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
&lt;/code>&lt;/pre>
&lt;p>正常我们就可以看到 etcd 集群正常了，但是由于控制平台的 3 个节点是先后安装的，所以前面两个节点的 etcd 中并不包含其他 etcd 节点的信息，所以我们需要同步所有控制平面节点的 etcd 集群配置：&lt;/p>
&lt;pre>&lt;code>$ cat /etc/kubernetes/manifests/etcd.yaml
......
- --initial-cluster=ydzs-master=https://10.151.30.11:2380,ydzs-master2=https://10.151.30.70:2380,ydzs-master3=https://10.151.30.71:2380
......
&lt;/code>&lt;/pre>
&lt;p>最后执行如下所示的命令查看集群是否正常：&lt;/p>
&lt;pre>&lt;code>$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
ydzs-master Ready master 299d v1.17.11
ydzs-master2 Ready master 34m v1.17.11
ydzs-master3 Ready master 10m v1.17.11
ydzs-node1 Ready &amp;lt;none&amp;gt; 299d v1.17.11
ydzs-node2 Ready &amp;lt;none&amp;gt; 299d v1.17.11
ydzs-node3 Ready &amp;lt;none&amp;gt; 297d v1.17.11
ydzs-node4 Ready &amp;lt;none&amp;gt; 297d v1.17.11
ydzs-node5 Ready &amp;lt;none&amp;gt; 225d v1.17.11
ydzs-node6 Ready &amp;lt;none&amp;gt; 225d v1.17.11
&lt;/code>&lt;/pre>
&lt;p>这里我们就可以看到 ydzs-master、ydzs-master2、ydzs-master3 3 个节点变成了 master 节点，我们也就完成了将单 master 升级为多 master 的高可用集群了。&lt;/p></description></item></channel></rss>