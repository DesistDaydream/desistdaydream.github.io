<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 性能优化 与 故障处理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</link><description>Recent content in 性能优化 与 故障处理 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: k8s CPU limit和throttling的迷思</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/k8s-cpu-limit%E5%92%8Cthrottling%E7%9A%84%E8%BF%B7%E6%80%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/k8s-cpu-limit%E5%92%8Cthrottling%E7%9A%84%E8%BF%B7%E6%80%9D/</guid><description>
&lt;p>原文链接：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://nanmu.me/zh-cn/posts/2021/myth-of-k8s-cpu-limit-and-throttle/">https://nanmu.me/zh-cn/posts/2021/myth-of-k8s-cpu-limit-and-throttle/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/QYJycJCaxB42xdEo3qHHHA">https://mp.weixin.qq.com/s/QYJycJCaxB42xdEo3qHHHA&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>你应当小心设定 k8s 中负载的 CPU limit，太小的值会给你的程序带来额外的、无意义的延迟，太大的值会带来过大的爆炸半径，削弱集群的整体稳定性。&lt;/p>
&lt;h2 id="request-和-limit">request 和 limit&lt;/h2>
&lt;p>k8s 的一大好处就是资源隔离，通过设定负载的 request 和 limit，我们可以方便地让不同程序共存于合适的节点上。&lt;/p>
&lt;p>其中，request 是给调度看的，调度会确保节点上所有负载的 CPU request 合计与内存 request 合计分别都不大于节点本身能够提供的 CPU 和内存，limit 是给节点（kubelet）看的，节点会保证负载在节点上只使用这么多 CPU 和内存。例如，下面配置意味着单个负载会调度到一个剩余 CPU request 大于 0.1 核，剩余 request 内存大于 200MB 的节点，并且负载运行时的 CPU 使用率不能高于 0.4 核（超过将被限流），内存使用不多余 300MB（超过将被 OOM Kill 并重启）。&lt;/p>
&lt;p>&lt;code>resources:   requests:     memory: 200Mi     cpu: &amp;quot;0.1&amp;quot;   limits:     memory: 300Mi     cpu: &amp;quot;0.4&amp;quot;&lt;/code>&lt;/p>
&lt;h2 id="cpu-的利用率">CPU 的利用率&lt;/h2>
&lt;p>CPU 和内存不一样，它是量子化的，只有“使用中”和“空闲”两个状态。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>我和老婆聊了聊 CPU 和内存的不同，她帮我画了一张插图 图/我的妻子&lt;/p>
&lt;p>当我们说内存的使用率是 60%时，我们是在说内存有 60%在&lt;strong>空间上&lt;/strong>已被使用，还有 40%的空间可以放入负载。但是，当我们说 CPU 的某个核的使用率是 60%时，我们是在说采样时间段内，CPU 的这个核在&lt;strong>时间上&lt;/strong>有 60%的时间在忙，40%的时间在睡大觉。&lt;/p>
&lt;p>你设定负载的 CPU limit 时，这个时空区别可能会带来一个让你意想不到的效果——过分的降速限流， 节点 CPU 明明不忙，但是节点故意不让你的负载全速使用 CPU，服务延时上升。&lt;/p>
&lt;h2 id="cpu-限流">CPU 限流&lt;/h2>
&lt;p>k8s 使用 CFS（Completely Fair Scheduler，完全公平调度）限制负载的 CPU 使用率，CFS 本身的机制比较复杂[1]，但是 k8s 的文档中给了一个简明的解释[2]，要点如下：&lt;/p>
&lt;ul>
&lt;li>CPU 使用量的计量周期为 100ms；&lt;/li>
&lt;li>CPU limit 决定每计量周期（100ms）内容器可以使用的 CPU 时间的上限；&lt;/li>
&lt;li>本周期内若容器的 CPU 时间用量达到上限，CPU 限流开始，容器只能在下个周期继续执行；&lt;/li>
&lt;li>1 CPU = 100ms CPU 时间每计量周期，以此类推，0.2 CPU = 20ms CPU 时间每计量周期，2.5 CPU = 250ms CPU 时间每计量周期；&lt;/li>
&lt;li>如果程序用了多个核，CPU 时间会累加统计。&lt;/li>
&lt;/ul>
&lt;p>举个例子，假设一个 API 服务在响应请求时需要使用 A, B 两个线程（2 个核），分别使用 60ms 和 80ms，其中 B 线程晚触发 20ms，我们看到 API 服务在 100ms 后可给出响应：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>没有 CPU 限制的情况，响应时间为 100ms&lt;/p>
&lt;p>如果 CPU limit 被设为 1 核，即每 100ms 内最多使用 100ms CPU 时间，API 服务的线程 B 会受到一次限流（灰色部分），服务在 140ms 后响应：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>CPU limit = 1，响应时间为 140ms&lt;/p>
&lt;p>如果 CPU limit 被设为 0.6 核，即每 100ms 内最多使用 60ms CPU 时间，API 服务的线程 A 会受到一次限流（灰色部分），线程 B 受到两次限流，服务在 220ms 后响应：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>CPU limit = 0.6，响应时间为 220ms&lt;/p>
&lt;p>注意，&lt;strong>即使此时 CPU 没有其他的工作要做，限流一样会执行&lt;/strong>，这是个死板不通融的机制。&lt;/p>
&lt;p>这是一个比较夸张的例子，一般的 API 服务是 IO 密集型的，CPU 时间使用量没那么大（你在跑模型推理？当我没说），但还是可以看到，限流会实打实地延伸 API 服务的延时。因此，对于延时敏感的服务，我们都应该尽量避免触发 k8s 的限流机制。&lt;/p>
&lt;p>下面这张图是我工作中一个 API 服务在 pod 级别的 CPU 使用率和 CPU 限流比率（CPU Throttling），我们看到，CPU 限流的情况在一天内的大部分时候都存在，限流比例在 10%上下浮动，这意味着服务的工作没能全速完成，在速度上打了 9 折。值得一提，这时 pod 所在节点仍然有富余的 CPU 资源，节点的整体 CPU 使用率没有超过 50%.&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>一个实际的降速限流的例子，服务的处理速度被 kubelet 降低了 10%&lt;/p>
&lt;p>你可能注意到，监控图表里的 CPU 使用率看上去没有达到 CPU limit（橙色横线），这是由于 CPU 使用率的统计周期（1min）太长造成的信号混叠（Aliasing）[3]，如果它的统计统计周期和 CFS 的一样（100ms），我们就能看到高过 CPU limit 的尖刺了。（这不是 bug，这是 feature）&lt;/p>
&lt;p>不过，内核版本低于 4.18 的 Linux 还真有个 bug 会造成不必要的 CPU 限流[4]。┑(￣ Д ￣)┍&lt;/p>
&lt;h2 id="避免-cpu-限流">避免 CPU 限流&lt;/h2>
&lt;p>有的开发者倾向于完全弃用 CPU limit[5]，裸奔直接跑，特别是内核版本不够有 bug 的时候[6]。&lt;/p>
&lt;p>我认为这么做还是太过放飞自我了，如果程序里有耗尽 CPU 的 bug（例如死循环，我不幸地遇到过），整个节点及其负载都会陷入不可用的状态，爆炸半径太大，特别是在大号的节点上（16 核及以上）。&lt;/p>
&lt;p>我有两个建议：&lt;/p>
&lt;ol>
&lt;li>监控一段时间应用的 CPU 利用率，基于利用率设定一个合适的 CPU limit（例如，日常利用率的 95 分位 * 10），同时该 limit 不要占到节点 CPU 核数的太大比例（例如 2/3），这样可以达到性能和安全的一个平衡。&lt;/li>
&lt;li>使用 automaxprocs[7]一类的工具让程序适配 CFS 调度环境，各个语言应该都有类似的库或者执行参数，根据 CFS 的特点调整后，程序更不容易遇到 CPU 限流[8]。&lt;/li>
&lt;/ol>
&lt;h2 id="结语">结语&lt;/h2>
&lt;p>上面说到的信号混叠（采样频率不足）和 Linux 内核 bug 让我困扰了一年多，现在想想，主要还是望文生义惹的祸，文档还是应该好好读，基础概念还是要搞清，遂记此文章于错而知新[9]。&lt;/p>
&lt;p>题外话，性能和资源利用率有时是相互矛盾的。对于延时不敏感的程序，CPU 限流率控制在 10%以内应该都是比较健康可接受的，量体裁衣，在线离线负载混合部署，可以提升硬件的资源利用率。有消息说腾讯云研发投产了基于服务优先级的抢占式调度[10]，这是一条更难但更有效的路，希望有朝一日在上游能看到他们的相关贡献。&lt;/p>
&lt;h3 id="引用链接">引用链接&lt;/h3>
&lt;p>[1]&lt;/p>
&lt;p>CFS 本身的机制比较复杂: &lt;a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">&lt;em>https://en.wikipedia.org/wiki/Completely_Fair_Scheduler&lt;/em>&lt;/a>&lt;/p>
&lt;p>[2]&lt;/p>
&lt;p>简明的解释: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run">&lt;em>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run&lt;/em>&lt;/a>&lt;/p>
&lt;p>[3]&lt;/p>
&lt;p>信号混叠（Aliasing）: &lt;a href="https://en.wikipedia.org/wiki/Aliasing">&lt;em>https://en.wikipedia.org/wiki/Aliasing&lt;/em>&lt;/a>&lt;/p>
&lt;p>[4]&lt;/p>
&lt;p>内核版本低于 4.18 的 Linux 还真有个 bug 会造成不必要的 CPU 限流: &lt;a href="https://github.com/kubernetes/kubernetes/issues/67577#issuecomment-466609030">&lt;em>https://github.com/kubernetes/kubernetes/issues/67577#issuecomment-466609030&lt;/em>&lt;/a>&lt;/p>
&lt;p>[5]&lt;/p>
&lt;p>完全弃用 CPU limit: &lt;a href="https://amixr.io/blog/what-wed-do-to-save-from-the-well-known-k8s-incident/">&lt;em>https://amixr.io/blog/what-wed-do-to-save-from-the-well-known-k8s-incident/&lt;/em>&lt;/a>&lt;/p>
&lt;p>[6]&lt;/p>
&lt;p>内核版本不够有 bug 的时候: &lt;a href="https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718">&lt;em>https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718&lt;/em>&lt;/a>&lt;/p>
&lt;p>[7]&lt;/p>
&lt;p>automaxprocs: &lt;a href="https://github.com/uber-go/automaxprocs">&lt;em>https://github.com/uber-go/automaxprocs&lt;/em>&lt;/a>&lt;/p>
&lt;p>[8]&lt;/p>
&lt;p>程序更不容易遇到 CPU 限流: &lt;a href="https://github.com/uber-go/automaxprocs/issues/12#issuecomment-405976401">&lt;em>https://github.com/uber-go/automaxprocs/issues/12#issuecomment-405976401&lt;/em>&lt;/a>&lt;/p>
&lt;p>[9]&lt;/p>
&lt;p>错而知新: &lt;em>&lt;a href="https://nanmu.me/zh-cn/categories/">https://nanmu.me/zh-cn/categories/&lt;/a>错而知新/&lt;/em>&lt;/p>
&lt;p>[10]&lt;/p>
&lt;p>基于服务优先级的抢占式调度: &lt;a href="https://cloud.tencent.com/developer/article/1876817">&lt;em>https://cloud.tencent.com/developer/article/1876817&lt;/em>&lt;/a>&lt;/p>
&lt;p>原文链接：&lt;a href="https://nanmu.me/zh-cn/posts/2021/myth-of-k8s-cpu-limit-and-throttle/">&lt;strong>https://nanmu.me/zh-cn/posts/2021/myth-of-k8s-cpu-limit-and-throttle/&lt;/strong>&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>&lt;strong>你可能还喜欢&lt;/strong>&lt;/p>
&lt;p>点击下方图片即可阅读&lt;/p>
&lt;p>&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640">
&lt;/a>&lt;/p>
&lt;p>SRE 到底是干什么的？？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>&lt;strong>云原生是一种信仰  🤘&lt;/strong>&lt;/p>
&lt;p>&lt;strong>关注公众号&lt;/strong>&lt;/p>
&lt;p>&lt;strong>后台回复 ◉k8s◉ 获取史上最方便快捷的 Kubernetes 高可用部署工具，只需一条命令，连 ssh 都不需要！&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p>
&lt;p>点击  &amp;ldquo;阅读原文&amp;rdquo;  获取&lt;strong>更好的阅读体验！&lt;/strong>&lt;/p>
&lt;p>&lt;strong>发现朋友圈变“安静”了吗？&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/5dcde1bf-2f81-48b1-a6fe-43f89db28c9f/640" alt="">&lt;/p></description></item><item><title>Docs: kubernetes优化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote>
&lt;p>增加可以打开的文件数与线程数,防止 pod 无故无法启动&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt;&amp;gt; /etc/security/limits.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* soft nproc 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* hard nproc 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* soft nofile 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* hard nofile 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="馆长推荐的优化参数">馆长推荐的优化参数&lt;/h2>
&lt;h1 id="-endif-">&lt;a href="https://github.com/moby/moby/issues/31208">https://github.com/moby/moby/issues/31208&lt;/a>
# ipvsadm -l &amp;ndash;timout
# 修复 ipvs 模式下长连接 timeout 问题 小于 900 即可
{% if proxy.mode is defined and proxy.mode == &amp;lsquo;ipvs&amp;rsquo; %}
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
{% endif %}&lt;/h1>
&lt;p>net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_max_tw_buckets = 5000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_synack_retries = 2&lt;/p>
&lt;p>net.core.somaxconn = 10000
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.netfilter.nf_conntrack_max = 2310720
fs.inotify.max_user_watches=89100
fs.may_detach_mounts = 1
fs.file-max = 52706963
fs.nr_open = 52706963
net.bridge.bridge-nf-call-arptables = 1&lt;/p>
&lt;p>{% if not kubelet.swap %}
vm.swappiness = 0
{% endif %}&lt;/p>
&lt;p>vm.overcommit_memory=1
vm.panic_on_oom=0
vm.max_map_count = 262144&lt;/p></description></item><item><title>Docs: kubernetes优化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/</guid><description/></item><item><title>Docs: 故障处理案例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</guid><description>
&lt;h1 id="案例列表">案例列表&lt;/h1>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/m4x_o0WC26oivNAPwGflVg">公众号-云原生实验室，JVM 内存与 K8s 容器内存不一致引发的 OOMKilled 总结&lt;/a>&lt;/p>
&lt;h1 id="kube-proxy-无法绑定-nodeport-端口">kube-proxy 无法绑定 NodePort 端口&lt;/h1>
&lt;h2 id="故障现象">故障现象&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>其他有相同现象的人：
&lt;ul>
&lt;li>&lt;a href="https://zhangguanzhang.github.io/2019/07/08/nodeport-err/">馆长&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ieevee.com/tech/2019/07/20/svc-nodeport.html">ieevee&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>kube-proxy 日志报错：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@lichenhao:~# kubectl logs -n kube-system kube-proxy-4thfl | more
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>E0507 06:05:09.433545 &lt;span style="color:#ae81ff">1&lt;/span> proxier.go:1445&lt;span style="color:#f92672">]&lt;/span> can&lt;span style="color:#e6db74">&amp;#39;t open &amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34; (:33306/tcp), skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">E0507 06:05:09.602044 1 proxier.go:1445] can&amp;#39;&lt;/span>t open &lt;span style="color:#e6db74">&amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>:33306/tcp&lt;span style="color:#f92672">)&lt;/span>, skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>E0507 06:05:39.333119 &lt;span style="color:#ae81ff">1&lt;/span> proxier.go:1445&lt;span style="color:#f92672">]&lt;/span> can&lt;span style="color:#e6db74">&amp;#39;t open &amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34; (:33306/tcp), skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">E0507 06:06:09.494578 1 proxier.go:1445] can&amp;#39;&lt;/span>t open &lt;span style="color:#e6db74">&amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>:33306/tcp&lt;span style="color:#f92672">)&lt;/span>, skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="故障排查">故障排查&lt;/h2>
&lt;p>这个 kube-proxy 在 master-2 上，去 master-2 上看，发现根本没有人占用 33306。反倒是 kube-apiserver 作为客户端，使用 33306 端口，与 etcd 的 2379 进行互联&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-2 ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ss -ntap | grep 33306&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ESTAB &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> 127.0.0.1:33306 127.0.0.1:2379 users:&lt;span style="color:#f92672">((&lt;/span>&lt;span style="color:#e6db74">&amp;#34;kube-apiserver&amp;#34;&lt;/span>,pid&lt;span style="color:#f92672">=&lt;/span>2746,fd&lt;span style="color:#f92672">=&lt;/span>77&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ESTAB &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> 127.0.0.1:2379 127.0.0.1:33306 users:&lt;span style="color:#f92672">((&lt;/span>&lt;span style="color:#e6db74">&amp;#34;etcd&amp;#34;&lt;/span>,pid&lt;span style="color:#f92672">=&lt;/span>2768,fd&lt;span style="color:#f92672">=&lt;/span>100&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="故障处理">故障处理&lt;/h2>
&lt;p>将 kube-apiserver 的 manifest 从 /etc/kubernetes/manifests 目录中移出。待 kube-proxy 创建完端口后，再移回 manifest&lt;/p>
&lt;h2 id="故障分析">故障分析&lt;/h2>
&lt;p>kubernetes 这样设计，这不是给 Local Process 挖坑吗，内核在随机选择本地端口的时候，很可能会命中 kubernetes svc 的端口号呀。
其实 Kubernetes 已经尽力了。
当创建 nodePort 类型的 svc 时，kube-proxy 除了会下发 iptables 规则，还会创建一个监听 Socket，该 Socket 监听的端口号就是 nodePort，因此：&lt;/p>
&lt;ul>
&lt;li>当内核指定 bind 该端口号时，会返回端口已使用&lt;/li>
&lt;li>当内核随机选择本地端口号时，不会命中该端口&lt;/li>
&lt;/ul>
&lt;p>因此，正常情况下，Local Process 不会进坑。
但是，如果 Local Process 先启动，kube-proxy 后启动，则会出现上文描述的情况。
此时，kube-proxy 仍然会下发 iptables 规则，并且尝试 bind 该端口号，但会不成功，因为已经被 Local Process 占用了。&lt;/p>
&lt;pre>&lt;code>E0729 01:48:43.034098 1 proxier.go:1072] can't open &amp;quot;nodePort for default/nginx:&amp;quot; (:31325/tcp), skipping this nodePort: listen tcp :31325: bind: address already in use
E0729 01:49:13.064492 1 proxier.go:1072] can't open &amp;quot;nodePort for default/nginx:&amp;quot; (:31325/tcp), skipping this nodePort: listen tcp :31325: bind: address already in use
E0729 01:49:43.094846 1 proxier.go:1072] can't open &amp;quot;nodePort for default/nginx:&amp;quot; (:31325/tcp), skipping this nodePort: listen tcp :31325: bind: address already in use
&lt;/code>&lt;/pre>
&lt;p>但由于 iptables 已经下发，因此 Local Process 只能空守着端口号流眼泪，眼睁睁的看着报文被劫走。&lt;/p>
&lt;h1 id="深信服-与-flannel-vxlan-8472-端口冲突">深信服 与 Flannel VxLan 8472 端口冲突&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ufbiwm/1625537665271-2ae56e76-3de0-4598-ac39-d3dd0a165198.png" alt="image.png">
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ufbiwm/1625537959161-bafbdab6-d95c-46b2-b4b7-1440ce01be81.png" alt="image.png">
&lt;a href="https://cloud.tencent.com/developer/article/1746944">https://cloud.tencent.com/developer/article/1746944&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://segmentfault.com/a/1190000037782599">https://segmentfault.com/a/1190000037782599&lt;/a>&lt;/p></description></item><item><title>Docs: 故障处理案例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</guid><description/></item><item><title>Docs: 故障处理技巧</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">官方文档,监控、日志和调试-调试运行中的 Pods&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubernetes 作为分布式容器调度系统荣，难免出现问题。&lt;/p>
&lt;p>Kubernetes 让运维管理设备的方式发生了根本的转变，从一台一台设备登录，变为统一管理。在 1.19 版本官方文档的 &lt;a href="https://v1-19.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#node-shell-session">debug 章节&lt;/a>中，提到了这么一句话：&lt;/p>
&lt;blockquote>
&lt;p>If none of these approaches work, you can find the host machine that the pod is running on and SSH into that host, but this should generally not be necessary given tools in the Kubernetes API. Therefore, if you find yourself needing to ssh into a machine, please file a feature request on GitHub describing your use case and why these tools are insufficient.&lt;/p>
&lt;/blockquote>
&lt;p>Kubernetes 集群会积极推进让维护工作不再通过登录每一台设备才能进行调试。&lt;/p>
&lt;h1 id="pod-无法启动时让其强制启动">Pod 无法启动时，让其强制启动&lt;/h1>
&lt;p>当我们发现 Pod 无法启动时，除了日常通过 kubectl 命令，查看日志等常规手段以外，还有可能需要让 Pod 强制启动，以便更深入排障&lt;/p>
&lt;p>首先，编辑 Pod 的控制器，删除各种探针&lt;/p>
&lt;p>然后利用 pod.spec.containers.tty 和 pod.spec.containers.command 两个字段，为 Pod 分配一个终端，并保持 Pod 运行状态&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">myapp-bj-test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">command&lt;/span>: [&lt;span style="color:#ae81ff">sh]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tty&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这时，我们就可以通过 kubectl exec 命令进入这个容器中，进行各种调试了。&lt;/p>
&lt;h1 id="kubernetes-debug">Kubernetes Debug&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://mp.weixin.qq.com/s/PrmR-7vub9oVz-EFEZTGaQ">https://mp.weixin.qq.com/s/PrmR-7vub9oVz-EFEZTGaQ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/iPXKS36GKzfd404oT39vrQ">https://mp.weixin.qq.com/s/iPXKS36GKzfd404oT39vrQ&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>曾几何时，我们将自己的应用运行在 Kubernetes 上，每当出现容器异常崩溃时，我们往往都是一边重启容器，一边面对崩溃的容器无从下手。通常在业务研发自己 build 的镜像内包含了 shell，我们还能通过在 command 中嵌入一个[&amp;ldquo;sleep&amp;rdquo;, &amp;ldquo;3600&amp;rdquo;]命令来阻塞容器内服务启动，不过也有时候会出现不知道从哪里冒出来一个 distroless 镜像，这时可能最先崩溃的就是运维了。那是一种运维这个职业自诞生以来，第一次感受到手足无措并脱离掌控的无助感。于是在 k8s 环境下无法 debug 容器的梗开始在坊间广为吐槽。
第一个打破魔咒的是 kubectl-debug，它包含了&lt;strong>agent&lt;/strong>和&lt;strong>debug-tools&lt;/strong>两个部分。也是目前全网内搜到文档最全的解决方案。不过目前它的开发似乎已经停止，上一次提交还是在 8 个月之前，而最近一次 Release 版本也停留在两年前。更难以接受的是，当前它无法被集成在容器运行时为 Containerd 的 k8s 集群。
尽管 kubectl-debug 曾经确实是一款非常好用的容器调试工具，但如今 Kubernetes 已经有了更好的容器调试解决方案，Ephemeral Containers&lt;/p>
&lt;h2 id="ephemeral-containers">Ephemeral Containers&lt;/h2>
&lt;p>Ephemeral Containers 字如其名，它就是一个临时容器。这是一个自 Kubernetes v1.16 中作为 alpha 引入的新功能，虽然当前它还没有 GA，不过自从在 Kubernetes v1.18 之后，在 kubectl 内已经集成了 debug 客户端，我们几乎可以完整的使用并体验它的新特性。
临时容器的目标是为 Kubernetes 用户提供一个故障诊断工具，同时具备满足以下需求：&lt;/p>
&lt;ul>
&lt;li>作为一个开箱即用的平台化工具&lt;/li>
&lt;li>不依赖于已经包含在容器镜像中的工具&lt;/li>
&lt;li>不需要直接登陆计算节点(可以通过 Kubernetes API 的管理访问 Node)&lt;/li>
&lt;/ul>
&lt;p>不过也有东西是临时容器不计划支持的，比如对 windows 上启用临时容器就不太友好。
启用临时容器的特性也非常简单，在 kubernetes v1.16 之后的版本中将启动参数&amp;ndash;feature-gates=EphemeralContainers=true 配置到 kube-api 和 kubelet 服务上重启即可。
在 1.20 之前，kubectl debug 工具被放在 alpha 中，注意不同版本的命令操作差别 这里推荐使用客户端为 1.20+的版本体验会更好
那么我们有了 Ephemeral Containers 能做哪些事情呢？&lt;/p>
&lt;h3 id="1-pod-troubleshooting">1. POD Troubleshooting&lt;/h3>
&lt;p>如上文所说，我们可以直接通过 kubectl debug 命令进行容器调试。最直接简单的对一个 pod 进行调试命令如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug mypod -it --image&lt;span style="color:#f92672">=&lt;/span>busybox
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>默认情况下用户不指定临时容器名称的话，debug 容器名称就由 kubectl 自动生成一个唯一 id 的名称。如果用户需要自己指定容器名称则使用&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug mypod -c debugger --image&lt;span style="color:#f92672">=&lt;/span>busybox
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>有了临时容器除了日常 debug 功能外，我们可以扩展出很多新花样的玩法。比如批量跑某个命名空间下的安全扫描的脚本而不用干扰原容器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> pod in &lt;span style="color:#66d9ef">$(&lt;/span>kubectl get -o name pod&lt;span style="color:#66d9ef">)&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">do&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl debug --image security/pod_scanner -p $pod /sanner.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="2-pod-troubleshooting-by-copy">2. POD Troubleshooting by Copy&lt;/h4>
&lt;p>对于没有开启 Ephemeral Containers 特性的集群，我们就只能通过复制模式来调试容器。它的原理是复制一个指定 pod 的新容器，并将 debug 作为 sidecar 跟随新容器一起启动。通过这种方式也能达到曲线救国的目的。此种方式的几个参数还是挺有意思：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>--copy-to 指定新pod的名称
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--replace&lt;span style="color:#f92672">=&lt;/span>true 是否删除原容器
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--same-node&lt;span style="color:#f92672">=&lt;/span>true 是否调度到和原容器一样的node上
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--share-processes&lt;span style="color:#f92672">=&lt;/span>true 是否共享容器pid空间
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>例如我们就可以启动一个跟需要调试 pod 一样配置的 debug 容器如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug mypod -it &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--container&lt;span style="color:#f92672">=&lt;/span>debug &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--image&lt;span style="color:#f92672">=&lt;/span>busybox &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--copy-to&lt;span style="color:#f92672">=&lt;/span>my-debugger &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--same-node&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--share-processes&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3-node-troubleshooting">3. Node Troubleshooting&lt;/h4>
&lt;p>对！你没看错！利用 Ephemeral Containers 还能对 Worker 节点进行调试。当以节点为目标调用时，kubectl debug 将创建一个带有 node 名称的 pod，并且调度到该节点。同时该容器还具备了 hostIPC、hostNetwork 和 hostPID 这些特权模式。不可思议的是 Worker 节点的根文件系统还被 mount 到了 debug 容器下的/host 目录下。
直接执行这个命令就能 debug 主机。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug node/mynode -it --image&lt;span style="color:#f92672">=&lt;/span>busybox
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="debug-镜像">Debug 镜像&lt;/h2>
&lt;p>工欲善其事，必先利其器。不管怎样我们都需要一套工具完善的 debug 镜像，在处理问题时能够得心应手。虽然网上也有不少 debug 镜像，不过都还是不如自己构建来的畅快。
这里小白分享一个 Debug 镜像的 Dockerfile，大家可以根据自己条件修改即可。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>FROM golang:alpine as grpcurl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ENV XXX 添加 go 代理
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RUN apk update &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> apk add --virtual build-dependencies git &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> apk add bash curl jq &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> go get -u github.com/fullstorydev/grpcurl &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>FROM alpine:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RUN sed -i &lt;span style="color:#e6db74">&amp;#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g&amp;#39;&lt;/span> /etc/apk/repositories &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> apk update &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> apk add --no-cache vim bash tcpdump curl wget strace mysql-client iproute2 redis jq iftop tzdata tar nmap bind-tools htop &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>RUN wget -O /usr/bin/httpstat https://github.com/davecheney/httpstat/releases/download/v1.0.0/httpstat-linux-amd64-v1.0.0 &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> chmod +x /usr/bin/httpstat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>COPY --from&lt;span style="color:#f92672">=&lt;/span>grpcurl /go/bin/grpcurl /usr/bin/grpcurl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ENV TZ&lt;span style="color:#f92672">=&lt;/span>Asia/Shanghai LC_ALL&lt;span style="color:#f92672">=&lt;/span>C.UTF-8 LANG&lt;span style="color:#f92672">=&lt;/span>C.UTF-8 LANGUAGE&lt;span style="color:#f92672">=&lt;/span>C.UTF-8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ENTRYPOINT &lt;span style="color:#f92672">[&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/bin/bash&amp;#34;&lt;/span> &lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker build -t lchdzh/k8s-debug:v1 .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl debug -n ingress-controller nginx-hw-cloud-ingress-nginx-controller-85m49 -it &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--container&lt;span style="color:#f92672">=&lt;/span>debug &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--image&lt;span style="color:#f92672">=&lt;/span>lchdzh/k8s-debug:v1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--copy-to&lt;span style="color:#f92672">=&lt;/span>my-debugger &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--same-node&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--share-processes&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>``debug 镜像内支持的工具包如下图
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ib9dxg/1627884711514-0a090b8c-a82b-481f-ac33-960e41a91080.png" alt="image.png">&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文主要讲述了 kubernetes 在 v1.18 版本之后被提上 alpha 的 Ephemeral Containers 特性，通过临时容器我们可以 debug 容器，甚至还可以 debug 主机。它确实是一个非常方便和足以替代 kubectl-debug 的解决方案。不过，目前临时容器对于用户权限这块并没有特别的说明，特别是用特权模式调试主机的时候，希望后面能够借助 PSP（Pod Security Policy）做一个额外的补充。&lt;/p>
&lt;h1 id="heading">&lt;/h1></description></item><item><title>Docs: 故障处理技巧</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/</guid><description/></item></channel></rss>