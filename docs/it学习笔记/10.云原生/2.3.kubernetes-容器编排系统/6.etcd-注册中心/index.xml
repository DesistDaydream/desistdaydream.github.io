<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 6.Etcd 注册中心</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/6.etcd-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/</link><description>Recent content in 6.Etcd 注册中心 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/6.etcd-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Etcd 集群注册中心的实现程序</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/6.etcd-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/etcd-%E9%9B%86%E7%BE%A4%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%A8%8B%E5%BA%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/6.etcd-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/etcd-%E9%9B%86%E7%BE%A4%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%A8%8B%E5%BA%8F/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>etcd 详细用法详见 ETCD 简介&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>作为 kubernetes 集群的存储系统使用(也可以算是集群的注册中心)，保存了集群的所有配置信息，需要高可用，如果需要在生产环境下使用，则需要在单独部署&lt;/p>
&lt;p>Note：etcd 只接收 apiserver 的请求&lt;/p>
&lt;p>每个 etcd 一般使用两个端口进行工作，一个端口面向客户端提供服务(port/2379)，另一个端口集群内部通信(port/2380)&lt;/p>
&lt;p>想要正常运行 ETCD，需要注意以下几点：&lt;/p>
&lt;ol>
&lt;li>配置 etcd 的证书，如果不使用证书，则不法分子有可能直接去修改 etcd 数据&lt;/li>
&lt;li>ca.crt(证书 CN：etcd-ca) # 给 apiserver 发客户端证书，给 etcd 发服务端证书以及对等证书&lt;/li>
&lt;li>peer.crt(证书 CN：HostName) # etcd 集群各节点属于对等节点，使用 peer 类型证书(一般分为 server 证书和 client 证书，但是 etcd 集群之间不存在服务端和客户端的区别)&lt;/li>
&lt;li>apiserver-etcd-client.crt(证书 CN：kube-apiserver-etcd-client) # 与 server.crt 证书对应。apiserver 作为 etcd 的客户端所用的证书&lt;/li>
&lt;li>server.crt(证书 CN：HostName) # 与 apiserver-etcd-client.crt 证书对应。etcd 作为 apiserver 的服务端所用的证书&lt;/li>
&lt;li>修改 etcd 的配置文件&lt;/li>
&lt;/ol>
&lt;h1 id="etcd-metrics">Etcd Metrics&lt;/h1>
&lt;p>详见：k8s 主要组件 metrics 获取指南&lt;/p>
&lt;h1 id="etcdctl-命令行工具使用说明">etcdctl 命令行工具使用说明&lt;/h1>
&lt;p>详见：etcdctl 命令行工具&lt;/p>
&lt;p>k8s 集群中使用 etcdctl 的技巧：可以创建一个别名，以便后续使用 etcdctl 命令的时候，可以使用别名来自动加载证书，比如下面&lt;/p>
&lt;p>测试环境&lt;/p>
&lt;ol>
&lt;li>echo &amp;lsquo;alias etcdctl=&amp;ldquo;kubectl exec -it -n kube-system etcd-master-1.tj-test &amp;ndash; etcdctl &amp;ndash;key=/etc/kubernetes/pki/etcd/peer.key &amp;ndash;cert=/etc/kubernetes/pki/etcd/peer.crt &amp;ndash;cacert=/etc/kubernetes/pki/etcd/ca.crt &amp;ndash;endpoints=172.38.40.212:2379,172.38.40.213:2379,172.38.40.214:2379&amp;rdquo;&amp;rsquo; &amp;raquo; /etc/bashrc&lt;/li>
&lt;/ol>
&lt;p>测试环境外部 etcd&lt;/p>
&lt;ol>
&lt;li>echo &amp;lsquo;alias etcdctl=&amp;ldquo;export ETCDCTL_API=3; etcdctl &amp;ndash;key=/etc/ssl/etcd/ssl/admin-master-1.tj-test-key.pem &amp;ndash;cert=/etc/ssl/etcd/ssl/admin-master-1.tj-test.pem &amp;ndash;cacert=/etc/ssl/etcd/ssl/ca.pem &amp;ndash;endpoints=172.38.40.212:2379,172.38.40.213:2379,172.38.40.214:2379&amp;rdquo;&amp;rsquo; &amp;raquo; /etc/bashrc&lt;/li>
&lt;/ol>
&lt;p>无锡环境&lt;/p>
&lt;ol>
&lt;li>echo &amp;lsquo;alias etcdctl=&amp;ldquo;kubectl exec -it -n kube-system etcd-master-1.wx &amp;ndash; etcdctl &amp;ndash;key=/etc/kubernetes/pki/etcd/peer.key &amp;ndash;cert=/etc/kubernetes/pki/etcd/peer.crt &amp;ndash;cacert=/etc/kubernetes/pki/etcd/ca.crt &amp;ndash;endpoints=172.40.0.3:2379,172.40.0.4:2379,172.40.0.5:2379&amp;rdquo;&amp;rsquo; &amp;raquo; /root/.bashrc&lt;/li>
&lt;/ol>
&lt;p>etcdctl 工具也可以直接从 etcd 容器的目录 /usr/local/bin 下，直接拷贝到宿主机上使用。这样使用起来也更方便&lt;/p>
&lt;ol>
&lt;li>echo &amp;lsquo;alias etcdctl=&amp;ldquo;export ETCDCTL_API=3; etcdctl &amp;ndash;key=/etc/kubernetes/pki/etcd/peer.key &amp;ndash;cert=/etc/kubernetes/pki/etcd/peer.crt &amp;ndash;cacert=/etc/kubernetes/pki/etcd/ca.crt &amp;ndash;endpoints=172.38.40.212:2379,172.38.40.213:2379,172.38.40.214:2379&amp;rdquo;&amp;rsquo; &amp;raquo; /root/.bashrc&lt;/li>
&lt;/ol>
&lt;h1 id="etcd-参数详解">Etcd 参数详解&lt;/h1>
&lt;p>参考：Etcd 命令行 flag 详解
下面是&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
creationTimestamp: null
labels:
component: etcd
tier: control-plane
name: etcd
namespace: kube-system
spec:
containers:
- command:
- etcd
- --advertise-client-urls=https://10.10.100.101:2379
- --cert-file=/etc/kubernetes/pki/etcd/server.crt
- --client-cert-auth=true
- --data-dir=/var/lib/etcd
- --initial-advertise-peer-urls=https://10.10.100.101:2380
- --initial-cluster=master-1.test.tjiptv.net=https://10.10.100.101:2380
- --key-file=/etc/kubernetes/pki/etcd/server.key
- --listen-client-urls=https://127.0.0.1:2379,https://10.10.100.101:2379
- --listen-metrics-urls=http://127.0.0.1:2381
- --listen-peer-urls=https://10.10.100.101:2380
- --name=master-1.test.tjiptv.net
- --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
- --peer-client-cert-auth=true
- --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
- --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
- --snapshot-count=10000
- --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
image: k8s.gcr.io/etcd:3.3.15-0
imagePullPolicy: IfNotPresent
livenessProbe:
failureThreshold: 8
httpGet:
host: 127.0.0.1
path: /health
port: 2381
scheme: HTTP
initialDelaySeconds: 15
timeoutSeconds: 15
name: etcd
resources: {}
volumeMounts:
- mountPath: /var/lib/etcd
name: etcd-data
- mountPath: /etc/kubernetes/pki/etcd
name: etcd-certs
hostNetwork: true
priorityClassName: system-cluster-critical
volumes:
- hostPath:
path: /etc/kubernetes/pki/etcd
type: DirectoryOrCreate
name: etcd-certs
- hostPath:
path: /var/lib/etcd
type: DirectoryOrCreate
name: etcd-data
&lt;/code>&lt;/pre></description></item><item><title>Docs: Etcd 中存储的数据之探究</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/6.etcd-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/etcd-%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%8E%A2%E7%A9%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/6.etcd-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/etcd-%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%8E%A2%E7%A9%B6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;p>K8s 的架构复杂，涉及到概念非常多，其基础组件包含 ETCD、kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy 等，其运行时环境为 docker 或 Rkt，当然还包含很多插件。在我看来，k8s 是 DevOps 的未来，因此不禁想写一些它的故事。&lt;/p>
&lt;p>ETCD 在 k8s 技术栈的地位，就仿佛数据库（Mysql、Postgresql 或 oracle 等）在 Web 应用中的地位，它存储了 k8s 集群中所有的元数据（以 key-value 的方式）。那么很现实的一个问题是：这些元数据是如何组织并保存的呢？本文就对此问题探究一番。&lt;/p>
&lt;p>探究正文&lt;/p>
&lt;p>相关环境&lt;/p>
&lt;ul>
&lt;li>
&lt;p>两台 2 个核 4G 内存（2C4G）的虚拟机，ip 分别为 192.168.205.137 和 192.168.205.139&lt;/p>
&lt;/li>
&lt;li>
&lt;p>k8s 相关控件-1.8.6&lt;/p>
&lt;/li>
&lt;li>
&lt;p>etcd-3.3.10&lt;/p>
&lt;/li>
&lt;li>
&lt;p>docker-18.06.1-ce&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>k8s 中 ETCD 数据的增删改查&lt;/p>
&lt;p>首先应该明确，K8s 中所有元数据的增删改查都是由 kube-apiserver 来执行的，那么这些数据在 ETCD 中必然有一套存储规范，这样才能保证在集群中部署成千上万的应用时不会出差错。在此基础上可以认为，只要掌握了 k8s 在 ETCD 中存储数据的规范，便可以像 k8s 一样手动来操作 ETCD 了（虽然不建议这么做）。不过更大的价值是能对 k8s 的理解更进一步，便于以后 debug 或者二次开发 k8s 的某些功能时更有底气。&lt;/p>
&lt;p>初探 ETCD 中的数据&lt;/p>
&lt;p>本文对 ETCD 的操作主要使用了其官方客户端工具 etcdctl，这里不对 etcdctl 进行详解了（需要用一整篇博客来介绍它才行），只就用到的一些命令进行阐释。&lt;/p>
&lt;p>获取 ETCD 中的所有的 key 值&lt;/p>
&lt;p>下面的命令可以获取 ETCD 中的所有的 key 值：&lt;/p>
&lt;pre>&lt;code># 获取ETCD中的所有数据
# --prefix 表示获取所有key值头为某个字符串的数据， 由于传入的是&amp;quot;&amp;quot;，所以会匹配所有的值
# --keys-only 表示只返回key而不返回value
# 对输出的结果使用grep过滤掉空行
/$ ETCDCTL_API=3 etcdctl get &amp;quot;&amp;quot; --prefix --keys-only |grep -Ev &amp;quot;^$&amp;quot;
# 输出结果如下所示，实际数据会非常整齐
/registry/apiextensions.k8s.io/customresourcedefinitions/globalbgpconfigs.crd.projectcalico.org
/registry/apiextensions.k8s.io/customresourcedefinitions/globalfelixconfigs.crd.projectcalico.org
/registry/apiextensions.k8s.io/customresourcedefinitions/globalnetworkpolicies.crd.projectcalico.org
# ... 略过很多条目
/registry/namespaces/default/registry/namespaces/kube-public
/registry/namespaces/kube-system/registry/pods/kube-system/canal-mljsv
/registry/pods/kube-system/canal-qlvh6
# ... 略过很多条目
/registry/services/endpoints/kube-system/kube-scheduler
/registry/services/specs/default/kubernetes
/registry/services/specs/kube-system/kube-dnscompact_rev_key
# 总共有240条记录
/$ etcdctl get &amp;quot;&amp;quot; --prefix --keys-only |grep -Ev &amp;quot;^$&amp;quot;|wc -l240
&lt;/code>&lt;/pre>
&lt;h2 id="etcd-中-key-值的规律">ETCD 中 key 值的规律&lt;/h2>
&lt;p>通过观察可以简单得出下面几个规律：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>k8s 主要把自己的数据注册在/registry/前缀下面（在 ETCD-v3 版本后没有了目录的概念，只能一切皆前缀了）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过观察 k8s 中 deployment、namespace、pod 等在 ETCD 中的表示，可以知道这部分资源的 key 的格式为/registry/#{k8s 对象}/#{命名空间}/#{具体实例名}。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>存在一个与众不同的 key 值 compact_rev_key，搜索可以知道这是 apiserver/compact.go 中用来记录无效数据版本使用的；运行 etcdctl get compact_rev_key 可以发现，输出的是一个整形数值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在查看 ETCD 时，k8s 中除了必要的网络插件 canal，未部署其他的应用，此时 ETCD 中只有 240 条数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>有了上面的规律，可以初步得出一个结论：在研究 k8s 时重点关注/registry/前缀的 key 及其 value 即可。&lt;/p>
&lt;p>ETCD 中的 value 值&lt;/p>
&lt;p>通过上面的内容知道，k8s 在 ETCD 中保存数据时 key 的取值非常讲究，规律非常容易概括出来。那么这些 key 值所对应的值是什么样子呢？我试着输出了/registry/ranges/serviceips 和/registry/services/endpoints/default/kubernetes 这两个 key 所对应的值，效果见下面编码展示区。&lt;/p>
&lt;pre>&lt;code># 获取&amp;quot;/registry/ranges/serviceips&amp;quot;所对应的值
# 发现这里有很多奇怪的字符=。=
# 可以大体推断出来，集群所有service的ip范围为10.96.0.0/12， 与api-server的yaml文件中配置的一致
/$ etcdctl get /registry/ranges/serviceips
/registry/ranges/serviceips
k8s
v1RangeAllocation&amp;amp;
&amp;quot;*28Bz
10.96.0.0/12&amp;quot;
# 获取&amp;quot;/registry/services/endpoints/default/kubernetes&amp;quot;所对应的值
# 发现这里有很多奇怪的字符=。=
# 在default命名空间的kubernetes这个service所对应的endpoint有两个ip
# 分别为192.168.205.137和192.168.205.139
/$ etcdctl get /registry/services/endpoints/default/kubernetes
/registry/services/endpoints/default/kubernetes
k8s
v1 Endpoints�
O
kubernetesdefault&amp;quot;*$0b6bb724-f066-11e8-be14-000c29d2cb3a2ں��z;
192.168.205.137
192.168.205.139
https�2TCP&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>可以很明显看出来，ETCD 中保存的并不是输出友好的数据（比如 json 或 xml 等就是输出友好型数据）。当然，如果进一步研究可以知道，ETCD 保存的是 Protocol Buffers 序列化后的值。如果大家对 Protobuf 有研究，可以知道这个协议也是个 key-value 的协议，只不过会把其 key-value 值按照特定的算法进行压缩，不过并没有压缩的很厉害，显式输出这些值多少也能获取到一些信息；比如 /registry/services/endpoints/default/kubernetes 对应的 192.168.205.137、192.168.205.139 等值。&lt;/p>
&lt;p>ETCD 中其他的 key 及其 value&lt;/p>
&lt;p>通过上面的探索，对 ETCD 中存储的数据有了大体的了解，接下来就可以开始更加刺激的冒险了。&lt;/p>
&lt;p>据说 flannel 需要使用 ETCD 保存网络元数据&lt;/p>
&lt;p>那么，flannel 在 ETCD 中保存的数据是什么，保存在哪个 key 中了呢？下面把所有网关相关的几个关键词 canal|flannel|calico 输出可以知道，里面只有一个可能包含 flannel 所需数据的 key，即/registry/configmaps/kube-system/canal-config，输出内容后对比关于 flannel 的 etcd 配置这篇文章，很大程度可以认为就是它了（需要进一步去 canal 项目的源码中去确认）。&lt;/p>
&lt;p>/$ etcdctl get &amp;quot;&amp;quot; &amp;ndash;prefix &amp;ndash;keys-only |grep -Ev &amp;ldquo;^$&amp;quot;|grep&amp;quot;canal|flannel|calico&amp;rdquo;# &amp;hellip; 忽略很多条/registry/configmaps/kube-system/canal-config# &amp;hellip; 忽略很多条# 可以看到里面有一个配置项 net-conf， 对比flannel的配置，可以认为这个地方很可能就是canal项目中flannel在etcd中需要的值。这里设置了网段为&amp;quot;10.244.0.0/16&amp;quot;/$ etcdctl get /registry/configmaps/kube-system/canal-config# &amp;hellip; 省略很多 net-conf.jsonI{&amp;ldquo;Network&amp;rdquo;:&amp;ldquo;10.244.0.0/16&amp;rdquo;, &amp;ldquo;Backend&amp;rdquo;:{&amp;ldquo;Type&amp;rdquo;:&amp;ldquo;vxlan&amp;rdquo;}}&lt;/p>
&lt;p>default 命名空间中 endpoint 实例 kubernetes 的值&lt;/p>
&lt;h1 id="在-k8s-的-ha-部署时会发现-default-命名空间有一个默认的-service-明明没有-selector如果不熟悉这个概念可以自行去搜索了解一下却有实实在在的-endpoints-值-那么这个值是如何写入的呢-kubectl-describe-svc-kubernetesname-kubernetesnamespace-defaultlabels-componentapiserver-providerkubernetesannotations-noneselector-nonetype-clusteripip-109601port-https-443tcptargetport-6443tcpendpoints-19216820513764431921682051396443session-affinity-clientipevents-none-通过在-etcd-中检索可以获取到两个-key-值-一个对应的是-default-命名空间中的-service另一个是对应的-endpoint-etcdctl-get----prefix---keys-only-grep--ev-grepdefaultgrepkubernetesregistryservicesendpointsdefaultkubernetesregistryservicesspecsdefaultkubernetes">在 k8s 的 HA 部署时，会发现 default 命名空间有一个默认的 service# 明明没有 selector（如果不熟悉这个概念可以自行去搜索了解一下），却有实实在在的 Endpoints 值# 那么这个值是如何写入的呢？/$ kubectl describe svc kubernetesName: kubernetesNamespace: defaultLabels: component=apiserver provider=kubernetesAnnotations: &lt;!-- raw HTML omitted -->Selector: &lt;!-- raw HTML omitted -->Type: ClusterIPIP: 10.96.0.1Port: https 443/TCPTargetPort: 6443/TCPEndpoints: 192.168.205.137:6443,192.168.205.139:6443Session Affinity: ClientIPEvents: &lt;!-- raw HTML omitted --># 通过在 ETCD 中检索可以获取到两个 key 值# 一个对应的是 default 命名空间中的 service，另一个是对应的 endpoint/$ etcdctl get &amp;quot;&amp;quot; &amp;ndash;prefix &amp;ndash;keys-only |grep -Ev &amp;ldquo;^$&amp;quot;|grep&amp;quot;default&amp;rdquo;|grep&amp;quot;kubernetes&amp;quot;/registry/services/endpoints/default/kubernetes/registry/services/specs/default/kubernetes&lt;/h1>
&lt;p>我把/registry/services/endpoints/default/kubernetes 输入到搜索引擎搜索了一下，发现有人在 github 上抛出类似的问题，从其 The three apiservers (Ip Adresses .31,.32,.33) are constantly overwriting the etcd-key /registry/services/endpoints/default/kubernetes 可以推测出来，这个值是 api-server 这个控件主动去写入的。&lt;/p>
&lt;p>这能得出一个结论：在部署高可用集群时，如果想把多个 api-server 注册到集群，那么所有的 api-server 的服务都将会出现在 default 命名空间的 kubernetes 这个 endpoints 上；这也就意味着难以把集群中的一个 api-server 单独隔离出来而不让它对外提供服务（我当前想 debug 的一个问题需要这么操作，得出这个结论表示很无奈）。&lt;/p>
&lt;p>小结&lt;/p>
&lt;p>文本对 k8s 数据仓库 ETCD 进行了探究，总结了 ETCD 保存 k8s 数据时 key 值的规范，并尝试查看了 value 值的内容。最后对几个感兴趣的 key 值及其 value 值进行了探索。通过探究可以知道，k8s 把集群的信息非常有条理地保存在 ETCD 中，key 值定义有章可循，比较方便 debug；同时，虽然 ETCD 中的 value 值是 protobuf 序列化后的数据，不适合展示，不过输出到文本后依然有一定的参考价值。&lt;/p>
&lt;p>参考&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Production-Grade Container Orchestration - Kubernetes Kubernetes 官网&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubernetes 是什么 _ Kubernetes(K8S)中文文档_Kubernetes 中文社区 k8s 中文文档&lt;/p>
&lt;/li>
&lt;li>
&lt;p>GitHub - cookeem/kubeadm-ha: Kubernetes high availiability deploy based on kubeadm (English/中文 for v1.11.x/v1.9.x/v1.7.x/v1.6.x) k8s 高可用部署方案&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Installing Calico for policy and flannel for networking 网络插件的安装&lt;/p>
&lt;/li>
&lt;li>
&lt;p>flannel Container Networking | Configuring flannel Networking 描述了 flannel 配置 etcd 作为 datastore 的做法，可以推敲出 etcd 中保存的值可能的样子&lt;/p>
&lt;/li>
&lt;li>
&lt;p>flannel/configuration.md at master · coreos/flannel · GitHub flannel 配置 etcd 作为 datastore 的文档&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubernetes service multiple apiserver endpoints · Issue #19989 · kubernetes/kubernetes · GitHub 从这里的描述可以看出 api-server 本身主动向 ETCD 写数据&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>