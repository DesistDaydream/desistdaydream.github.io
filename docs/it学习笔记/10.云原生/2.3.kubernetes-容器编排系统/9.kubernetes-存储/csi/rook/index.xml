<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Rook</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/</link><description>Recent content in Rook on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Rook+Ceph 部署与清理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/rook+ceph-%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%B8%85%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/rook+ceph-%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%B8%85%E7%90%86/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;h1 id="rookceph-部署">Rook+Ceph 部署&lt;/h1>
&lt;p>官方文档：&lt;a href="https://rook.github.io/docs/rook/master/ceph-quickstart.html">https://rook.github.io/docs/rook/master/ceph-quickstart.html&lt;/a>&lt;/p>
&lt;h2 id="部署环境准备">部署环境准备&lt;/h2>
&lt;p>kubernetes 集群准备&lt;/p>
&lt;p>在集群中至少有三个节点可用，满足 ceph 高可用要求，这里已配置 master 节点使其支持运行 pod。&lt;/p>
&lt;p>rook 使用存储方式&lt;/p>
&lt;p>rook 默认使用所有节点的所有资源，rook operator 自动在所有节点上启动 OSD 设备，Rook 会用如下标准监控并发现可用设备：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>设备没有分区&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设备没有格式化的文件系统&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Rook 不会使用不满足以上标准的设备。另外也可以通过修改配置文件，指定哪些节点或者设备会被使用。&lt;/p>
&lt;p>添加新磁盘&lt;/p>
&lt;p>这里在所有节点添加 1 块 50GB 的新磁盘：/dev/sdb，作为 OSD 盘，提供存储空间，添加完成后扫描磁盘，确保主机能够正常识别到：&lt;/p>
&lt;p>#扫描 SCSI 总线并添加 SCSI 设备&lt;/p>
&lt;p>for host in $(ls /sys/class/scsi_host) ; do echo &amp;ldquo;- - -&amp;rdquo; &amp;gt; /sys/class/scsi_host/$host/scan; done&lt;/p>
&lt;p>#重新扫描 SCSI 总线&lt;/p>
&lt;p>for scsi_device in $(ls /sys/class/scsi_device/); do echo 1 &amp;gt; /sys/class/scsi_device/$scsi_device/device/rescan; done&lt;/p>
&lt;p>#查看已添加的磁盘，能够看到 sdb 说明添加成功&lt;/p>
&lt;p>lsblk&lt;/p>
&lt;p>本次搭建环境为 3 master 2 node，其中 master-3 去掉了污点，让其可以运行 pod&lt;/p>
&lt;h2 id="部署-rook-operator">部署 Rook Operator&lt;/h2>
&lt;p>克隆 rook github 仓库到本地(注意：一般不使用 master 分支版本)&lt;/p>
&lt;ol>
&lt;li>
&lt;p>git clone &amp;ndash;single-branch &amp;ndash;branch master &lt;a href="https://github.com/rook/rook.git">https://github.com/rook/rook.git&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cd rook/cluster/examples/kubernetes/ceph/&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>创建 Rook Operator&lt;/p>
&lt;ol>
&lt;li>
&lt;p>kubectl apply -f common.yaml&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubectl apply -f operator.yaml&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>common.yaml 会创建如下资源：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>namespace：rook-ceph，之后的所有 rook 相关的 pod 都会创建在该 namespace 下面&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CRD：创建多个 CRDs&lt;/p>
&lt;/li>
&lt;li>
&lt;p>role &amp;amp; clusterrole：用户资源控制&lt;/p>
&lt;/li>
&lt;li>
&lt;p>serviceaccount：ServiceAccount 资源，给 Rook 创建的 Pod 使用&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>operator.yaml 会创建如下资源&lt;/p>
&lt;ol>
&lt;li>
&lt;p>deployment：rook-ceph-operator，部署 rook ceph 相关的组件&lt;/p>
&lt;/li>
&lt;li>
&lt;p>config # rook-ceph-operator 的配置文件&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>部署 rook-ceph-operator 过程中，会触发以 DaemonSet 的方式在集群部署 rook-discover&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl get pod -n rook-ceph
NAME READY STATUS RESTARTS AGE
rook-ceph-operator-667756ddb6-spqdw 1/1 Running 0 2m10s
rook-discover-2d8zm 1/1 Running 0 67s
rook-discover-fcg47 1/1 Running 0 67s
rook-discover-klkb6 1/1 Running 0 67s
&lt;/code>&lt;/pre>
&lt;h2 id="创建-rook-cluster">创建 rook Cluster&lt;/h2>
&lt;p>等待 rook-discover 与 rook-ceph-operator 容器正常运行，就可以部署 ceph cluster 了。&lt;/p>
&lt;ol>
&lt;li>kubectl apply -f cluster.yaml&lt;/li>
&lt;/ol>
&lt;p>它会创建如下资源：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>serviceaccount：ServiceAccount 资源，给 Ceph 集群的 Pod 使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>role &amp;amp; rolebinding：用户资源控制&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cluster：rook-ceph，创建的 Ceph 集群&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>该配置文件中 dataDirHostPath 字段用来定义数据盘目录&lt;/p>
&lt;p>Ceph 集群部署成功后，可以查看到的 pods 如下，其中 osd 数量取决于你的节点数量：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl get pod -n rook-ceph
NAME READY STATUS RESTARTS AGE
csi-cephfsplugin-8dn2w 3/3 Running 0 2m25s
csi-cephfsplugin-lm4r6 3/3 Running 0 2m25s
csi-cephfsplugin-provisioner-598854d87f-m8msp 6/6 Running 0 2m25s
csi-cephfsplugin-provisioner-598854d87f-w28b6 6/6 Running 0 2m25s
csi-cephfsplugin-sdbp5 3/3 Running 0 2m26s
csi-rbdplugin-452q7 3/3 Running 0 2m28s
csi-rbdplugin-5vnpc 3/3 Running 0 2m28s
csi-rbdplugin-bw6wl 3/3 Running 0 2m28s
csi-rbdplugin-provisioner-dbc67ffdc-678n7 6/6 Running 0 2m27s
csi-rbdplugin-provisioner-dbc67ffdc-jqltc 6/6 Running 0 2m27s
rook-ceph-crashcollector-master-3.tj-test-6989ff959-7zvhc 1/1 Running 0 52s
rook-ceph-crashcollector-node-1.tj-test-6b6c5d9647-cd876 1/1 Running 0 83s
rook-ceph-crashcollector-node-2.tj-test-679479f5b6-6rkf9 1/1 Running 0 28s
rook-ceph-mgr-a-5d985d4477-z5j82 1/1 Running 0 30s
rook-ceph-mon-a-6786946fcb-89z8w 1/1 Running 0 84s
rook-ceph-mon-b-84c758775f-n5rx2 1/1 Running 0 69s
rook-ceph-mon-c-84dd9549b6-npvkx 1/1 Running 0 52s
rook-ceph-operator-667756ddb6-spqdw 1/1 Running 0 6m45s
rook-ceph-osd-1-59554d79b9-9fbpw 0/1 Pending 0 0s
rook-ceph-osd-prepare-master-3.tj-test-pz7kx 1/1 Running 0 28s
rook-ceph-osd-prepare-node-1.tj-test-kkplm 1/1 Running 0 27s
rook-ceph-osd-prepare-node-2.tj-test-tcvfk 1/1 Running 0 26s
rook-discover-2d8zm 1/1 Running 0 5m42s
rook-discover-fcg47 1/1 Running 0 5m42s
rook-discover-klkb6 1/1 Running 0 5m42s
&lt;/code>&lt;/pre>
&lt;p>可以看出部署的 Ceph 集群有：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Ceph Monitors：默认启动三个 ceph-mon，可以在 cluster.yaml 里配置&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ceph Mgr：默认启动一个，可以在 cluster.yaml 里配置&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ceph OSDs：根据 cluster.yaml 里的配置启动，默认在所有的可用节点上启动&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="配置-ceph-dashboard">配置 ceph dashboard&lt;/h2>
&lt;ol>
&lt;li>kubectl apply -f dashboard-external-https.yaml&lt;/li>
&lt;/ol>
&lt;p>查看一下 nodeport 暴露的端口，这里是 30004 端口：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl get service -n rook-ceph | grep dashboard
rook-ceph-mgr-dashboard ClusterIP 10.108.174.177 &amp;lt;none&amp;gt; 8443/TCP 8m56s
rook-ceph-mgr-dashboard-external-https NodePort 10.110.152.30 &amp;lt;none&amp;gt; 8443:30004/TCP 12s
&lt;/code>&lt;/pre>
&lt;p>获取 Dashboard 的登陆账号和密码&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&amp;quot;{['data']['password']}&amp;quot; | base64 --decode &amp;amp;&amp;amp; echo
SP70Y6t&amp;quot;BkJ$3yGR'm_=
&lt;/code>&lt;/pre>
&lt;p>找到 username 和 password 字段，我这里是&lt;/p>
&lt;ul>
&lt;li>
&lt;p>admin&lt;/p>
&lt;/li>
&lt;li>
&lt;p>SP70Y6t&amp;quot;BkJ$3yGR&amp;rsquo;m_=&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>打开浏览器输入任意一个 Node 的 IP+nodeport 端口，这里使用 master 节点 ip 访问：&lt;/p>
&lt;p>&lt;a href="https://172.38.40.215:30004/">https://172.38.40.215:30004/&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712843-55431619-67e2-4013-9772-c425ecc99973.jpeg" alt="">&lt;/p>
&lt;p>登录后界面如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712971-c344974d-5f4a-4a02-be48-670379281117.jpeg" alt="">&lt;/p>
&lt;p>查看 hosts 状态：&lt;/p>
&lt;p>运行了 1 个 mgr、3 个 mon 和 3 个 osd&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712837-40fd19dd-8bd3-419c-a96b-82ad4dd423a4.jpeg" alt="">&lt;/p>
&lt;p>查看 monitors 状态：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712933-4405ce2d-607d-4b26-8dec-c9dbad6a5202.jpeg" alt="">&lt;/p>
&lt;p>查看 OSD 状态&lt;/p>
&lt;p>3 个 osd 状态正常，每个容量 100GB.&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712820-790e5743-70ff-41f3-b522-82f7ff7f5e33.jpeg" alt="">&lt;/p>
&lt;h2 id="部署-ceph-toolbox">部署 Ceph toolbox&lt;/h2>
&lt;p>默认启动的 Ceph 集群，是开启 Ceph 认证的，这样你登陆 Ceph 组件所在的 Pod 里，是没法去获取集群状态，以及执行 CLI 命令，这时需要部署 Ceph toolbox，命令如下：&lt;/p>
&lt;ol>
&lt;li>kubectl apply -f toolbox.yaml&lt;/li>
&lt;/ol>
&lt;p>部署成功后，pod 如下：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl -n rook-ceph get pods -o wide | grep ceph-tools
rook-ceph-tools-7cc7fd5755-rz64q 1/1 Running 0 6s 10.244.4.165 node-2.tj-test &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>然后可以登陆该 pod 后，执行 Ceph CLI 命令：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl exec -it -n rook-ceph rook-ceph-tools-7cc7fd5755-rz64q -- /bin/bash
# 查看ceph集群状态
[root@rook-ceph-tools-7cc7fd5755-rz64q /]# ceph status
cluster:
id: fb54f9a4-47d1-4c57-98b2-f5f7c16cd3d9
health: HEALTH_OK
services:
mon: 3 daemons, quorum a,b,c (age 13m)
mgr: a(active, since 12m)
osd: 3 osds: 3 up (since 12m), 3 in (since 12m)
data:
pools: 1 pools, 1 pgs
objects: 0 objects, 0 B
usage: 3.0 GiB used, 297 GiB / 300 GiB avail
pgs: 1 active+clean
&lt;/code>&lt;/pre>
&lt;p>查看 ceph 配置文件&lt;/p>
&lt;pre>&lt;code>[root@rook-ceph-tools-7cc7fd5755-rz64q /]# cd /etc/ceph
[root@rook-ceph-tools-7cc7fd5755-rz64q ceph]# ls
ceph.conf keyring
[root@rook-ceph-tools-7cc7fd5755-rz64q ceph]# cat ceph.conf
[global]
mon_host = 10.106.96.12:6789,10.100.114.27:6789,10.96.163.133:6789
[client.admin]
keyring = /etc/ceph/keyring
[root@rook-ceph-tools-7cc7fd5755-rz64q ceph]# cat keyring
[client.admin]
key = AQDKn2xfuxpvIhAAixmcxq5Y0JFSIyQWXFtE0w==
&lt;/code>&lt;/pre>
&lt;h1 id="rook-提供-rbd-服务">Rook 提供 RBD 服务&lt;/h1>
&lt;p>rook 可以提供以下 3 类型的存储：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Block: Create block storage to be consumed by a pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Object: Create an object store that is accessible inside or outside the Kubernetes cluster&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Shared File System: Create a file system to be shared across multiple pods&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>在提供块存储之前，需要先创建 StorageClass 和存储池。K8S 需要这两类资源，才能和 Rook 交互，进而分配持久卷（PV）。&lt;/p>
&lt;p>在 kubernetes 集群里，要提供 rbd 块设备服务，需要有如下步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>创建 rbd-provisioner pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 rbd 对应的 storageclass&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 pvc，使用 rbd 对应的 storageclass&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 pod 使用 rbd pvc&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>通过 rook 创建 Ceph Cluster 之后，rook 自身提供了 rbd-provisioner 服务，所以我们不需要再部署其 provisioner。&lt;/p>
&lt;p>备注：代码位置 pkg/operator/ceph/provisioner/provisioner.go&lt;/p>
&lt;h2 id="创建-pool-和-storageclass">创建 pool 和 StorageClass&lt;/h2>
&lt;p>创建块存储&lt;/p>
&lt;ol>
&lt;li>kubectl create -f csi/rbd/storageclass.yaml&lt;/li>
&lt;/ol>
&lt;p>创建了一个名为 replicapool 的存储池，和名为 rook-ceph-block 的 storageClass。&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl apply -f csi/rbd/storageclass.yaml
cephblockpool.ceph.rook.io/replicapool created
storageclass.storage.k8s.io/rook-ceph-block created
&lt;/code>&lt;/pre>
&lt;p>查看创建的 storageclass:&lt;/p>
&lt;pre>&lt;code>[root@master-1 ceph]# kubectl get storageclasses.storage.k8s.io
NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE
rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 6s
&lt;/code>&lt;/pre>
&lt;p>登录 ceph dashboard 查看创建的存储池：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712857-87c55f33-b37a-4aa6-a5b4-330da191df62.jpeg" alt="">&lt;/p>
&lt;h2 id="使用存储">使用存储&lt;/h2>
&lt;p>以官方 wordpress 示例为例，创建一个经典的 wordpress 和 mysql 应用程序来使用 Rook 提供的块存储，这两个应用程序都将使用 Rook 提供的 block volumes。&lt;/p>
&lt;p>启动 mysql 和 wordpress ：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>kubectl apply -f rook/cluster/examples/kubernetes/mysql.yaml&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubectl apply -f rook/cluster/examples/kubernetes/wordpress.yaml&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这 2 个应用都会创建一个块存储卷，并且挂载到各自的 pod 中，查看声明的 pvc 和 pv：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ~]# kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
mysql-pv-claim Bound pvc-6946e476-b03f-45ce-b482-f4313c4d50b3 20Gi RWO rook-ceph-block 18s
wp-pv-claim Bound pvc-29627c5c-7a45-460f-a2ab-ce9d8887bed6 20Gi RWO rook-ceph-block 16s
[root@master-1 ~]# kubectl get pv
NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
pvc-29627c5c-7a45-460f-a2ab-ce9d8887bed6 20Gi RWO Delete Bound default/wp-pv-claim rook-ceph-block 26s
pvc-6946e476-b03f-45ce-b482-f4313c4d50b3 20Gi RWO Delete Bound default/mysql-pv-claim rook-ceph-block 26s
&lt;/code>&lt;/pre>
&lt;p>注意：这里的 pv 会自动创建，当提交了包含 StorageClass 字段的 PVC 之后，Kubernetes 就会根据这个 StorageClass 创建出对应的 PV，这是用到的是 Dynamic Provisioning 机制来动态创建 pv，PV 支持 Static 静态请求，和动态创建两种方式。&lt;/p>
&lt;p>登录 ceph dashboard 查看创建的 images&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712855-9b6ab1a5-76f7-4814-bcd9-39350af05000.jpeg" alt="">&lt;/p>
&lt;p>在 Ceph 集群端检查：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ~]# kubectl -n rook-ceph exec -it rook-ceph-tools-7cc7fd5755-rz64q bash -- /bin/bash
[root@rook-ceph-tools-7cc7fd5755-rz64q /]# rbd info -p replicapool csi-vol-32fc614e-fe70-11ea-a2af-76b93ba918d2
rbd image 'csi-vol-32fc614e-fe70-11ea-a2af-76b93ba918d2':
size 20 GiB in 5120 objects
order 22 (4 MiB objects)
snapshot_count: 0
id: 639b799e1bd
block_name_prefix: rbd_data.639b799e1bd
format: 2
features: layering
op_features:
flags:
create_timestamp: Thu Sep 24 14:14:06 2020
access_timestamp: Thu Sep 24 14:14:06 2020
modify_timestamp: Thu Sep 24 14:14:06 2020e
&lt;/code>&lt;/pre>
&lt;p>登陆 pod 检查 rbd 设备：&lt;/p>
&lt;pre>&lt;code>[root@master-1 ~]# kubectl get pod -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
wordpress-7bfc545758-d88nx 1/1 Running 0 2m46s 10.244.4.166 node-2.tj-test &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
wordpress-mysql-764fc64f97-khcgl 1/1 Running 0 2m48s 10.244.2.52 master-3.tj-test &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
[root@master-1 ~]# kubectl exec -it wordpress-7bfc545758-d88nx -- /bin/bash
root@wordpress-7bfc545758-d88nx:/var/www/html# mount | grep rbd
/dev/rbd0 on /var/www/html type ext4 (rw,relatime,stripe=1024,data=ordered)
root@wordpress-7bfc545758-d88nx:/var/www/html# df -h
Filesystem Size Used Avail Use% Mounted on
.....
/dev/rbd0 20G 70M 20G 1% /var/www/html
.....
&lt;/code>&lt;/pre>
&lt;p>一旦 wordpress 和 mysql pods 处于运行状态，获取 wordpress 应用程序的集群 IP 并使用浏览器访问：&lt;/p>
&lt;pre>&lt;code>[centos@k8s-master ~]$ kubectl get svc wordpress
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
wordpress LoadBalancer 10.98.178.189 &amp;lt;pending&amp;gt; 80:30001/TCP 136m
&lt;/code>&lt;/pre>
&lt;p>访问 wordpress:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lte8up/1616117712888-0690e884-3549-4303-805b-95506d413d7c.jpeg" alt="">&lt;/p>
&lt;h1 id="清理-ceph-集群">清理 Ceph 集群&lt;/h1>
&lt;p>官方文档：&lt;a href="https://rook.github.io/docs/rook/master/ceph-teardown.html">https://rook.github.io/docs/rook/master/ceph-teardown.html&lt;/a>&lt;/p>
&lt;p>清理 Ceph 集群先，需要先清楚 Ceph 上的所有数据(PV、PVC、volumes 等等)。清理这些资源是非常重要的，如果不删除，可能会导致 Ceph 集群无法正确清理&lt;/p>
&lt;p>清理完成后，开始删除已创建的 Ceph 集群&lt;/p>
&lt;p>删除 CephCluster 这个 CRD&lt;/p>
&lt;p>在清除了上述这些块和文件资源之后，就可以删除 Rook 群集。&lt;/p>
&lt;pre>&lt;code>kubectl -n rook-ceph delete cephcluster rook-ceph
&lt;/code>&lt;/pre>
&lt;p>在继续下一步之前，请验证是否已删除群集 CRD。&lt;/p>
&lt;pre>&lt;code>kubectl -n rook-ceph get cephcluster
&lt;/code>&lt;/pre>
&lt;p>删除 Rook Operator 及其相关资源&lt;/p>
&lt;pre>&lt;code>kubectl delete -f operator.yaml
kubectl delete -f common.yaml
&lt;/code>&lt;/pre>
&lt;p>删除设备上的数据 Delete the data on hosts&lt;/p>
&lt;p>重要说明：最后的清理步骤要求删除群集中每个主机上的文件。群集 CRD 中指定的 dataDirHostPath 属性下的所有文件都需要删除。否则，启动新群集时将保持不一致状态。&lt;/p>
&lt;p>连接到每台计算机，然后删除 /var/lib/rook 或 dataDirHostPath 指定的路径。&lt;/p>
&lt;p>In the future this step will not be necessary when we build on the K8s local storage feature.&lt;/p>
&lt;p>If you modified the demo settings, additional cleanup is up to you for devices, host paths, etc.&lt;/p>
&lt;p>Zapping Devices&lt;/p>
&lt;p>被 Rook 创建的 osds 所使用的节点上的磁盘，可以通过以下方法重置为可用状态：&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env bash
DISK=&amp;quot;/dev/sdb&amp;quot;
# 将磁盘转换为新的可用状态（全部压缩非常重要，b/c MBR必须干净）
# 您必须对所有磁盘运行此步骤。
sgdisk --zap-all $DISK
# 用 dd 命令清理硬盘
dd if=/dev/zero of=&amp;quot;$DISK&amp;quot; bs=1M count=100 oflag=direct,dsync
# 使用 blkdiscard 而不是 dd 清理 ssd 等磁盘
blkdiscard $DISK
# These steps only have to be run once on each node
# 如果rook使用ceph-volume设置了osds，则拆除会留下一些映射的设备来锁定磁盘。
ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %
# eph-volume 安装程序可以将 ceph-&amp;lt;UUID&amp;gt; 目录保留在 /dev 中（不必要的混乱）
rm -rf /dev/ceph-*
&lt;/code>&lt;/pre>
&lt;p>删除 Ceph 集群后，在之前部署 Ceph 组件节点的/var/lib/rook/目录，会遗留下 Ceph 集群的配置信息。&lt;/p>
&lt;p>若之后再部署新的 Ceph 集群，先把之前 Ceph 集群的这些信息删除，不然启动 monitor 会失败；&lt;/p>
&lt;pre>&lt;code># cat clean-rook-dir.sh
hosts=(
master0
master1
master2
)
for host in ${hosts[@]} ; do
ssh $host &amp;quot;rm -rf /var/lib/rook/*&amp;quot;
done
&lt;/code>&lt;/pre>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code># kubectl delete -f cluster.yaml
&lt;/code>&lt;/pre></description></item><item><title>Docs: Rook+Ceph 的 osd 管理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/rook+ceph-%E7%9A%84-osd-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/rook+ceph-%E7%9A%84-osd-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>官方文档：&lt;a href="https://rook.github.io/docs/rook/master/ceph-osd-mgmt.html">https://rook.github.io/docs/rook/master/ceph-osd-mgmt.html&lt;/a>&lt;/p></description></item><item><title>Docs: Rook+Ceph 现存问题</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/rook+ceph-%E7%8E%B0%E5%AD%98%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/csi/rook/rook+ceph-%E7%8E%B0%E5%AD%98%E9%97%AE%E9%A2%98/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>k8s 版本：1.18.8&lt;/p>
&lt;h1 id="rook-ceph-的问题">rook-ceph 的问题&lt;/h1>
&lt;h2 id="使用-pvc-后出现">使用 pvc 后出现&lt;/h2>
&lt;p>Failed to list *v1beta1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io)&lt;/p>
&lt;p>解决方式：暂无&lt;/p>
&lt;h2 id="清理-rook-ceph-后出现">清理 rook-ceph 后出现&lt;/h2>
&lt;p>Operation for &amp;ldquo;/var/lib/kubelet/plugins/rbd.csi.ceph.com/csi.sock&amp;rdquo; failed.&amp;quot;&lt;/p>
&lt;p>问题跟踪：&lt;a href="https://github.com/rook/rook/issues/4359">https://github.com/rook/rook/issues/4359&lt;/a>&lt;/p>
&lt;h2 id="failed-to-list-v1partialobjectmetadata-the-server-could-not-find-the-requested-resource">Failed to list *v1.PartialObjectMetadata: the server could not find the requested resource&lt;/h2>
&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/79610">https://github.com/kubernetes/kubernetes/issues/79610&lt;/a>&lt;/p>
&lt;p>CRD 删除后，controller-manager 依然会持续跟踪，并报告错误&lt;/p>
&lt;p>解决方式：&lt;/p>
&lt;p>暂无&lt;/p></description></item></channel></rss>