<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 2.CGroup</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/</link><description>Recent content in 2.CGroup on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: CGroup FS</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/cgroup-fs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/cgroup-fs/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote>
&lt;h1 id="sysfscgroup">/sys/fs/cgroup/*&lt;/h1>
&lt;h3 id="cgroupv1">CGroupV1&lt;/h3>
&lt;p>CGroupV1 根目录下的每个目录的名称都是一个子系统的名称，每个子系统都有其自己独立的资源控制配置文件。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls -l /sys/fs/cgroup/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 blkio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">11&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpu -&amp;gt; cpu,cpuacct
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">11&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpuacct -&amp;gt; cpu,cpuacct
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpu,cpuacct
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpuset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 devices
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">4&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 freezer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 hugetlb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 memory
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">16&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 net_cls -&amp;gt; net_cls,net_prio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 net_cls,net_prio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">16&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 net_prio -&amp;gt; net_cls,net_prio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 perf_event
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 pids
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">2&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 rdma
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 systemd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 unified
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="cpu--cpu-子系统">./cpu # CPU 子系统&lt;/h4>
&lt;ul>
&lt;li>./cpu.cfs_quota_us 与 ./cpu.cfs_period_us # 用来限制进程每运行 cfs_period_us 一段时间，只能被分配到的总量为 cfs_quota_us 的 CPU 时间
&lt;ul>
&lt;li>cfs_quota_us 默认值为-1，不做任何限制，如果修改为 20000(20ms)则表示 CPU 只能使用到 20%的&lt;/li>
&lt;li>cfs_period_us 默认值为 100000(100ms)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>./cpu.shares #&lt;/li>
&lt;li>./cpu.stat #
&lt;ul>
&lt;li>nr_periods #&lt;/li>
&lt;li>nr_throttled #&lt;/li>
&lt;li>throttled_time #&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="cgroupv2">CGroupV2&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls -l /sys/fs/cgroup/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cgroup.controllers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.max.depth
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.max.descendants
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cgroup.procs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.stat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cgroup.subtree_control
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.threads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cpu.pressure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cpuset.cpus.effective
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cpuset.mems.effective
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>drwxr-xr-x &lt;span style="color:#ae81ff">2&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 init.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 io.cost.model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 io.cost.qos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 io.pressure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 memory.pressure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>drwxr-xr-x &lt;span style="color:#ae81ff">44&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:53 system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>drwxr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:53 user.slice
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Linux Cgroup 系列（二）：玩转 CPU</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/linux-cgroup-%E7%B3%BB%E5%88%97%E4%BA%8C%E7%8E%A9%E8%BD%AC-cpu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/linux-cgroup-%E7%B3%BB%E5%88%97%E4%BA%8C%E7%8E%A9%E8%BD%AC-cpu/</guid><description>
&lt;p>上篇文章主要介绍了 cgroup 的一些基本概念，包括其在 &lt;code>CentOS&lt;/code> 系统中的默认设置和控制工具，并以 CPU 为例阐述 cgroup 如何对资源进行控制。这篇文章将会通过具体的示例来演示如何通过 cgroup 来限制 &lt;code>CPU&lt;/code> 的使用以及不同的 cgroup 设置对性能的影响。&lt;/p>
&lt;h2 id="1-查看当前-cgroup-信息">&lt;strong>1. 查看当前 cgroup 信息&lt;/strong>&lt;/h2>
&lt;hr>
&lt;p>有两种方法来查看系统的当前 cgroup 信息。第一种方法是通过 &lt;code>systemd-cgls&lt;/code> 命令来查看，它会返回系统的整体 cgroup 层级，cgroup 树的最高层由 &lt;code>slice&lt;/code> 构成，如下所示：&lt;/p>
&lt;pre>&lt;code>$ systemd-cgls --no-page
├─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 22
├─user.slice
│ ├─user-1000.slice
│ │ └─session-11.scope
│ │ ├─9507 sshd: tom [priv]
│ │ ├─9509 sshd: tom@pts/3
│ │ └─9510 -bash
│ └─user-0.slice
│ └─session-1.scope
│ ├─ 6239 sshd: root@pts/0
│ ├─ 6241 -zsh
│ └─11537 systemd-cgls --no-page
└─system.slice
├─rsyslog.service
│ └─5831 /usr/sbin/rsyslogd -n
├─sshd.service
│ └─5828 /usr/sbin/sshd -D
├─tuned.service
│ └─5827 /usr/bin/python2 -Es /usr/sbin/tuned -l -P
├─crond.service
│ └─5546 /usr/sbin/crond -n
&lt;/code>&lt;/pre>
&lt;p>可以看到系统 cgroup 层级的最高层由 &lt;code>user.slice&lt;/code> 和 &lt;code>system.slice&lt;/code> 组成。因为系统中没有运行虚拟机和容器，所以没有 &lt;code>machine.slice&lt;/code>，所以当 CPU 繁忙时，&lt;code>user.slice&lt;/code> 和 &lt;code>system.slice&lt;/code> 会各获得 &lt;code>50%&lt;/code> 的 CPU 使用时间。user.slice 下面有两个子 slice：&lt;code>user-1000.slice&lt;/code> 和 &lt;code>user-0.slice&lt;/code>，每个子 slice 都用 User ID (&lt;code>UID&lt;/code>) 来命名，因此我们很容易识别出哪个 slice 属于哪个用户。例如：从上面的输出信息中可以看出 &lt;code>user-1000.slice&lt;/code> 属于用户 tom，&lt;code>user-0.slice&lt;/code> 属于用户 root。&lt;code>systemd-cgls&lt;/code> 命令提供的只是 cgroup 层级的静态信息快照，要想查看 cgroup 层级的动态信息，可以通过 &lt;code>systemd-cgtop&lt;/code> 命令查看：&lt;/p>
&lt;pre>&lt;code>$ systemd-cgtop
Path Tasks %CPU Memory Input/s Output/s
/ 161 1.2 161.0M - -
/system.slice - 0.1 - - -
/system.slice/vmtoolsd.service 1 0.1 - - -
/system.slice/tuned.service 1 0.0 - - -
/system.slice/rsyslog.service 1 0.0 - - -
/system.slice/auditd.service 1 - - - -
/system.slice/chronyd.service 1 - - - -
/system.slice/crond.service 1 - - - -
/system.slice/dbus.service 1 - - - -
/system.slice/gssproxy.service 1 - - - -
/system.slice/lvm2-lvmetad.service 1 - - - -
/system.slice/network.service 1 - - - -
/system.slice/polkit.service 1 - - - -
/system.slice/rpcbind.service 1 - - - -
/system.slice/sshd.service 1 - - - -
/system.slice/system-getty.slice/getty@tty1.service 1 - - - -
/system.slice/systemd-journald.service 1 - - - -
/system.slice/systemd-logind.service 1 - - - -
/system.slice/systemd-udevd.service 1 - - - -
/system.slice/vgauthd.service 1 - - - -
/user.slice 3 - - - -
/user.slice/user-0.slice/session-1.scope 3 - - - -
/user.slice/user-1000.slice 3 - - - -
/user.slice/user-1000.slice/session-11.scope 3 - - - -
/user.slice/user-1001.slice/session-8.scope 3 - - - -
&lt;/code>&lt;/pre>
&lt;p>systemd-cgtop 提供的统计数据和控制选项与 &lt;code>top&lt;/code> 命令类似，但该命令只显示那些开启了资源统计功能的 service 和 slice。比如：如果你想开启 &lt;code>sshd.service&lt;/code> 的资源统计功能，可以进行如下操作：&lt;/p>
&lt;pre>&lt;code>$ systemctl set-property sshd.service CPUAccounting=true MemoryAccounting=true
&lt;/code>&lt;/pre>
&lt;p>该命令会在 &lt;code>/etc/systemd/system/sshd.service.d/&lt;/code> 目录下创建相应的配置文件：&lt;/p>
&lt;pre>&lt;code>$ ll /etc/systemd/system/sshd.service.d/
总用量 8
4 -rw-r--r-- 1 root root 28 5月 31 02:24 50-CPUAccounting.conf
4 -rw-r--r-- 1 root root 31 5月 31 02:24 50-MemoryAccounting.conf
$ cat /etc/systemd/system/sshd.service.d/50-CPUAccounting.conf
[Service]
CPUAccounting=yes
$ cat /etc/systemd/system/sshd.service.d/50-MemoryAccounting.conf
[Service]
MemoryAccounting=yes
&lt;/code>&lt;/pre>
&lt;p>配置完成之后，再重启 &lt;code>sshd&lt;/code> 服务：&lt;/p>
&lt;pre>&lt;code>$ systemctl daemon-reload
$ systemctl restart sshd
&lt;/code>&lt;/pre>
&lt;p>这时再重新运行 systemd-cgtop 命令，就能看到 sshd 的资源使用统计了：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788822-11ad9f57-7e88-4ee1-af1e-af4f5a8fe0e0.png" alt="">&lt;/p>
&lt;h2 id="2-分配-cpu-相对使用时间">&lt;strong>2. 分配 CPU 相对使用时间&lt;/strong>&lt;/h2>
&lt;hr>
&lt;p>通过上篇文章的学习我们知道了 CPU &lt;code>shares&lt;/code> 可以用来设置 CPU 的相对使用时间，接下来我们就通过实践来验证一下。&lt;/p>
&lt;blockquote>
&lt;p>下面所做的实验都是在单核 CPU 的系统上进行的，多核与单核的情况完全不同，文末会单独讨论。&lt;/p>
&lt;/blockquote>
&lt;p>测试对象是 1 个 service 和两个普通用户，其中用户 &lt;code>tom&lt;/code> 的 UID 是 1000，可以通过以下命令查看：&lt;/p>
&lt;pre>&lt;code>$ cat /etc/passwd|grep tom
tom:x:1000:1000::/home/tom:/bin/bash
&lt;/code>&lt;/pre>
&lt;p>创建一个 &lt;code>foo.service&lt;/code>：&lt;/p>
&lt;pre>&lt;code>$ cat /etc/systemd/system/foo.service
[Unit]
Description=The foo service that does nothing useful
After=remote-fs.target nss-lookup.target
[Service]
ExecStart=/usr/bin/sha1sum /dev/zero
ExecStop=/bin/kill -WINCH ${MAINPID}
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>&lt;code>/dev/zero&lt;/code> 在 linux 系统中是一个特殊的设备文件，当你读它的时候，它会提供无限的空字符，因此 foo.service 会不断地消耗 CPU 资源。现在我们将 foo.service 的 CPU shares 改为 &lt;code>2048&lt;/code>：&lt;/p>
&lt;pre>&lt;code>$ mkdir /etc/systemd/system/foo.service.d
$ cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/systemd/system/foo.service.d/50-CPUShares.conf
[Service]
CPUShares=2048
EOF
&lt;/code>&lt;/pre>
&lt;p>由于系统默认的 CPU shares 值为 &lt;code>1024&lt;/code>，所以设置成 2048 后，在 CPU 繁忙的情况下，&lt;code>foo.service&lt;/code> 会尽可能获取 &lt;code>system.slice&lt;/code> 的所有 CPU 使用时间。现在通过 &lt;code>systemctl start foo.service&lt;/code> 启动 foo 服务，并使用 &lt;code>top&lt;/code> 命令查看 CPU 使用情况：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122789915-32785528-b825-4ff2-85f9-875db0a1eaae.png" alt="">&lt;/p>
&lt;p>目前没有其他进程在消耗 CPU，所以 foo.service 可以使用几乎 100% 的 CPU。&lt;/p>
&lt;p>现在我们让用户 &lt;code>tom&lt;/code> 也参与进来，先将 &lt;code>user-1000.slice&lt;/code> 的 CPU shares 设置为 &lt;code>256&lt;/code>：&lt;/p>
&lt;pre>&lt;code>$ systemctl set-property user-1000.slice CPUShares=256
&lt;/code>&lt;/pre>
&lt;p>使用用户 &lt;code>tom&lt;/code> 登录该系统，然后执行命令 &lt;code>sha1sum /dev/zero&lt;/code>，再次查看 CPU 使用情况：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788830-f36c9b9f-747e-49d2-8cb5-42dce4e69e81.png" alt="">&lt;/p>
&lt;p>现在是不是感到有点迷惑了？foo.service 的 CPU shares 是 &lt;code>2048&lt;/code>，而用户 tom 的 CPU shares 只有 &lt;code>256&lt;/code>，难道用户 &lt;code>tom&lt;/code> 不是应该只能使用 10% 的 CPU 吗？回忆一下我在上一节提到的，当 CPU 繁忙时，&lt;code>user.slice&lt;/code> 和 &lt;code>system.slice&lt;/code> 会各获得 &lt;code>50%&lt;/code> 的 CPU 使用时间。而这里恰好就是这种场景，同时 &lt;code>user.slice&lt;/code> 下面只有 sha1sum 进程比较繁忙，所以会获得 50% 的 CPU 使用时间。最后让用户 &lt;code>jack&lt;/code> 也参与进来，他的 CPU shares 是默认值 1024。使用用户 &lt;code>jack&lt;/code> 登录该系统，然后执行命令 &lt;code>sha1sum /dev/zero&lt;/code>，再次查看 CPU 使用情况：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788820-4de33b45-2e29-4dab-87cc-b378a4f037bb.png" alt="">&lt;/p>
&lt;p>上面我们已经提到，这种场景下 &lt;code>user.slice&lt;/code> 和 &lt;code>system.slice&lt;/code> 会各获得 &lt;code>50%&lt;/code> 的 CPU 使用时间。用户 tom 的 CPU shares 是 &lt;code>256&lt;/code>，而用户 jack 的 CPU shares 是 &lt;code>1024&lt;/code>，因此用户 jack 获得的 CPU 使用时间是用户 tom 的 &lt;code>4&lt;/code> 倍。&lt;/p>
&lt;h2 id="3-分配-cpu-绝对使用时间">&lt;strong>3. 分配 CPU 绝对使用时间&lt;/strong>&lt;/h2>
&lt;hr>
&lt;p>上篇文章已经提到，如果想严格控制 CPU 资源，设置 CPU 资源的使用上限，即不管 CPU 是否繁忙，对 CPU 资源的使用都不能超过这个上限，可以通过 &lt;code>CPUQuota&lt;/code> 参数来设置。下面我们将用户 tom 的 CPUQuota 设置为 &lt;code>5%&lt;/code>：&lt;/p>
&lt;pre>&lt;code>$ systemctl set-property user-1000.slice CPUQuota=5%
&lt;/code>&lt;/pre>
&lt;p>这时你会看到用户 tom 的 sha1sum 进程只能获得 5% 左右的 CPU 使用时间。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788812-e5ef85ea-f7f6-48b0-8c44-87a0a1fc0782.png" alt="">&lt;/p>
&lt;p>如果此时停止 &lt;code>foo.service&lt;/code>，关闭用户 jack 的 sha1sum 进程，你会看到用户 tom 的 sha1sum 进程仍然只能获得 &lt;code>5%&lt;/code>左右的 CPU 使用时间。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788841-c53eb572-e7b8-4ff0-ac62-b9cf69c3d1c9.png" alt="">&lt;/p>
&lt;p>如果某个非核心服务很消耗 CPU 资源，你可以通过这种方法来严格限制它对 CPU 资源的使用，防止对系统中其他重要的服务产生影响。&lt;/p>
&lt;h2 id="4-动态设置-cgroup">&lt;strong>4. 动态设置 cgroup&lt;/strong>&lt;/h2>
&lt;hr>
&lt;p>cgroup 相关的所有操作都是基于内核中的 cgroup virtual filesystem，使用 cgroup 很简单，挂载这个文件系统就可以了。系统默认情况下都是挂载到 &lt;code>/sys/fs/cgroup&lt;/code> 目录下，当 service 启动时，会将自己的 cgroup 挂载到这个目录下的子目录。以 &lt;code>foo.service&lt;/code> 为例：先进入 &lt;code>system.slice&lt;/code> 的 CPU 子系统：&lt;/p>
&lt;pre>&lt;code>$ cd /sys/fs/cgroup/cpu,cpuacct/system.slice
&lt;/code>&lt;/pre>
&lt;p>查看 foo.service 的 cgroup 目录：&lt;/p>
&lt;pre>&lt;code>$ ls foo.*
zsh: no matches found: foo.*
&lt;/code>&lt;/pre>
&lt;p>因为 foo.service 没有启动，所以没有挂载 cgroup 目录，现在启动 foo.service，再次查看它的 cgroup 目录：&lt;/p>
&lt;pre>&lt;code>$ ls foo.serice
cgroup.clone_children cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release
cgroup.event_control cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks
&lt;/code>&lt;/pre>
&lt;p>也可以查看它的 PID 和 CPU shares：&lt;/p>
&lt;pre>&lt;code>$ cat foo.service/tasks
20225
$ cat foo.service/cpu.shares
2048
&lt;/code>&lt;/pre>
&lt;h2 id="5-如果是多核-cpu-呢">&lt;strong>5. 如果是多核 CPU 呢？&lt;/strong>&lt;/h2>
&lt;hr>
&lt;p>上面的所有实验都是在单核 CPU 上进行的，下面我们简单讨论一下多核的场景，以 2 个 CPU 为例。&lt;/p>
&lt;p>首先来说一下 CPU shares，shares 只能针对单核 CPU 进行设置，也就是说，无论你的 shares 值有多大，该 cgroup 最多只能获得 100% 的 CPU 使用时间（即 1 核 CPU）。还是用本文第 2 节的例子，将 foo.service 的 CPU shares 设置为 2048，启动 foo.service，这时你会看到 foo.service 仅仅获得了 100% 的 CPU 使用时间，并没有完全使用两个 CPU 核：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788828-fb879234-a995-4b2f-acaa-658edd6a40dd.png" alt="">&lt;/p>
&lt;p>再使用用户 &lt;code>tom&lt;/code> 登录系统，执行命令 &lt;code>sha1sum /dev/zero&lt;/code>，你会发现用户 tom 的 sha1sum 进程和 foo.service 各使用 1 个 CPU 核：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eh4oaq/1616122788823-67ecc2d3-96dd-42dd-a2ce-316553ecb94f.png" alt="">&lt;/p>
&lt;p>再来说说 CPUQuota，这个上篇文章结尾已经提过了，如要让一个 cgroup 完全使用两个 CPU 核，可以通过 CPUQuota 参数来设置。例如：&lt;/p>
&lt;pre>&lt;code>$ systemctl set-property foo.service CPUQuota=200%
&lt;/code>&lt;/pre>
&lt;p>至于进程最后能不能完全使用两个 CPU 核，就要看它自身的设计支持不支持了。&lt;/p></description></item></channel></rss>