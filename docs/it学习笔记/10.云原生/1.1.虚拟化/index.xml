<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 1.1.虚拟化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</link><description>Recent content in 1.1.虚拟化 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 1.1.虚拟化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_getting_started_guide/index">RedHat 7 虚拟化入门指南&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/virtualization-in-rhel-8-an-overview_configuring-and-managing-virtualization#what-is-virtualization-in-rhel-8-virt-overview">Redhat 8 官方对虚拟化的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_getting_started_guide/index">RedHat 7 对“虚拟化性能不行”这个误区的辟谣&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ubuntu.com/server/docs/virtualization-introduction">Ubuntu 官方文档，虚拟化-介绍&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Virtualization(虚拟化)&lt;/strong> 是用于运行软件的广义的计算机术语。通常情况下，&lt;strong>Virtualization(虚拟化)&lt;/strong> 体现在让单个可以运行多个操作系统，这些操作系统同时运行，而又是互相独立的。&lt;/p>
&lt;p>虚拟化是云计算的基础。简单的说，虚拟化使得在一台物理的服务器上可以跑多台虚拟机，虚拟机共享物理机的 CPU、内存、IO 硬件资源，但逻辑上虚拟机之间是相互隔离的。物理机我们一般称为 &lt;strong>Host(宿主机)&lt;/strong>，宿主机上面的虚拟机称为 &lt;strong>Guest(客户机)&lt;/strong>。那么 Host 是如何将自己的硬件资源虚拟化，并提供给 Guest 使用的呢？这个主要是通过一个叫做 Hypervisor 的程序实现的。&lt;/p>
&lt;h2 id="hypervisor">Hypervisor&lt;/h2>
&lt;p>参考：&lt;a href="https://www.redhat.com/zh/topics/virtualization/what-is-a-hypervisor">https://www.redhat.com/zh/topics/virtualization/what-is-a-hypervisor&lt;/a>&lt;/p>
&lt;p>Hypervisor 是用来创建与运行虚拟机的软件、固件或硬件。被 Hypervisor 用来运行一个或多个虚拟机的设备称为 Host Machine(宿主机)，这些虚拟机则称为 Guest Machine(客户机)。&lt;strong>Hypervisor 有时也被称为 Virtual Machine Monitor (虚拟机监视器，简称 VMM)&lt;/strong>&lt;/p>
&lt;h1 id="虚拟化技术的分类">虚拟化技术的分类&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ihdpea/1616124416735-5e89f29f-21cd-4fed-af5e-194227de3048.png" alt="">
根据 Hypervisor 的实现方式和所处的位置，虚拟化又分为两种：1 型虚拟化和 2 型虚拟化&lt;/p>
&lt;ol>
&lt;li>半虚拟化（para-virtualization）：TYPE1，也叫裸金属虚拟化比如 Vmware ESXi、Xen 等是一款类似于操作系统的 Hypervisor，直接运行在硬件之上，需要修改 Guest OS 的内核，让 VM 知道自己是虚拟机&lt;/li>
&lt;li>完全虚拟化（full-virtualization）：TYPE2，物理机上首先安装常规的操作系统，比如 Redhat、Ubuntu 和 Windows。Hypervisor 作为 OS 上的一个程序模块运行，并对管理虚拟机进行管理。比如 Vmware Workstation、KVM 等是一款类似于软件的 Hypervisor，运行于操作系统之上，VM 不知道自己是虚拟机
&lt;ol>
&lt;li>BT：软件，二进制翻译。性能很差&lt;/li>
&lt;li>HVM：硬件，硬件辅助的虚拟化。性能很好。现阶段 KVM 主要基于硬件辅助进行虚拟化
&lt;ol>
&lt;li>&lt;strong>硬件辅助全虚拟化主要使用了支持虚拟化功能的 CPU 进行支撑，CPU 可以明确的分辨出来自 GuestOS 的特权指令，并针对 GuestOS 进行特权操作，而不会影响到 HostOS。&lt;/strong>&lt;/li>
&lt;li>从更深入的层次来说，虚拟化 CPU 形成了新的 CPU 执行状态 —— _ Non-Root Mode&amp;amp; Root Mode_ 。从上图中可以看见，GuestOS 运行在 Non-Root Mode 的 Ring 0 核心态中，这表明 GuestOS 能够直接执行特却指令而不再需要 &lt;em>特权解除&lt;/em> 和 &lt;em>陷入模拟&lt;/em> 机制。并且在硬件层上面紧接的就是虚拟化层的 VMM，而不需要 HostOS。这是因为在硬件辅助全虚拟化的 VMM 会以一种更具协作性的方式来实现虚拟化 —— &lt;em>将虚拟化模块加载到 HostOS 的内核中&lt;/em>，例如：KVM，KVM 通过在 HostOS 内核中加载&lt;strong>KVM Kernel Module&lt;/strong>来将 HostOS 转换成为一个 VMM。所以此时 VMM 可以看作是 HostOS，反之亦然。这种虚拟化方式创建的 GuestOS 知道自己是正在虚拟化模式中运行的 GuestOS，KVM 就是这样的一种虚拟化实现解决方案。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>OS 级别虚拟化：容器级虚拟化，准确来说不能叫虚拟化了，只能叫容器技术无 Hypervisor，将用户空间分隔为多个，彼此互相隔离，每个 VM 中没有独立内核，OpenVZ、LXC(Linux container)、libcontainer 等，比如 Docker，Docker 的基础是 LXC。&lt;/li>
&lt;li>模拟(Emulation)：比如 QEMU，PearPC，Bochs&lt;/li>
&lt;li>库虚拟化：WINE&lt;/li>
&lt;li>应用程序虚拟化：JVM&lt;/li>
&lt;li>理论上 Type1 和 Typ2 之间的区别
&lt;ol>
&lt;li>1 型虚拟化一般对硬件虚拟化功能进行了特别优化，性能上比 2 型要高；&lt;/li>
&lt;li>2 型虚拟化因为基于普通的操作系统，会比较灵活，比如支持虚拟机嵌套。嵌套意味着可以在 KVM 虚拟机中再运行 KVM。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h1 id="虚拟化总结云计算基础实现云功能的灵活调度">虚拟化总结(云计算基础，实现云功能的灵活调度)&lt;/h1>
&lt;p>所谓的云计算：当一台虚拟机需要跨越多个物理机进行数据交互，比如拿来运行 VM 的物理主机不止一台，在每台物理机上按需启动既定数量的 VM，每个 VM 有多少 CPU 和 MEM，每个 VM 启动在哪个物理机上，启动 VM 需要的存储设备在什么地方，存储设备中的系统是临时安装，还是通过一个已经装好的系统模板直接使用，还有多个 VM 跨物理主机进行网络通信等等一系列工作，可以使用一个虚拟化管理工具(VM Manager)来实现，这个管理器的功能即可称为云计算。在没有这个管理器的时候，人们只能人为手工从把 VM 从一台物理机移动到另一台物理机，非常不灵活。&lt;/p>
&lt;p>计算机五大部件：运算器(cpu)，控制器(cpu)，存储器(memory)，输入与输出设备(磁盘 I/O，网络 I/O)。&lt;/p>
&lt;p>一般情况，VM 的 CPU 与 Memory 无法跨主机使用；但是磁盘 I/O 与网络 I/O 则可以跨主机使用。云计算的灵活性（即 VM 或者单个云计算节点挂了但是不影响数据，可以重新启动在任一一个节点等类似的功能）&lt;/p>
&lt;p>磁盘 I/O 的灵活调度&lt;/p>
&lt;p>所以，在启动一个 VM 的时候，分为这么几个启动步骤，模拟 CPU 和内存，模拟存储，模拟网络。当在多个 node 的虚拟化集群中创建完一个 VM 并想启动的时候，又分为两种情况：&lt;/p>
&lt;ol>
&lt;li>当该 VM 的虚拟存储放在某个节点上的时候，则该 VM 只能启动在该节点上，因为没有存储就没法加载系统镜像，何谈启动呢&lt;/li>
&lt;li>当该 VM 的虚拟存储放在虚拟化集群的后端存储服务器或者共享存储空间的时候，则该 VM 可以根据调度策略在任一节点启动,然后把该 VM 对应的虚拟存储挂载或下载到需要启动的节点上即可（这个所谓的虚拟存储，可以称为模板，每次 VM 启动的时候，都可以通过这个模板直接启动而不用重新安装系统了）&lt;/li>
&lt;/ol>
&lt;p>这种可以灵活调度 VM，而不让 VM 固定启动在一个虚拟机上的机制，这就是云功能的基础，用户不用关心具体运行在哪个节点上，都是由系统自动调度的。&lt;/p>
&lt;p>网络 I/O 的灵活调度&lt;/p>
&lt;p>同样的，在一个 VM 从 node1 移动到 node2 的时候，除了存储需要跟随移动外，还需要网络也跟随移动，移动的前提是所有 node 的网络配置是一样的，不管是隔离模型，还是路由模型，还是 nat 模型，还是桥接模型，都需要给每个 node 进行配置，但是，会有这么几个情况，&lt;/p>
&lt;ol>
&lt;li>一个公司，有 2 个部门，有两台物理 server，node1 最多有 4 个 VM，node2 最多有 4 个 VM，其中一个部门需要 5 台 VM，另一个部门需要 3 台 VM，而两个部门又要完全隔离，这时候可以通过对 vSwitch 进行 vlan 划分来进行隔离，。这时候就一个开源的软件应运而生，就是 Open vSwtich，简称为 OVS。&lt;/li>
&lt;li>普通 VLAN 只有 4096 个，对于公有云来说，该 vlan 数量远远不够，这时候，vxlan 技术应运而生&lt;/li>
&lt;li>每个公司有多个部门,每个部门有的需要连接公网，有的不需要连接公网,如果想隔离开两个公司，仅仅依靠虚拟交换机从二层隔离，无法隔离全面，这时候 vRouter 虚拟路由器技术应运而生，通过路由来隔离，并通过路由来访问，而这台 vRouter 就是由 linux 的 net namespace 功能来创建的&lt;/li>
&lt;/ol>
&lt;p>Openstack 中创建的每个 network 就相当于一个 vSwitch，创建的每个 route 就相当于一个 vRoute 即 net namespace，然后把 network 绑定到 route 上，就相当于把 vSwitch 连接到了 vRoute，所以，在绑定完成之后，会在 route 列表中看到个端口的 IP，这个 IP 就是 vSwitch 子网(在创建 vSwitch 的时候会设置一个可用的网段)中的一个 IP，就相当于交换机连到路由器后，路由器上这个端口的 IP&lt;/p>
&lt;h1 id="实际上一个虚拟机就是宿主机上的一个文件虚拟化程序可以通过这个文件来运行虚拟机">实际上，一个虚拟机就是宿主机上的一个文件，虚拟化程序可以通过这个文件来运行虚拟机&lt;/h1></description></item><item><title>Docs: Computing Virtualization(计算虚拟化)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/computing-virtualization%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/computing-virtualization%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote>
&lt;p>CPU 虚拟化(vCPU=virtual CPU #虚拟 CPU)&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gd8p4x/1616124392775-cccdd43b-0f21-4877-8c78-c4d6c7352728.png" alt="">&lt;/p>
&lt;p>使用如下命令可以查看该 CPU 是否支持虚拟化&lt;/p>
&lt;p>egrep -o &amp;lsquo;(vmx|svm)&amp;rsquo; /proc/cpuinfo&lt;/p>
&lt;p>如果有输出 vmx 或者 svm，就说明当前的 CPU 支持 KVM。CPU 厂商 Intel 和 AMD 都支持虚拟化了，除非是非常老的 CPU。&lt;/p>
&lt;p>在 CUP 虚拟化的图片中，宿主机有两个物理 CPU，上面起了两个虚机 VM1 和 VM2。 VM1 有两个 vCPU，VM2 有 4 个 vCPU。可以看到 VM1 和 VM2 分别有两个和 4 个线程在两个物理 CPU 上调度。&lt;/p>
&lt;p>虚机的 vCPU 总数可以超过物理 CPU 数量，这个叫 CPU overcommit（超配）。 KVM 允许 overcommit，这个特性使得虚机能够充分利用宿主机的 CPU 资源，但前提是在同一时刻，不是所有的虚机都满负荷运行。 当然，如果每个虚机都很忙，反而会影响整体性能，所以在使用 overcommit 的时候，需要对虚机的负载情况有所了解，需要测试。&lt;/p></description></item><item><title>Docs: Memory Virtualization(内存虚拟化)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/memory-virtualization%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/memory-virtualization%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;p>为了在一台机器上运行多个虚拟机，KVM 需要实现 VA（虚拟内存） -&amp;gt; PA（物理内存） -&amp;gt; MA（机器内存）之间的地址转换。虚机 OS 控制虚拟地址到客户内存物理地址的映射 （VA -&amp;gt; PA），但是虚机 OS 不能直接访问实际机器内存，因此 KVM 需要负责映射客户物理内存到实际机器内存 （PA -&amp;gt; MA）。内存也是可以 overcommit 的，即所有虚机的内存之和可以超过宿主机的物理内存。但使用时也需要充分测试，否则性能会受影响。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tqgo41/1616124379716-c4fa6e3c-3050-4d41-935c-8cd582fe465e.png" alt="">&lt;/p></description></item><item><title>Docs: Network Virtual(网络虚拟化)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote>
&lt;p>在传统网络环境中，一台物理主机包含一个或多个网卡（NIC），要实现与其他物理主机之间的通信，需要通过自身的 NIC 连接到外部的网络设施，如交换机上；为了对应用进行隔离，往往是将一个应用部署在一台物理设备上，这样会存在两个问题：&lt;/p>
&lt;ol>
&lt;li>是某些应用大部分情况可能处于空闲状态，&lt;/li>
&lt;li>是当应用增多的时 候，只能通过增加物理设备来解决扩展性问题。不管怎么样，这种架构都会对物理资源造成极大的浪费。&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，可以借助虚拟化技术对一台物理资源进行抽象，将一张物理网卡虚拟成多张虚拟网卡（vNIC），通过虚拟机来隔离不同的应用。&lt;/p>
&lt;ol>
&lt;li>针对问题 1），可以利用虚拟化层 Hypervisor 的调度技术，将资源从空闲的应用上调度到繁忙的应用上，达到资源的合理利用；&lt;/li>
&lt;li>针对问题 2），可以根据物理设备的资源使用情况进行横向扩容，除非设备资源已经用尽，否则没有必要新增设备。&lt;/li>
&lt;/ol>
&lt;p>综上所述：SDN 主要是通过系统的功能，模拟出网络设备中的路由器，交换机，端口，网线等等，这些现实中的数通设备都可以通过软件来模拟实现&lt;/p>
&lt;p>网络虚拟化的几种最基础模型：&lt;/p>
&lt;ol>
&lt;li>隔离模型：在 host 上创建一个 vSwitch(bridge device)：每个 VM 的 TAP 设备直接添加至 vswitch 上，VM 通过 vSwitch 互相通信，与外界隔离&lt;/li>
&lt;li>路由模型：基于隔离模型，在 vSwitch 添加一个端口，作为 host 上的虚拟网卡使用(就是 VMware workstation 中创建的那些虚拟网卡，其中的 IP 作为虚拟机的网关)，并打开 host 的核心转发功能，使数据从 VM 发送到 host；该模型数据包可以从 VM 上出去，但是外界无法回到 VM，如果想让外部访问 VM，需要添加 NAT 功能，变成 NAT 模型&lt;/li>
&lt;li>NAT 模型：配置 Linux 自带的 NAT(可通过 iptables 定义)功能，所有 VM 的 IP 被 NAT 成物理网卡 IP，这是一种常用的虚拟网络模型&lt;/li>
&lt;li>桥接模型：可以想象成把物理机网卡变成一台 vSwitch，然后给物理机创建一个虚拟网卡，虚拟机和物理机都连接到 vSwitch，相当于把虚拟机直接接入到网络中，从网络角度看，VM 相当于同网段的一台 host&lt;/li>
&lt;li>隧道模型：VM 的数据包在经过某个具备隧道功能的虚拟网络设备时，可以在数据包外层再封装一层 IP，以 IP 套 IP 的隧道方式，与对方互通&lt;/li>
&lt;/ol>
&lt;p>网络虚拟化术语&lt;/p>
&lt;ol>
&lt;li>Network Stack：网络栈，包括网卡（Network Interface）、回环设备（LoopbackDevice）、路由表（Routing Table）和 iptables 规则。对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。&lt;/li>
&lt;li>port：当成虚拟交换机上的端口，可以打 VLAN TAG&lt;/li>
&lt;li>interface：当成接口，类似于连到端口的网线，可以设置 TYPE。&lt;a href="https://blog.csdn.net/number1killer/article/details/79226772">注意端口与接口的概念&lt;/a>&lt;/li>
&lt;li>bridge，port，interface：一个 BRIDGE 上可以配置多个 PORT，一个 PORT 上可以配置多个 INTERFACE&lt;/li>
&lt;/ol>
&lt;h1 id="网路虚拟化的解决方案">网路虚拟化的解决方案&lt;/h1>
&lt;p>基于 Linux 本身的网络虚拟化方案&lt;/p>
&lt;ol>
&lt;li>Linux Bridge # 虚拟网络基础资源，用于二层网络虚拟化&lt;/li>
&lt;li>Namespace # 网络名称空间，用于三层网络虚拟化&lt;/li>
&lt;/ol>
&lt;p>高级虚拟化方案&lt;/p>
&lt;ol>
&lt;li>Open vSwitch # 开源的虚拟交换机，用于二层网络虚拟化&lt;/li>
&lt;/ol>
&lt;h2 id="linux-bridgelinux-网桥">Linux Bridge(Linux 网桥)&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9d2b14eaa119000192206f">Linux 上抽象网络设备的原理及使用&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9d2a2712d5ba000162d3e2">云计算底层技术-虚拟网络设备(Bridge,VLAN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9d294eeaa11900019215fc">云计算底层技术-虚拟网络设备(tun tap,veth)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>每台 VM 都有一套独立的网络栈，如果想让两台 VM 互相通信怎么办呢？最直接的办法就是把两台 VM 用一根网线连接起来，而如果想要实现多台 VM 通信，那就需要用把它们连接到一台交换机上。这个&lt;strong>简单的虚拟交换机&lt;/strong>的功能就叫 &lt;code>Bridge(网桥)&lt;/code>，而&lt;strong>虚拟交换机想要实现普通物理交换机的&lt;/strong>上的功能，比如接口，网线，端口，Vlan 等 都有专门对应的虚拟网络设备来模拟实现。而交换机中的 VLAN 则是 Linux 自己本身实现的。&lt;/p>
&lt;h3 id="虚拟网络设备的类型">虚拟网络设备的类型&lt;/h3>
&lt;p>&lt;strong>Veth Pair：&lt;/strong>(Virtual Ethernet Pair)虚拟以太网设备对，可以理解为一跟网线。是 Linux 内核由模块实现的虚拟网络设备，该设备被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。创建方式详见 ip link add 命令。可以把 Veth 连接到 vSwich 与 vSwitch、vSwitch 与 nameSpace、namespace 与 namespace 上。&lt;/p>
&lt;p>veth 的作用是反转数据流量，从一段接收到数据后，会把该数据流进行反转变成发送，发送到对端后，对端接收到数据流之后，再次反转会发送给绑定到该端的 namesapce 中。ethtool -S VethNAME 可以使用该命令通过 Veth 的一半查看另一半网卡的序号&lt;/p>
&lt;p>&lt;strong>TAP/TUN&lt;/strong>：该设备会创建一个 /dev/tunX 的文件并作用在内核空间，与用户空间的 APP 相连(比如 VM)，当这个 VM 通过其 Hypervisor 通信时，会把数据写入该/dev/tunX 文件，并交给内核，内核会处理这些数据并通过网卡发送。TAP 工作在二层，TUN 工作在三层。详见 TUN TAP 设备浅析(一) &amp;ndash; 原理浅析&lt;/p>
&lt;ul>
&lt;li>Veth Pari 与 TAP/TUN 设备在 VM 与 Container 中的使用注意事项以及原因
&lt;ul>
&lt;li>为什么 VM 要使用 TAP/TUN,而 Container 不用？因为 VM 数据包在从其进程发送到 Host 的时候，由于 VM 有自己的内核，那么这个数据包相当于已经经过了一个 VM 的网络栈，这时候就不能直接发送给 Host 的网络栈再进行处理了，所以需要 TAP/TUN 设备来作为一个转折点，接收 VM 的数据包，并以一个已经经过网络栈处理过的姿态直接进入内核的网络设备。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bridge(网桥)&lt;/strong>：在 Linux 中能够起到虚拟交换机作用的网络设备，但不同于 TAP/TUN 这种单端口的设备，Bridge 实现虚拟为多端口，本质上是一个虚拟交换机，具备和物理交换机类似的功能。Bridge 可以绑定其他 Linux 网络设备作为从设备，并将这些从设备虚拟化为端口，当一个从设备被绑定到 Bridge 上时，就相当于真实网络中的交换机端口上插入了一根连有终端的网线。&lt;/p>
&lt;p>注意：一旦一块虚拟网卡被连接到 Bridge 上，该设备会变成 该 Bridge 的“从设备”。从设备会被剥夺调用网络协议栈处理数据包的资格，从而降级成网桥上的一个端口。而这个端口的唯一作用就是接收流入的数据包，然后把这些数据包的“生杀大权”(比如转发或者丢弃等)全部交给对应的 Bridge 进行处理&lt;/p>
&lt;h3 id="linux-如何实现-vlan">Linux 如何实现 VLAN&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kinqyh/1616124322231-b84ec223-407c-41fd-b0bf-0a1c61faa7c9.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>eth0 是宿主机上的物理网卡，有一个命名为 eth0.10 的子设备与之相连。 eth0.10 就是 VLAN 设备了，其 VLAN ID 就是 VLAN 10。 eth0.10 挂在命名为 brvlan10 的 Linux Bridge 上，虚机 VM1 的虚拟网卡 vent0 也挂在 brvlan10 上。&lt;/li>
&lt;li>这样的配置其效果就是： 宿主机用软件实现了一个交换机（当然是虚拟的），上面定义了一个 VLAN10。 eth0.10，brvlan10 和 vnet0 都分别接到 VLAN10 的 Access 口上。而 eth0 就是一个 Trunk 口。VM1 通过 vnet0 发出来的数据包会被打上 VLAN10 的标签。&lt;/li>
&lt;li>eth0.10 的作用是：定义了 VLAN10&lt;/li>
&lt;li>brvlan10 的作用是：Bridge 上的其他网络设备自动加入到 VLAN10 中&lt;/li>
&lt;li>再增加一个 VLAN20&lt;/li>
&lt;li>样虚拟交换机就有两个 VLAN 了，VM1 和 VM2 分别属于 VLAN10 和 VLAN20。 对于新创建的虚机，只需要将其虚拟网卡放入相应的 Bridge，就能控制其所属的 VLAN。&lt;/li>
&lt;li>VLAN 设备总是以母子关系出现，母子设备之间是一对多的关系。 一个母设备（eth0）可以有多个子设备（eth0.10，eth0.20 ……），而一个子设备只有一个母设备。&lt;/li>
&lt;/ol>
&lt;p>Linux Bridge + VLAN = 虚拟交换机&lt;/p>
&lt;ol>
&lt;li>物理交换机存在多个 VLAN，每个 VLAN 拥有多个端口。 同一 VLAN 端口之间可以交换转发，不同 VLAN 端口之间隔离。 所以交换机其包含两层功能：交换与隔离。&lt;/li>
&lt;li>Linux 的 VLAN 设备实现的是隔离功能，但没有交换功能。 一个 VLAN 母设备（比如 eth0）不能拥有两个相同 ID 的 VLAN 子设备，因此也就不可能出现数据交换情况。&lt;/li>
&lt;li>Linux Bridge 专门实现交换功能。 将同一 VLAN 的子设备都挂载到一个 Bridge 上，设备之间就可以交换数据了。&lt;/li>
&lt;/ol>
&lt;p>总结起来，Linux Bridge 加 VLAN 在功能层面完整模拟现实世界里的二层交换机。eth0 相当于虚拟交换机上的 trunk 口，允许 vlan10 和 vlan20 的数据通过。&lt;/p>
&lt;p>eth0.10，vent0 和 brvlan10 都可以看着 vlan10 的 access 口。&lt;/p>
&lt;p>eth0.20，vent1 和 brvlan20 都可以看着 vlan20 的 access 口。&lt;/p>
&lt;h2 id="open-vswitch--开放的虚拟交换机">Open vSwitch # 开放的虚拟交换机&lt;/h2>
&lt;p>详见 &lt;a href="https://www.yuque.com/go/doc/33175901">Open vSwitch&lt;/a>&lt;/p>
&lt;h2 id="network-namespace--网络名称空间">Network Namespace # 网络名称空间&lt;/h2>
&lt;p>Network Namespace 可以简单得理解为 Linux 上的 **虚拟路由器(vRouter)。**详见：&lt;a href="https://www.yuque.com/go/doc/33173252">Network Namespace 详解&lt;/a>&lt;/p>
&lt;p>Network Namespace 在逻辑上是网络栈的另一个副本，具有自己的路由、防火墙规则、网络设备等功能。默认情况下，每个进程从其父进程继承其网络名称空间。在最初的时候，所有进程都共享来自系统启动的 PID 为 1 的父进程的名称空间。整个系统 PID 为 1 的进程的 Network Namespace 就是整台机器和系统的网络栈。Linux 内核可以通过 clone()函数的功能在默认的网络名称空间中，克隆出来一个具备相同功能的网络栈，该克隆出来的 Network Namespace 为绑定上来的进程提供一个完全独立的网络协议栈，多个进程也可同时共享同一个 Network Namespace。Host 上一个网络设备只能存在于一个 Network Namespace 当中，而不同的 Network Namespace 之间要想通信，可以通过虚拟网络设备来提供一种类似于管道的抽象，在不同的 Network Namespace 中建立隧道实现通信。当一个 Network Namespace 被销毁时，插入到该 Netwrok Namespace 上的虚拟网络设备会被自动移回最开始默认整台设备的 Network Namespace&lt;/p>
&lt;p>Network Namespace 的应用场景&lt;/p>
&lt;ol>
&lt;li>可以把 Net Namepace 就相当于在物理机上创建了一台 vRouter，这台 vRouter 就是一块 namespace，把与 VM 连接的 vSwitch 连接到这台 vRouter，然后 VM 通过 vRouter 与外部或者另一部分被隔离的网络通信，这样即可实现对这台 vSwitch 以及与之关联的 VM 进行网络隔离（如果要与外部通信，那么需要使用桥接模型，把物理网卡模拟成 vSwitch，然后把该 vSwitch 关联到该 vRouter）&lt;/li>
&lt;li>Network Namespace 还可用于承载 Container 技术的网络功能，一个 Container 占据一个 Namespace，通过使用 Veth 设备连接 Namespace 与 Bridge 相连来实现各 Namespace 中的 Container 之间互相通信。具体详见 &lt;a href="https://www.yuque.com/go/doc/33173252">Network Namespace 详解&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>管理 Network Namesapce 的方式：&lt;/p>
&lt;ol>
&lt;li>通过 ip netns 命令来管理，该命令的用法详见&lt;a href="https://www.yuque.com/go/doc/33221906"> Iproute2 命令行工具&lt;/a> 中的 netns 子命令&lt;/li>
&lt;/ol>
&lt;h1 id="overlay-network-叠加网络">Overlay Network 叠加网络&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kinqyh/1616124322243-b9ecf172-ef2b-4452-8c3c-364af95526aa.jpeg" alt="">
Overlay Network 产生的原因&lt;/p>
&lt;p>在网桥的概念中，各个 VM 可以通过 Host 上的 vSwitch 来进行通信，那么当需要访问另外一台 Host 上不同网段的 VM 的时候呢？&lt;/p>
&lt;p>实现 overlay network 的方式有 gre 等&lt;/p>
&lt;p>VXLAN&lt;/p>
&lt;p>概念详见 flannel.note 中的 vxlan 模型&lt;/p></description></item><item><title>Docs: Network Virtual(网络虚拟化)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description/></item><item><title>Docs: Storage Virtualization 存储虚拟化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/storage-virtualization-%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/storage-virtualization-%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;h1 id="存储虚拟化io">存储虚拟化(I/O)&lt;/h1>
&lt;h1 id="kvm-模式的存储虚拟化">KVM 模式的存储虚拟化&lt;/h1>
&lt;h2 id="第一种存储虚拟化是通过存储池storage-pool和卷volume来管理的">第一种：存储虚拟化是通过存储池（Storage Pool）和卷（Volume）来管理的。&lt;/h2>
&lt;ol>
&lt;li>Storage Pool 是宿主机上可以看到的一片存储空间，可以是多种类型。&lt;/li>
&lt;li>文件目录类型的 Storage Pool 。KVM 将宿主机目录 /var/lib/libvirt/images/ 作为默认的 Storage Pool。&lt;/li>
&lt;li>KVM 将 HOST 目录 /var/lib/libvirt/images/ 作为默认的 Storage Pool&lt;/li>
&lt;li>KVM 所有可以使用的 Storage Pool 都定义在宿主机的 /etc/libvirt/storage 目录下，每个 Pool 一个 xml 文件，默认有一个 default.xml&lt;/li>
&lt;li>LVM 类型的 Storage Pool。宿主机上 VG 中的 LV 也可以作为虚拟磁盘分配给虚拟机使用。不过，LV 由于没有磁盘的 MBR 引导记录，不能作为虚拟机的启动盘，只能作为数据盘使用。&lt;/li>
&lt;li>KVM 还支持 iSCSI，Ceph 等多种类型的 Storage Pool，最常用的就是目录类型，其他类型可以参考文档&lt;a href="http://libvirt.org/storage.html">http://libvirt.org/storage.html&lt;/a>&lt;/li>
&lt;li>Volume 是在 Storage Pool 中划分出的一块空间，宿主机将 Volume 分配给虚拟机，Volume 在虚拟机中看到的就是一块硬盘。Volume 是 Storage Pool 目录下面的文件，一个文件就是一个 Volume(使用文件做 Volume 有很多优点：存储方便、移植性好、可复制)。Volume 分为几种类型，类型如下&lt;/li>
&lt;li>qcow2 #是推荐使用的格式，QEMU V2 磁盘镜像格式，cow 表示 copy on write，能够节省磁盘空间，支持 AES 加密，支持 zlib 压缩，支持多快照，功能很多。&lt;/li>
&lt;li>raw #是默认格式，即原始磁盘镜像格式，移植性好，性能好，但大小固定，不能节省磁盘空间。&lt;/li>
&lt;li>vmdk #是 VMWare 的虚拟磁盘格式，也就是说 VMWare 虚机可以直接在 KVM 上 运行。&lt;/li>
&lt;/ol></description></item></channel></rss>