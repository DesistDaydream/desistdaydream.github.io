<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 10.云原生</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/</link><description>Recent content in 10.云原生 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: /etc/kubernetes 目录误删恢复</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/etc_kubernetes-%E7%9B%AE%E5%BD%95%E8%AF%AF%E5%88%A0%E6%81%A2%E5%A4%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/etc_kubernetes-%E7%9B%AE%E5%BD%95%E8%AF%AF%E5%88%A0%E6%81%A2%E5%A4%8D/</guid><description>
&lt;h1 id="故障现象">故障现象&lt;/h1>
&lt;p>参考：&lt;a href="https://mp.weixin.qq.com/s/O3fJF5aZuxPOKa7lIjrHnQ">阳明公众号原文&lt;/a>&lt;/p>
&lt;p>Kubernetes 是一个很牛很牛的平台，Kubernetes 的架构可以让你轻松应对各种故障，今天我们将来破坏我们的集群、删除证书，然后再想办法恢复我们的集群，进行这些危险的操作而不会对已经运行的服务造成宕机。&lt;/p>
&lt;blockquote>
&lt;p>如果你真的想要执行接下来的操作，还是建议别在生产环境去折腾，虽然理论上不会造成服务宕机，但是如果出现了问题，&lt;strong>可千万别骂我~~~&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>我们知道 Kubernetes 的控制平面是由几个组件组成的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>etcd：作为整个集群的数据库使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-apiserver：集群的 API 服务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-controller-manager：整个集群资源的控制操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-scheduler：核心调度器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubelet：是运行在节点上用来真正管理容器的组件&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这些组件都由一套针对客户端和服务端的 TLS 证书保护，用于组件之间的认证和授权，大部分情况下它们并不是直接存储在 Kubernetes 的数据库中的，而是以普通文件的形式存在。&lt;/p>
&lt;pre>&lt;code># tree /etc/kubernetes/pki/
/etc/kubernetes/pki/
├── apiserver.crt
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
├── apiserver.key
├── apiserver-kubelet-client.crt
├── apiserver-kubelet-client.key
├── ca.crt
├── ca.key
├── CTNCA.pem
├── etcd
│ ├── ca.crt
│ ├── ca.key
│ ├── healthcheck-client.crt
│ ├── healthcheck-client.key
│ ├── peer.crt
│ ├── peer.key
│ ├── server.crt
│ └── server.key
├── front-proxy-ca.crt
├── front-proxy-ca.key
├── front-proxy-client.crt
├── front-proxy-client.key
├── sa.key
└── sa.pub
&lt;/code>&lt;/pre>
&lt;p>控制面板的组件以静态 Pod (我这里用 kubeadm 搭建的集群)的形式运行在 master 节点上，默认资源清单位于 &lt;code>/etc/kubernetes/manifests&lt;/code> 目录下。通常来说这些组件之间会进行互相通信，基本流程如下所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ghm4g3/1616115588241-7c7f556a-1526-43e4-847a-d78a70821f6b.png" alt="">&lt;/p>
&lt;p>组件之间为了通信，他们需要使用到 TLS 证书。假设我们已经有了一个部署好的集群，接下来让我们开始我们的破坏行为。&lt;/p>
&lt;pre>&lt;code>rm -rf /etc/kubernetes/
&lt;/code>&lt;/pre>
&lt;p>在 master 节点上，这个目录包含：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>etcd 的一组证书和 CA（在 &lt;code>/etc/kubernetes/pki/etcd&lt;/code> 目录下）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一组 kubernetes 的证书和 CA（在 &lt;code>/etc/kubernetes/pki&lt;/code> 目录下）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>还有 kube-controller-manager、kube-scheduler、cluster-admin 以及 kubelet 这些使用的 kubeconfig 文件&lt;/p>
&lt;/li>
&lt;li>
&lt;p>etcd、kube-apiserver、kube-scheduler 和 kube-controller-manager 的静态 Pod 资源清单文件（位于 &lt;code>/etc/kubernetes/manifests&lt;/code> 目录）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>现在我们就上面这些全都删除了，如果是在生产环境做了这样的操作，可能你现在正瑟瑟发抖吧~&lt;/p>
&lt;p>修复控制平面&lt;/p>
&lt;p>首先我也确保下我们的所有控制平面 Pod 已经停止了。&lt;/p>
&lt;pre>&lt;code># 如果你用 docker 也是可以的
crictl rm `crictl ps -aq`
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>注意：kubeadm 默认不会覆盖现有的证书和 kubeconfigs，为了重新颁发证书，你必须先手动删除旧的证书。&lt;/p>
&lt;/blockquote>
&lt;p>接下来我们首先恢复 etcd，在**一个 master **节点上执行下面的命令生成 etcd 集群的证书：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs etcd-ca --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令将为我们的 etcd 集群生成一个新的 CA，由于所有其他证书都必须由它来签署，我们也将把它和私钥复制到其他 master 节点(如果你是多 master)。&lt;/p>
&lt;pre>&lt;code>/etc/kubernetes/pki/etcd/ca.{key,crt}
&lt;/code>&lt;/pre>
&lt;p>接下来让我们在&lt;strong>所有 master&lt;/strong> 节点上为它重新生成其余的 etcd 证书和静态资源清单。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs etcd-healthcheck-client --config=kubeadm-config.yaml
kubeadm init phase certs etcd-peer --config=kubeadm-config.yaml
kubeadm init phase certs etcd-server --config=kubeadm-config.yaml
kubeadm init phase etcd local --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令执行后，你应该已经有了一个正常工作的 etcd 集群了。&lt;/p>
&lt;pre>&lt;code># crictl ps
CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID
ac82b4ed5d83a 0369cf4303ffd 2 seconds ago Running etcd 0 bc8b4d568751b
&lt;/code>&lt;/pre>
&lt;p>接下来我们对 Kubernetes 服务做同样的操作，在其中&lt;strong>一个 master&lt;/strong> 节点上执行如下的命令：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs all --config=kubeadm-config.yaml
kubeadm init phase kubeconfig all --config=kubeadm-config.yaml
kubeadm init phase control-plane all --config=kubeadm-config.yaml
rm -rf /root/.kube/*
cp -f /etc/kubernetes/admin.conf ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>上面的命令将生成 Kubernetes 的所有 SSL 证书，以及 Kubernetes 服务的静态 Pods 清单和 kubeconfigs 文件。&lt;/p>
&lt;p>如果你使用 kubeadm 加入 kubelet，你还需要更新 &lt;code>kube-public&lt;/code> 命名空间中的 cluster-info 配置，因为它仍然包含你的旧 CA 的哈希值。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase bootstrap-token --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>由于其他 master 节点上的所有证书也必须由单一 CA 签署，所以我们将其复制到其他控制面节点，并在每个节点上重复上述命令。&lt;/p>
&lt;pre>&lt;code>/etc/kubernetes/pki/{ca,front-proxy-ca}.{key,crt}
/etc/kubernetes/pki/sa.{key,pub}
&lt;/code>&lt;/pre>
&lt;p>顺便说一下，作为手动复制证书的替代方法，你也可以使用 Kubernetes API，如下所示的命令：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase upload-certs --upload-certs --config=kubeadm-config.yaml
# 上一条命令输出的 certificate-key 替换 ${MasterJoinKey}
kubeadm token create --ttl=2h --certificate-key=${MasterJoinKey} --print-join-command
&lt;/code>&lt;/pre>
&lt;p>该命令将加密并上传证书到 Kubernetes，时间为 2 小时，所以你可以按以下方式注册 master 节点：&lt;/p>
&lt;pre>&lt;code># 注意替换上面命令输出的 join 命令的内容
kubeadm join phase control-plane-prepare all kubernetes-apiserver:6443 --control-plane --token cs0etm.ua7fbmwuf1jz946l --discovery-token-ca-cert-hash sha256:555f6ececd4721fed0269d27a5c7f1c6d7ef4614157a18e56ed9a1fd031a3ab8 --certificate-key 385655ee0ab98d2441ba8038b4e8d03184df1806733eac131511891d1096be73
kubeadm join phase control-plane-join all
&lt;/code>&lt;/pre>
&lt;p>需要注意的是，Kubernetes API 还有一个配置，它为 &lt;code>front-proxy&lt;/code> 客户端持有 CA 证书，它用于验证从 apiserver 到 webhooks 和聚合层服务的请求。不过 kube-apiserver 会自动更新它。到在这个阶段，我们已经有了一个完整的控制平面了。&lt;/p>
&lt;h2 id="修复-kubelet">修复 kubelet&lt;/h2>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/
kubeadm init phase kubeconfig kubelet --config=kubeadm-config.yaml
kubeadm init phase kubelet-start --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;h2 id="修复工作节点">修复工作节点&lt;/h2>
&lt;p>现在我们可以使用下面的命令列出集群的所有节点：&lt;/p>
&lt;pre>&lt;code>kubectl get nodes
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>若报错：Unable to connect to the server: x509: certificate signed by unknown authority
删除 /root/.kube/config 文件，并重新拷贝一遍&lt;/p>
&lt;/blockquote>
&lt;p>当然正常现在所有节点的状态都是 NotReady，这是因为他们仍然还使用的是旧的证书，为了解决这个问题，我们将使用 kubeadm 来执行重新加入集群节点。&lt;/p>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/ /etc/kubernetes/kubelet.conf
kubeadm init phase kubeconfig kubelet --config=kubeadm-config.yaml
kubeadm init phase kubelet-start --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>但要加入工作节点，我们必须生成一个新的 token。&lt;/p>
&lt;pre>&lt;code>kubeadm token create --print-join-command
&lt;/code>&lt;/pre>
&lt;p>然后在工作节点分别执行下面的命令：&lt;/p>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/ /etc/kubernetes/pki/ /etc/kubernetes/kubelet.conf
kubeadm join phase kubelet-start kubernetes-apiserver:6443 --token cs0etm.ua7fbmwuf1jz946l --discovery-token-ca-cert-hash sha256:555f6ececd4721fed0269d27a5c7f1c6d7ef4614157a18e56ed9a1fd031a3ab8
&lt;/code>&lt;/pre>
&lt;p>上面的操作会把你所有的 kubelet 重新加入到集群中，它并不会影响任何已经运行在上面的容器，但是，如果集群中有多个节点并且不同时进行，则可能会遇到一种情况，即 kube-controller-mananger 开始从 NotReady 节点重新创建容器，并尝试在活动节点上重新调度它们。&lt;/p>
&lt;p>为了防止这种情况，我们可以暂时停掉 master 节点上的 controller-manager。&lt;/p>
&lt;pre>&lt;code>rm /etc/kubernetes/manifests/kube-controller-manager.yaml
crictl rmp `crictl ps --name kube-controller-manager -q`
&lt;/code>&lt;/pre>
&lt;p>一旦集群中的所有节点都被加入，你就可以为 controller-manager 生成一个静态资源清单，在所有 master 节点上运行下面的命令。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase control-plane controller-manager
&lt;/code>&lt;/pre>
&lt;p>如果 kubelet 被配置为请求由你的 CA 签署的证书(选项 serverTLSBootstrap: true)，你还需要批准来自 kubelet 的 CSR：&lt;/p>
&lt;pre>&lt;code>kubectl get csrkubectl certificate approve &amp;lt;csr&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="修复-serviceaccounts">修复 ServiceAccounts&lt;/h2>
&lt;p>因为我们丢失了 &lt;code>/etc/kubernetes/pki/sa.key&lt;/code> ，这个 key 用于为集群中所有 &lt;code>ServiceAccounts&lt;/code> 签署 &lt;code>jwt tokens&lt;/code>，因此，我们必须为每个 sa 重新创建 tokens。这可以通过类型为 &lt;code>kubernetes.io/service-account-token&lt;/code> 的 Secret 中删除 token 字段来完成。&lt;/p>
&lt;pre>&lt;code>kubectl get secret --all-namespaces | awk '/kubernetes.io\/service-account-token/ { print &amp;quot;kubectl patch secret -n &amp;quot; $1 &amp;quot; &amp;quot; $2 &amp;quot; -p {\\\&amp;quot;data\\\&amp;quot;:{\\\&amp;quot;token\\\&amp;quot;:null}}&amp;quot;}' | sh -x
&lt;/code>&lt;/pre>
&lt;p>删除之后，kube-controller-manager 会自动生成用新密钥签名的新令牌。不过需要注意的是并非所有的微服务都能即时更新 tokens，因此很可能需要手动重新启动使用 tokens 的容器。&lt;/p>
&lt;pre>&lt;code>kubectl get pod --field-selector 'spec.serviceAccountName!=default' --no-headers --all-namespaces | awk '{print &amp;quot;kubectl delete pod -n &amp;quot; $1 &amp;quot; &amp;quot; $2 &amp;quot; --wait=false --grace-period=0&amp;quot;}'
&lt;/code>&lt;/pre>
&lt;p>例如，这个命令会生成一个命令列表，会将所有使用非默认的 serviceAccount 的 Pod 删除，我建议从 kube-system 命名空间执行，因为 kube-proxy 和 CNI 插件都安装在这个命名空间中，它们对于处理你的微服务之间的通信至关重要。&lt;/p>
&lt;p>到这里我们的集群就恢复完成了。&lt;/p>
&lt;blockquote>
&lt;p>参考链接：&lt;a href="https://itnext.io/breaking-down-and-fixing-kubernetes-4df2f22f87c3">https://itnext.io/breaking-down-and-fixing-kubernetes-4df2f22f87c3&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: 1.1.Rancher 介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.1.rancher-%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.1.rancher-%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;h1 id="rancher-介绍">Rancher 介绍&lt;/h1>
&lt;p>官方文档：&lt;a href="https://rancher.com/">https://rancher.com/&lt;/a>&lt;/p>
&lt;p>Rancher 是为使用容器的公司打造的容器管理平台。Rancher 简化了使用 Kubernetes 的流程，开发者可以随处运行 Kubernetes（Run Kubernetes Everywhere），满足 IT 需求规范，赋能 DevOps 团队。&lt;/p>
&lt;p>Rancher 在现阶段可以看作是一个解决方案，是一套产品的统称，这套产品包括如下几个：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>K3S # 用于运行高可用 Rancher 的底层平台。是一个轻量的 kubernetes，一个 k3s 二进制文件即可包含所有 kubernetes 的主要组件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Rancher Server # Rancher 管理程序，常部署于 k3s 之上，用来管理其下游 k8s 集群。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RKE # Rancher 创建的 kubernetes 集群。是一个可以通过名为 rke 的二进制文件以及一个 yaml 文件，即可启动 kubernetes 集群的引擎。RKE 与 kubernetes 的关系，类似于 docker 与 containerd 的关系。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="rancher-server-介绍">Rancher Server 介绍&lt;/h2>
&lt;p>Rancher Server 由认证代理(Authentication Proxy)、Rancher API Server、集群控制器(Cluster Controller)、数据存储(比如 etcd、mysql 等)和集群代理(Cluster Agent) 组成。除了 Cluster Agent 以外，其他组件都部署在 Rancher Server 中。(这些组件都集中在一起，一般可以通过 docker 直接启动一个 Rancher Server。)&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kxmsmg/1616114814016-9de5267d-0813-4790-826c-7c4448e40861.png" alt="">&lt;/p>
&lt;p>Rancher Server 可以管理多种 k8s 集群&lt;/p>
&lt;ol>
&lt;li>
&lt;p>通过 Rancher Server 来创建一个 RKE 集群&lt;/p>
&lt;/li>
&lt;li>
&lt;p>托管的 kubernetes 集群。e.g.Amazon EKS、Azure AKS、Google GKE 等等&lt;/p>
&lt;/li>
&lt;li>
&lt;p>导入已有的 kubernetes 集群。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="rancher-与下游集群交互的方式">Rancher 与下游集群交互的方式&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kxmsmg/1616114813966-db373999-6c8f-4541-a09f-5f20eaa656ce.png" alt="">&lt;/p>
&lt;p>通过 Rancher 管理的 kubernetes 集群(不管是导入的还是通过 Rancher 创建的)，都会在集群中部署两种 agent，来与 Rancher 进行交互。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cattle-cluster-agent # 上图中的 Cluster Agent。用于本集群与 Rancher Server 的 Cluster Controller(集群控制器)的通信&lt;/p>
&lt;/li>
&lt;li>
&lt;p>连接 Rancher 与本集群的 API Server&lt;/p>
&lt;/li>
&lt;li>
&lt;p>管理集群内的工作负载，比如 Rancher Server 下发一个部署 pod 的任务，集群代理就会与本集群 API 交互来处理任务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据每个集群的设置，配置 Role 和 RoleBindings&lt;/p>
&lt;/li>
&lt;li>
&lt;p>实现集群和 Rancher Server 之间的消息传输，包括事件，指标，健康状况和节点信息等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cattle-node-agent # 上图中的 Node Agent。用于处理本节点的任务，比如升级 kubernetes 版本以及创建或者还原 etcd 快照等等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Note：如果 Cluster Agent 不可用，下游集群中的其中一个 Node Agent 会创建一个通信管道，由节点 Agent 连接到集群控制器，实现下游集群和 Rancher 之间的通信。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一般使用 DaemonSet 的方式部署到集群中，以保证每个节点都有一个代理可以执行 Rancher Server 下发的任务。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="rancher-配置">Rancher 配置&lt;/h1>
&lt;p>Rancher 套件中的各组件配置详见各自组件配置详解&lt;/p>
&lt;h2 id="k3s-配置">K3S 配置&lt;/h2>
&lt;h2 id="rancher-server-配置">Rancher Server 配置&lt;/h2>
&lt;h2 id="rancher-创建的集群配置">Rancher 创建的集群配置&lt;/h2>
&lt;p>Rancher 创建的集群是为 RKE 集群，配置详见：RKE 配置详解&lt;/p></description></item><item><title>Docs: 1.1.虚拟化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_getting_started_guide/index">RedHat 7 虚拟化入门指南&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/virtualization-in-rhel-8-an-overview_configuring-and-managing-virtualization#what-is-virtualization-in-rhel-8-virt-overview">Redhat 8 官方对虚拟化的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_getting_started_guide/index">RedHat 7 对“虚拟化性能不行”这个误区的辟谣&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ubuntu.com/server/docs/virtualization-introduction">Ubuntu 官方文档，虚拟化-介绍&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Virtualization(虚拟化)&lt;/strong> 是用于运行软件的广义的计算机术语。通常情况下，&lt;strong>Virtualization(虚拟化)&lt;/strong> 体现在让单个可以运行多个操作系统，这些操作系统同时运行，而又是互相独立的。&lt;/p>
&lt;p>虚拟化是云计算的基础。简单的说，虚拟化使得在一台物理的服务器上可以跑多台虚拟机，虚拟机共享物理机的 CPU、内存、IO 硬件资源，但逻辑上虚拟机之间是相互隔离的。物理机我们一般称为 &lt;strong>Host(宿主机)&lt;/strong>，宿主机上面的虚拟机称为 &lt;strong>Guest(客户机)&lt;/strong>。那么 Host 是如何将自己的硬件资源虚拟化，并提供给 Guest 使用的呢？这个主要是通过一个叫做 Hypervisor 的程序实现的。&lt;/p>
&lt;h2 id="hypervisor">Hypervisor&lt;/h2>
&lt;p>参考：&lt;a href="https://www.redhat.com/zh/topics/virtualization/what-is-a-hypervisor">https://www.redhat.com/zh/topics/virtualization/what-is-a-hypervisor&lt;/a>&lt;/p>
&lt;p>Hypervisor 是用来创建与运行虚拟机的软件、固件或硬件。被 Hypervisor 用来运行一个或多个虚拟机的设备称为 Host Machine(宿主机)，这些虚拟机则称为 Guest Machine(客户机)。&lt;strong>Hypervisor 有时也被称为 Virtual Machine Monitor (虚拟机监视器，简称 VMM)&lt;/strong>&lt;/p>
&lt;h1 id="虚拟化技术的分类">虚拟化技术的分类&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ihdpea/1616124416735-5e89f29f-21cd-4fed-af5e-194227de3048.png" alt="">
根据 Hypervisor 的实现方式和所处的位置，虚拟化又分为两种：1 型虚拟化和 2 型虚拟化&lt;/p>
&lt;ol>
&lt;li>半虚拟化（para-virtualization）：TYPE1，也叫裸金属虚拟化比如 Vmware ESXi、Xen 等是一款类似于操作系统的 Hypervisor，直接运行在硬件之上，需要修改 Guest OS 的内核，让 VM 知道自己是虚拟机&lt;/li>
&lt;li>完全虚拟化（full-virtualization）：TYPE2，物理机上首先安装常规的操作系统，比如 Redhat、Ubuntu 和 Windows。Hypervisor 作为 OS 上的一个程序模块运行，并对管理虚拟机进行管理。比如 Vmware Workstation、KVM 等是一款类似于软件的 Hypervisor，运行于操作系统之上，VM 不知道自己是虚拟机
&lt;ol>
&lt;li>BT：软件，二进制翻译。性能很差&lt;/li>
&lt;li>HVM：硬件，硬件辅助的虚拟化。性能很好。现阶段 KVM 主要基于硬件辅助进行虚拟化
&lt;ol>
&lt;li>&lt;strong>硬件辅助全虚拟化主要使用了支持虚拟化功能的 CPU 进行支撑，CPU 可以明确的分辨出来自 GuestOS 的特权指令，并针对 GuestOS 进行特权操作，而不会影响到 HostOS。&lt;/strong>&lt;/li>
&lt;li>从更深入的层次来说，虚拟化 CPU 形成了新的 CPU 执行状态 —— _ Non-Root Mode&amp;amp; Root Mode_ 。从上图中可以看见，GuestOS 运行在 Non-Root Mode 的 Ring 0 核心态中，这表明 GuestOS 能够直接执行特却指令而不再需要 &lt;em>特权解除&lt;/em> 和 &lt;em>陷入模拟&lt;/em> 机制。并且在硬件层上面紧接的就是虚拟化层的 VMM，而不需要 HostOS。这是因为在硬件辅助全虚拟化的 VMM 会以一种更具协作性的方式来实现虚拟化 —— &lt;em>将虚拟化模块加载到 HostOS 的内核中&lt;/em>，例如：KVM，KVM 通过在 HostOS 内核中加载&lt;strong>KVM Kernel Module&lt;/strong>来将 HostOS 转换成为一个 VMM。所以此时 VMM 可以看作是 HostOS，反之亦然。这种虚拟化方式创建的 GuestOS 知道自己是正在虚拟化模式中运行的 GuestOS，KVM 就是这样的一种虚拟化实现解决方案。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>OS 级别虚拟化：容器级虚拟化，准确来说不能叫虚拟化了，只能叫容器技术无 Hypervisor，将用户空间分隔为多个，彼此互相隔离，每个 VM 中没有独立内核，OpenVZ、LXC(Linux container)、libcontainer 等，比如 Docker，Docker 的基础是 LXC。&lt;/li>
&lt;li>模拟(Emulation)：比如 QEMU，PearPC，Bochs&lt;/li>
&lt;li>库虚拟化：WINE&lt;/li>
&lt;li>应用程序虚拟化：JVM&lt;/li>
&lt;li>理论上 Type1 和 Typ2 之间的区别
&lt;ol>
&lt;li>1 型虚拟化一般对硬件虚拟化功能进行了特别优化，性能上比 2 型要高；&lt;/li>
&lt;li>2 型虚拟化因为基于普通的操作系统，会比较灵活，比如支持虚拟机嵌套。嵌套意味着可以在 KVM 虚拟机中再运行 KVM。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h1 id="虚拟化总结云计算基础实现云功能的灵活调度">虚拟化总结(云计算基础，实现云功能的灵活调度)&lt;/h1>
&lt;p>所谓的云计算：当一台虚拟机需要跨越多个物理机进行数据交互，比如拿来运行 VM 的物理主机不止一台，在每台物理机上按需启动既定数量的 VM，每个 VM 有多少 CPU 和 MEM，每个 VM 启动在哪个物理机上，启动 VM 需要的存储设备在什么地方，存储设备中的系统是临时安装，还是通过一个已经装好的系统模板直接使用，还有多个 VM 跨物理主机进行网络通信等等一系列工作，可以使用一个虚拟化管理工具(VM Manager)来实现，这个管理器的功能即可称为云计算。在没有这个管理器的时候，人们只能人为手工从把 VM 从一台物理机移动到另一台物理机，非常不灵活。&lt;/p>
&lt;p>计算机五大部件：运算器(cpu)，控制器(cpu)，存储器(memory)，输入与输出设备(磁盘 I/O，网络 I/O)。&lt;/p>
&lt;p>一般情况，VM 的 CPU 与 Memory 无法跨主机使用；但是磁盘 I/O 与网络 I/O 则可以跨主机使用。云计算的灵活性（即 VM 或者单个云计算节点挂了但是不影响数据，可以重新启动在任一一个节点等类似的功能）&lt;/p>
&lt;p>磁盘 I/O 的灵活调度&lt;/p>
&lt;p>所以，在启动一个 VM 的时候，分为这么几个启动步骤，模拟 CPU 和内存，模拟存储，模拟网络。当在多个 node 的虚拟化集群中创建完一个 VM 并想启动的时候，又分为两种情况：&lt;/p>
&lt;ol>
&lt;li>当该 VM 的虚拟存储放在某个节点上的时候，则该 VM 只能启动在该节点上，因为没有存储就没法加载系统镜像，何谈启动呢&lt;/li>
&lt;li>当该 VM 的虚拟存储放在虚拟化集群的后端存储服务器或者共享存储空间的时候，则该 VM 可以根据调度策略在任一节点启动,然后把该 VM 对应的虚拟存储挂载或下载到需要启动的节点上即可（这个所谓的虚拟存储，可以称为模板，每次 VM 启动的时候，都可以通过这个模板直接启动而不用重新安装系统了）&lt;/li>
&lt;/ol>
&lt;p>这种可以灵活调度 VM，而不让 VM 固定启动在一个虚拟机上的机制，这就是云功能的基础，用户不用关心具体运行在哪个节点上，都是由系统自动调度的。&lt;/p>
&lt;p>网络 I/O 的灵活调度&lt;/p>
&lt;p>同样的，在一个 VM 从 node1 移动到 node2 的时候，除了存储需要跟随移动外，还需要网络也跟随移动，移动的前提是所有 node 的网络配置是一样的，不管是隔离模型，还是路由模型，还是 nat 模型，还是桥接模型，都需要给每个 node 进行配置，但是，会有这么几个情况，&lt;/p>
&lt;ol>
&lt;li>一个公司，有 2 个部门，有两台物理 server，node1 最多有 4 个 VM，node2 最多有 4 个 VM，其中一个部门需要 5 台 VM，另一个部门需要 3 台 VM，而两个部门又要完全隔离，这时候可以通过对 vSwitch 进行 vlan 划分来进行隔离，。这时候就一个开源的软件应运而生，就是 Open vSwtich，简称为 OVS。&lt;/li>
&lt;li>普通 VLAN 只有 4096 个，对于公有云来说，该 vlan 数量远远不够，这时候，vxlan 技术应运而生&lt;/li>
&lt;li>每个公司有多个部门,每个部门有的需要连接公网，有的不需要连接公网,如果想隔离开两个公司，仅仅依靠虚拟交换机从二层隔离，无法隔离全面，这时候 vRouter 虚拟路由器技术应运而生，通过路由来隔离，并通过路由来访问，而这台 vRouter 就是由 linux 的 net namespace 功能来创建的&lt;/li>
&lt;/ol>
&lt;p>Openstack 中创建的每个 network 就相当于一个 vSwitch，创建的每个 route 就相当于一个 vRoute 即 net namespace，然后把 network 绑定到 route 上，就相当于把 vSwitch 连接到了 vRoute，所以，在绑定完成之后，会在 route 列表中看到个端口的 IP，这个 IP 就是 vSwitch 子网(在创建 vSwitch 的时候会设置一个可用的网段)中的一个 IP，就相当于交换机连到路由器后，路由器上这个端口的 IP&lt;/p>
&lt;h1 id="实际上一个虚拟机就是宿主机上的一个文件虚拟化程序可以通过这个文件来运行虚拟机">实际上，一个虚拟机就是宿主机上的一个文件，虚拟化程序可以通过这个文件来运行虚拟机&lt;/h1></description></item><item><title>Docs: 1.2.Rancher 部署与清理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.2.rancher-%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%B8%85%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.2.rancher-%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%B8%85%E7%90%86/</guid><description>
&lt;h1 id="rancher-部署">Rancher 部署&lt;/h1>
&lt;p>常见问题：&lt;a href="https://www.bookstack.cn/read/rancher-2.4.4-zh/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.md">https://www.bookstack.cn/read/rancher-2.4.4-zh/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.md&lt;/a>&lt;/p>
&lt;h2 id="快速部署体验">快速部署体验&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker run -d --name&lt;span style="color:#f92672">=&lt;/span>rancher-server --restart&lt;span style="color:#f92672">=&lt;/span>unless-stopped &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 60080:80 -p 60443:443 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /opt/rancher:/var/lib/rancher &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --privileged &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> rancher/rancher:v2.5.3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果想让 rancher 可以验证外部 https 的自建 CA 证书，需要在启动前将证书导入 rancher-server 中，效果如下：&lt;/p>
&lt;p>参考链接： &lt;a href="https://rancher.com/docs/rancher/v2.x/en/installation/resources/chart-options/#additional-trusted-cas">https://rancher.com/docs/rancher/v2.x/en/installation/resources/chart-options/#additional-trusted-cas&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://rancher.com/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/#custom-ca-certificate">https://rancher.com/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/#custom-ca-certificate&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker run -d --name&lt;span style="color:#f92672">=&lt;/span>rancher-server --restart&lt;span style="color:#f92672">=&lt;/span>unless-stopped &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 60080:80 -p 60443:443 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /opt/rancher:/var/lib/rancher &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /host/certs:/container/certs &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e SSL_CERT_DIR&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;/container/certs&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --privileged &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> rancher/rancher:latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在宿主机的 /host/certs 目录中存放要导入的证书，比如可以把 harbor 的证书与私钥放入该目录中，这样 rancher 就可以添加 https 的 harbor 仓库了&lt;/p>
&lt;p>还可以传递 CATTLE_SYSTEM_DEFAULT_REGISTRY 环境变量，让 rancher 内部使用私有镜像地址。比如&lt;/p>
&lt;pre>&lt;code>docker run -d --name=rancher-server --restart=unless-stopped \
-p 60080:80 -p 60443:443 \
-v /opt/rancher:/var/lib/rancher \
--privileged \
-e CATTLE_SYSTEM_DEFAULT_REGISTRY=&amp;quot;registry.wx-net.ehualu.com&amp;quot; \
rancher/rancher:latest
&lt;/code>&lt;/pre>
&lt;h2 id="高可用部署">高可用部署&lt;/h2>
&lt;p>Rancher 的高可用本质上就是将 Rancher 作为 kubernetes 上的一个服务对外提供(这个 k8s 集群通常只用来运行 Rancher)。Rancher 的数据储存在 k8s 集群的后端存储中(i.e.ETCD)。由于原生 k8s 部署负责，资源需求大，不易维护等问题，Rancher 官方推出了一个简化版的 k8s，即 &lt;a href="https://github.com/rancher/k3s">k3s&lt;/a>。k3s 是一个简化版的 k8s，可以实现基本的 k8s 功能，但是部署更简单，资源需求更少，更易维护。&lt;a href="https://github.com/rancher/k3s">k3s&lt;/a> 介绍详见 k3s 介绍&lt;/p>
&lt;h3 id="可选启动-k3s-集群">(可选)启动 k3s 集群&lt;/h3>
&lt;p>启动 mysql 用于 k3s 存储数据&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d --name k3s-mysql --restart&lt;span style="color:#f92672">=&lt;/span>always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-v /opt/k3s-cluster/mysql/conf:/etc/mysql/conf.d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-v /opt/k3s-cluster/mysql/data:/var/lib/mysql &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-e MYSQL_ROOT_PASSWORD&lt;span style="color:#f92672">=&lt;/span>root &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-p 3306:3306 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>mysql:5.7.29 --default-time-zone&lt;span style="color:#f92672">=&lt;/span>+8:00 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--character-set-server&lt;span style="color:#f92672">=&lt;/span>utf8mb4 --collation-server&lt;span style="color:#f92672">=&lt;/span>utf8mb4_general_ci &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--explicit_defaults_for_timestamp&lt;span style="color:#f92672">=&lt;/span>true --lower_case_table_names&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --max_allowed_packet&lt;span style="color:#f92672">=&lt;/span>128M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>启动 k3s&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR&lt;span style="color:#f92672">=&lt;/span>cn sh -s - server &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--docker --datastore-endpoint&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;mysql://root:root@tcp(172.38.40.212:3306)/k3s&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 配置 kubectl 的 kubeconfig 文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp /etc/rancher/k3s/k3s.yaml /root/.kube/config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;source &amp;lt;(kubectl completion bash)&amp;#34;&lt;/span> &amp;gt;&amp;gt; ~/.bashrc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="部署-rancher">部署 Rancher&lt;/h3>
&lt;p>创建证书参考：&lt;a href="https://thoughts.teambition.com/workspaces/5f90e312c800160016ea22fb/docs/5fa4f848eaa1190001257bba">&lt;strong>自建 CA 脚本&lt;/strong>&lt;/a>，该脚本参考：&lt;a href="https://docs.rancher.cn/docs/rancher2/installation/options/self-signed-ssl/_index">https://docs.rancher.cn/docs/rancher2/installation/options/self-signed-ssl/_index&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建用于运行 Rancher 的名称空间&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create namespace cattle-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建CA证书&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out ca.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -x509 -new -nodes -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=k3s-rancher.desistdaydream.ltd&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key ca.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out k3s-rancher.desistdaydream.ltd.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -sha512 -new &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=k3s-rancher.desistdaydream.ltd&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key k3s-rancher.desistdaydream.ltdn.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out k3s-rancher.desistdaydream.ltd.csr
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; v3.ext &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">authorityKeyIdentifier=keyid,issuer
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">basicConstraints=CA:FALSE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">extendedKeyUsage = serverAuth
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">subjectAltName = @alt_names
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[alt_names]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">DNS.1=k3s-rancher.desistdaydream.ltd
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl x509 -req -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -extfile v3.ext &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -CA ca.crt -CAkey ca.key -CAcreateserial &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -in k3s-rancher.desistdaydream.ltd.csr &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out k3s-rancher.desistdaydream.ltd.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将证书添加到 secret 资源，以便让 Rancher 读取&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp k3s-rancher.desistdaydream.ltd.crt tls.crt &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp k3s-rancher.desistdaydream.ltd.key tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret tls tls-rancher-ingress &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert&lt;span style="color:#f92672">=&lt;/span>tls.crt &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key&lt;span style="color:#f92672">=&lt;/span>tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ca.crt cacerts.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret generic tls-ca &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-file&lt;span style="color:#f92672">=&lt;/span>cacerts.pem&lt;span style="color:#f92672">=&lt;/span>./cacerts.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 添加 rancher 的 helm 仓库&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo add rancher-stable http://rancher-mirror.oss-cn-beijing.aliyuncs.com/server-charts/stable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 部署指定版本的 Rancher&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm install rancher rancher-stable/rancher &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --namespace cattle-system &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set hostname&lt;span style="color:#f92672">=&lt;/span>k3s-rancher.desistdaydream.ltd &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set ingress.tls.source&lt;span style="color:#f92672">=&lt;/span>secret &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set privateCA&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="高可用离线部署">高可用离线部署&lt;/h2>
&lt;p>与在线部署类似，但是需要一个私有镜像仓库，所需相关的部署镜像需要先推送到私有镜像仓库中。&lt;/p>
&lt;p>需要提前准备的文件列表：&lt;/p>
&lt;p>在 &lt;a href="https://github.com/rancher/rancher/releases">https://github.com/rancher/rancher/releases&lt;/a> 页面，下载推送镜像所需的文件，这里以 2.4.5 为例。一共需要三个文件。&lt;/p>
&lt;p>rancher-images.txt rancher 镜像列表。&lt;/p>
&lt;p>rancher-save-images.sh 根据镜像列表文件打包所有镜像。&lt;/p>
&lt;p>rancher-load-images.sh 将打包好的镜像推送到私有仓库。&lt;/p>
&lt;ol>
&lt;li>rancher-images.tar.gz # rancher 的镜像
&lt;ol>
&lt;li>curl -LO &lt;a href="https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-images.txt">https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-images.txt&lt;/a>&lt;/li>
&lt;li>curl -LO &lt;a href="https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-save-images.sh">https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-save-images.sh&lt;/a>&lt;/li>
&lt;li>sort -u rancher-images.txt -o rancher-images.txt&lt;/li>
&lt;li>./rancher-save-images.sh &amp;ndash;image-list ./rancher-images.txt&lt;/li>
&lt;li>保存完成后，会生成名为 rancher-images.tar.gz 的镜像打包文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>rancher-load-images.sh # 推送 rancher 镜像的脚本
&lt;ol>
&lt;li>curl -LO &lt;a href="https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-load-images.sh">https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-load-images.sh&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>mysql.tar # mysql 镜像。&lt;/li>
&lt;li>k3s # k3s 二进制文件&lt;/li>
&lt;li>k3s-airgap-images-amd64.tar # k3s 运行所需的镜像
&lt;ol>
&lt;li>从 &lt;a href="https://github.com/rancher/k3s/releases">此处&lt;/a> 下载 k3s 二进制文件以及镜像的压缩包。二进制文件名称为 &lt;a href="https://github.com/rancher/k3s/releases/download/v1.18.6%2Bk3s1/k3s">k3s&lt;/a>。镜像压缩包的文件名为 &lt;a href="https://github.com/rancher/k3s/releases/download/v1.18.6%2Bk3s1/k3s-airgap-images-amd64.tar">k3s-airgap-images-amd64.tar&lt;/a>。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>install.sh # 部署 k3s 的脚本
&lt;ol>
&lt;li>从 &lt;a href="https://get.k3s.io">此处&lt;/a> 获取离线安装所需的脚本。国内用户从 &lt;a href="http://mirror.cnrancher.com/">这个页面&lt;/a>的 k3s 目录下获取脚本，脚本名为 k3s-install.sh。&lt;/li>
&lt;li>curl -LO &lt;a href="https://raw.githubusercontent.com/rancher/k3s/master/install.sh">https://raw.githubusercontent.com/rancher/k3s/master/install.sh&lt;/a>&lt;/li>
&lt;li>curl -LO &lt;a href="https://docs.rancher.cn/k3s/k3s-install.sh">https://docs.rancher.cn/k3s/k3s-install.sh&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>rancher-2.4.5.tgz # 用于部署 rancher 的 helm chart。
&lt;ol>
&lt;li>helm repo add rancher-stable &lt;a href="https://releases.rancher.com/server-charts/stable">https://releases.rancher.com/server-charts/stable&lt;/a>&lt;/li>
&lt;li>helm pull rancher-stable/rancher&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>helm # helm 二进制文件
&lt;ol>
&lt;li>从 git 上下载 tar 包，解压获取二进制文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>kubectl #kubectl 二进制文件，用于在 Rancher 创建的集群节点上操作集群。&lt;/li>
&lt;/ol>
&lt;h3 id="部署私有镜像仓库">部署私有镜像仓库&lt;/h3>
&lt;p>略。&lt;/p>
&lt;p>推送用于部署 Rancher 所需的镜像，到私有镜像仓库。&lt;/p>
&lt;pre>&lt;code># 拷贝 rancher-images.tar.gz 文件到当前目录
# 假如私有镜像仓库的访问路径为 http://172.38.40.180
./rancher-load-images.sh --image-list ./rancher-images.txt --registry 172.38.40.180
&lt;/code>&lt;/pre>
&lt;h3 id="启动-mysql">启动 mysql&lt;/h3>
&lt;p>注意修改 ${CustomRegistry} 变量为指定的私有镜像仓库的仓库名&lt;/p>
&lt;pre>&lt;code>docker run -d --name k3s-mysql --restart=always \
-v /opt/k3s-cluster/mysql/conf:/etc/mysql/conf.d \
-v /opt/k3s-cluster/mysql/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=root \
-p 3306:3306 \
${CustomRegistry}mysql:5.7.29 --default-time-zone=+8:00 \
--character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci \
--explicit_defaults_for_timestamp=true --lower_case_table_names=1 --max_allowed_packet=128M
docker run -d --name k3s-mysql --restart=always \
-v /opt/k3s-cluster/mysql/conf:/etc/mysql/conf.d \
-v /opt/k3s-cluster/mysql/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=root \
-p 3306:3306 ${CustomRegistry}/mysql:5.7.29 --default-time-zone=+8:00
&lt;/code>&lt;/pre>
&lt;h3 id="部署-k3s">部署 k3s&lt;/h3>
&lt;p>准备部署环境
需要 k3s、k3s-airgap-images-amd64.tar、install.sh 文件，拷贝到同一个目录中。并在需要部署 k3s 节点的设备上执行如下命令。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将下载好的k3s镜像包放到指定目录中，k3s 启动时直接使用该目录的镜像压缩包，加载并启动容器。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p /var/lib/rancher/k3s/agent/images/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将二进制文件放在每个节点的 /usr/local/bin 目录中。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod &lt;span style="color:#ae81ff">755&lt;/span> ./k3s &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp ./k3s /usr/local/bin/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 拷贝 helm 二进制文件到 /usr/local/bin 目录下&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod &lt;span style="color:#ae81ff">755&lt;/span> ./helm &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp ./helm /usr/local/bin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 离线安装脚本放在任意路径下。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv ./k3s-install.sh ./install.sh &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> chmod &lt;span style="color:#ae81ff">755&lt;/span> install.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 准备 k3s containerd 操作私有镜像仓库的配置文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p /etc/rancher/k3s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/rancher/k3s/registries.yaml &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">mirrors:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> registry-test.ehualu.com:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> endpoint:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - &amp;#34;http://172.38.40.180&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">configs:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;registry-test.ehualu.com&amp;#34;:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> auth:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> username: admin
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> password: Harbor12345
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 配置解析以访问私有仓库&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt;&amp;gt; /etc/hosts &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">172.38.40.180 registry-test.ehualu.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>开始部署 k3s&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 在 install.sh 所在目录执行部署命令&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>INSTALL_K3S_SKIP_DOWNLOAD&lt;span style="color:#f92672">=&lt;/span>true INSTALL_K3S_EXEC&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;server --docker --datastore-endpoint=mysql://root:root@tcp(172.38.40.212:3306)/k3s&amp;#39;&lt;/span> ./install.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note：&lt;/p>
&lt;ol>
&lt;li>若 k3s.service 无法启动，报错 msg=&amp;ldquo;starting kubernetes: preparing server: creating storage endpoint: building kine: dial tcp: unknown network tcp&amp;rdquo;，则修改 /etc/systemd/system/k3s.service 文件。
&lt;ol>
&lt;li>将其中 &amp;lsquo;&amp;ndash;datastore-endpoint=mysql://root:root@tcp(172.38.40.214:3306)/k3s&amp;rsquo; \ 这行改为 &amp;lsquo;&amp;ndash;datastore-endpoint=mysql://root:root@tcp(172.38.40.214:3306)/k3s&amp;rsquo;  \&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>配置 kubectl config 文件&lt;/p>
&lt;p>虽然高版本 k3s 在 /usr/local/bin/ 目录下生成了 kubectl 的软连接，但是 kubeconfig 文件依然需要拷贝到 .kube 目录中，因为 helm 也会使用。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 获取 kubectl 二进制文件并放入 /usr/local/bin/ 目录中&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 从别的机器 copy 一个对应 k3s 版本的 kubeclt 二进制文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 拷贝 kubeconfig 文件到 kubectl 配置目录，并配置命令补全功能&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp /etc/rancher/k3s/k3s.yaml /root/.kube/config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;source &amp;lt;(kubectl completion bash)&amp;#34;&lt;/span> &amp;gt;&amp;gt; ~/.bashrc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="部署-rancher-1">部署 Rancher&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建所需 namespaces&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create namespace cattle-system
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>创建 Rancher 所需证书&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建证书&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out ca.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -x509 -new -nodes -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=rancher.ehualu.com&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key ca.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out ehualu.com.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -sha512 -new &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=rancher.ehualu.com&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key ehualu.com.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ehualu.com.csr
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; v3.ext &lt;span style="color:#e6db74">&amp;lt;&amp;lt;-EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">authorityKeyIdentifier=keyid,issuer
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">basicConstraints=CA:FALSE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">extendedKeyUsage = serverAuth
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">subjectAltName = @alt_names
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[alt_names]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">DNS.1=rancher.ehualu.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl x509 -req -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -extfile v3.ext &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -CA ca.crt -CAkey ca.key -CAcreateserial &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -in ehualu.com.csr &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ehualu.com.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将证书添加到 secret 资源，以便让 Rancher 读取&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ehualu.com.crt tls.crt &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp ehualu.com.key tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret tls tls-rancher-ingress &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert&lt;span style="color:#f92672">=&lt;/span>tls.crt &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key&lt;span style="color:#f92672">=&lt;/span>tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ca.crt cacerts.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret generic tls-ca &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-file&lt;span style="color:#f92672">=&lt;/span>cacerts.pem&lt;span style="color:#f92672">=&lt;/span>./cacerts.pem
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>部署&lt;/p>
&lt;pre>&lt;code># 通过 helm 生成部署 rancher 的 yaml。
helm template rancher ./rancher-2.4.5.tgz --output-dir . \
--namespace cattle-system \
--set hostname=rancher.ehualu.com \
--set rancherImage=registry.ehualu.com/rancher/rancher \
--set ingress.tls.source=secret \
--set privateCA=true \
--set systemDefaultRegistry=registry.ehualu.com \
--set useBundledSystemChart=true \
--set rancherImageTag=v2.4.5
# 部署 Rancher
kubectl -n cattle-system apply -R -f ./rancher
&lt;/code>&lt;/pre>
&lt;h1 id="rancher-升级">Rancher 升级&lt;/h1>
&lt;p>Rancher 本身的升级，就是 k8s 集群中服务的升级，使用 helm 更新即可，新版 pod 创建后销毁旧版 pod。&lt;/p>
&lt;p>Rancher 管理的 k8s 集群升级参考 &lt;a href="https://docs.rancher.cn/docs/rancher2/cluster-admin/upgrading-kubernetes/_index/">官方文档&lt;/a>，在 web 页面点两下就好很简单 。&lt;/p>
&lt;h1 id="rancher-清理">Rancher 清理&lt;/h1>
&lt;h2 id="清理-rancher">清理 Rancher&lt;/h2>
&lt;p>官方文档：&lt;a href="https://rancher.com/docs/rancher/v2.x/en/faq/removing-rancher/#what-if-i-don-t-want-rancher-anymore">https://rancher.com/docs/rancher/v2.x/en/faq/removing-rancher/#what-if-i-don-t-want-rancher-anymore&lt;/a>&lt;/p>
&lt;p>通过 &lt;a href="https://rancher.com/docs/rancher/v2.x/en/system-tools/#remove">rancher 系统工具的 remove 子命令&lt;/a>来清理 k8s 集群上 rancher&lt;/p>
&lt;h2 id="清理通过-rancher-创建的-k8s-集群">清理通过 Rancher 创建的 k8s 集群&lt;/h2>
&lt;p>官方文档：&lt;a href="https://docs.rancher.cn/docs/rancher2/cluster-admin/cleaning-cluster-nodes/_index/">https://docs.rancher.cn/docs/rancher2/cluster-admin/cleaning-cluster-nodes/_index/&lt;/a>&lt;/p>
&lt;p>在 Rancher web UI 上删除集群后，手动执行一些 &lt;a href="https://rancher2.docs.rancher.cn/docs/cluster-admin/cleaning-cluster-nodes/_index#%E6%89%8B%E5%8A%A8%E4%BB%8E%E9%9B%86%E7%BE%A4%E4%B8%AD%E5%88%A0%E9%99%A4-rancher-%E7%BB%84%E4%BB%B6">命令&lt;/a> 以删除在节点上生成的数据，并重启相关节点。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#！/bin/bash&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理所有 Docker 容器、镜像和卷：&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker rm -f &lt;span style="color:#66d9ef">$(&lt;/span>docker ps -qa&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker rmi -f &lt;span style="color:#66d9ef">$(&lt;/span>docker images -q&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker volume rm &lt;span style="color:#66d9ef">$(&lt;/span>docker volume ls -q&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 卸载挂载&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> mount in &lt;span style="color:#66d9ef">$(&lt;/span>mount | grep tmpfs | grep &lt;span style="color:#e6db74">&amp;#39;/var/lib/kubelet&amp;#39;&lt;/span> | awk &lt;span style="color:#e6db74">&amp;#39;{ print $3 }&amp;#39;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span> /var/lib/kubelet /var/lib/rancher; &lt;span style="color:#66d9ef">do&lt;/span> umount $mount; &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理目录及数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rm -rf /etc/ceph &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /etc/cni &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /etc/kubernetes &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /opt/cni &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /opt/rke &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /run/secrets/kubernetes.io &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /run/calico &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /run/flannel &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/calico &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/etcd &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/cni &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/kubelet &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/rancher/rke/log &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/log/containers &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/log/kube-audit &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/log/pods &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/run/calico
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理 iptables 与 网络设备，需要重启设备，也可以自己手动清理&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># reboot&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 1.2.实现虚拟化的工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/</guid><description/></item><item><title>Docs: 1.3.OpenStack 虚拟机编排系统</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</guid><description/></item><item><title>Docs: 1.3.Rancher 配置</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.3.rancher-%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.3.rancher-%E9%85%8D%E7%BD%AE/</guid><description>
&lt;p>Rancher Server URL 的修改&lt;/p>
&lt;p>当 Rancher Server URL 变更后(比如从 40443 变到 60443)，则还需要连带修改以下几部分&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Rancher Web 页面最上面的标签，进入&lt;code>系统设置&lt;/code>，修改&lt;code>server-url&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>k8s 集群中，修改 cattle-system 名称空间中，名为&lt;code>cattle-credentials-XXX&lt;/code>的 secret 资源中的 .data.url 字段的值，这个值是用 base64 编码的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>echo -n &amp;ldquo;https://X.X.X.X:60443&amp;rdquo; | bas64 ，通过该命令获取编码后的 url，然后填入 .data.url 字段中&lt;/p>
&lt;/li>
&lt;li>
&lt;p>k8s 集群中，修改 cattle-cluster-agent-XX 和 cattle-node-agent-XX 这些 pod 的 env 参数，将其中的 CATTLE_SERVER 的值改为想要的 URL。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cattle-node-agent 在 2.5.0 版本之后没有了，就不用改了。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>导入集群的 yaml 文件位置&lt;/p>
&lt;p>打开 &lt;code>https://RancherIP/v3/cluster/集群ID/clusterregistrationtokens&lt;/code> 页面&lt;/p>
&lt;p>在 data 字段下，可以看到获取 yaml 文件的 URL，可能会有多组，一般以时间最新的那组为准。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggn0dn/1616114779749-bd6fd7cc-32cb-41b8-9122-2047f125c4a7.png" alt="">&lt;/p></description></item><item><title>Docs: 1.API、Resource(资源)、Object(对象)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">官方文档，概念-概述-Kubernetes API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/">官方文档，参考-API 概述&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubernetes API 使我们可以查询和操纵 Kubernetes API 中资源的状态。Kubernetes API 符合 &lt;a href="https://www.yuque.com/go/doc/33220383">RESTful 规范&lt;/a>。&lt;/p>
&lt;p>Kubernetes 把自身一切抽象理解为 &lt;strong>Resource(资源)&lt;/strong>，也叫 &lt;strong>API Resource&lt;/strong>(有地方也叫 Group Resource)。对集群的所有操作都是通过对 Kubernetes API 的 HTTP(s) 请求来实现的。可以使用命令 kubectl api-resources 命令查看所有支持的资源。&lt;/p>
&lt;p>kubernetes 控制平面的核心是 &lt;strong>API Server&lt;/strong>。API Server 是实现了 Kubernets API 的应用程序，并为 Kubernetes 公开了一个 HTTP(s) 的 API，以供用户、集群中的不同部分和集群外部组件相互通信。&lt;/p>
&lt;p>Kubernetes 中各种资源(对象)的数据都通过 API 接口被提交到后端的持久化存储（etcd）中，Kubernetes 集群中的各部件之间通过该 API 接口实现解耦合，同时 Kubernetes 集群中一个重要且便捷的管理工具 kubectl 也是通过访问该 API 接口实现其强大的管理功能的。&lt;/p>
&lt;blockquote>
&lt;p>Note：kubectl 就是代替用户执行各种 http 请求的工具&lt;/p>
&lt;/blockquote>
&lt;p>在 Kubernetes 系统中，在大多数情况下，API 定义和实现都符合标准的 HTTP REST 格式，比如通过标准的 HTTP 动词（POST、PUT、GET、DELETE）来完成对相关资源对象的查询、创建、修改、删除等操作。但同时，Kubernetes 也为某些非标准的 REST 行为实现了附加的 API 接口，例如 Watch 某个资源的变化、进入容器执行某个操作等。另外，某些 API 接口可能违背严格的 REST 模式，因为接口返回的不是单一的 JSON 对象，而是其他类型的数据，比如 JSON 对象流或非结构化的文本日志数据等。&lt;/p>
&lt;p>另外，从另一个角度看，其实 kubernetes 就是提供了一个 web 服务，只是这个 web 服务不像传统的 B/S 架构那样，可以通过浏览器直接操作~kubernetes API 就是这个 web 服务的入口。&lt;/p>
&lt;blockquote>
&lt;p>注意：Kubernetes 的 API 与传统意义上的 API 不太一样。传统 API，一个 API 就是一个功能；而 Kubernetes API 中，一个 API 实际上又可以当作功能，也可以当作一个资源。对 API 的操作，就是对 Kubernets 资源进行操作&lt;/p>
&lt;/blockquote>
&lt;h2 id="api-resource资源-分类">API Resource(资源) 分类&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/">API 参考&lt;/a>、&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/">1.19 版本 API 参考(一页模式)&lt;/a>(要查看其他版本，修改 URL 最后的版本号即可)。&lt;/p>
&lt;/blockquote>
&lt;p>资源大体可以分为下面几类：&lt;/p>
&lt;ol>
&lt;li>**workload(工作负载) **# 用于在集群上管理和运行容器
&lt;ol>
&lt;li>Pod，Deployment，StatefuSet，DaemonSet，Job 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Discovery &amp;amp; LB(服务发现及均衡)&lt;/strong> # 可以使用这些资源类型的对象将工作负载“缝合”到一个外部可访问的、负载均衡的服务中。
&lt;ol>
&lt;li>Service，Ingress 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Config &amp;amp; Storage(配置与存储)&lt;/strong> # 这种类型的资源是用于将初始化数据注入到应用程序中并保留容器外部数据的对象。
&lt;ol>
&lt;li>Volume，ConifgMap，secret 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Cluster(集群级资源)&lt;/strong> # 这种类型的资源对象定义了群集本身的配置方式。这些通常仅由集群运营商使用。
&lt;ol>
&lt;li>Namesapces,Node,Role,ClusterRole,RoleBinding,ClusterRoleBinding 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Metadata(元数据型资源)&lt;/strong> # 这种类型的资源是用于配置集群中其他资源行为的对象。
&lt;ol>
&lt;li>HPA，PodTemplate，LimitRange 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>各种资源所用的 manifest 文件中的各个字段的含义就可以参考该页面找到详解。&lt;/p>
&lt;h2 id="api-resource资源-的-url-结构">API Resource(资源) 的 URL 结构&lt;/h2>
&lt;p>在 Kubernetes 中，资源的 URL 结构是由：Group（组）、Version（版本）和 Resource（资源种类）三个部分组成的。(还有一种 /metrics，/healthz 之类的结构，这里面的资源是系统自带的，不在任何组里)&lt;/p>
&lt;p>通过这样的结构，整个 Kubernetes 里的所有资源，实际上就可以用如下图的树形结构表示出来：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sz9hgm/1616120310758-dc53a2df-2a39-45e9-92e3-9beb5d9101f0.png" alt="">&lt;/p>
&lt;p>比如，如果要创建一个 CronJob 资源&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">batch/v2alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">CronJob&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在这个 YAML 文件中，“CronJob”就是资源的种类(Resource)，“batch”就是它的组(Group)，v2alpha1 就是它的版本(Version)。&lt;/p>
&lt;p>现阶段，有两个 API Groups 正在使用&lt;/p>
&lt;ol>
&lt;li>&lt;strong>core group(核心组)&lt;/strong> # 在/api/v1 路径下(由于某些历史原因而并没有在/apis/core/v1 路径下)。核心组是不需要 Group 的（即：它们 Group 是 &lt;code>&amp;quot;&amp;quot;&lt;/code>）。URI 路径为/api/v1，并且在定义资源的 manifest 文件中 apiVersion 字段的值不用包含组名，直接使用 v1 即可&lt;/li>
&lt;li>&lt;strong>named groups(已命名组)&lt;/strong> # URI 路径为/apis/$GROUP_NAME/$VERSION，在定义资源的 manifest 文件中 apiVersion 中省略 apis，使用 GroupName/Version&lt;/li>
&lt;/ol>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>有的资源是 cluster 级别的(比如 node)，有的资源是 namespace 级别的(比如 pod)，对于 namespace 级别的资源，可以在 Version 和 Resource 中间添加 namespace 字段以获取指定 namespace 下的资源。i.e./api/v1/namespaces/$NAMESPACE/pods/ ($NAMESPACE 就是具体的 namesapce 的名称)。&lt;/li>
&lt;li>所以 namesapce 级别资源的对象的 URI 应该像这样：/api/v1/namespaces/kube-system/pods/coredns-5644d7b6d9-tw4rh&lt;/li>
&lt;li>而 cluster 级别资源的对象的 URI 则是：/api/v1/nodes/master1&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>所有资源类型要么受集群范围限制（/apis/GROUP/VERSION/&lt;em>），要么受命名空间限制（/apis/GROUP/VERSION/namespaces/NAMESPACE/&lt;/em>）&lt;/strong>&lt;/p>
&lt;p>集群范围的资源：&lt;/p>
&lt;ol>
&lt;li>GET /apis/GROUP/VERSION/RESOURCETYPE #返回指定资源类型的资源集合(返回的是一个 list 列表，比如 NodeList 等)&lt;/li>
&lt;li>GET /apis/GROUP/VERSION/RESOURCETYPE/NAME #返回指定资源类型下名为 NAME 的的资源&lt;/li>
&lt;/ol>
&lt;p>名称空间范围的资源：&lt;/p>
&lt;ol>
&lt;li>GET /apis/GROUP/VERSION/RESOURCETYPE #返回所有名称空间指定资源类型的实例集合(返回的是一个 list 列表，比如 podList、serviceList 等)&lt;/li>
&lt;li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE #返回 NAMESPACE 下指定 ResourceType 的所有实例集合(返回的是一个 list 列表，比如 podList、serviceList 等)&lt;/li>
&lt;li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME #返回 NAMESPACE 下指定 ResourceType，名为 NAME 的实例&lt;/li>
&lt;/ol>
&lt;h1 id="declarative-api声明式-api-的特点">Declarative API(声明式 API) 的特点：&lt;/h1>
&lt;ol>
&lt;li>首先，所谓 &lt;strong>Declarative(声明式)&lt;/strong>，指的就是我只需要提交一个定义好的 API 对象来 **Declarative(声明) **我所期望的状态是什么样子。&lt;/li>
&lt;li>其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。&lt;/li>
&lt;li>最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。&lt;/li>
&lt;/ol>
&lt;p>所以说，声明式 API，才是 Kubernetes 项目编排能力“赖以生存”的核心所在。而想要实现 声明式 API，离不开 Controller 控制器，K8S 的大脑 的工作。&lt;/p>
&lt;h1 id="api-url-使用示例">API URL 使用示例&lt;/h1>
&lt;p>下面是在 1.18.8 版本下获取到的 api 路径结构&lt;/p>
&lt;p>根路径将列出所有可用路径&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/ -s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;paths&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/api&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/api/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/admissionregistration.k8s.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/admissionregistration.k8s.io/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/admissionregistration.k8s.io/v1beta1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiextensions.k8s.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiextensions.k8s.io/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiextensions.k8s.io/v1beta1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiregistration.k8s.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果访问到错误的资源，还会返回 404 的响应码&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-s&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/api/v1/service
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Status&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Failure&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;the server could not find the requested resource&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NotFound&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;details&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">404&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在“组/版本”下面可以看到该“组/版本”下所包含的 API 资源列表&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-s&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/api/v1/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;APIResourceList&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;groupVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resources&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">.......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;configmaps&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;singularName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespaced&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ConfigMap&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;verbs&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;create&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;delete&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;deletecollection&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;get&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;list&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;patch&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;watch&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;shortNames&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;cm&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;storageVersionHash&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;qFsyl6wFWjQ=&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;endpoints&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;singularName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespaced&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Endpoints&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;verbs&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;create&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;delete&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;deletecollection&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;get&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;list&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;patch&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;watch&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;shortNames&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ep&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;storageVersionHash&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;fWeeMqaN/OA=&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在“资源”下可以看到该“资源”下所包含的所有对象，下图是 pod 资源的列表，包含所有 pod 对象及其信息&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-s&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/api/v1/pods | more
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;PodList&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;selfLink&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/api/v1/pods&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resourceVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;618871&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;items&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent-cc6ddc6dc-7f89l&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;generateName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent-cc6ddc6dc-&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;selfLink&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/api/v1/namespaces/cattle-system/pods/cattle-cluster-agent-cc6ddc6dc-7f89l&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;72f4a825-feb2-416a-900d-d8401acc9a18&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resourceVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;452264&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;creationTimestamp&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2020-09-13T09:59:49Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;app&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;pod-template-hash&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cc6ddc6dc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;ownerReferences&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;apps/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ReplicaSet&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent-cc6ddc6dc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;7d4b6cbe-d6d1-46e3-99e5-8410095880c7&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;controller&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;blockOwnerDeletion&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;managedFields&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 1.Authenticating(认证)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/1.authenticating%E8%AE%A4%E8%AF%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/1.authenticating%E8%AE%A4%E8%AF%81/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">官方文档,参考-API 访问控制-认证&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Authenticating(动词) 也称为 Authentication(名词) 身份验证。指明客户端是否有权限访问 API Server。&lt;/p>
&lt;p>就好比我们在登录一个网站时，需要输入账户和密码的概念类似。在使用 API Server 时，也是通过类似的方式，使用账户来登录 API server(虽然不是真的登录)。&lt;/p>
&lt;h2 id="accounts--kubernetes-集群中的账号">Accounts # Kubernetes 集群中的账号&lt;/h2>
&lt;p>Accounts 是一个在认证授权系统里的逻辑概念。Accounts 需要通过认证概念中的东西(比如证书、token、或者用户名和密码等)来建立。类似于登陆系统的账户。而在 Kubernetes 中，Accounts 分为如下两类&lt;/p>
&lt;ol>
&lt;li>UserAccoun(用户账户，简称 User)&lt;/li>
&lt;li>ServiceAccount(服务账户，简称 SA)&lt;/li>
&lt;/ol>
&lt;h3 id="user-account-用户账号">User Account 用户账号&lt;/h3>
&lt;p>详见：[User Account 详解](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/7.API%20 访问控制/1.Authenticating(认证)/User%20Account%20 详解.md Account 详解.md)
User 不属于 K8S 中的一个资源。这类 Account 适用于：客户端访问集群时使用(比如使用 kubectl、scheduler 等访问 api)&lt;/p>
&lt;p>一个 User 可以管理多个 k8s 集群、也可以多个 User 管理一个集群，权限不同。User 只有在 KubeConfig 文件中才具有实际意义。&lt;/p>
&lt;p>由于 User 不属于 K8S 资源，那么则无法通过 API 调用来添加 User Account。但是任何提供了由群集的证书颁发机构(CA)签名的有效证书的用户都将被视为已认证。基于这种情况，Kubernetes 使用证书中的 subject 字段中的 Common Name(通用名称,即 CN)的值，作为用户名。接下来，基于授权概念中的 RBAC 子系统会确定用户是否有权针对某资源执行特定的操作。&lt;/p>
&lt;p>如果想创建一个 User，则可以通过证书的方式来创建。比如像下面这样， 这就创建了一个名为 lch 的 User Account。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>openssl genrsa -out lch.key &lt;span style="color:#ae81ff">2048&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -new -key lch.key -out lch.csr -subj &lt;span style="color:#e6db74">&amp;#34;/CN=lch&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果想使用 lch 这个 UA，则需要使用 kubectl config set-credentials 命令指定 lch 所需的相关凭证即可。还需要为 lch 绑定[授权概念](&amp;lt;✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/7.API%20 访问控制/2.Authorization(授权).md&amp;raquo;)中的 Role 以便让该用户具有某些操作权限，然后 lch 这个 UA 即可对所绑定的集群有 Role 中所指定的操作权限。其中为 -subj 选项中 CN 的值就是 User 的名称。这个值也是在后面为 User 赋予 RBAC 权限的 rolebinding 时所使用的 &lt;code>subjects.name&lt;/code> 字段的值。&lt;/p>
&lt;p>进一步的细节可参阅 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">证书请求&lt;/a> 下普通用户主题。&lt;/p>
&lt;h3 id="service-account-服务账号">Service Account 服务账号&lt;/h3>
&lt;blockquote>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33165738">Service Account 详解&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>SA 属于 K8S 中的一个资源。这类 Account 适用于：Pod 访问集群时使用。&lt;/p>
&lt;p>为什么需要 Service Account 呢？&lt;/p>
&lt;p>Service Account(服务账户) 概念的引入是基于这样的使用场景：运行在 pod 里的进程需要调用 Kubernetes API 以及非 Kubernetes API 的其它服务。Service Account 是给 pod 里面 Container 中的进程使用的，它为 pod 提供必要的身份认证。(与用户控制 kubectl 去调用 API 一样，这里相当于 Pod 中 Container 在调用 API 的时候需要的认证)&lt;/p>
&lt;h3 id="useraccount-与-serviceaccount-的区别httpskubernetesiodocsreferenceaccess-authn-authzservice-accounts-adminuser-accounts-versus-service-accounts">&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#user-accounts-versus-service-accounts">UserAccount 与 ServiceAccount 的区别&lt;/a>&lt;/h3>
&lt;ol>
&lt;li>UA 用来给人。SA 用来给运行在 pod 中的进程&lt;/li>
&lt;li>UA 作用于全局，UA 的名字在集群的所有 namespace 中必须是唯一的。SA 作用于 namespace&lt;/li>
&lt;li>UA 于 SA 的账户审核注意事项是不同的，UA 的凭证信息需要在使用 kubectl config 命令时候的手动指定；SA 的凭证信息在创建 SA 后会自动生成对应的 secret 并把凭证信息保存其中。&lt;/li>
&lt;/ol>
&lt;h2 id="accounts-group--账户组useraccount-与-serviceaccount-都有-group">Accounts Group # 账户组，UserAccount 与 ServiceAccount 都有 Group&lt;/h2>
&lt;p>UA 与 SA 都可以属于一个或多个 Group&lt;/p>
&lt;p>Group 是 Account 的集合，本身并没有操作权限，但附加于 Group 上的权限可由其内部的所有用户继承，以实现高效的授权管理机制。Kubernetes 有几个内建的用于特殊目的的 Group：&lt;/p>
&lt;ol>
&lt;li>system:unauthenticated&lt;/li>
&lt;li>system:authenticated&lt;/li>
&lt;li>system:serviceaccounts&lt;/li>
&lt;li>system:serviceaccounts:&lt;!-- raw HTML omitted -->&lt;/li>
&lt;/ol>
&lt;p>kubeconfig 会给 UserAccount 提供与 APIServer 交互时所用的证书&lt;/p>
&lt;p>Secret 会给 ServiceAccount 提供与 APIServer 交互时所用的证书&lt;/p>
&lt;h1 id="authentication-strategies-认证策略httpskubernetesiozhdocsreferenceaccess-authn-authzauthenticationauthentication-strategiesieaccount-可用的认证方式">&lt;a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#authentication-strategies">Authentication Strategies 认证策略&lt;/a>(i.e.Account 可用的认证方式)&lt;/h1>
&lt;p>Kubernetes 接受的认证方式有如下几种：&lt;/p>
&lt;ol>
&lt;li>client certificates&lt;/li>
&lt;li>bearer tokens&lt;/li>
&lt;li>an authenticating proxy&lt;/li>
&lt;li>HTTP basic auth&lt;/li>
&lt;/ol>
&lt;p>向 API Server 发起 HTTPS 请求时，kubernetes 通过身份验证插件对请求进行身份验证。&lt;/p>
&lt;h2 id="x509-client-certshttpskubernetesiodocsreferenceaccess-authn-authzauthenticationx509-client-certsx509-客户端证书">&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs">X509 Client Certs&lt;/a>(X509 客户端证书)&lt;/h2>
&lt;p>通过给 API 服务器传递 &amp;ndash;client-ca-file=SOMEFILE 选项，就可以启动客户端证书身份认证。 所引用的文件必须包含一个或者多个证书机构，用来验证向 API 服务器提供的客户端证书。 如果提供了客户端证书并且证书被验证通过，则 subject 中的公共名称（Common Name）就被 作为请求的用户名。 自 Kubernetes 1.4 开始，客户端证书还可以通过证书的 organization 字段标明用户的组成员信息。 要包含用户的多个组成员信息，可以在证书种包含多个 organization 字段。&lt;/p>
&lt;p>例如，使用 openssl 命令行工具生成一个证书签名请求：&lt;/p>
&lt;pre>&lt;code>openssl req -new -key jbeda.pem -out jbeda-csr.pem -subj &amp;quot;/CN=jbeda/O=app1/O=app2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>此命令将使用用户名 jbeda 生成一个证书签名请求（CSR），且该用户属于 &amp;ldquo;app&amp;rdquo; 和 &amp;ldquo;app2&amp;rdquo; 两个用户组。&lt;/p>
&lt;p>参阅&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/">管理证书&lt;/a>了解如何生成客户端证书&lt;/p>
&lt;h2 id="static-token-filehttpskubernetesiodocsreferenceaccess-authn-authzauthenticationstatic-token-file静态令牌文件">&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file">Static Token File&lt;/a>(静态令牌文件)&lt;/h2>
&lt;p>当 API 服务器的命令行设置了 &amp;ndash;token-auth-file=SOMEFILE 选项时，会从文件中 读取持有者令牌。目前，令牌会长期有效，并且在不重启 API 服务器的情况下 无法更改令牌列表。&lt;/p>
&lt;p>令牌文件是一个 CSV 文件，包含至少 3 个列：令牌、用户名和用户的 UID。 其余列被视为可选的组名。&lt;/p>
&lt;p>说明：&lt;/p>
&lt;p>如果要设置的组名不止一个，则对应的列必须用双引号括起来，例如&lt;/p>
&lt;pre>&lt;code>token,user,uid,&amp;quot;group1,group2,group3&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>在请求中放入持有者令牌&lt;/p>
&lt;p>当使用持有者令牌来对某 HTTP 客户端执行身份认证时，API 服务器希望看到 一个名为 Authorization 的 HTTP 头，其值格式为 Bearer THETOKEN。 持有者令牌必须是一个可以放入 HTTP 头部值字段的字符序列，至多可使用 HTTP 的编码和引用机制。 例如：如果持有者令牌为 31ada4fd-adec-460c-809a-9e56ceb75269，则其 出现在 HTTP 头部时如下所示：&lt;/p>
&lt;pre>&lt;code>Authorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269
# 比如一个 curl 请求中，可以通过 -H 参数加入请求头
curl --cacert ${CAPATH} -H &amp;quot;Authorization: Bearer ${TOKEN}&amp;quot; https://${IP}:6443/
&lt;/code>&lt;/pre></description></item><item><title>Docs: 1.Namespaces</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/1.namespaces/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/1.namespaces/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Linux_namespaces">Wiki,Linux_namespaces&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://segmentfault.com/a/1190000009732550">思否，Linux Namespace 和 Cgroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.junmajinlong.com/virtual/namespace">骏马金龙博客，Linux namespace&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/rhO5GUuWycRiFxdYaV-yiQ">公众号，YP 小站-Namespace 机制详解&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/lscMpc5BWAEzjgYw6H0wBw">公众号，开发内功修炼-Linux 网络名称空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/jJ9peydbNSd6Fv5bsJR3yA">公众号，MoeLove-彻底搞懂容器技术的基石：namespace&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/o5nZZzOTNXOFjv2aaIZ6OA">https://mp.weixin.qq.com/s/o5nZZzOTNXOFjv2aaIZ6OA&lt;/a>(下)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux Namespaces(Linux 名称空间) 是 Linux 内核的一个特性，Namespaces 可以对内核资源进行划分，使得一组进程看到一组资源，而另一组进程看到一组不同的资源。&lt;/p>
&lt;blockquote>
&lt;p>这里的资源包括 进程 ID、主机名、用户 ID、网络 等等。&lt;/p>
&lt;/blockquote>
&lt;p>如果把 Linux 操作系统比作一个大房子，那名称空间指的就是这个房子中的一个个房间，住在每个房间里的人都自以为独享了整个房子的资源，但其实大家仅仅只是在共享的基础之上互相隔离，共享指的是共享全局的资源，而隔离指的是局部上彼此保持隔离，因而名称空间的本质就是指：一种在空间上隔离的概念，当下盛行的许多容器虚拟化技术（典型代表如 LXC、Docker）就是基于 Linux 名称空间的概念而来的。&lt;/p>
&lt;p>很早以前的 Unix 有一个叫 chroot 的系统调用(通过修改根目录把用户 &lt;strong>jail(监狱)&lt;/strong> 到一个特定目录下)，chroot 提供了一种简单的隔离模式(隔离目录)：chroot 内部的文件系统无法访问外部的内容，详见 ftp 实现工具，chroot 说明。Linux Namespace 就是基于 chroot 的概念扩展而来，提供了对系统下更多资源的隔离机制。&lt;/p>
&lt;p>操作系统通过虚拟内存技术，使得每个用户进程都认为自己拥有所有的物理内存，这是操作系统对内存的虚拟化。操作系统通过分时调度系统，每个进程都能被【公平地】调度执行，即每个进程都能获取到 CPU，使得每个进程都认为自己在进程活动期间拥有所有的 CPU 时间，这是操作系统对 CPU 的虚拟化。&lt;/p>
&lt;p>从这两种虚拟化方式可推知，当使用某种虚拟化技术去管理进程时，进程会认为自己拥有某种物理资源的全部。&lt;/p>
&lt;p>虚拟内存和分时系统均是对物理资源进行虚拟化，其实操作系统中还有很多非物理资源，比如用户权限系统资源、网络协议栈资源、文件系统挂载路径资源等。通过 Linux 的 namespace 功能，可以对这些非物理全局资源进行虚拟化。&lt;/p>
&lt;p>Linux namespace 是在当前运行的系统环境中创建(隔离)另一个进程的运行环境出来，并在此运行环境中将一些必要的系统全局资源进行【虚拟化】。进程可以运行在指定的 namespace 中，因此，namespace 中的每个进程都认为自己拥有所有这些虚拟化的全局资源。&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gp34mf/1640133594098-db8cd29e-8628-4117-a5ec-ea14de312485.webp" alt="">
Linux Namespaces 的灵感来自 &lt;a href="https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs">Plan 9 from Bell Labs&lt;/a> 中大量使用的名称空间功能。Plan 9 from Bell Labs 是贝尔实验室弄出来的分布式操作系统。&lt;/p>
&lt;h2 id="linux-namespace-类型">Linux Namespace 类型&lt;/h2>
&lt;p>Note：随着技术的发展，Linux 内核支持的的 Namespace 类型在逐步增加&lt;/p>
&lt;p>目前，Linux 已经支持 8 种全局资源的虚拟化(每种资源都是随着 Linux 内核版本的迭代而逐渐加入的，因此有些内核版本可能不具备某种 namespace)：&lt;/p>
&lt;ul>
&lt;li>cgroup namespace：该 namespace 可单独管理自己的 cgroup&lt;/li>
&lt;li>ipc namespace：该 namespace 有自己的 IPC，比如共享内存、信号量等&lt;/li>
&lt;li>network namespace：该 namespace 有自己的网络资源，包括网络协议栈、网络设备、路由表、防火墙、端口等&lt;/li>
&lt;li>mount namespace：该 namespace 有自己的挂载信息，即拥有独立的目录层次&lt;/li>
&lt;li>pid namespace：该 namespace 有自己的进程号，使得 namespace 中的进程 PID 单独编号，比如可以 PID=1&lt;/li>
&lt;li>time namespace：该 namespace 有自己的启动时间点信息和单调时间，比如可设置某个 namespace 的开机时间点为 1 年前启动，再比如不同的 namespace 创建后可能流逝的时间不一样&lt;/li>
&lt;li>user namespace：该 namespace 有自己的用户权限管理机制(比如独立的 UID/GID)，使得 namespace 更安全&lt;/li>
&lt;li>uts namespace：该 namepsace 有自己的主机信息，包括主机名(hostname)、NIS domain name&lt;/li>
&lt;/ul>
&lt;p>用户可以同时创建具有多种资源类型的 namespace，比如创建一个同时具有 uts、pid 和 user 的 namespace。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>类型&lt;/th>
&lt;th>功能说明&lt;/th>
&lt;th>系统调用参数&lt;/th>
&lt;th>内核版本&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MNT Namespace&lt;/td>
&lt;td>提供磁盘挂载点和文件系统的隔离能力&lt;/td>
&lt;td>CLONE_NEWNS&lt;/td>
&lt;td>2.4.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>IPC Namespace&lt;/td>
&lt;td>提供进程间通信的隔离能力&lt;/td>
&lt;td>CLONE_NEWIPC&lt;/td>
&lt;td>2.6.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Net Namespace&lt;/td>
&lt;td>提供网络隔离能力&lt;/td>
&lt;td>CLONE_NEWNET&lt;/td>
&lt;td>2.6.29&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UTS Namespace&lt;/td>
&lt;td>提供主机名隔离能力&lt;/td>
&lt;td>CLONE_NEWUTS&lt;/td>
&lt;td>2.6.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PID Namespace&lt;/td>
&lt;td>提供进程隔离能力&lt;/td>
&lt;td>CLONE_NEWPID&lt;/td>
&lt;td>2.6.24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>User Namespace&lt;/td>
&lt;td>提供用户隔离能力&lt;/td>
&lt;td>CLONE_NEWUSER&lt;/td>
&lt;td>3.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CGroup Namespace&lt;/td>
&lt;td>Cgroup root directory&lt;/td>
&lt;td>&lt;/td>
&lt;td>4.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="理解-linux-namespace">理解 Linux namespace&lt;/h2>
&lt;p>用户可以创建指定类型的 namespace 并将进程放入该 namespace 中运行，这表示从当前的系统运行环境中隔离一个进程的运行环境，在此 namespace 中运行的进程将认为自己享有该 namespace 中的独立资源。&lt;/p>
&lt;p>实际上，即使用户没有手动创建 Linux namespace，Linux 系统开机后也会创建一个默认的 namespace，称为 root namespace，所有进程默认都运行在 root namespace 中，每个进程都认为自己拥有该 namespace 中的所有系统全局资源。&lt;/p>
&lt;p>回顾一下 Linux 的开机启动流程，内核加载成功后将初始化系统运行环境，这个运行环境就是 root namespace 环境，系统运行环境初始化完成后，便可以认为操作系统已经开始工作了。&lt;/p>
&lt;p>每一个 namespace 都基于当前内核，无论是默认的 root namespace 还是用户创建的每一个 namespace，都基于当前内核工作。所以可以认为 namespace 是内核加载后启动的一个特殊系统环境，用户进程可以在此环境中独立享用资源。更严格地说，root namespace 直接基于内核，而用户创建的 namespace 运行环境基于当前所在的 namespace。之所以用户创建的 namespace 不直接基于内核环境，是因为每一个 namespace 可能都会修改某些运行时内核参数。&lt;/p>
&lt;p>比如，用户创建的 uts namespace1 中修改了主机名为 ns1，然后在 namespace1 中创建 uts namespace2 时，namespace2 默认将共享 namespace1 的其他资源并拷贝 namespace1 的主机名资源，因此 namespace2 的主机名初始时也是 ns1。当然，namespace2 是隔离的，可以修改其主机名为 ns2，这不会影响其他 namespace，修改后，将只有 namespace2 中的进程能看到其主机名为 ns2。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gp34mf/1616122902978-3836fe05-d484-4fba-8626-939d6795c4d2.png" alt="">&lt;/p>
&lt;p>可以通过如下方式查看某个进程运行在哪一个 namespace 中，即该进程享有的独立资源来自于哪一个 namespace。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ls -l /proc/&amp;lt;PID&amp;gt;/ns&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ls -l /proc/$$/ns | awk &lt;span style="color:#e6db74">&amp;#39;{print $1,$(NF-2),$(NF-1),$NF}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx cgroup -&amp;gt; cgroup:&lt;span style="color:#f92672">[&lt;/span>4026531835&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx ipc -&amp;gt; ipc:&lt;span style="color:#f92672">[&lt;/span>4026531839&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx mnt -&amp;gt; mnt:&lt;span style="color:#f92672">[&lt;/span>4026531840&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx net -&amp;gt; net:&lt;span style="color:#f92672">[&lt;/span>4026531992&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid_for_children -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx user -&amp;gt; user:&lt;span style="color:#f92672">[&lt;/span>4026531837&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx uts -&amp;gt; uts:&lt;span style="color:#f92672">[&lt;/span>4026531838&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ls -l /proc/1/ns | awk &lt;span style="color:#e6db74">&amp;#39;{print $1,$(NF-2),$(NF-1),$NF}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx cgroup -&amp;gt; cgroup:&lt;span style="color:#f92672">[&lt;/span>4026531835&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx ipc -&amp;gt; ipc:&lt;span style="color:#f92672">[&lt;/span>4026531839&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx mnt -&amp;gt; mnt:&lt;span style="color:#f92672">[&lt;/span>4026531840&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx net -&amp;gt; net:&lt;span style="color:#f92672">[&lt;/span>4026531992&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid_for_children -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx user -&amp;gt; user:&lt;span style="color:#f92672">[&lt;/span>4026531837&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx uts -&amp;gt; uts:&lt;span style="color:#f92672">[&lt;/span>4026531838&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这些文件表示当前进程打开的 namespace 资源，每一个文件都是一个软链接，所指向的文件是一串格式特殊的名称。冒号后面中括号内的数值表示该 namespace 的 inode。如果不同进程的 namespace inode 相同，说明这些进程属于同一个 namespace。&lt;/p>
&lt;p>从结果上来看，每个进程都运行在多个 namespace 中，且 pid=1 和 pid=$$(当前 Shell 进程)两个进程的 namespace 完全一样，说明它们运行在相同的环境下(root namespace)。&lt;/p>
&lt;pre>&lt;code># namespace概念和细节相关man文档。这些 man 手册在 3.10 内核及之前版本是没有的
man namespaces
man uts_namespaces
man network_namespaces
man ipc_namespaces
man pid_namespaces
man mount_namespaces
man user_namespaces
man time_namespaces
man cgroup_namespaces
# namespace管理工具
man unshare # 创建namespace
man nscreate # 创建namespace，老版本的内核没有该工具
man nsenter # 切换namespace
man lsns # 查看当前已创建的namespace
&lt;/code>&lt;/pre>
&lt;h1 id="namespace-的具体实现">Namespace 的具体实现&lt;/h1>
&lt;p>对于 Linux 系统来说，自己本身就是一个 Namespace。系统启动的第一个进程 systemd 自己就有对应的 6 个名称空间，可以通过 lsns 命令看到 pid 为 1 的进程所使用的 Namespace，我们平时操作的地方就是 systemd 所在的 jail，所以能看到的 &lt;code>/&lt;/code> 就是 systemd 所在 jail 规定出来的 &lt;code>/&lt;/code>&lt;/p>
&lt;p>Linux Namespace 主要使用三个系统调用来实现&lt;/p>
&lt;ul>
&lt;li>&lt;strong>clone()&lt;/strong> # 实现线程的系统调用，用来创建一个新的进程 。&lt;/li>
&lt;li>&lt;strong>unshare()&lt;/strong> # 使某进程脱离某个 Namespace&lt;/li>
&lt;li>&lt;strong>setns()&lt;/strong> # 把某进程加入到某个 Namespace&lt;/li>
&lt;/ul>
&lt;p>每个 NameSpace 的说明：&lt;/p>
&lt;ol>
&lt;li>当调用 clone 时，设定了 CLONE_NEWPID，就会创建一个新的 PID Namespace，clone 出来的新进程将成为 Namespace 里的第一个进程。一个 PID Namespace 为进程提供了一个独立的 PID 环境，PID Namespace 内的 PID 将从 1 开始，在 Namespace 内调用 fork，vfork 或 clone 都将产生一个在该 Namespace 内独立的 PID。新创建的 Namespace 里的第一个进程在该 Namespace 内的 PID 将为 1，就像一个独立的系统里的 init 进程一样。该 Namespace 内的孤儿进程都将以该进程为父进程，当该进程被结束时，该 Namespace 内所有的进程都会被结束。PID Namespace 是层次性，新创建的 Namespace 将会是创建该 Namespace 的进程属于的 Namespace 的子 Namespace。子 Namespace 中的进程对于父 Namespace 是可见的，一个进程将拥有不止一个 PID，而是在所在的 Namespace 以及所有直系祖先 Namespace 中都将有一个 PID。系统启动时，内核将创建一个默认的 PID Namespace，该 Namespace 是所有以后创建的 Namespace 的祖先，因此系统所有的进程在该 Namespace 都是可见的。&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWIPC，就会创建一个新的 IPC Namespace，clone 出来的进程将成为 Namespace 里的第一个进程。一个 IPC Namespace 有一组 System V IPC objects 标识符构成，这标识符有 IPC 相关的系统调用创建。在一个 IPC Namespace 里面创建的 IPC object 对该 Namespace 内的所有进程可见，但是对其他 Namespace 不可见，这样就使得不同 Namespace 之间的进程不能直接通信，就像是在不同的系统里一样。当一个 IPC Namespace 被销毁，该 Namespace 内的所有 IPC object 会被内核自动销毁。
&lt;ol>
&lt;li>PID Namespace 和 IPC Namespace 可以组合起来一起使用，只需在调用 clone 时，同时指定 CLONE_NEWPID 和 CLONE_NEWIPC，这样新创建的 Namespace 既是一个独立的 PID 空间又是一个独立的 IPC 空间。不同 Namespace 的进程彼此不可见，也不能互相通信，这样就实现了进程间的隔离&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWNS，就会创建一个新的 mount Namespace。每个进程都存在于一个 mount Namespace 里面，mount Namespace 为进程提供了一个文件层次视图。如果不设定这个 flag，子进程和父进程将共享一个 mount Namespace，其后子进程调用 mount 或 umount 将会影响到所有该 Namespace 内的进程。如果子进程在一个独立的 mount Namespace 里面，就可以调用 mount 或 umount 建立一份新的文件层次视图。该 flag 配合 pivot_root 系统调用，可以为进程创建一个独立的目录空间。&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWNET，就会创建一个新的 Network Namespace。一个 Network Namespace 为进程提供了一个完全独立的网络协议栈的视图。包括网络设备接口，IPv4 和 IPv6 协议栈，IP 路由表，防火墙规则，sockets 等等。一个 Network Namespace 提供了一份独立的网络环境，就跟一个独立的系统一样。一个物理设备只能存在于一个 Network Namespace 中，可以从一个 Namespace 移动另一个 Namespace 中。虚拟网络设备(virtual network device)提供了一种类似管道的抽象，可以在不同的 Namespace 之间建立隧道。利用虚拟化网络设备，可以建立到其他 Namespace 中的物理设备的桥接。当一个 Network Namespace 被销毁时，物理设备会被自动移回 init Network Namespace，即系统最开始的 Namespace&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWUTS，就会创建一个新的 UTS Namespace。一个 UTS Namespace 就是一组被 uname 返回的标识符。新的 UTS Namespace 中的标识符通过复制调用进程所属的 Namespace 的标识符来初始化。Clone 出来的进程可以通过相关系统调用改变这些标识符，比如调用 sethostname 来改变该 Namespace 的 hostname。这一改变对该 Namespace 内的所有进程可见。CLONE_NEWUTS 和 CLONE_NEWNET 一起使用，可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样。&lt;/li>
&lt;/ol>
&lt;p>以上所有 clone flag 都可以一起使用，为进程提供了一个独立的运行环境。LXC 正是通过在 clone 时设定这些 flag，为进程创建一个有独立 PID，IPC，FS，Network，UTS 空间的 container。一个 container 就是一个虚拟的运行环境，对 container 里的进程是透明的，它会以为自己是直接在一个系统上运行的。&lt;/p>
&lt;h1 id="namespace-关联文件">Namespace 关联文件&lt;/h1>
&lt;p>主信息：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>/proc/PID/ns/*&lt;/strong> # 由于 namespace 都是与进程相关联，那么可以通过从每个进程的 ns 目录查看相关进程的 namespace 使用情况&lt;/li>
&lt;/ul>
&lt;p>Network Namespace：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>/var/run/netns/NAME&lt;/strong> # 该目录为 &lt;code>ip netns&lt;/code> 命令所能调取查看的目录
&lt;ol>
&lt;li>如果想让 &lt;code>ip netns&lt;/code> 命令查看到网络名称空间的信息，则需要把 /proc/PID/ns/net 文件链接到该目录即可&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 1.nova</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.nova/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.nova/</guid><description>
&lt;p>语法格式：nova [OPTIONS] [SubCommand [OPTIONS]]&lt;/p>
&lt;p>nova list [OPTIONS] #列出SERVER相关信息&lt;/p>
&lt;p>nova show #显示指定SERVER的详细信息，非常详细&lt;/p>
&lt;p>nova instacne-action-list #列出指定SERVER的操作，创建、启动、停止、删除时间等，&lt;/p>
&lt;p>语法格式：nova [OPTIONS] [SubCommand [OPTIONS]]&lt;/p>
&lt;p>注意：语法中的SERVER指的都是已经创建的虚拟服务器，SERVER可以用实例的NAME(实例名)或者UUID(实例的ID)来表示，SERVER的ID和NAME可以用过nova list命令查到&lt;/p>
&lt;p>UUID:Universally Unique Identifier,即通用唯一识别码&lt;/p>
&lt;p>可以使用nova help SubCommand命令查看相关子命令的使用方法&lt;/p>
&lt;p>nova list [OPTIONS] #列出SERVER相关信息&lt;/p>
&lt;ol>
&lt;li>
&lt;p>OPTIONS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--all-tenants #显示所有租户的SERVER信息，可简写为&amp;ndash;all-t&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--tenant [] #显示指定租户的SERVER信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova list &amp;ndash;all-t #显示所有正在运行的实例，可以查看实例以及ID和主机名&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova list &amp;ndash;all-t &amp;ndash;host &lt;code>cat /etc/uuid&lt;/code> #显示&lt;code>cat /etc/uuid&lt;/code>命令输出的主机名的节点运行的实例信息&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nova show #显示指定SERVER的详细信息，非常详细&lt;/p>
&lt;ol>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova show ID #以实例ID展示该实例的详细信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova show ID | grep host #以实例ID查看所在节点的主机名&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nova instacne-action-list #列出指定SERVER的操作，创建、启动、停止、删除时间等，&lt;/p>
&lt;ol>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova instance-action-list ID|NAME #以实例ID显示该实例的活动信息，包括启动、停止、创建时间等&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nova&lt;/p>
&lt;p>nova [&amp;ndash;version] [&amp;ndash;debug] [&amp;ndash;os-cache] [&amp;ndash;timings]&lt;/p>
&lt;pre>&lt;code> [--os-region-name ] [--service-type ]
[--service-name ]
[--os-endpoint-type ]
[--os-compute-api-version ]
[--endpoint-override ] [--profile HMAC_KEY]
[--insecure] [--os-cacert ]
[--os-cert ] [--os-key ] [--timeout ]
[--os-auth-type ] [--os-auth-url OS_AUTH_URL]
[--os-system-scope OS_SYSTEM_SCOPE] [--os-domain-id OS_DOMAIN_ID]
[--os-domain-name OS_DOMAIN_NAME] [--os-project-id OS_PROJECT_ID]
[--os-project-name OS_PROJECT_NAME]
[--os-project-domain-id OS_PROJECT_DOMAIN_ID]
[--os-project-domain-name OS_PROJECT_DOMAIN_NAME]
[--os-trust-id OS_TRUST_ID]
[--os-default-domain-id OS_DEFAULT_DOMAIN_ID]
[--os-default-domain-name OS_DEFAULT_DOMAIN_NAME]
[--os-user-id OS_USER_ID] [--os-username OS_USERNAME]
[--os-user-domain-id OS_USER_DOMAIN_ID]
[--os-user-domain-name OS_USER_DOMAIN_NAME]
[--os-password OS_PASSWORD]
...
&lt;/code>&lt;/pre>
&lt;p>Command-line interface to the OpenStack Nova API.&lt;/p>
&lt;p>Positional arguments:位置参数&lt;/p>
&lt;p>子命令&lt;/p>
&lt;pre>&lt;code>add-secgroup Add a Security Group to a server.
agent-create Create new agent build.
agent-delete Delete existing agent build.
agent-list List all builds.
agent-modify Modify existing agent build.
aggregate-add-host Add the host to the specified aggregate.
aggregate-create Create a new aggregate with the specified
details.
aggregate-delete Delete the aggregate.
aggregate-list Print a list of all aggregates.
aggregate-remove-host Remove the specified host from the specified
aggregate.
aggregate-set-metadata Update the metadata associated with the
aggregate.
aggregate-show Show details of the specified aggregate.
aggregate-update Update the aggregate's name and optionally
availability zone.
availability-zone-list List all the availability zones.
backup Backup a server by creating a 'backup' type
snapshot.
boot Boot a new server.
cell-capacities Get cell capacities for all cells or a given
cell.
cell-show Show details of a given cell.
clear-password Clear the admin password for a server from the
metadata server. This action does not actually
change the instance server password.
console-log Get console log output of a server.
delete Immediately shut down and delete specified
server(s).
diagnostics Retrieve server diagnostics.
evacuate Evacuate server from failed host.
flavor-access-add Add flavor access for the given tenant.
flavor-access-list Print access information about the given
flavor.
flavor-access-remove Remove flavor access for the given tenant.
flavor-create Create a new flavor.
flavor-delete Delete a specific flavor
flavor-key Set or unset extra_spec for a flavor.
flavor-list Print a list of available 'flavors' (sizes of
servers).
flavor-show Show details about the given flavor.
flavor-update Update the description of an existing flavor.
(Supported by API versions '2.55' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
force-delete Force delete a server.
get-mks-console Get an MKS console to a server. (Supported by
API versions '2.8' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
get-password Get the admin password for a server. This
operation calls the metadata service to query
metadata information and does not read
password information from the server itself.
get-rdp-console Get a rdp console to a server.
get-serial-console Get a serial console to a server.
get-spice-console Get a spice console to a server.
get-vnc-console Get a vnc console to a server.
host-evacuate Evacuate all instances from failed host.
host-evacuate-live Live migrate all instances of the specified
host to other available hosts.
host-meta Set or Delete metadata on all instances of a
host.
host-servers-migrate Cold migrate all instances off the specified
host to other available hosts.
hypervisor-list List hypervisors. (Supported by API versions
'2.0' - '2.latest') [hint: use '--os-compute-
api-version' flag to show help message for
proper version]
hypervisor-servers List servers belonging to specific
hypervisors.
hypervisor-show Display the details of the specified
hypervisor.
hypervisor-stats Get hypervisor statistics over all compute
nodes.
hypervisor-uptime Display the uptime of the specified
hypervisor.
image-create Create a new image by taking a snapshot of a
running server.
instance-action Show an action.
instance-action-list List actions on a server. (Supported by API
versions '2.0' - '2.latest') [hint: use '--os-
compute-api-version' flag to show help message
for proper version]
interface-attach Attach a network interface to a server.
interface-detach Detach a network interface from a server.
interface-list List interfaces attached to a server.
keypair-add Create a new key pair for use with servers.
keypair-delete Delete keypair given by its name. (Supported
by API versions '2.0' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
keypair-list Print a list of keypairs for a user (Supported
by API versions '2.0' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
keypair-show Show details about the given keypair.
(Supported by API versions '2.0' - '2.latest')
[hint: use '--os-compute-api-version' flag to
show help message for proper version]
limits Print rate and absolute limits.
list List servers.
list-extensions List all the os-api extensions that are
available.
list-secgroup List Security Group(s) of a server.
live-migration Migrate running server to a new machine.
live-migration-abort Abort an on-going live migration. (Supported
by API versions '2.24' - '2.latest') [hint:
use '--os-compute-api-version' flag to show
help message for proper version]
live-migration-force-complete
Force on-going live migration to complete.
(Supported by API versions '2.22' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
lock Lock a server. A normal (non-admin) user will
not be able to execute actions on a locked
server.
meta Set or delete metadata on a server.
migrate Migrate a server.
migration-list Print a list of migrations. (Supported by API
versions '2.0' - '2.latest') [hint: use '--os-
compute-api-version' flag to show help message
for proper version]
pause Pause a server.
quota-class-show List the quotas for a quota class.
quota-class-update Update the quotas for a quota class.
(Supported by API versions '2.0' - '2.latest')
[hint: use '--os-compute-api-version' flag to
show help message for proper version]
quota-defaults List the default quotas for a tenant.
quota-delete Delete quota for a tenant/user so their quota
will Revert back to default.
quota-show List the quotas for a tenant/user.
quota-update Update the quotas for a tenant/user.
(Supported by API versions '2.0' - '2.latest')
[hint: use '--os-compute-api-version' flag to
show help message for proper version]
reboot Reboot a server.
rebuild Shutdown, re-image, and re-boot a server.
refresh-network Refresh server network information.
remove-secgroup Remove a Security Group from a server.
rescue Reboots a server into rescue mode, which
starts the machine from either the initial
image or a specified image, attaching the
current boot disk as secondary.
reset-network Reset network of a server.
reset-state Reset the state of a server.
resize Resize a server.
resize-confirm Confirm a previous resize.
resize-revert Revert a previous resize (and return to the
previous VM).
restore Restore a soft-deleted server.
resume Resume a server.
server-group-create Create a new server group with the specified
details.
server-group-delete Delete specific server group(s).
server-group-get Get a specific server group.
server-group-list Print a list of all server groups.
server-migration-list Get the migrations list of specified server.
(Supported by API versions '2.23' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
server-migration-show Get the migration of specified server.
(Supported by API versions '2.23' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
server-tag-add Add one or more tags to a server. (Supported
by API versions '2.26' - '2.latest') [hint:
use '--os-compute-api-version' flag to show
help message for proper version]
server-tag-delete Delete one or more tags from a server.
(Supported by API versions '2.26' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
server-tag-delete-all Delete all tags from a server. (Supported by
API versions '2.26' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
server-tag-list Get list of tags from a server. (Supported by
API versions '2.26' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
server-tag-set Set list of tags to a server. (Supported by
API versions '2.26' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
service-delete Delete the service by UUID ID. (Supported by
API versions '2.0' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
service-disable Disable the service. (Supported by API
versions '2.0' - '2.latest') [hint: use '--os-
compute-api-version' flag to show help message
for proper version]
service-enable Enable the service. (Supported by API versions
'2.0' - '2.latest') [hint: use '--os-compute-
api-version' flag to show help message for
proper version]
service-force-down Force service to down. (Supported by API
versions '2.11' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
service-list Show a list of all running services. Filter by
host &amp;amp; binary.
set-password Change the admin password for a server.
shelve Shelve a server.
shelve-offload Remove a shelved server from the compute node.
show Show details about the given server.
ssh SSH into a server.
start Start the server(s).
stop Stop the server(s).
suspend Suspend a server.
trigger-crash-dump Trigger crash dump in an instance. (Supported by API versions '2.17' - '2.latest') [hint:
use '--os-compute-api-version' flag to show
help message for proper version]
unlock Unlock a server.
unpause Unpause a server.
unrescue Restart the server from normal boot disk
again.
unshelve Unshelve a server.
update Update the name or the description for a
server.
usage Show usage data for a single tenant.
usage-list List usage data for all tenants.
version-list List all API versions.
volume-attach Attach a volume to a server.
volume-attachments List all the volumes attached to a server.
volume-detach Detach a volume from a server.
volume-update Update the attachment on the server. Migrates
the data from an attached volume to the
specified available volume and swaps out the
active attachment to the new volume.
bash-completion Prints all of the commands and options to
stdout so that the nova.bash_completion script
doesn't have to hard code them.
help Display help about this program or one of its
subcommands.
&lt;/code>&lt;/pre>
&lt;p>Optional arguments:可选参数&lt;/p>
&lt;p>--version show program&amp;rsquo;s version number and exit&lt;/p>
&lt;p>--debug Print debugging output.&lt;/p>
&lt;p>--os-cache Use the auth token cache. Defaults to False if&lt;/p>
&lt;pre>&lt;code> env[OS_CACHE] is not set.
&lt;/code>&lt;/pre>
&lt;p>--timings Print call timing info.&lt;/p>
&lt;p>--os-region-name&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_REGION_NAME].
&lt;/code>&lt;/pre>
&lt;p>--service-type&lt;/p>
&lt;pre>&lt;code> Defaults to compute for most actions.
&lt;/code>&lt;/pre>
&lt;p>--service-name&lt;/p>
&lt;pre>&lt;code> Defaults to env[NOVA_SERVICE_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-endpoint-type&lt;/p>
&lt;pre>&lt;code> Defaults to env[NOVA_ENDPOINT_TYPE],
env[OS_ENDPOINT_TYPE] or publicURL.
&lt;/code>&lt;/pre>
&lt;p>--os-compute-api-version&lt;/p>
&lt;pre>&lt;code> Accepts X, X.Y (where X is major and Y is
minor part) or &amp;quot;X.latest&amp;quot;, defaults to
env[OS_COMPUTE_API_VERSION].
&lt;/code>&lt;/pre>
&lt;p>--endpoint-override&lt;/p>
&lt;pre>&lt;code> Use this API endpoint instead of the Service
Catalog. Defaults to
env[NOVACLIENT_ENDPOINT_OVERRIDE].
&lt;/code>&lt;/pre>
&lt;p>--profile HMAC_KEY HMAC key to use for encrypting context data&lt;/p>
&lt;pre>&lt;code> for performance profiling of operation. This
key should be the value of the HMAC key
configured for the OSprofiler middleware in
nova; it is specified in the Nova
configuration file at &amp;quot;/etc/nova/nova.conf&amp;quot;.
Without the key, profiling will not be
triggered even if OSprofiler is enabled on the
server side.
&lt;/code>&lt;/pre>
&lt;p>--os-auth-type , &amp;ndash;os-auth-plugin&lt;/p>
&lt;pre>&lt;code> Authentication type to use
&lt;/code>&lt;/pre>
&lt;p>API Connection Options:API连接选项&lt;/p>
&lt;p>Options controlling the HTTP API Connections&lt;/p>
&lt;p>--insecure Explicitly allow client to perform &amp;ldquo;insecure&amp;rdquo;&lt;/p>
&lt;pre>&lt;code> TLS (https) requests. The server's certificate
will not be verified against any certificate
authorities. This option should be used with
caution.
&lt;/code>&lt;/pre>
&lt;p>--os-cacert Specify a CA bundle file to use in verifying a&lt;/p>
&lt;pre>&lt;code> TLS (https) server certificate. Defaults to
env[OS_CACERT].
&lt;/code>&lt;/pre>
&lt;p>--os-cert Defaults to env[OS_CERT].&lt;/p>
&lt;p>--os-key Defaults to env[OS_KEY].&lt;/p>
&lt;p>--timeout Set request timeout (in seconds).&lt;/p>
&lt;p>Authentication Options:认证选项&lt;/p>
&lt;p>Options specific to the password plugin.&lt;/p>
&lt;p>--os-auth-url OS_AUTH_URL Authentication URL&lt;/p>
&lt;p>--os-system-scope OS_SYSTEM_SCOPE&lt;/p>
&lt;pre>&lt;code> Scope for system operations
&lt;/code>&lt;/pre>
&lt;p>--os-domain-id OS_DOMAIN_ID Domain ID to scope to&lt;/p>
&lt;p>--os-domain-name OS_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> Domain name to scope to
&lt;/code>&lt;/pre>
&lt;p>--os-project-id OS_PROJECT_ID, &amp;ndash;os-tenant-id OS_PROJECT_ID&lt;/p>
&lt;pre>&lt;code> Project ID to scope to
&lt;/code>&lt;/pre>
&lt;p>--os-project-name OS_PROJECT_NAME, &amp;ndash;os-tenant-name OS_PROJECT_NAME&lt;/p>
&lt;pre>&lt;code> Project name to scope to
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-id OS_PROJECT_DOMAIN_ID&lt;/p>
&lt;pre>&lt;code> Domain ID containing project
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-name OS_PROJECT_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> Domain name containing project
&lt;/code>&lt;/pre>
&lt;p>--os-trust-id OS_TRUST_ID Trust ID&lt;/p>
&lt;p>--os-default-domain-id OS_DEFAULT_DOMAIN_ID&lt;/p>
&lt;pre>&lt;code> Optional domain ID to use with v3 and v2
parameters. It will be used for both the user
and project domain in v3 and ignored in v2
authentication.
&lt;/code>&lt;/pre>
&lt;p>--os-default-domain-name OS_DEFAULT_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> Optional domain name to use with v3 API and v2
parameters. It will be used for both the user
and project domain in v3 and ignored in v2
authentication.
&lt;/code>&lt;/pre>
&lt;p>--os-user-id OS_USER_ID User id&lt;/p>
&lt;p>--os-username OS_USERNAME, &amp;ndash;os-user-name OS_USERNAME&lt;/p>
&lt;pre>&lt;code> Username
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-id OS_USER_DOMAIN_ID&lt;/p>
&lt;pre>&lt;code> User's domain id
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-name OS_USER_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> User's domain name
&lt;/code>&lt;/pre>
&lt;p>--os-password OS_PASSWORD User&amp;rsquo;s password&lt;/p>
&lt;p>See &amp;ldquo;nova help COMMAND&amp;rdquo; for help on a specific command.&lt;/p></description></item><item><title>Docs: 1.openstack Command Line基础</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.openstack-command-line%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.openstack-command-line%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>常用命令&lt;/p>
&lt;p>openstack 命令行控制简介&lt;/p>
&lt;p>openstack CLI 的认证方式：&lt;/p>
&lt;p>语法格式：openstack [OPTIONS] Command [CommandArguments]&lt;/p>
&lt;p>Command&lt;/p>
&lt;p>openstack.cli #CLI 命令行控制命令组&lt;/p>
&lt;p>openstack.common # common 通用命令组&lt;/p>
&lt;p>openstack.compute.v2 #compute 计算服务命令组&lt;/p>
&lt;p>openstack.identity.v3 #identity 身份服务命令组&lt;/p>
&lt;p>openstack.image.v2 #image 镜像服务命令组&lt;/p>
&lt;p>openstack.network.v2 #network 网络服务命令组&lt;/p>
&lt;p>openstack.neutronclient.v2 #neutron 客户端命令组&lt;/p>
&lt;p>openstack.object_store.v1 #objectStore 对象存储服务命令组&lt;/p>
&lt;p>openstack.volume.v2 #volume 卷服务命令组&lt;/p>
&lt;p>常用命令&lt;/p>
&lt;p>虚拟机的：server、console&lt;/p>
&lt;p>网络的：network、subnet、port、router&lt;/p>
&lt;p>镜像的：image&lt;/p>
&lt;p>存储的：&lt;/p>
&lt;p>实例类型：flavor&lt;/p>
&lt;p>openstack 命令行控制简介&lt;/p>
&lt;p>Openstack Command Line Client 官方介绍：&lt;a href="https://docs.openstack.org/python-openstackclient/rocky/">https://docs.openstack.org/python-openstackclient/rocky/&lt;/a>&lt;/p>
&lt;p>OpenStackClient(又名 OSC)是&lt;strong>openstack 的命令行客户端&lt;/strong>，这个客户端将 compute、identity、image、object、storage 和 blockStorage 这些的 APIs 一起放在一个具有统一命令结构的 shell 中。e.g.nova、neutron、glance 等命令，都会集中在 openstack 的子命令中。&lt;/p>
&lt;p>openstack CLI 的认证：&lt;/p>
&lt;p>使用 openstack CLI 需要进行认证，才能通过该客户端与 openstack 各个组件的 API 进行交互，否则，是没有权限对 openstack 集群进行任何控制的。&lt;/p>
&lt;p>认证方式一般是通过环境变量来进行的。不同的安装方式，认证方式不同&lt;/p>
&lt;ol>
&lt;li>
&lt;p>kolla-ansible 部署的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>直接使用 kolla 提供的配置文件，加载文件中的环境变量即可。i.e.source /etc/kolla/admin-openrc.sh&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>如果不进行认证，一般会出现以下几种报错&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ERROR (CommandError): You must provide a user name/id (via &amp;ndash;os-username, &amp;ndash;os-user-id, env[OS_USERNAME] or env[OS_USER_ID]) or an auth token (via &amp;ndash;os-token).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Missing value auth-url required for auth plugin password&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>语法格式：openstack [OPTIONS] Command [CommandArguments]&lt;/p>
&lt;p>启动一个 shell 来执行 openstack 客户端中的 Command，或者直接使用 openstack+Command 来对 openstack 进行管理与控制&lt;/p>
&lt;p>OPTIONS #global options 全局选项，用来控制 openstack 程序，大部分都是关于认证的选项&lt;/p>
&lt;p>Command&lt;/p>
&lt;p>所有 openstack 的可用的 Command 可以通过&lt;code>openstack command list&lt;/code> 命令所列出的列表来查看。这些命令通过组来划分，每个命令组代表对一类服务的控制命令&lt;/p>
&lt;p>openstack.cli #CLI 命令行控制命令组&lt;/p>
&lt;p>command list [&amp;ndash;group ] #按组列出 openstack 可以支持的所有 Command，可以在选项中指定要查看的具体组名，只查看该组的命令。GroupKeyword 可以使组名中的关键字，不用使用完整的组名&lt;/p>
&lt;p>module list [&amp;ndash;all] #显示 OSC 程序已经安装的 python 模块&lt;/p>
&lt;p>openstack.common # common 通用命令组&lt;/p>
&lt;p>availability&lt;/p>
&lt;p>configuration&lt;/p>
&lt;p>extension&lt;/p>
&lt;p>limits&lt;/p>
&lt;p>project&lt;/p>
&lt;p>quota&lt;/p>
&lt;p>versions&lt;/p>
&lt;p>openstack.compute.v2 #compute 计算服务命令组&lt;/p>
&lt;p>aggregate&lt;/p>
&lt;p>compute&lt;/p>
&lt;p>console #实例控制台相关控制命令&lt;/p>
&lt;p>openstack console log show [&amp;ndash;lines ] SERVER&lt;/p>
&lt;p>openstack console url show [&amp;ndash;novnc | &amp;ndash;xvpvnc | &amp;ndash;spice][&amp;ndash;rdp | &amp;ndash;serial | &amp;ndash;mks] SERVER&lt;/p>
&lt;p>flavor #实例类型相关控制命令&lt;/p>
&lt;p>openstack flavor SubCommand [OPTIONS] [ARGS]&lt;/p>
&lt;p>SubCommand 包括：&lt;/p>
&lt;ol>
&lt;li>list&lt;/li>
&lt;/ol>
&lt;p>host&lt;/p>
&lt;p>hypervisor&lt;/p>
&lt;p>ip&lt;/p>
&lt;p>keypair&lt;/p>
&lt;p>server #控制 openstack 上所开虚拟机的命令&lt;/p>
&lt;p>openstack server SubCommand [OPTIONS] [ARGS]&lt;/p>
&lt;p>SubCommand 包括：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>list #列出 openstack 上所开的虚拟机 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>OPTIONS &lt;/p>
&lt;/li>
&lt;li>
&lt;p>--long #列出更多的关于虚拟机的信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>openstack server list #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>set #设置服务器属性&lt;/p>
&lt;/li>
&lt;li>
&lt;p>OPTIONS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>openstack server set &amp;ndash;root-password centos7 #为名为 centos7 的实例修改 root 密码&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>usage&lt;/p>
&lt;p>openstack.identity.v3 #identity 身份服务命令组&lt;/p>
&lt;p>access&lt;/p>
&lt;p>application&lt;/p>
&lt;p>catalog&lt;/p>
&lt;p>consumer&lt;/p>
&lt;p>credential&lt;/p>
&lt;p>domain #domain 域，是用户、组、项目的集合，每个组合项目仅有一个域&lt;/p>
&lt;p>ec2&lt;/p>
&lt;p>endpoint&lt;/p>
&lt;p>federation&lt;/p>
&lt;p>group&lt;/p>
&lt;p>identity&lt;/p>
&lt;p>implied&lt;/p>
&lt;p>limit&lt;/p>
&lt;p>mapping&lt;/p>
&lt;p>policy&lt;/p>
&lt;p>project&lt;/p>
&lt;p>region&lt;/p>
&lt;p>registered&lt;/p>
&lt;p>request&lt;/p>
&lt;p>role&lt;/p>
&lt;p>service&lt;/p>
&lt;p>token&lt;/p>
&lt;p>trust&lt;/p>
&lt;p>user&lt;/p>
&lt;p>openstack.image.v2 #image 镜像服务命令组&lt;/p>
&lt;p>image&lt;/p>
&lt;p>openstack.network.v2 #network 网络服务命令组&lt;/p>
&lt;p>address&lt;/p>
&lt;p>floating&lt;/p>
&lt;p>ip&lt;/p>
&lt;p>network&lt;/p>
&lt;p>port&lt;/p>
&lt;p>router&lt;/p>
&lt;p>security&lt;/p>
&lt;p>subnet&lt;/p>
&lt;p>openstack.neutronclient.v2 #neutron 客户端命令组&lt;/p>
&lt;p>bgp&lt;/p>
&lt;p>bgpvpn&lt;/p>
&lt;p>firewall&lt;/p>
&lt;p>network&lt;/p>
&lt;p>sfc&lt;/p>
&lt;p>vpn&lt;/p>
&lt;p>openstack.object_store.v1 #objectStore 对象存储服务命令组&lt;/p>
&lt;p>container&lt;/p>
&lt;p>object&lt;/p>
&lt;p>openstack.volume.v2 #volume 卷服务命令组&lt;/p>
&lt;p>backup&lt;/p>
&lt;p>consistency&lt;/p>
&lt;p>snapshot&lt;/p>
&lt;p>volume&lt;/p></description></item><item><title>Docs: 10.1.bootstrap 认证配置步骤介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/kubelet-%E7%89%B9%E6%80%A7/10.1.bootstrap-%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/kubelet-%E7%89%B9%E6%80%A7/10.1.bootstrap-%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;p>kubelet 授权 kube-apiserver 的一些操作 exec run logs 等&lt;/p>
&lt;p>&lt;strong>RBAC 只需创建一次就可以&lt;/strong>&lt;/p>
&lt;pre>&lt;code>kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>创建 bootstrap kubeconfig 文件&lt;/strong>&lt;/p>
&lt;p>注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token&lt;/p>
&lt;pre>&lt;code>kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-clientgroup --kubeconfig ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>查看生成的 token&lt;/p>
&lt;pre>&lt;code>kubeadm token list --kubeconfig ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS&lt;/p>
&lt;p>** 2kcmsb.hyl5s4g0l1mkff9z** &lt;strong>23h&lt;/strong> 2018-11-16T11:08:00+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-clientgroup&lt;/p>
&lt;p>配置集群参数，生成 kubernetes-clientgroup-bootstrap.kubeconfig&lt;/p>
&lt;pre>&lt;code>kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=https://192.168.1.7:6443 \ #master节点ip
--kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>配置客户端认证&lt;/p>
&lt;pre>&lt;code>kubectl config set-credentials kubelet-bootstrap \
--token= 2kcmsb.hyl5s4g0l1mkff9z \ #上面生成的token
--kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>配置关联&lt;/p>
&lt;pre>&lt;code>kubectl config set-context default \
--cluster=kubernetes \
--user=kubelet-bootstrap \
--kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>配置默认关联&lt;/p>
&lt;pre>&lt;code>kubectl config use-context default --kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>拷贝生成的 kubernetes-clientgroup-bootstrap.kubeconfig 文件到其它所有的 node 节点，并重命名&lt;/p>
&lt;pre>&lt;code>scp kubernetes-clientgroup-bootstrap.kubeconfig 192.168.1.8:/etc/kubernetes/bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>配置 bootstrap RBAC 权限&lt;/strong>&lt;/p>
&lt;pre>&lt;code>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
&lt;/code>&lt;/pre>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>否则报如下错误
failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &amp;quot;system:bootstrap:1jezb7&amp;quot; cannot create
certificatesigningrequests.certificates.k8s.io at the cluster scope
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>创建自动批准相关 CSR 请求的 ClusterRole&lt;/strong>&lt;/p>
&lt;pre>&lt;code>vi /etc/kubernetes/tls-instructs-csr.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver
rules:
- apiGroups: [&amp;quot;certificates.k8s.io&amp;quot;]
resources: [&amp;quot;certificatesigningrequests/selfnodeserver&amp;quot;]
verbs: [&amp;quot;create&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>导入 yaml 文件&lt;/p>
&lt;pre>&lt;code>kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml
&lt;/code>&lt;/pre>
&lt;p>clusterrole.rbac.authorization.k8s.io &amp;ldquo;system:certificates.k8s.io:certificatesigningrequests:selfnodeserver&amp;rdquo; created&lt;/p>
&lt;p>查看创建的 ClusterRole&lt;/p>
&lt;pre>&lt;code>kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>将 ClusterRole 绑定到适当的用户组&lt;/strong>&lt;/p>
&lt;pre>&lt;code># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求
kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers
# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求
kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes
# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求
kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes
查看已有绑定 kubectl get clusterrolebindings
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>动态 kubelet 配置&lt;/strong>&lt;/p>
&lt;p>创建 kubelet 服务文件&lt;/p>
&lt;pre>&lt;code>mkdir -p /var/lib/kubelet
vim /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
--hostname-override=k8s-wjoyxt \ #本地node节点的hostname
--pod-infra-container-image=jicki/pause-amd64:3.1 \ #pod的基础镜像，即gcr的gcr.io/google_containers/pause-amd64:3.1镜像
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
--config=/etc/kubernetes/kubelet.config.json \
--cert-dir=/etc/kubernetes/ssl \
--logtostderr=true \
--v=2
[Install]
WantedBy=multi-user.target创建 kubelet config 配置文件
&lt;/code>&lt;/pre>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>vim /etc/kubernetes/kubelet.config.json
{
&amp;quot;kind&amp;quot;: &amp;quot;KubeletConfiguration&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;kubelet.config.k8s.io/v1beta1&amp;quot;,
&amp;quot;authentication&amp;quot;: {
&amp;quot;x509&amp;quot;: {
&amp;quot;clientCAFile&amp;quot;: &amp;quot;/etc/kubernetes/ssl/ca.pem&amp;quot;
},
&amp;quot;webhook&amp;quot;: {
&amp;quot;enabled&amp;quot;: true,
&amp;quot;cacheTTL&amp;quot;: &amp;quot;2m0s&amp;quot;
},
&amp;quot;anonymous&amp;quot;: {
&amp;quot;enabled&amp;quot;: false
}
},
&amp;quot;authorization&amp;quot;: {
&amp;quot;mode&amp;quot;: &amp;quot;Webhook&amp;quot;,
&amp;quot;webhook&amp;quot;: {
&amp;quot;cacheAuthorizedTTL&amp;quot;: &amp;quot;5m0s&amp;quot;,
&amp;quot;cacheUnauthorizedTTL&amp;quot;: &amp;quot;30s&amp;quot;
}
},
&amp;quot;address&amp;quot;: &amp;quot;172.16.6.66&amp;quot;, #本地node节点的IP
&amp;quot;port&amp;quot;: 10250,
&amp;quot;readOnlyPort&amp;quot;: 0,
&amp;quot;cgroupDriver&amp;quot;: &amp;quot;cgroupfs&amp;quot;,
&amp;quot;hairpinMode&amp;quot;: &amp;quot;promiscuous-bridge&amp;quot;,
&amp;quot;serializeImagePulls&amp;quot;: false,
&amp;quot;RotateCertificates&amp;quot;: true,
&amp;quot;featureGates&amp;quot;: {
&amp;quot;RotateKubeletClientCertificate&amp;quot;: true,
&amp;quot;RotateKubeletServerCertificate&amp;quot;: true
},
&amp;quot;MaxPods&amp;quot;: &amp;quot;512&amp;quot;,
&amp;quot;failSwapOn&amp;quot;: false,
&amp;quot;containerLogMaxSize&amp;quot;: &amp;quot;10Mi&amp;quot;,
&amp;quot;containerLogMaxFiles&amp;quot;: 5,
&amp;quot;clusterDomain&amp;quot;: &amp;quot;cluster.local.&amp;quot;,
&amp;quot;clusterDNS&amp;quot;: [&amp;quot;10.254.0.2&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>以上配置中:&lt;/p>
&lt;p>cluster.local. 为 kubernetes 集群的 domain&lt;/p>
&lt;p>10.254.0.2 预分配的 dns 地址&lt;/p>
&lt;p>&amp;ldquo;clusterDNS&amp;rdquo;: [&amp;ldquo;10.254.0.2&amp;rdquo;] 可配置多个 dns 地址，逗号可开, 可配置宿主机 dns&lt;/p>
&lt;p>&lt;strong>启动 Kubelet 服务&lt;/strong>&lt;/p>
&lt;pre>&lt;code>systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
&lt;/code>&lt;/pre>
&lt;p>验证 nodes&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/crt3fo/1616120007572-6678d863-7c6f-4000-8c7f-6e7c17ff42ca.png" alt="">&lt;/p>
&lt;p>注意:这里的 ROLES 是节点标签&lt;/p>
&lt;p>关于 kubectl get node 中的 ROLES 的标签&lt;/p>
&lt;p>单 Master 打标签 kubectl label node es-60 node-role.kubernetes.io/master=&amp;quot;&amp;quot;，当标签为 NoSchedule，表示不进行资源调度&lt;/p>
&lt;p>更新标签命令为 kubectl label nodes es-60 node-role.kubernetes.io/master=:NoSchedule &amp;ndash;overwrite&lt;/p>
&lt;p>单 Node 打标签 kubectl label node es-61 node-role.kubernetes.io/node=&amp;quot;&amp;quot;&lt;/p>
&lt;p>关于删除 label 可使用 - 号相连 如: kubectl label nodes es-61 node-role.kubernetes.io/node-&lt;/p>
&lt;p>查看自动生成的证书配置文件&lt;/p>
&lt;pre>&lt;code>ls -lt /etc/kubernetes/ssl/kubelet-*
&lt;/code>&lt;/pre></description></item><item><title>Docs: 2.1.容器</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.1.%E5%AE%B9%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.1.%E5%AE%B9%E5%99%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/OS-level_virtualization">Wiki,OS-level virtualization&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Container(容器)&lt;/strong> 是一种基础工具；泛指任何可以用于容纳其它物品的工具，可以部分或完全封闭，被用于容纳、储存、运输物品。物体可以被放置在容器中，而容器则可以保护内容物。人类使用容器的历史至少有十万年，甚至可能有数百万年的历史。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1616122918518-107de7cd-51b4-4427-8281-8e81f7c2383d.png" alt="">
自 1979 年，Unix 版本 7 引用 Chroot Jail 以及 Chroot 系统调用开始，直到 2013 年开源出的 Docker，2014 年开源出来的 Kubernetes，直到现在的云原生生态的火热。容器技术已经逐步成为主流的基础技术之一。&lt;/p>
&lt;h2 id="一什么是容器">一、什么是容器&lt;/h2>
&lt;p>IT 里的容器技术是英文单词 Linux Container 的直译。Container 这个单词有集装箱、容器的含义（主要偏集装箱意思）。不过，在中文环境下，咱们要交流要传授，如果翻译成“集装箱技术” 就有点拗口，所以结合中国人的吐字习惯和文化背景，更喜欢用容器这个词。不过，如果要形象的理解 Linux Container 技术的话，还是得念成集装箱会比较好。我们知道，海边码头里的集装箱是运载货物用的，它是一种按规格标准化的钢制箱子。集装箱的特色，在于其格式划一，并可以层层重叠，所以可以大量放置在特别设计的远洋轮船中（早期航运是没有集装箱概念的，那时候货物杂乱无章的放，很影响出货和运输效率）。有了集装箱，那么这就更加快捷方便的为生产商提供廉价的运输服务。&lt;/p>
&lt;p>因此，IT 世界里借鉴了这一理念。早期，大家都认为硬件抽象层基于 hypervisor 的虚拟化方式可以最大程度上提供虚拟化管理的灵活性。各种不同操作系统的虚拟机都能通过 hypervisor（KVM、XEN 等）来衍生、运行、销毁。然而，随着时间推移，用户发现 hypervisor 这种方式麻烦越来越多。为什么？因为对于 hypervisor 环境来说，每个虚拟机都需要运行一个完整的操作系统以及其中安装好的大量应用程序。但实际生产开发环境里，我们更关注的是自己部署的应用程序，如果每次部署发布我都得搞一个完整操作系统和附带的依赖环境，那么这让任务和性能变得很重和很低下。&lt;/p>
&lt;p>基于上述情况，人们就在想，有没有其他什么方式能让人更加的关注应用程序本身，底层多余的操作系统和环境我可以共享和复用？换句话来说，那就是我部署一个服务运行好后，我再想移植到另外一个地方，我可以不用再安装一套操作系统和依赖环境。这就像集装箱运载一样，我把货物一辆兰博基尼跑车（好比开发好的应用 APP），打包放到一容器集装箱里，它通过货轮可以轻而易举的从上海码头（CentOS7.2 环境）运送到纽约码头（Ubuntu14.04 环境）。而且运输期间，我的兰博基尼（APP）没有受到任何的损坏（文件没有丢失），在另外一个码头卸货后，依然可以完美风骚的赛跑（启动正常）。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1616122918592-560b0741-407f-4354-bc87-d0ef48160754.png" alt="">&lt;/p>
&lt;h2 id="二容器技术的实现方式lxcrunckata-等">二、容器技术的实现方式，lxc、runc、kata 等&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1616122918517-21a8c653-b45e-44a9-b54e-86ca53db6fd7.png" alt="">&lt;/p>
&lt;p>Linux Container(LXC)容器技术的诞生（2008 年）就解决了 IT 世界里“集装箱运输”的问题。Linux Container（简称 LXC）它是一种 内核轻量级的操作系统层 虚拟化技术，也称为容器的运行时(runtime 运行环境)。Linux Container 主要由 Namespace 和 Cgroup 两大机制来保证实现。那么 Namespace 和 Cgroup 是什么呢？刚才我们上面提到了集装箱，集装箱的作用当然是可以对货物进行打包隔离了，不让 A 公司的货跟 B 公司的货混在一起，不然卸货就分不清楚了。那么 Namespace 也是一样的作用，做隔离。光有隔离还没用，我们还需要对货物进行资源的管理。同样的，航运码头也有这样的管理机制：货物用什么样规格大小的集装箱，货物用多少个集装箱，货物哪些优先运走，遇到极端天气怎么暂停运输服务怎么改航道等等&amp;hellip; 通用的，与此对应的 Cgroup 就负责资源管理控制作用，比如进程组使用 CPU/MEM 的限制，进程组的优先级控制，进程组的挂起和恢复等等。&lt;/p>
&lt;p>经过多年的发展，陆续推出了 runc、kata 等容器底层技术&lt;/p>
&lt;p>runc 是 lxc 的替代品，官方说明：&lt;a href="https://www.docker.com/blog/runc/">https://www.docker.com/blog/runc/&lt;/a>&lt;/p>
&lt;p>kata 是自带内核的虚拟机型的容器 runtime，官方网址：&lt;a href="https://katacontainers.io/">https://katacontainers.io/&lt;/a>&lt;/p>
&lt;h2 id="三容器技术的特点">三、容器技术的特点&lt;/h2>
&lt;p>容器的特点其实我们拿跟它跟硬件抽象层虚拟化 hypervisor 技术对比就清楚了，我们之前也提到过，传统的虚拟化（虚拟机）技术，创建环境和部署应用都很麻烦，而且应用的移植性也很繁琐，比如你要把 vmware 里的虚拟机迁移到 KVM 里就很繁琐（需要做镜像格式的转换）。那么有了容器技术就简单了，总结下容器技术主要有三个特点：&lt;/p>
&lt;ul>
&lt;li>极其轻量：只打包了必要的 Bin/Lib；&lt;/li>
&lt;li>秒级部署：根据镜像的不同，容器的部署大概在毫秒与秒之间（比虚拟机强很多）；&lt;/li>
&lt;li>易于移植：一次构建，随处部署；&lt;/li>
&lt;li>弹性伸缩：Kubernetes、Swam、Mesos 这类开源、方便、好使的容器管理平台有着非常强大的弹性管理能力。&lt;/li>
&lt;/ul>
&lt;h2 id="四容器的标准化-open-container-initiativeoci">四、容器的标准化 Open Container Initiative(OCI)&lt;/h2>
&lt;p>当前，docker 几乎是容器的代名词，很多人以为 docker 就是容器。其实，这是错误的认识(docker 只是可以实现容器的引擎, docker 调用 containerd，containerd 再调用 runc 来启动一个容器)。除了 docker 还有 podman 等等。所以，容器世界里并不是只有 docker 一家。既然不是一家就很容易出现分歧。任何技术出现都需要一个标准来规范它，不然各搞各的很容易导致技术实现的碎片化，出现大量的冲突和冗余。因此，在 2015 年，由 Google，Docker、CoreOS、IBM、微软、红帽等厂商联合发起的 &lt;a href="https://www.opencontainers.org/">OCI(Open Container Initiative)&lt;/a> 项目成立了，并于 2016 年 4 月推出了第一个开放容器标准。标准主要包括 runtime(运行时)标准 和 image(镜像)标准。标准的推出，有助于替成长中市场带来稳定性，让企业能放心采用容器技术，用户在打包、部署应用程序后，可以自由选择不同的容器 Runtime；同时，镜像打包、建立、认证、部署、命名也都能按照统一的规范来做。&lt;/p>
&lt;p>两种标准主要包含以下内容：&lt;/p>
&lt;ul>
&lt;li>容器运行时标准 （runtime spec）
&lt;ul>
&lt;li>creating：使用 create 命令创建容器，这个过程称为创建中 b). created：容器创建出来，但是还没有运行，表示镜像和配置没有错误，容器能够运行在当前平台 c).&lt;/li>
&lt;li>running：容器的运行状态，里面的进程处于 up 状态，正在执行用户设定的任务 d)&lt;/li>
&lt;li>stopped：容器运行完成，或者运行出错，或者 stop 命令之后，容器处于暂停状态。这个状态，容器还有很多信息保存在平台中，并没有完全被删除&lt;/li>
&lt;li>&amp;hellip;.等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>容器镜像标准（image spec）
&lt;ul>
&lt;li>文件系统：以 layer 保存的文件系统，每个 layer 保存了和上层之间变化的部分，layer 应该保存哪些文件，怎么表示增加、修改和删除的文件等;&lt;/li>
&lt;li>config 文件：保存了文件系统的层级信息（每个层级的 hash 值，以及历史信息），以及容器运行时需要的一些信息（比如环境变量、工作目录、命令参数、mount 列表），指定了镜像在某个特定平台和系统的配置。比较接近我们使用 docker inspect&lt;/li>
&lt;li>&amp;hellip;.等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="五容器的主要应用场景">五、容器的主要应用场景&lt;/h2>
&lt;p>容器技术的诞生其实主要解决了 PAAS 的层的技术实现。像 OpenStack、Cloudstack 这样的技术是解决 IAAS 层的问题。IAAS 层和 PAAS 层大家估计也听得很多了，关于他们的区别和特性我这里不在描述。那么容器技术主要应用在哪些场景呢？目前主流的有以下几种：&lt;/p>
&lt;ul>
&lt;li>1.容器化传统应用 容器不仅能提高现有应用的安全性和可移植性，还能节约成本。
&lt;ul>
&lt;li>每个企业的环境中都有一套较旧的应用来服务于客户或自动执行业务流程。即使是大规模的单体应用，通过容器隔离的增强安全性、以及可移植性特点，也能从 容器 中获益，从而降低成本。一旦容器化之后，这些应用可以扩展额外的服务或者转变到微服务架构之上。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>持续集成和持续部署 (CI/CD) 通过 Docker 加速应用管道自动化和应用部署，交付速度提高至少 13 倍。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>现代化开发流程快速、持续且具备自动执行能力，最终目标是开发出更加可靠的软件。通过持续集成 (CI) 和持续部署 (CD)，每次开发人员签入代码并顺利测试之后，IT 团队都能够集成新代码。作为开发运维方法的基础，CI/CD 创造了一种实时反馈回路机制，持续地传输小型迭代更改，从而加速更改，提高质量。CI 环境通常是完全自动化的，通过 git 推送命令触发测试，测试成功时自动构建新镜像，然后推送到 Docker 镜像库。通过后续的自动化和脚本，可以将新镜像的容器部署到预演环境，从而进行进一步测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ol start="3">
&lt;li>微服务 加速应用架构现代化进程。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>应用架构正在从采用瀑布模型开发法的单体代码库转变为独立开发和部署的松耦合服务。成千上万个这样的服务相互连接就形成了应用。Docker 允许开发人员选择最适合于每种服务的工具或技术栈，隔离服务以消除任何潜在的冲突，从而避免“地狱式的矩阵依赖”。这些容器可以独立于应用的其他服务组件，轻松地共享、部署、更新和瞬间扩展。Docker 的端到端安全功能让团队能够构建和运行最低权限的微服务模型，服务所需的资源（其他应用、涉密信息、计算资源等）会适时被创建并被访问。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ol start="4">
&lt;li>IT 基础设施优化 充分利用基础设施，节省资金。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Docker 和容器有助于优化 IT 基础设施的利用率和成本。优化不仅仅是指削减成本，还能确保在适当的时间有效地使用适当的资源。容器是一种轻量级的打包和隔离应用工作负载的方法，所以 Docker 允许在同一物理或虚拟服务器上毫不冲突地运行多项工作负载。企业可以整合数据中心，将并购而来的 IT 资源进行整合，从而获得向云端的可迁移性，同时减少操作系统和服务器的维护工作。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="container-的基本核心概念">Container 的基本核心概念&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1641604740718-d450d041-4e36-462a-844d-d330f0a8715e.png" alt="image.png">&lt;/p>
&lt;h2 id="image镜像">Image(镜像)&lt;/h2>
&lt;p>镜像就是一个只读的模板。&lt;/p>
&lt;p>例如：一个镜像可以包含一个完整的 CentOS 操作系统环境，里面仅安装了 Apache 或用户需要的其他应用程序。&lt;/p>
&lt;p>镜像可以用来创建 Container。&lt;/p>
&lt;h3 id="reference引用">Reference(引用)&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/engine/reference/commandline/images/">https://docs.docker.com/engine/reference/commandline/images/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在互联网上，我们通过 &lt;strong>Reference(引用)&lt;/strong> 表示唯一一个 Image，就像 URL 之于 HTTP 的 Resource 一样，&lt;strong>Reference 就是 Image 的 URL&lt;/strong>。&lt;/p>
&lt;h4 id="syntax语法">Syntax(语法)&lt;/h4>
&lt;p>&lt;strong>Scheme://Registry/[Namespace/]Repository:{Tag|Digest}&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>**Scheme:// **# 访问 Registry 时所使用的协议，比如 HTTP、HTTPS&lt;/li>
&lt;li>&lt;strong>Registry(注册中心)&lt;/strong> # 提供 Image 管理服务的提供商，通常是一个域名
&lt;ul>
&lt;li>现阶段常见的 Registry 有：
&lt;ul>
&lt;li>docker.io&lt;/li>
&lt;li>k8s.gcr.io&lt;/li>
&lt;li>quay.io&lt;/li>
&lt;li>ghcr.io&lt;/li>
&lt;li>&amp;hellip;&amp;hellip; 等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Namespace(名称空间)&lt;/strong> # 在一个 Registry 中可能会有多个同名的 Repository，所以需要通过 Namespace 将这些 Repository 隔开。
&lt;ul>
&lt;li>docker.io 将用户注册的账户名称作为 Namespace，若 Namespace 被省略，则 Image 就是这个 Registry 官方的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Repository(仓库)&lt;/strong> # 顾名思义，存放镜像的仓库&lt;/li>
&lt;li>&lt;strong>Tag(标签)&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>Digest(摘要)&lt;/strong> # Image 内容的 sha256 计算结果。通常是互联网唯一的&lt;/li>
&lt;/ul>
&lt;p>假如我在 docker.io 注册了一个账号 lchdzh 用来存放容器镜像，有一个 k8s-debug 的镜像，版本号是 v1，我想把镜像放在 dd_k8s 仓库中。&lt;/p>
&lt;ul>
&lt;li>那么正常的 Image Reference 是：&lt;code>docker.io/lchdzh/dd_k8s:k8s-debug-v1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>但是，后来人们一般情况 Repository 都存放同一个软件的 Image，把 Tag 仅仅当做了镜像的版本&lt;/p>
&lt;ul>
&lt;li>那么上面例子的 Image Reference 就变成了：&lt;code>docker.io/lchdzh/k8s-debug:v1&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="registry注册中心">Registry(注册中心)&lt;/h3>
&lt;p>Registry 可以理解为一个网站，通过 https 协议与 docker daemon 交互;也可以自己搭建私有单位 registry，提供多个功能&lt;/p>
&lt;ul>
&lt;li>用于存储 image 的 Repository 功能，一个 Registry 上有多个 Repository&lt;/li>
&lt;li>用户来获取 image 时的认证功能&lt;/li>
&lt;li>当前 registry 所有 image 的索引 功能&lt;/li>
&lt;/ul>
&lt;p>Registry 上有多个 Repository，每个 Repository 中又包含了多个 TAG(标签)。一个 registry 中分两种：顶层仓库与用户仓库，顶层仓库里的 Repository 是这个 Registry 官方所创建的，用户仓库里的 Repository 是在该 Registry 创建的用户所创建的。image 名字中有 namespace 的就是用户仓库，没有就是顶层仓库&lt;/p>
&lt;h3 id="repository仓库">Repository(仓库)&lt;/h3>
&lt;p>想要定位一个 Registry 下的一个 Repository，至少需要两部分&lt;/p>
&lt;ul>
&lt;li>Namespace(名称空间) # 有的也称为 ProjectID。
&lt;ul>
&lt;li>Docker 将用户注册的账户名称作为 Namespace，若 Namespace 被省略，则就是这个 Registry 官方的。所以也可以这么理解。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Repository(仓库) # 仓库名称&lt;/li>
&lt;/ul>
&lt;p>很多时候都将 Namespace 和 Repository 合起来，统一称为 Repository&lt;/p>
&lt;p>仓库分为公开仓库(Public)和私有仓库(Private)两种形式。当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下载在另外一台机器上使用这个镜像时候，只需需要从仓库上 pull 下来就可以了。&lt;/p>
&lt;p>注意：Docker 仓库的概念跟 Git 类似，Registry 可以理解为 GitHub 这样的托管服务。&lt;/p>
&lt;h3 id="tag标签">Tag(标签)&lt;/h3>
&lt;p>Repository 可以存放不同的 Image(比如 nginx,redis,centos 等)，通过 Tag 来区分这些 Image。说白了，Tag 就是 Image 的名称。&lt;/p>
&lt;h2 id="container容器">Container(容器)&lt;/h2>
&lt;p>容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的，保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括 root 用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。注意：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。&lt;/p>
&lt;p>Image 与 Container 的关系，就好比是程序与进程之间的关系。Image 类似程序是静态的。Container 类似进程是动态的，是有生命周期的。&lt;/p>
&lt;h2 id="联合文件系统">联合文件系统&lt;/h2>
&lt;ul>
&lt;li>当我们在下载镜像的时候，会发现每一层都有一个 id，这是 &lt;strong>Layer(层)&lt;/strong> 的概念，是 **UnionFS(联合文件系统) **中的重要概念&lt;/li>
&lt;li>联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下&lt;/li>
&lt;li>联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。&lt;/li>
&lt;li>不同容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率。&lt;/li>
&lt;/ul>
&lt;h1 id="rootless-containers">Rootless Containers&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/rootless-containers">GitHub 项目,rootless-containers&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Rootless Containers(无根容器)&lt;/strong> 是指非特权用户能够创建、运行和以各种方式管理容器。这个术语还包括围绕容器的各种工具，这些工具也可以作为非特权用户运行。&lt;/p>
&lt;p>运行 Rootless Containers 通常需要弃用 CGroupV2 来限制 CPU、内存、I/O、PID 这些资源的消耗。&lt;/p></description></item><item><title>Docs: 2.2.实现容器的工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;h1 id="oci-runtime-规范的实现">OCI Runtime 规范的实现&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/NPxLLhRkpNdTgVcKQSLcFA">公众号-k8s 技术圈，Containerd 深度剖析-runtime 篇&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>当人们想到容器运行时，可能会想到一连串的相关概念；runc、runv、lxc、lmctfy、Docker（containerd）、rkt、cri-o。每一个都是基于不同的场景而实现的，均实现了不同的功能。如 containerd 和 cri-o，实际均可使用 runc 来运行容器，但其实现了如镜像管理、容器 API 等功能，可以将这些看作是比 runc 具备的更高级的功能。
可以发现，容器运行时是相当复杂的。每个运行时都涵盖了从低级到高级的不同部分，如下图所示。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ctvy4o/1653965809357-01c7d7f1-81d0-49f1-beaa-15bd63e7acd6.png" alt="">
根据功能范围划分，将其分为 &lt;strong>Low level Container Runtime(低级容器运行时)&lt;/strong> 和 &lt;strong>High level Container Runtime(高级容器运行时)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>低级容器运行时 # 只关注容器的本身运行&lt;/li>
&lt;li>高级容器运行时 # 支持更多高级功能的运行时，如镜像管理及一些 gRPC/Web APIs，通常被称为&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，低级运行时和高级运行时有本质区别，各自解决的问题也不同。&lt;/p>
&lt;h2 id="低级运行时">低级运行时&lt;/h2>
&lt;p>低级运行时的功能有限，通常执行运行容器的低级任务。大多数开发者日常工作中不会使用到。其一般指按照 OCI 规范、能够接收可运行 roofs 文件系统和配置文件并运行隔离进程的实现。这种运行时只负责将进程运行在相对隔离的资源空间里，不提供存储实现和网络实现。但是其他实现可以在系统中预设好相关资源，低级容器运行时可通过 config.json 声明加载对应资源。低级运行时的特点是底层、轻量，限制也很一目了然：&lt;/p>
&lt;ul>
&lt;li>只认识 rootfs 和 config.json，没有其他镜像能力&lt;/li>
&lt;li>不提供网络实现&lt;/li>
&lt;li>不提供持久实现&lt;/li>
&lt;li>无法跨平台等&lt;/li>
&lt;/ul>
&lt;h3 id="runc">RunC&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/opencontainers/runc">GitHub 项目，opencontainers/runc&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>runc 是一个 CLI 工具，用于根据 OCI 规范生成和运行容器。&lt;/p>
&lt;h3 id="youki">youki&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containers/youki">GitHub 项目，containers/youki&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>使用 Rust 语言写的，类似于 Runc 的容器运行时，&lt;/p>
&lt;h3 id="sysbox">Sysbox&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/nestybox/sysbox">GitHub 项目，nestybox/sysbox&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Sysbox 是一个新型的 OCI 容器运行时，对标 runc。相比于 runc，Sysbox 在以下两个方面做了增强：&lt;/p>
&lt;ul>
&lt;li>增强容器隔离性：Sysbox 为所有容器开启 user namespace（即容器中的 root 用户映射为主机中的普通用户），在容器中隐藏宿主机的信息，锁定容器的初始挂载，等等。&lt;/li>
&lt;li>容器不仅可以运行普通进程，还可以运行 systemd、Docker、K8s、K3s 等系统级软件，一定程度上可以替换虚拟机。&lt;/li>
&lt;/ul>
&lt;p>最初 Sysbox 只支持 Docker，但最新版本 v0.4.0 已支持直接作为 Kubernetes 的 CRI 运行时。&lt;/p>
&lt;h3 id="kata-container">Kata Container&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kata-containers/kata-containers">GitHub 项目，kata-containers/kata-containers&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kata Containers 是一个开源项目和社区，致力于构建轻量级虚拟机 (vm) 的标准实现，该虚拟机感觉和性能类似于容器，但提供 vm 的工作负载隔离和安全性优势。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ctvy4o/1616122531941-6b13921a-78c5-45a1-9a38-6695b517bca8.png" alt="">&lt;/p>
&lt;h2 id="高级运行时">高级运行时&lt;/h2>
&lt;p>高级运行时负责容器镜像的传输和管理，解压镜像，并传递给低级运行时来运行容器。通常情况下，高级运行时提供一个守护程序和一个 API，远程应用程序可以使用它来运行容器并监控它们，它们位于低层运行时或其他高级运行时之上。&lt;/p>
&lt;p>高层运行时也会提供一些看似很低级的功能。例如，管理网络命名空间，并允许容器加入另一个容器的网络命名空间。
这里有一个类似逻辑分层图，可以帮助理解这些组件是如何结合在一起工作的。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ctvy4o/1653966607306-f97afdfd-66fd-4d4d-ab04-364b1b60f27e.png" alt="">&lt;/p>
&lt;h3 id="docker">Docker&lt;/h3>
&lt;h3 id="containerd">Containerd&lt;/h3></description></item><item><title>Docs: 2.3.Kubernetes 容器编排系统</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</guid><description/></item><item><title>Docs: 2.Authorization(授权)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/2.authorization%E6%8E%88%E6%9D%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/2.authorization%E6%8E%88%E6%9D%83/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">官方文档,参考-API 访问控制-授权&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 Kubernetes 中，在 &lt;strong>Authorization(i.e.授予访问权限，简称：授权)&lt;/strong> 之前必须进行过 &lt;a href="https://www.yuque.com/go/doc/33165791">Authenticating(认证)&lt;/a>&lt;/p>
&lt;h2 id="授权流程">授权流程&lt;/h2>
&lt;h3 id="确定是允许还是拒绝请求">确定是允许还是拒绝请求&lt;/h3>
&lt;p>Kubernetes 使用 API 服务器授权 API 请求。它根据所有策略评估所有请求属性来决定允许或拒绝请求。 一个 API 请求的所有部分必须被某些策略允许才能继续。这意味着默认情况下拒绝权限。&lt;/p>
&lt;p>（尽管 Kubernetes 使用 API 服务器，但是依赖于特定种类对象的特定字段的访问控制和策略由准入控制器处理。）&lt;/p>
&lt;p>配置多个授权模块时，将按顺序检查每个模块。 如果任何授权模块批准或拒绝请求，则立即返回该决定，并且不会与其他授权模块协商。 如果所有模块对请求没有意见，则拒绝该请求。一个拒绝响应返回 HTTP 状态代码 403 。&lt;/p>
&lt;h3 id="审查您的请求属性">审查您的请求属性&lt;/h3>
&lt;p>Kubernetes 仅审查以下 API 请求属性：&lt;/p>
&lt;ul>
&lt;li>user - 身份验证期间提供的 user 字符串。&lt;/li>
&lt;li>group - 经过身份验证的用户所属的组名列表。&lt;/li>
&lt;li>extra - 由身份验证层提供的任意字符串键到字符串值的映射。&lt;/li>
&lt;li>API - 指示请求是否针对 API 资源。&lt;/li>
&lt;li>Request path - 各种非资源端点的路径，如 /api 或 /healthz。&lt;/li>
&lt;li>API request verb - API 动词 get，list，create，update，patch，watch，proxy，redirect，delete 和 deletecollection 用于资源请求。要确定资源 API 端点的请求动词，请参阅确定请求动词。&lt;/li>
&lt;li>HTTP request verb - HTTP 动词 get，post，put 和 delete 用于非资源请求。&lt;/li>
&lt;li>Resource - 正在访问的资源的 ID 或名称（仅限资源请求） - 对于使用 get，update，patch 和 delete 动词的资源请求，您必须提供资源名称。&lt;/li>
&lt;li>Subresource - 正在访问的子资源（仅限资源请求）。&lt;/li>
&lt;li>Namespace - 正在访问的对象的名称空间（仅适用于命名空间资源请求）。&lt;/li>
&lt;li>API group - 正在访问的 API 组（仅限资源请求）。空字符串表示核心 API 组。&lt;/li>
&lt;/ul>
&lt;h3 id="确定请求动词">确定请求动词&lt;/h3>
&lt;p>要确定资源 API 端点的请求动词，需要检查所使用的 HTTP 动词以及请求是否对单个资源或资源集合起作用：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>HTTP 动词&lt;/th>
&lt;th>request 动词&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>POST&lt;/td>
&lt;td>create&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GET, HEAD&lt;/td>
&lt;td>get (单个资源)，list (资源集合)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PUT&lt;/td>
&lt;td>update&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PATCH&lt;/td>
&lt;td>patch&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DELETE&lt;/td>
&lt;td>delete (单个资源)，deletecollection (资源集合)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Kubernetes 有时使用专门的动词检查授权以获得额外的权限。例如：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod 安全策略&lt;/a> 检查 policy API 组中 podsecuritypolicies 资源的 use 动词的授权。&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping">RBAC &lt;/a>检查 rbac.authorization.k8s.io API 组中 roles 和 clusterroles 资源的 bind 动词的授权。&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">认证&lt;/a> layer 检查核心 API 组中 users，groups 和 serviceaccounts 的 impersonate 动词的授权，以及 authentication.k8s.io API 组中的 userextras&lt;/li>
&lt;/ul>
&lt;h1 id="授权的实现方式">授权的实现方式&lt;/h1>
&lt;p>在 Kubernetes 中，可以通过多种方式来实现 Authorization(授权) 功能&lt;/p>
&lt;h2 id="rbac-授权">RBAC 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/&lt;/a>&lt;/li>
&lt;li>RBAC 概念：&lt;a href="https://www.yuque.com/go/doc/33177747">https://www.yuque.com/go/doc/33177747&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>RBAC&lt;/strong> # 基于角色的访问控制（RBAC）是一种基于企业内个人用户的角色来管理对计算机或网络资源的访问的方法。在这种语境中，权限是单个用户执行特定任务的能力，例如查看，创建或修改文件。要了解有关使用 RBAC 模式的更多信息，请参阅 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC 模式&lt;/a>。&lt;/p>
&lt;ul>
&lt;li>当指定的 RBAC（基于角色的访问控制）使用 rbac.authorization.k8s.io API 组来驱动授权决策时，允许管理员通过 Kubernetes API 动态配置权限策略。&lt;/li>
&lt;li>要启用 RBAC，请使用 &amp;ndash;authorization-mode = RBAC 启动 apiserver 。&lt;/li>
&lt;/ul>
&lt;p>详见 ：&lt;a href="https://www.yuque.com/go/doc/43619714">RBAC 授权章节&lt;/a>&lt;/p>
&lt;h2 id="abac-授权">ABAC 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">https://kubernetes.io/docs/reference/access-authn-authz/abac/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>ABAC&lt;/strong> # 基于属性的访问控制（ABAC）定义了一种访问控制范例，通过使用将属性组合在一起的策略，将访问权限授予用户。策略可以使用任何类型的属性（用户属性，资源属性，对象，环境属性等）。要了解有关使用 ABAC 模式的更多信息，请参阅 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">ABAC 模式&lt;/a>。&lt;/p>
&lt;h2 id="node-授权">Node 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">https://kubernetes.io/docs/reference/access-authn-authz/node/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Node&lt;/strong> # 一个专用授权程序，根据计划运行的 pod 为 kubelet 授予权限。了解有关使用节点授权模式的更多信息，请参阅&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">节点授权.&lt;/a>&lt;/p>
&lt;h2 id="webhook-授权">Webhook 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/">https://kubernetes.io/docs/reference/access-authn-authz/webhook/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**Webhook **# WebHook 是一个 HTTP 回调：发生某些事情时调用的 HTTP POST；通过 HTTP POST 进行简单的事件通知。实现 WebHook 的 Web 应用程序会在发生某些事情时将消息发布到 URL。要了解有关使用 Webhook 模式的更多信息，请参阅 Webhook 模式。&lt;/p></description></item><item><title>Docs: 2.CGroup</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Cgroups">Wiki,Cgroups&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">Manual(手册),cgroup(7)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html">Linux Kernel 官方文档,Linux 内核用户和管理员指南-Control Group V1&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux Kernel 官方文档,Linux 内核用户和管理员指南-Control Group V2&lt;/a>&lt;/li>
&lt;li>红帽文档：
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/system_design_guide/using-control-groups-through-a-virtual-file-system_setting-limits-for-applications">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/system_design_guide/using-control-groups-through-a-virtual-file-system_setting-limits-for-applications&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/index">https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/index&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://segmentfault.com/a/1190000009732550">思否，Linux Namespace 和 Cgroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://fuckcloudnative.io/posts/understanding-cgroups-part-1-basics/">https://fuckcloudnative.io/posts/understanding-cgroups-part-1-basics/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Control Groups(控制组，简称 CGroups)&lt;/strong> 是一个 Linux 内核特性，用于限制、隔离一组进程集合的资源使用，资源包括 CPU、内存、磁盘 IO、网络 等。CGroups 由 Google 的两位工程师开发，自 2008 年 1 月发布的 Linux 2.6.24 版本的内核中提供此能力。到目前为止，CGroups 分 v1 和 v2 两个版本，v1 实现较早，功能比较多，但是由于它里面的功能都是零零散散的实现的，所以规划的不是很好，导致了一些使用和维护上的不便，v2 的出现就是为了解决 v1 中这方面的问题，在最新的 4.5 内核中，cgroup v2 声称已经可以用于生产环境了，但它所支持的功能还很有限，随着 v2 一起引入内核的还有 cgroup namespace。v1 和 v2 可以混合使用，但是这样会更复杂，所以一般没人会这样用。&lt;/p>
&lt;p>在 Linux 里，一直以来就有对进程进行分组的概念和需求，比如 session group， progress group 等，后来随着人们对这方面的需求越来越多，比如需要追踪一组进程的内存和 IO 使用情况等，于是出现了 cgroup，用来统一将进程进行分组，并在分组的基础上对进程进行监控和资源控制管理等。&lt;/p>
&lt;p>Cgroup 是 Linux kernel 的一项功能：它是在一个系统中运行的层级制进程组，你可对其进行资源分配（如 CPU 时间、系统内存、网络带宽或者这些资源的组合）。通过使用 cgroup，系统管理员在分配、排序、拒绝、管理和监控系统资源等方面，可以进行精细化控制。硬件资源可以在应用程序和用户间智能分配，从而增加整体效率。&lt;/p>
&lt;p>cgroup 和 namespace 类似，也是将进程进行分组，但它的目的和 namespace 不一样，namespace 是为了隔离进程组之间的资源，而 cgroup 是为了对一组进程进行统一的资源监控和限制。CGroup 还能对进程进行优先级设置、审计、以及将进程挂起和恢复等操作&lt;/p>
&lt;h2 id="术语">术语&lt;/h2>
&lt;p>cgroup 在不同的上下文中代表不同的意思，可以指整个 Linux 的 cgroup 技术，也可以指一个具体进程组。&lt;/p>
&lt;p>cgroup 是 Linux 下的一种将进程按组进行管理的机制，在用户层看来，cgroup 技术就是把系统中的所有进程组织成一颗一颗独立的树，每棵树都包含系统的所有进程，树的每个节点是一个进程组，而每颗树又和一个或者多个 subsystem 关联，树的作用是将进程分组，而 subsystem 的作用就是对这些组进行操作。&lt;/p>
&lt;p>cgroup 主要包括下面两部分：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>subsystem(子系统)&lt;/strong> # 一个 subsystem 就是一个内核模块，他被关联到一颗 cgroup 树之后，就会 在树的每个节点（进程组）上做具体的操作。subsystem 经常被称作 resource controller，因为它主要被用来调度或者限制每个进程组的资源，但是这个说法不完全准确，因为有时我们将进程分组只是为了做一些监控，观察一下他们的状态，比如 perf_event subsystem。到目前为止，Linux 支持 12 种 subsystem，比如限制 CPU 的使用时间，限制使用的内存，统计 CPU 的使用情况，冻结和恢复一组进程等，后续会对它们一一进行介绍。&lt;/li>
&lt;li>&lt;strong>hierarchy(层次结构)&lt;/strong> # 一个 hierarchy 可以理解为一棵 cgroup 树，树的每个节点就是一个进程组，每棵树都会与零到多个 subsystem 关联。在一颗树里面，会包含 Linux 系统中的所有进程，但每个进程只能属于一个节点（进程组）。系统中可以有很多颗 cgroup 树，每棵树都和不同的 subsystem 关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组，只是这些进程组和不同的 subsystem 关联。目前 Linux 支持 12 种 subsystem，如果不考虑不与任何 subsystem 关联的情况（systemd 就属于这种情况），Linux 里面最多可以建 12 颗 cgroup 树，每棵树关联一个 subsystem，当然也可以只建一棵树，然后让这棵树关联所有的 subsystem。当一颗 cgroup 树不和任何 subsystem 关联的时候，意味着这棵树只是将进程进行分组，至于要在分组的基础上做些什么，将由应用程序自己决定，systemd 就是一个这样的例子。&lt;/li>
&lt;/ul>
&lt;h2 id="cgroup-子系统类型">CGroup 子系统类型&lt;/h2>
&lt;p>可以通过 /proc/cgroups 文件查看当前系统支持哪些 subsystem：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/cgroups&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#subsys_name hierarchy num_cgroups enabled&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpuset 6 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu 8 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpuacct 8 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>blkio 4 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>memory 12 236 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>devices 11 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>freezer 9 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>net_cls 10 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>perf_event 5 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>net_prio 10 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hugetlb 2 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pids 3 103 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rdma 7 1 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>整理一下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>subsys_name&lt;/th>
&lt;th>hierarchy&lt;/th>
&lt;th>num_cgroups&lt;/th>
&lt;th>enabled&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cpuset&lt;/td>
&lt;td>6&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu&lt;/td>
&lt;td>8&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpuacct&lt;/td>
&lt;td>8&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>blkio&lt;/td>
&lt;td>4&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory&lt;/td>
&lt;td>12&lt;/td>
&lt;td>236&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>devices&lt;/td>
&lt;td>11&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>freezer&lt;/td>
&lt;td>9&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>net_cls&lt;/td>
&lt;td>10&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>perf_event&lt;/td>
&lt;td>5&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>net_prio&lt;/td>
&lt;td>10&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hugetlb&lt;/td>
&lt;td>2&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pids&lt;/td>
&lt;td>3&lt;/td>
&lt;td>103&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>rdma&lt;/td>
&lt;td>7&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>从左到右，字段的含义分别是：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>subsys_name&lt;/strong> # subsystem 的名字
&lt;ul>
&lt;li>blkio # 块设备 IO&lt;/li>
&lt;li>cpu # 基于 CFS 对 CPU 时间配额进行限制的子系统，CFS 概念详见：CPU 管理 章节中的 CFS 调度器。该子系统是 cgroup 对进程使用 CPU 资源进行限制的主要手段&lt;/li>
&lt;li>cpuacct # CPU 资源使用报告&lt;/li>
&lt;li>cpuset # 多处理器平台上的 CPU 集合&lt;/li>
&lt;li>devices # 设备访问&lt;/li>
&lt;li>freezer # 挂载器或恢复任务&lt;/li>
&lt;li>hungetlb #&lt;/li>
&lt;li>memory # 内存用量及报告&lt;/li>
&lt;li>net_cls # cgroup 中的任务创建的数据包的类别标识符&lt;/li>
&lt;li>net_prio #&lt;/li>
&lt;li>perf_event # 对 cgroup 中的任务进行统一性能测试&lt;/li>
&lt;li>pids #&lt;/li>
&lt;li>rdma #&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>hierarchy&lt;/strong> # subsystem 所关联到的 cgroup 树的 ID，如果多个 subsystem 关联到同一颗 cgroup 树，那么他们的这个字段将一样，比如这里的 cpu 和 cpuacct 就一样，表示他们绑定到了同一颗树。如果出现下面的情况，这个字段将为 0：
&lt;ul>
&lt;li>当前 subsystem 没有和任何 cgroup 树绑定&lt;/li>
&lt;li>当前 subsystem 已经和 cgroup v2 的树绑定&lt;/li>
&lt;li>当前 subsystem 没有被内核开启&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>num_cgroups&lt;/strong> # subsystem 所关联的 cgroup 树中进程组的个数，也即树上节点的个数&lt;/li>
&lt;li>&lt;strong>enabled&lt;/strong> # 1 表示开启，0 表示没有被开启(可以通过设置内核的启动参数“cgroup_disable”来控制 subsystem 的开启).&lt;/li>
&lt;/ul>
&lt;h1 id="cgroup-关联文件">CGroup 关联文件&lt;/h1>
&lt;h2 id="sysfscgroup--cgroup-根目录">/sys/fs/cgroup/* # CGroup 根目录。&lt;/h2>
&lt;p>CGroup 的相关操作都是基于内核中的 &lt;strong>CGroup Virtual Filesystem(控制组虚拟文件系统)&lt;/strong>。所以，使用 CGroup 首先需要挂载这个文件系统，通常，现代系统在启动时，都默认会挂载相关的 CGroup 文件系统：&lt;/p>
&lt;ul>
&lt;li>**CGroupV1，**该目录下的每个目录都是 CGroup 子系统的名称。其中包含该子系统中所关联的进程的资源控制信息。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># mount -t cgroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/systemd type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,xattr,name&lt;span style="color:#f92672">=&lt;/span>systemd&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/pids type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,pids&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/blkio type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,blkio&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,cpu,cpuacct&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/memory type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,memory&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...... 略
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>**CGroupV2，**则只会有一个 cgroup2 on /sys/fs/cgroup type cgroup2 (&amp;hellip;&amp;hellip;) 的挂载项&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># mount -t cgroup2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup2 on /sys/fs/cgroup type cgroup2 &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里面的 &lt;strong>/sys/fs/cgroup&lt;/strong> 目录，就称为 **CGroup 的根目录。CGroup 文件系统的 V1 与 V2 的根目录下的内容，各不相同，详见 &lt;strong>[&lt;/strong>《CGroup FS》 **](✏IT 学习笔记/☁️10.云原生/2.1.容器/2.CGroup/CGroup%20FS.md FS.md)&lt;strong>章节&lt;/strong>&lt;/p>
&lt;h2 id="procpidcgroup--进程号为-pid-的进程所属的-cgroup-信息">/proc/PID/cgroup # 进程号为 PID 的进程所属的 cgroup 信息。&lt;/h2>
&lt;p>在** &lt;strong>/proc/PID/cgroup&lt;/strong> **文件中会指定进程所使用的 CGropu 的相对路径。文件中每行都是进程所属的 CGroup 子系统，每行子系统信息由以 &lt;code>:&lt;/code> 分割的三个字段组成&lt;/p>
&lt;ul>
&lt;li>&lt;strong>hierarchy-ID&lt;/strong> # Hierarchy 唯一标识符。与 /proc/cgroups 文件中的 Hierarchy ID 相同。
&lt;ul>
&lt;li>CGroup v2 版本该字段始终为 0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>controller-list&lt;/strong> # 绑定到 Hierarchy ID 的控制器列表。也就是 CGroup 的子系统。
&lt;ul>
&lt;li>CGroup v2 版本该字段为空&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>cgroup-path&lt;/strong> # 进程所属 CGroup 子系统的信息的路径。这是一个相对路径。
&lt;ul>
&lt;li>这里面的 &lt;code>/&lt;/code> 就是指 CGroup 的根节点中对应子系统的目录
&lt;ul>
&lt;li>对于 CGroupV1 来说通常是 /sys/fs/cgroup/SUBSYSTEM。所以，一个完整的 cgroup-path 应该是 &lt;code>/sys/fs/cgroup/SUBSYSTEM/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope/*&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>cgroup 的 v1 和 v2 版本显示的信息不同&lt;/p>
&lt;h3 id="cgroupv1">CGroupV1&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/1185/cgroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>12:memory:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>11:devices:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10:net_cls,net_prio:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>9:freezer:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>8:cpu,cpuacct:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>7:rdma:/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>6:cpuset:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>5:perf_event:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>4:blkio:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3:pids:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2:hugetlb:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>1:name&lt;span style="color:#f92672">=&lt;/span>systemd:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0::/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>比如，1185 进程的 cpu 子系统的 CGroup 信息，就在 &lt;code>/sys/fs/cgroup/cpu/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope/&lt;/code> 目录中：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls /sys/fs/cgroup/cpu/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.clone_children cpuacct.usage cpuacct.usage_percpu_sys cpuacct.usage_user cpu.shares cpu.uclamp.min
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.procs cpuacct.usage_all cpuacct.usage_percpu_user cpu.cfs_period_us cpu.stat notify_on_release
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpuacct.stat cpuacct.usage_percpu cpuacct.usage_sys cpu.cfs_quota_us cpu.uclamp.max tasks
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="cgropuv2">CGropuV2&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/1277/cgroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0::/system.slice/docker-020cfdfbd4cd43981570f4fa7def9a2b600025b2e60e3150e742a5049562f30f.scope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>比如，1277 进程的 CGroup 信息，就在 &lt;code>/sys/fs/cgroup/system.slice/docker-020cfdfbd4cd43981570f4fa7def9a2b600025b2e60e3150e742a5049562f30f.scope/&lt;/code> 目录中：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls /sys/fs/cgroup/system.slice/docker-020cfdfbd4cd43981570f4fa7def9a2b600025b2e60e3150e742a5049562f30f.scope/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.controllers cgroup.procs cpu.max cpuset.mems cpu.weight io.weight memory.low memory.stat rdma.max
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.events cgroup.stat cpu.pressure cpuset.mems.effective cpu.weight.nice memory.current memory.max pids.current
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.freeze cgroup.subtree_control cpuset.cpus cpu.stat io.max memory.events memory.min pids.events
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.max.depth cgroup.threads cpuset.cpus.effective cpu.uclamp.max io.pressure memory.events.local memory.oom.group pids.max
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.max.descendants cgroup.type cpuset.cpus.partition cpu.uclamp.min io.stat memory.high memory.pressure rdma.current
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="其他文件">其他文件&lt;/h2>
&lt;p>&lt;strong>/proc/cgroups&lt;/strong> # 当前系统支持的所有 CGroup 子系统&lt;/p>
&lt;h1 id="systemd-的-slice-单元">systemd 的 slice 单元&lt;/h1>
&lt;p>在 Systemd 作为 1 号进程的系统中，进程的 CGroup 都可以配置为由 Systemd 管理，其中 Slice 类型的单元就是用来控制 CGroup 的。默认会创建 3 个顶级 Slice&lt;/p>
&lt;ul>
&lt;li>&lt;strong>system.slice&lt;/strong> # 所有 Service Unit 的默认。&lt;/li>
&lt;li>&lt;strong>user.lice&lt;/strong> # 所有用户进程的默认。&lt;/li>
&lt;li>&lt;strong>machine.slice&lt;/strong> # 所有虚拟机和容器的默认。&lt;/li>
&lt;/ul>
&lt;p>&lt;code>systemd-cgls&lt;/code> 命令可以查看 CGroup 的层次结构&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># systemd-cgls&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Control group /:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├─931 bpfilter_umh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├─user.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─user-1000.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─user@1000.service …
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ └─init.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─81271 /lib/systemd/systemd --user
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ └─81276 &lt;span style="color:#f92672">(&lt;/span>sd-pam&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─session-431.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─81902 sshd: lichenhao &lt;span style="color:#f92672">[&lt;/span>priv&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─81998 sshd: lichenhao@pts/1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82001 -bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82100 su - root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82101 -bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82697 systemd-cgls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ └─82698 pager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─session-432.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─82013 sshd: lichenhao &lt;span style="color:#f92672">[&lt;/span>priv&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─82097 sshd: lichenhao@notty
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─82098 /usr/lib/openssh/sftp-server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├─init.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─1 /sbin/init nospectre_v2 nopti noibrs noibpb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└─system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├─irqbalance.service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └─524 /usr/sbin/irqbalance --foreground
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├─uniagent.service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └─537 /usr/local/uniagent/bin/uniagent
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├─containerd.service …
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├─ &lt;span style="color:#ae81ff">714&lt;/span> /usr/bin/containerd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├─ &lt;span style="color:#ae81ff">1140&lt;/span> /usr/bin/containerd-shim-runc-v2 -namespace moby -id b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460 -address /run/containerd/containerd.sock
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├─31778 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 4c5ec4bc9717bb9fd2a2ea7b507ac3c0e16da95fa87974152f0fe3b3a653cef9 -address /run/containerd/containerd.sock
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>systemd-cgtop&lt;/code> 命令可以查看 CGroup 的动态信息。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Control Group Tasks %CPU Memory Input/s Output/s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/ &lt;span style="color:#ae81ff">221&lt;/span> 1.0 3.1G - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>user.slice &lt;span style="color:#ae81ff">11&lt;/span> 0.7 1.5G - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice &lt;span style="color:#ae81ff">139&lt;/span> 0.4 1.2G - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/containerd.service &lt;span style="color:#ae81ff">46&lt;/span> 0.2 276.0M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/cloudResetPwdUpdateAgent.service &lt;span style="color:#ae81ff">18&lt;/span> 0.2 102.4M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/docker-4c5ec4…d2a2ea7b507ac3c0e16da95fa87974152f0fe3b3a653cef9.scope &lt;span style="color:#ae81ff">1&lt;/span> 0.1 1.3M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/multipathd.service &lt;span style="color:#ae81ff">7&lt;/span> 0.0 13.8M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>init.scope &lt;span style="color:#ae81ff">1&lt;/span> - 7.6M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/ModemManager.service &lt;span style="color:#ae81ff">3&lt;/span> - 6.8M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/NetworkManager.service &lt;span style="color:#ae81ff">3&lt;/span> - 13.6M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/accounts-daemon.service &lt;span style="color:#ae81ff">3&lt;/span> - 6.5M - -
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="cgroupv2">CGroupV2&lt;/h1>
&lt;h2 id="检查-cgroup-v2-是否已启用">检查 cgroup v2 是否已启用&lt;/h2>
&lt;p>如果 &lt;code>/sys/fs/cgroup/cgroup.controllers&lt;/code> 存在于您的系统上，则您使用的是 v2，否则您使用的是 v1。
已知以下发行版默认使用 cgroup v2：&lt;/p>
&lt;ul>
&lt;li>Fedora（31 起）&lt;/li>
&lt;li>Arch Linux（自 2021 年 4 月起）&lt;/li>
&lt;li>openSUSE Tumbleweed（自 2021 年起）&lt;/li>
&lt;li>Debian GNU/Linux（从 11 开始）&lt;/li>
&lt;li>Ubuntu（自 21.10 起）&lt;/li>
&lt;/ul>
&lt;h2 id="启用-cgroup-v2">启用 cgroup v2&lt;/h2>
&lt;p>为容器启用 cgroup v2 需要内核 4.15 或更高版本。建议使用内核 5.2 或更高版本。
然而，将 cgroup v2 控制器委派给非 root 用户需要最新版本的 systemd。建议使用 systemd 244 或更高版本。
要使用 cgroup v2 引导主机，请将以下字符串添加到 GRUB_CMDLINE_LINUXin 行/etc/default/grub，然后运行 sudo update-grub.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>systemd.unified_cgroup_hierarchy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="启用-cpucpuset-和-io-委派">启用 CPU、CPUSET 和 I/O 委派&lt;/h2>
&lt;p>默认情况下，非 root 用户只能获取 memory 控制器和 pids 要委托的控制器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ cat /sys/fs/cgroup/user.slice/user-&lt;span style="color:#66d9ef">$(&lt;/span>id -u&lt;span style="color:#66d9ef">)&lt;/span>.slice/user@&lt;span style="color:#66d9ef">$(&lt;/span>id -u&lt;span style="color:#66d9ef">)&lt;/span>.service/cgroup.controllers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>memory pids
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>要允许委派其他控制器，例如 cpu、cpuset 和 io，请运行以下命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ sudo mkdir -p /etc/systemd/system/user@.service.d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF | sudo tee /etc/systemd/system/user@.service.d/delegate.conf
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[Service]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">Delegate=cpu cpuset io memory pids
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo systemctl daemon-reload
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>建议使用委派 cpuset 以及 cpu. 委派 cpuset 需要 systemd 244 或更高版本。
更改 systemd 配置后，您需要重新登录或重新启动主机。建议重启主机。&lt;/p></description></item><item><title>Docs: 2.Kubelet 节点代理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">官方文档，参考-组件工具-kubelet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubelet 是在每个节点上运行的主要“节点代理”。它可以使用以下之一向 APIServer 注册节点：用于覆盖主机名的标志；或云提供商的特定逻辑。&lt;/p>
&lt;p>kubelet 根据 PodSpec 起作用。 PodSpec 是一个描述 Pod 的 YAML 或 JSON 对象。 kubelet 接受通过各种机制（主要是通过 apiserver）提供的一组 PodSpec，并确保这些 PodSpec 中描述的容器正在运行且运行状况良好。 Kubelet 不管理不是 Kubernetes 创建的容器。一般情况， PodSpec 都是由在 k8s 对象的 yaml 文件中定义的。&lt;/p>
&lt;p>kubelet 负责维护容器(CNI)的生命周期，同时也负责 Volume（CVI）和 Network（CNI）的管理。kubernetes 集群的宿主机上，启动的每一个 pod 都有由 kubelet 这个组件管理的。&lt;/p>
&lt;p>kubelet 在每个 Node 上都会启动一个 kubelet daemon 进程，默认监听在 &lt;strong>10250&lt;/strong> 端口。该进程用于处理 Master 节点(主要是 apiserver)下发到本节点的任务，管理 Pod 以及 Pod 中的容器。每个 kubelet 进程会在 APIServer 上注册节点自身信息，定期向 Master 节点汇报节点资源的使用情况，并通过 cAdvisor(kubelet 内部功能) 监控容器和节点资源。10248 为 kubelet 健康检查的 healthz 端口&lt;/p>
&lt;p>Note:如果 master 节点不运行 pod 的话，是不用部署 kubelet 的。&lt;/p>
&lt;p>kubelet 使用 PodSpec 来对其所在节点的 Pod 进行管理，PodSpec(Pod Specification)是描述 pod 的 yaml 或者 json 对象。PodSpec 一般是 yaml 或者 json 格式的文本文件。这些 PodSpecs 有 4 个来源&lt;/p>
&lt;ol>
&lt;li>apiserver：使用最多的方式，通过 kubectl 命令向 apiserver 提交 PodSpec 文件，然后 apiserver 再下发给相应节点的 node。还有一个通过 APIServer 监听 etcd 目录，同步 PodSpec&lt;/li>
&lt;li>File：kubelet 定期监控某个路径(默认路径为/etc/kubernetes/manifests)下所有文件，把这些文件当做 PodSpec，这种方式也就是所谓的 staticPod(静态 Pod)。默认情况下每 20 秒监控一下，可以通过 flag 进行配置，配置时可以指定具体的路径以及监控周期&lt;/li>
&lt;li>HTTP endpoint(URL)：使用&amp;ndash;manifest-url 参数，让 kubelet 每 20 秒检查一次 URL 指定的 endpoint(端点)&lt;/li>
&lt;li>HTTP server：kubelet 监听 HTTP 请求，并响应简单的 API 以提交新的 Pod 清单&lt;/li>
&lt;/ol>
&lt;h2 id="static-pod静态-pod">Static Pod：静态 Pod&lt;/h2>
&lt;p>所有以非 API Server 方式创建的 Pod 都叫 Static Pod。&lt;/p>
&lt;p>kubelet 的工作核心，就是一个控制循环。驱动这个控制循环运行的实践，包括四种&lt;/p>
&lt;ol>
&lt;li>Pod 更新事件&lt;/li>
&lt;li>Pod 生命周期变化&lt;/li>
&lt;li>kubelet 本身设置的执行周期&lt;/li>
&lt;li>定时的清理事件&lt;/li>
&lt;/ol>
&lt;p>注意：kubelet 调用下层容器运行时的执行过程，并不会直接调用 Docker 的 API，而是通过一组叫作 CRI（Container Runtime Interface，容器运行时接口）的 gRPC 接口来间接执行的。gRPC 接口规范详见官网：&lt;a href="https://grpc.io/docs/">https://grpc.io/docs/&lt;/a>&lt;/p>
&lt;p>kubelet 是集群的基础设施，其他主要组件如果不以 daemon 形式运行，则依赖 kubelet 以 pod 方式启动，这样，才可以组成集群最基本的形态。&lt;/p>
&lt;h2 id="kubelet-metrics">Kubelet Metrics&lt;/h2>
&lt;p>详见：k8s 主要组件 metrics 获取指南&lt;/p>
&lt;h1 id="kubelet-部署">Kubelet 部署&lt;/h1>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubelet
&lt;/code>&lt;/pre>
&lt;h1 id="kubelet-关联文件与配置">Kubelet 关联文件与配置&lt;/h1>
&lt;p>kubelet 可以通过多个地方读取其自身的配置并更改自己的行为方式，可以通过指定的 yaml 格式的文件读取配置信息，也可以直接指定命令行参数传递到 kubelet 程序中。&lt;/p>
&lt;p>&lt;strong>/var/lib/kubelet/*&lt;/strong> # kubelet 配置文件目录、以及运行时数据目录，包含基础配置文件、证书、通过 kubelet 启动的容器信息等等&lt;/p>
&lt;ul>
&lt;li>./config.yaml # kubelet 基础配置文件。一般在 kubelet 启动时使用 &amp;ndash;cofnig 参数指定该读取该文件的路径进行加载。
&lt;ul>
&lt;li>Note：该文件内容与 kubectl get configmap -n kube-system kubelet-config-X.XX -o yaml 命令所得结果一样
&lt;ul>
&lt;li>如果想要在 kubelet 运行时动态得更改其配置，则可以修改 configmap 中的内容，详见：&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>./kubeadm-flags.env # 该文件将内容作为 kubelet 参数，在 kubelet 启动时加载，常用来在 kubeadm 初始化时使用&lt;/li>
&lt;li>./pods/* # kubelet 启动的 Pod 的数据保存路径，其内目录名为 Pod 的 uid 。
&lt;ul>
&lt;li>./${POD_UID}/volumes/* # 对应 pod 挂载的 volume 保存路径，其内目录为 kubernetes.io~TYPE ，其中 TYPE 为 volume 的类型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>./pki/* # kubelet 与 apiserver 交互时所用到的证书存放目录。
&lt;ul>
&lt;li>./kubelet.crt # 在 kubelet 完成 TLS bootstrapping 后并且没有配置 &amp;ndash;feature-gates=RotateKubeletServerCertificate=true 时生成；这种情况下该文件为一个独立于 apiserver CA 的自签 CA 证书，有效期为 1 年；被用作 kubelet 10250 api 端口。当其他东西需要访问 kubelet 的 api 时，需要使用该证书作为认证。&lt;/li>
&lt;li>./kubelet-client-current.pem # 与 API server 通讯所用到的证书，与 apiserver 交互后生成。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/kubernetes/*&lt;/strong> # Kubernetes 系统组件运行时目录。&lt;/p>
&lt;ul>
&lt;li>./manifests/* # Kubelet 默认从该目录中读取 Pod 的 Manifests 文件，以运行静态类型 Pod。&lt;/li>
&lt;li>kubelet 在 k8s 集群交互时认证文件所在目录，kubelet 需要读取认证配置，用来与 apiserver 进行交互。
&lt;ul>
&lt;li>./bootstrap-kubelet.conf # 用于 TLS 引导程序的 KubeConfig 文件。该 kubeconfig 文件的用户信息为 token。该文件用于 kubelet 所在节点不在集群中时，向集群发起注册请求所用，如果节点已在集群中，则会自动生成 kubelet.conf 文件&lt;/li>
&lt;li>./kubelet.conf # 具有唯一 kubelet 标识的 KubeConfig 文件(与 kubectl 的 config 文件一样，用于 kubelet 与 apiserver 交互时提供认证信息)。该 kubeconfig 文件的用户信息为客户端证书和私钥，一般在 kubelet 启动时由 bootstrap-kubelet.conf 文件生成。
&lt;ul>
&lt;li>当该文件不存在时，会在 kubelet 启动时，由 bootstrap-kubelet.confg 生成&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/sysconfig/kubelet&lt;/strong> # 与 /var/lib/kubelet/kubeadm-flags.env 文件作用一样 ，将内容作为 kubelet 参数，在 kubelet 启动时加载。一般用于让用户指定 kubelet 的运行时参数 。 KUBELET_EXTRA_ARGS 在标志链中排在最后，并且在其他设置冲突时具有最高优先级。&lt;/p>
&lt;ul>
&lt;li>Note：对于 DEB 系统，配置文件位于：/etc/default/kubelet&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.confg&lt;/strong> #与 kubelet 守护进程运行参数&lt;/p>
&lt;h1 id="kubelet-的启动过程--kubelet-与-apiserver-的交互说明">Kubelet 的启动过程 &amp;amp;&amp;amp; Kubelet 与 APIServer 的交互说明&lt;/h1>
&lt;h2 id="kubelet-启动过程">kubelet 启动过程：&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>kubelet 启动流程源码分析&lt;/li>
&lt;li>&lt;a href="https://xiaohanliang.gitbook.io/notes/k8s/zhi-shi-yu-ding-yi/jie-dian-zu-jian-kubelet/kubelet-qi-dong-liu-cheng">https://xiaohanliang.gitbook.io/notes/k8s/zhi-shi-yu-ding-yi/jie-dian-zu-jian-kubelet/kubelet-qi-dong-liu-cheng&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.jianshu.com/p/e07d84cce9f9">https://www.jianshu.com/p/e07d84cce9f9&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ol>
&lt;li>读取配置 kubelet 配置文件。kubelet 启动时首先会根据 &lt;code>--config=PATH&lt;/code> 参数指定路径(默认 /var/lib/kubelet/config.yaml)读取配置文件，并根据其内配置加载 kubelet 相关参数，
&lt;ol>
&lt;li>根据 ca 配置路径加载 ca.crt 文件，并在 /var/lib/kubelet/pki 目录下生成关于 kubelet 的 10250 私有 api 端口所需的 crt 与 key 文件。&lt;/li>
&lt;li>如果该文件不存在或有问题，则启动失败。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>配置与 apiserver 通信的 kubeconfig 文件。根据 &amp;ndash;bootstrap-kubeconfig=PATH 参数加载 /etc/kubernetes/bootstrap-kubelet.conf 文件(如果不存在则根据 &amp;ndash;kubeconfig=PATH 参数加载 /etc/kubernetes/kubelet.conf 文件)，两个文件都不存在则报错
&lt;ol>
&lt;li>如果当前节点不在集群中，则会执行证书申请操作
&lt;ol>
&lt;li>首先 kubelet 向 bootstrap-kubelet.conf 文件内配置的 apiserver(文件中 server 的配置) 发送加入集群的申请申，同时 kubelet 会根据 bootstrap-kubelet.conf 文件生成 kubelet.conf 文件，将该 kubeconfig 文件中的 user 认证方式改为证书认证方式，并指定证书路径为/var/lib/kubelet/pki/kubelet-client-current.pem。&lt;/li>
&lt;li>在集群 master 上执行 kubectl get csr 命令获取当前申请列表，并使用 kubectl certificate approve XXXX 命令通过该节点的申请&lt;/li>
&lt;li>master 节点的 controller-manager 处理完该请求后，本地 kubelet 将生成 kubelet.conf 文件所需证书，并保存在 /var/lib/kubelet/pki/ 目录下，以 /var/lib/kubelet/pki/kubelet-client-current.pem-TIME 命名，并建立软件链接指向该文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>如果当前节点已在集群中，则会根据 bootstrap-kubelet.conf 文件生成 kubelet.conf 文件，且根据 kubelet.conf 文件中的证书信息与 apiserver 进行通信。&lt;/li>
&lt;li>如果当前节点已在集群且存在 kubelet.conf 文件，则使用该 kubeconfig 文件与 apiserver 进行交互后执行后续操作&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>初始化 kubelet 组件内部模块
&lt;ol>
&lt;li>会在 /var/lib/kubelet 目录下添加相关文件，用以驱动 pod 及 kubelet 相关功能。&lt;/li>
&lt;li>检查 cgroup 驱动设置与 CRI 的 cgroup 驱动设置是否一致，如果不一致则启动失败&lt;/li>
&lt;li>等等操作，后续有发现再补充&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>启动 kubelet 内部模块及服务(Note：当 csr 申请发送后，即可执行后续工作，无需等待申请审批通过)&lt;/li>
&lt;/ol>
&lt;h2 id="kubelet-启动后的工作原理">kubelet 启动后的工作原理&lt;/h2>
&lt;p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rf1qwi/1616120074647-12f084c2-e91c-475d-93d4-f4e3fd3e61ef.png" alt="">&lt;/p>
&lt;h3 id="kubelet-创建-pod-流程">kubelet 创建 pod 流程&lt;/h3>
&lt;p>参考：
&lt;a href="https://www.jianshu.com/p/5e0c9d1dbe95">https://www.jianshu.com/p/5e0c9d1dbe95&lt;/a>
&lt;a href="https://www.kubernetes.org.cn/6766.html">https://www.kubernetes.org.cn/6766.html&lt;/a>
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rf1qwi/1616120074625-d25d5fad-d58b-4142-958d-b1aee16d8e71.png" alt="">&lt;/p>
&lt;p>Note：注意 14，,15 步，kubelet 会先将生成配置(volume 挂载、配置主机名等等)，才会去启动 pod，哪怕 pod 启动失败，挂载依然存在。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rf1qwi/1616120074636-5f1f460b-6cf2-4190-ab01-6284178805e6.png" alt="">&lt;/p>
&lt;h1 id="kubelet-所管理三大板块">Kubelet 所管理三大板块&lt;/h1>
&lt;h2 id="kubelet-负责所在节点-containercri-的管理">kubelet 负责所在节点 Container(CRI) 的管理&lt;/h2>
&lt;p>CRI 的起源：
官方介绍：&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&lt;/a>&lt;/p>
&lt;p>Kubernetes 项目之所以要在 kubelet 中引入这样一层单独的抽象，是为了对 Kubernetes 屏蔽下层容器运行时的差异。因为 Kubernets 是一个编排工具，不只可以编排 Docker，还可以编排除 Docker 以外的其余的容器项目。而每种容器项目的实现方式不尽相同，为了解决这个问题，那么可以把 kubelet 对容器的操作，统一抽象成一个借口，这样，kubelet 就只需要跟这个借口打交道，而作为具体的容器项目，它们只需要自己提供一个该接口的实现，然后对 kubelet 暴露出 gRPC 服务即可(docker shim 现在集成在了 kubelet 中，以后会单独拿出来甚至废弃)&lt;/p>
&lt;p>CRI 的运作方式：&lt;/p>
&lt;ul>
&lt;li>当 kubernetes 通过编排能力声明一个 Pod 后，调度器会为这个 pod 选择一个具体的 Node 来运行，这时候，该 Node 上的 kubelet 会通过 SyncLoop 判断需要执行的具体操作，这个时候 kubelet 会调用一个叫做 GenericRuntime 的通用组件来发起创建 Pod 的 CRI 请求。&lt;/li>
&lt;li>CRI shim(CRI 垫片，宿主机与 kubelet 之间的东西)来响应 CRI 请求，然后把请求“翻译”成对后端容器项目的请求或者操作。&lt;/li>
&lt;li>每个容器项目都会自己实现一个 CRI shim，然后 CRI shim 收到的请求会转给对应的容器守护进程(e.g.docker 项目里的 dockerd)，由该守护进程进行容器的创建、更改、删除、exec 等操作&lt;/li>
&lt;/ul>
&lt;p>当前主流的 CRI 有如下几种：&lt;/p>
&lt;ul>
&lt;li>Docker(kubelet 默认的 CRI)
&lt;ul>
&lt;li>Note：kubelet 内置 dockershim，在启动或，会生成 dockersim.sock 文件，kubelet 与 crictl 都会默认与该文件关联&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CRI-O&lt;/li>
&lt;li>Containerd(通常与 docker 同时安装)&lt;/li>
&lt;li>Frakti(kata OCI 的实现)&lt;/li>
&lt;/ul>
&lt;p>kubelet 与 CRI 对接的方式&lt;/p>
&lt;ul>
&lt;li>kubelet 根据参数 &amp;ndash;container-runtime-endpoint 来决定其所绑定的 CRI sock。默认使用 docker 的 sock，路径为：/var/run/dockershim.sock
&lt;ul>
&lt;li>可以通过 crictl 工具来测试目标 sock 是否可用，crictl 用法详见：crictl 命令行工具&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>如果使用不同的 CRI 运行时，则需要为 kubelet 指定不同的 flag。 例如，当使用非 docker CRI 时， 则需要使用 &amp;ndash;container-runtime=remote 与 &amp;ndash;container-runtime-path-endpoint=&lt;!-- raw HTML omitted --> 指定 CRI 端点。endpoint 的值为指定 CRI 的 sock 文件。&lt;/li>
&lt;li>各个 CRI 需要进行配置才可与 kubelet 对接成功，如果不进行初始化配置，则 kubelet 无法获取到该 CRI 对于 k8s 的相关配置参数&lt;/li>
&lt;/ul>
&lt;p>kubelet 通过如下两个命令行标志来指定要使用的 CRI&lt;/p>
&lt;pre>&lt;code>--container-runtime=remote
--container-runtime-endpoint=unix:///run/containerd/containerd.sock
&lt;/code>&lt;/pre>
&lt;h2 id="kubelet-负责所在节点-networkcni-的管理">kubelet 负责所在节点 Network(CNI) 的管理&lt;/h2>
&lt;p>kubelet 对 cni(Container Network Interface 容器网络接口)的调用详见另一片关于网络介绍的文章：&lt;a href="https://www.yuque.com/go/doc/33164217">Kubernetes 的网络&lt;/a>&lt;/p>
&lt;p>kubelet 配置 pod 网络时，首先会读取下 /etc/cni/net.d/_ 目录下的配置，查看当前所使用的 CNI 插件及插件参数，比如现在是 flannel ，那么 flannel 会将 /run/flannel/subnet.env 文件的配置信息传递给 kubelet ，然后 kubelet 使用 /opt/cni/bin/_ 目录中的二进制文件，来处理处理 pod 的网络信息。&lt;/p>
&lt;h2 id="kubelet-负责所在节点-volumecvi-的管理">kubelet 负责所在节点 Volume(CVI) 的管理&lt;/h2></description></item><item><title>Docs: 2.Linux 上抽象网络设备的原理及使用</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/2.linux-%E4%B8%8A%E6%8A%BD%E8%B1%A1%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/2.linux-%E4%B8%8A%E6%8A%BD%E8%B1%A1%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/</guid><description>
&lt;h1 id="linux-抽象网络设备简介">Linux 抽象网络设备简介&lt;/h1>
&lt;p>和磁盘设备类似，Linux 用户想要使用网络功能，不能通过直接操作硬件完成，而需要直接或间接的操作一个 Linux 为我们抽象出来的设备，既通用的 Linux 网络设备来完成。一个常见的情况是，系统里装有一个硬件网卡，Linux 会在系统里为其生成一个网络设备实例，如 eth0，用户需要对 eth0 发出命令以配置或使用它了。更多的硬件会带来更多的设备实例，虚拟的硬件也会带来更多的设备实例。随着网络技术，虚拟化技术的发展，更多的高级网络设备被加入了到了 Linux 中，使得情况变得更加复杂。在以下章节中，将一一分析在虚拟化技术中经常使用的几种 Linux 网络设备抽象类型：Bridge、802.1.q VLAN device、VETH、TAP，详细解释如何用它们配合 Linux 中的 Route table、IP table 简单的创建出本地虚拟网络。&lt;/p>
&lt;h2 id="相关网络设备工作原理">相关网络设备工作原理&lt;/h2>
&lt;h3 id="bridge">Bridge&lt;/h3>
&lt;p>Bridge（桥）是 Linux 上用来做 TCP/IP 二层协议交换的设备，与现实世界中的交换机功能相似。Bridge 设备实例可以和 Linux 上其他网络设备实例连接，既 attach 一个从设备，类似于在现实世界中的交换机和一个用户终端之间连接一根网线。当有数据到达时，Bridge 会根据报文中的 MAC 信息进行广播、转发、丢弃处理。&lt;/p>
&lt;p>图 1.Bridge 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257424-79688f5a-dd7b-4f17-9afe-18d24df26e54.png" alt="">&lt;/p>
&lt;p>如图所示，Bridge 的功能主要在内核里实现。当一个从设备被 attach 到 Bridge 上时，相当于现实世界里交换机的端口被插入了一根连有终端的网线。这时在内核程序里，netdev_rx_handler_register()被调用，一个用于接受数据的回调函数被注册。以后每当这个从设备收到数据时都会调用这个函数可以把数据转发到 Bridge 上。当 Bridge 接收到此数据时，br_handle_frame()被调用，进行一个和现实世界中的交换机类似的处理过程：判断包的类别（广播/单点），查找内部 MAC 端口映射表，定位目标端口号，将数据转发到目标端口或丢弃，自动更新内部 MAC 端口映射表以自我学习。&lt;/p>
&lt;p>Bridge 和现实世界中的二层交换机有一个区别，图中左侧画出了这种情况：数据被直接发到 Bridge 上，而不是从一个端口接受。这种情况可以看做 Bridge 自己有一个 MAC 可以主动发送报文，或者说 Bridge 自带了一个隐藏端口和寄主 Linux 系统自动连接，Linux 上的程序可以直接从这个端口向 Bridge 上的其他端口发数据。所以当一个 Bridge 拥有一个网络设备时，如 bridge0 加入了 eth0 时，实际上 bridge0 拥有两个有效 MAC 地址，一个是 bridge0 的，一个是 eth0 的，他们之间可以通讯。由此带来一个有意思的事情是，Bridge 可以设置 IP 地址。通常来说 IP 地址是三层协议的内容，不应该出现在二层设备 Bridge 上。但是 Linux 里 Bridge 是通用网络设备抽象的一种，只要是网络设备就能够设定 IP 地址。当一个 bridge0 拥有 IP 后，Linux 便可以通过路由表或者 IP 表规则在三层定位 bridge0，此时相当于 Linux 拥有了另外一个隐藏的虚拟网卡和 Bridge 的隐藏端口相连，这个网卡就是名为 bridge0 的通用网络设备，IP 可以看成是这个网卡的。当有符合此 IP 的数据到达 bridge0 时，内核协议栈认为收到了一包目标为本机的数据，此时应用程序可以通过 Socket 接收到它。一个更好的对比例子是现实世界中的带路由的交换机设备，它也拥有一个隐藏的 MAC 地址，供设备中的三层协议处理程序和管理程序使用。设备里的三层协议处理程序，对应名为 bridge0 的通用网络设备的三层协议处理程序，即寄主 Linux 系统内核协议栈程序。设备里的管理程序，对应 bridge0 寄主 Linux 系统里的应用程序。&lt;/p>
&lt;p>Bridge 的实现当前有一个限制：当一个设备被 attach 到 Bridge 上时，那个设备的 IP 会变的无效，Linux 不再使用那个 IP 在三层接受数据。举例如下：如果 eth0 本来的 IP 是 192.168.1.2，此时如果收到一个目标地址是 192.168.1.2 的数据，Linux 的应用程序能通过 Socket 操作接受到它。而当 eth0 被 attach 到一个 bridge0 时，尽管 eth0 的 IP 还在，但应用程序是无法接受到上述数据的。此时应该把 IP 192.168.1.2 赋予 bridge0。&lt;/p>
&lt;p>另外需要注意的是数据流的方向。对于一个被 attach 到 Bridge 上的设备来说，只有它收到数据时，此包数据才会被转发到 Bridge 上，进而完成查表广播等后续操作。当请求是发送类型时，数据是不会被转发到 Bridge 上的，它会寻找下一个发送出口。用户在配置网络时经常忽略这一点从而造成网络故障。&lt;/p>
&lt;h3 id="vlan-device-for-8021q">VLAN device for 802.1.q&lt;/h3>
&lt;p>VLAN 又称虚拟网络，是一个被广泛使用的概念，有些应用程序把自己的内部网络也称为 VLAN。此处主要说的是在物理世界中存在的，需要协议支持的 VLAN。它的种类很多，按照协议原理一般分为：MACVLAN、802.1.q VLAN、802.1.qbg VLAN、802.1.qbh VLAN。其中出现较早，应用广泛并且比较成熟的是 802.1.q VLAN，其基本原理是在二层协议里插入额外的 VLAN 协议数据（称为 802.1.q VLAN Tag)，同时保持和传统二层设备的兼容性。Linux 里的 VLAN 设备是对 802.1.q 协议的一种内部软件实现，模拟现实世界中的 802.1.q 交换机。&lt;/p>
&lt;p>图 2 .VLAN 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257463-a0ff1a57-bfc0-40ea-b486-e6b2a4081b2e.png" alt="">&lt;/p>
&lt;p>如图所示，Linux 里 802.1.q VLAN 设备是以母子关系成对出现的，母设备相当于现实世界中的交换机 TRUNK 口，用于连接上级网络，子设备相当于普通接口用于连接下级网络。当数据在母子设备间传递时，内核将会根据 802.1.q VLAN Tag 进行对应操作。母子设备之间是一对多的关系，一个母设备可以有多个子设备，一个子设备只有一个母设备。当一个子设备有一包数据需要发送时，数据将被加入 VLAN Tag 然后从母设备发送出去。当母设备收到一包数据时，它将会分析其中的 VLAN Tag，如果有对应的子设备存在，则把数据转发到那个子设备上并根据设置移除 VLAN Tag，否则丢弃该数据。在某些设置下，VLAN Tag 可以不被移除以满足某些监听程序的需要，如 DHCP 服务程序。举例说明如下：eth0 作为母设备创建一个 ID 为 100 的子设备 eth0.100。此时如果有程序要求从 eth0.100 发送一包数据，数据将被打上 VLAN 100 的 Tag 从 eth0 发送出去。如果 eth0 收到一包数据，VLAN Tag 是 100，数据将被转发到 eth0.100 上，并根据设置决定是否移除 VLAN Tag。如果 eth0 收到一包包含 VLAN Tag 101 的数据，其将被丢弃。上述过程隐含以下事实：对于寄主 Linux 系统来说，母设备只能用来收数据，子设备只能用来发送数据。和 Bridge 一样，母子设备的数据也是有方向的，子设备收到的数据不会进入母设备，同样母设备上请求发送的数据不会被转到子设备上。可以把 VLAN 母子设备作为一个整体想象为现实世界中的 802.1.q 交换机，下级接口通过子设备连接到寄主 Linux 系统网络里，上级接口同过主设备连接到上级网络，当母设备是物理网卡时上级网络是外界真实网络，当母设备是另外一个 Linux 虚拟网络设备时上级网络仍然是寄主 Linux 系统网络。&lt;/p>
&lt;p>需要注意的是母子 VLAN 设备拥有相同的 MAC 地址，可以把它当成现实世界中 802.1.q 交换机的 MAC，因此多个 VLAN 设备会共享一个 MAC。当一个母设备拥有多个 VLAN 子设备时，子设备之间是隔离的，不存在 Bridge 那样的交换转发关系，原因如下：802.1.q VLAN 协议的主要目的是从逻辑上隔离子网。现实世界中的 802.1.q 交换机存在多个 VLAN，每个 VLAN 拥有多个端口，同一 VLAN 端口之间可以交换转发，不同 VLAN 端口之间隔离，所以其包含两层功能：交换与隔离。Linux VLAN device 实现的是隔离功能，没有交换功能。一个 VLAN 母设备不可能拥有两个相同 ID 的 VLAN 子设备，因此也就不可能出现数据交换情况。如果想让一个 VLAN 里接多个设备，就需要交换功能。在 Linux 里 Bridge 专门实现交换功能，因此将 VLAN 子设备 attach 到一个 Bridge 上就能完成后续的交换功能。总结起来，Bridge 加 VLAN device 能在功能层面完整模拟现实世界里的 802.1.q 交换机。&lt;/p>
&lt;p>Linux 支持 VLAN 硬件加速，在安装有特定硬件情况下，图中所述内核处理过程可以被放到物理设备上完成。&lt;/p>
&lt;h3 id="tap-设备与-veth-设备">TAP 设备与 VETH 设备&lt;/h3>
&lt;p>TUN/TAP 设备是一种让用户态程序向内核协议栈注入数据的设备，一个工作在三层，一个工作在二层，使用较多的是 TAP 设备。VETH 设备出现较早，它的作用是反转通讯数据的方向，需要发送的数据会被转换成需要收到的数据重新送入内核网络层进行处理，从而间接的完成数据的注入。&lt;/p>
&lt;p>图 3 .TAP 设备和 VETH 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257472-c3161ee0-667a-4db3-b478-6d4d3e6c1682.png" alt="">&lt;/p>
&lt;p>如图所示，当一个 TAP 设备被创建时，在 Linux 设备文件目录下将会生成一个对应 char 设备，用户程序可以像打开普通文件一样打开这个文件进行读写。当执行 write()操作时，数据进入 TAP 设备，此时对于 Linux 网络层来说，相当于 TAP 设备收到了一包数据，请求内核接受它，如同普通的物理网卡从外界收到一包数据一样，不同的是其实数据来自 Linux 上的一个用户程序。Linux 收到此数据后将根据网络配置进行后续处理，从而完成了用户程序向 Linux 内核网络层注入数据的功能。当用户程序执行 read()请求时，相当于向内核查询 TAP 设备上是否有需要被发送出去的数据，有的话取出到用户程序里，完成 TAP 设备的发送数据功能。针对 TAP 设备的一个形象的比喻是：使用 TAP 设备的应用程序相当于另外一台计算机，TAP 设备是本机的一个网卡，他们之间相互连接。应用程序通过 read()/write()操作，和本机网络核心进行通讯。(可以这么说，一台虚拟机的网卡就是一个物理机上的 tap 设备)&lt;/p>
&lt;p>VETH 设备总是成对出现，送到一端请求发送的数据总是从另一端以请求接受的形式出现。该设备不能被用户程序直接操作，但使用起来比较简单。创建并配置正确后，向其一端输入数据，VETH 会改变数据的方向并将其送入内核网络核心，完成数据的注入。在另一端能读到此数据。&lt;/p>
&lt;h2 id="网络设置举例说明">网络设置举例说明&lt;/h2>
&lt;p>为了更好的说明 Linux 网络设备的用法，下面将用一系列的例子，说明在一个复杂的 Linux 网络元素组合出的虚拟网络里，数据的流向。网络设置简介如下：一个中心 Bridge：bridge0 下 attach 了 4 个网络设备，包括 2 个 VETH 设备，1 个 TAP 设备 tap0，1 个物理网卡 eth0。在 VETH 的另外一端又创建了 VLAN 子设备。Linux 上共存在 2 个 VLAN 网络，既 vlan100 与 vlan200。物理网卡和外部网络相连，并且在它之下创建了一个 VLAN ID 为 200 的 VLAN 子设备。&lt;/p>
&lt;h3 id="从-vlan100-子设备发送-arp-报文">从 vlan100 子设备发送 ARP 报文&lt;/h3>
&lt;p>图 4 .ARP from vlan100 child device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257520-663ccc4b-26bd-4784-bda1-7b8dd04c75a6.png" alt="">&lt;/p>
&lt;p>如图所示，当用户尝试 ping 192.168.100.3 时，Linux 将会根据路由表，从 vlan100 子设备发出 ARP 报文，具体过程如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>用户 ping 192.168.100.3&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Linux 向 vlan100 子设备发送 ARP 信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ARP 报文被打上 VLAN ID 100 的 Tag 成为 ARP@vlan100，转发到母设备上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VETH 设备将这一发送请求转变方向，成为一个需要接受处理的报文送入内核网络模块。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>由于对端的 VETH 设备被加入到了 bridge0 上，并且内核发现它收到一个报文，于是报文被转发到 bridge0 上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bridge0 处理此 ARP@vlan100 信息，根据 TCP/IP 二层协议发现是一个广播请求，于是向它所知道的所有端口广播此报文，其中一路进入另一对 VETH 设备的一端，一路进入 TAP 设备 tap0，一路进入物理网卡设备 eth0。此时在 tap0 上，用户程序可以通过 read()操作读到 ARP@vlan100，eth0 将会向外界发送 ARP@vlan100，但 eth0 的 VLAN 子设备不会收到它，因为此数据方向为请求发送而不是请求接收。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VETH 将请求方向转换，此时在另一端得到请求接受的 ARP@vlan100 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对端 VETH 设备发现有数据需要接受，并且自己有两个 VLAN 子设备，于是执行 VLAN 处理逻辑。其中一个子设备是 vlan100，与 ARP@vlan100 吻合，于是去除 VLAN ID 100 的 Tag 转发到这个子设备上，重新成为标准的以太网 ARP 报文。另一个子设备由于 ID 不吻合，不会得到此报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此 VLAN 子设备又被 attach 到另一个桥 bridge1 上，于是转发自己收到的 ARP 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bridge1 广播 ARP 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最终另外一个 TAP 设备 tap1 收到此请求发送报文，用户程序通过 read()可以得到它。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="从-vlan200-子设备发送-arp-报文">从 vlan200 子设备发送 ARP 报文&lt;/h3>
&lt;p>图 5 .ARP from vlan200 child device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257503-55a18ca0-f189-4987-9a14-be02dff09d03.png" alt="">&lt;/p>
&lt;p>和前面情况类似，区别是 VLAN ID 是 200，对端的 vlan200 子设备设置为 reorder_hdr = 0，表示此设备被要求保留收到的报文中的 VLAN Tag。此时子设备会收到 ARP 报文，但是带了 VLAN ID 200 的 Tag，既 ARP@vlan200。&lt;/p>
&lt;h3 id="从中心-bridge-发送-arp-报文">从中心 bridge 发送 ARP 报文&lt;/h3>
&lt;p>图 5 .ARP from central bridge&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257514-72135ddd-006c-417e-a0b2-0a62c5906609.png" alt="">&lt;/p>
&lt;p>当 bridge0 拥有 IP 时，通过 Linux 路由表用户程序可以直接将 ARP 报文发向 bridge0。这时 tap0 和外部网络都能收到 ARP，但 VLAN 子设备由于 VLAN ID 过滤的原因，将收不到 ARP 信息。&lt;/p>
&lt;h3 id="从外部网络向物理网卡发送-arpvlan200-报文">从外部网络向物理网卡发送 ARP@vlan200 报文&lt;/h3>
&lt;p>图 6 .ARP from external network&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257531-ba4cbf5b-4940-4019-b46c-6e9a47946cbc.png" alt="">&lt;/p>
&lt;p>当外部网络连接在一个支持 VLAN 并且对应端口为 vlan200 时，此情况会发生。此时所有的 VLAN ID 为 200 的 VLAN 子设备都将接受到报文，如果设置 reorder_hdr=0 则会收到带 Tag 的 ARP@vlan200。&lt;/p>
&lt;h3 id="从-tap-设备以-ping-方式发送-arp">从 TAP 设备以 ping 方式发送 ARP&lt;/h3>
&lt;p>图 7 .ping from TAP device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257520-73844bea-b56f-413c-8688-ea99573c2b2b.png" alt="">&lt;/p>
&lt;p>给 tap0 赋予 IP 并加入路由，此时再 Ping 其对应网段的未知 IP 会产生 ARP 发送请求。需要注意的是此时由于 tap0 上存在的是发送而不是接收请求，因此 ARP 报文不会被转发到桥上，从而什么也不会发生。图中右边画了一个类似情况：从 vlan200 子设备发送 ARP 请求。由于缺少 VETH 设备反转请求方向，因此报文也不会被转发到桥上，而是直接通过物理网卡发往外部网络。&lt;/p>
&lt;h3 id="以文件操作方式从-tap-设备发送报文">以文件操作方式从 TAP 设备发送报文&lt;/h3>
&lt;p>图 8 .file operation on TAP device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257536-d6ac7a20-331e-421f-a7d0-58862524405e.png" alt="">&lt;/p>
&lt;p>用户程序指定 tap0 设备发送报文有两种方式：socket 和 file operation。当用 socket_raw 标志新建 socket 并指定设备编号时，可以要求内核将报文从 tap0 发送。但和前面的 ping from tap0 情况类似，由于报文方向问题，消息并不会被转发到 bridge0 上。当用 open()方式打开 tap 设备文件时，情况有所不同。当执行 write()操作时，内核认为 tap0 收到了报文，从而会触发转发动作，bridge0 将收到它。如果发送的报文如图所示，是一个以 A 为目的地的携带 VLAN ID 100 Tag 的单点报文，bridge0 将会找到对应的设备进行转发，对应的 VLAN 子设备将收到没有 VLAN ID 100 Tag 的报文。&lt;/p>
&lt;h2 id="linux-上配置网络设备命令举例">Linux 上配置网络设备命令举例&lt;/h2>
&lt;p>以 Redhat6.2 红帽 Linux 发行版为例，如果已安装 VLAN 内核模块和管理工具 vconfig，TAP/TUN 设备管理工具 tunctl，那么可以用以下命令设置前述网络设备：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>创建 Bridge：brctl addbr [BRIDGE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 Bridge：brctl delbr [BRIDGE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>attach 设备到 Bridge：brctl addif [BRIDGE NAME] [DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从 Bridge detach 设备：brctl delif [BRIDGE NAME] [DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查询 Bridge 情况：brctl show&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 VLAN 设备：vconfig add [PARENT DEVICE NAME] [VLAN ID]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 VLAN 设备：vconfig rem [VLAN DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设置 VLAN 设备 flag：vconfig set_flag [VLAN DEVICE NAME] [FLAG] [VALUE]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设置 VLAN 设备 qos：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>vconfig set_egress_map [VLAN DEVICE NAME] [SKB_PRIORITY] [VLAN_QOS]&lt;/p>
&lt;p>vconfig set_ingress_map [VLAN DEVICE NAME] [SKB_PRIORITY] [VLAN_QOS]&lt;/p>
&lt;ul>
&lt;li>
&lt;p>查询 VLAN 设备情况：cat /proc/net/vlan/[VLAN DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 VETH 设备：ip link add link [DEVICE NAME] type veth&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 TAP 设备：tunctl -p [TAP DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 TAP 设备：tunctl -d [TAP DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查询系统里所有二层设备，包括 VETH/TAP 设备：ip link show&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除普通二层设备：ip link delete [DEVICE NAME] type [TYPE]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>综上所述，Linux 已经提供一套基本工具供用户创建出各种内部网络，利用这些工具可以方便的创建出特定网络给应用程序使用，包括云计算中的初级内部虚拟网络。&lt;/p>
&lt;p>相关主题&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.linuxcommand.org/man_pages/vconfig8.html">Vconfig Man Page&lt;/a>，vconfig 工具帮助文档。&lt;/li>
&lt;li>&lt;a href="http://candelatech.com/~greear/vlan.html">802.1Q VLAN implementation for Linux&lt;/a>，Linux 中 VLAN 模块如何实现的文档说明。&lt;/li>
&lt;li>&lt;a href="http://www.policyrouting.org/iproute2.doc.html">IPROUTE2 Utility Suite Howto&lt;/a>，Linux 里的 IP 工具使用说明。&lt;/li>
&lt;li>&lt;a href="http://www.ibm.com/developerworks/linux/library/l-virtual-networking">Virtual networking in Linux&lt;/a>，以虚拟化应用为中心讲述主流的虚拟网络技术，主要以 openvswith 为例。&lt;/li>
&lt;li>&lt;a href="http://tldp.org/HOWTO/BRIDGE-STP-HOWTO/index.html">Linux BRIDGE-STP-HOWTO&lt;/a>，Linux 中的 bridge 设备使用说明。&lt;/li>
&lt;li>&lt;a href="http://www.linuxfoundation.org/collaborate/workgroups/networking/networkoverview">Linux Kernel Networking (Network Overview) by Rami Rosen&lt;/a>，Linux 内核里的各种网络概念的含义，目的及用法简单介绍。&lt;/li>
&lt;li>在 &lt;a href="http://www.ibm.com/developerworks/cn/linux/">developerWorks Linux &lt;/a>专区寻找为 Linux 开发人员（包括 &lt;a href="http://www.ibm.com/developerworks/cn/linux/newto/">Linux 新手入门&lt;/a>）准备的更多参考资料。&lt;/li>
&lt;/ul></description></item><item><title>Docs: 2.neutron</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/2.neutron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/2.neutron/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>语法格式：neutron [OPTIONS] [SubCommand [OPTIONS]]&lt;/p>
&lt;p>直接输入 neutron 可以进入 neutron 的 shell 模式，在 neutron 的 shell 中再执行相关命令&lt;/p>
&lt;p>net-list #列出网络信息&lt;/p>
&lt;p>neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.&lt;/p>
&lt;p>usage: neutron [&amp;ndash;version] [-v] [-q] [-h] [-r NUM]&lt;/p>
&lt;pre>&lt;code> [--os-service-type ]
[--os-endpoint-type ]
[--service-type ]
[--endpoint-type ]
[--os-auth-strategy ] [--os-cloud ]
[--os-auth-url ]
[--os-tenant-name | --os-project-name ]
[--os-tenant-id | --os-project-id ]
[--os-username ] [--os-user-id ]
[--os-user-domain-id ]
[--os-user-domain-name ]
[--os-project-domain-id ]
[--os-project-domain-name ]
[--os-cert ] [--os-cacert ]
[--os-key ] [--os-password ]
[--os-region-name ] [--os-token ]
[--http-timeout ] [--os-url ] [--insecure]
&lt;/code>&lt;/pre>
&lt;p>Command-line interface to the Neutron APIs (neutron CLI version: 6.7.0)&lt;/p>
&lt;p>optional arguments:&lt;/p>
&lt;p>--version show program&amp;rsquo;s version number and exit&lt;/p>
&lt;p>-v, &amp;ndash;verbose, &amp;ndash;debug&lt;/p>
&lt;pre>&lt;code> Increase verbosity of output and show tracebacks on
errors. You can repeat this option.
&lt;/code>&lt;/pre>
&lt;p>-q, &amp;ndash;quiet Suppress output except warnings and errors.&lt;/p>
&lt;p>-h, &amp;ndash;help Show this help message and exit.&lt;/p>
&lt;p>-r NUM, &amp;ndash;retries NUM&lt;/p>
&lt;pre>&lt;code> How many times the request to the Neutron server
should be retried if it fails.
&lt;/code>&lt;/pre>
&lt;p>--os-service-type&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_NETWORK_SERVICE_TYPE] or network.
&lt;/code>&lt;/pre>
&lt;p>--os-endpoint-type&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_ENDPOINT_TYPE] or public.
&lt;/code>&lt;/pre>
&lt;p>--service-type&lt;/p>
&lt;pre>&lt;code> DEPRECATED! Use --os-service-type.
&lt;/code>&lt;/pre>
&lt;p>--endpoint-type&lt;/p>
&lt;pre>&lt;code> DEPRECATED! Use --os-endpoint-type.
&lt;/code>&lt;/pre>
&lt;p>--os-auth-strategy&lt;/p>
&lt;pre>&lt;code> DEPRECATED! Only keystone is supported.
&lt;/code>&lt;/pre>
&lt;p>--os-cloud Defaults to env[OS_CLOUD].&lt;/p>
&lt;p>--os-auth-url&lt;/p>
&lt;pre>&lt;code> Authentication URL, defaults to env[OS_AUTH_URL].
&lt;/code>&lt;/pre>
&lt;p>--os-tenant-name&lt;/p>
&lt;pre>&lt;code> Authentication tenant name, defaults to
env[OS_TENANT_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-project-name&lt;/p>
&lt;pre>&lt;code> Another way to specify tenant name. This option is
mutually exclusive with --os-tenant-name. Defaults to
env[OS_PROJECT_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-tenant-id&lt;/p>
&lt;pre>&lt;code> Authentication tenant ID, defaults to
env[OS_TENANT_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-project-id&lt;/p>
&lt;pre>&lt;code> Another way to specify tenant ID. This option is
mutually exclusive with --os-tenant-id. Defaults to
env[OS_PROJECT_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-username&lt;/p>
&lt;pre>&lt;code> Authentication username, defaults to env[OS_USERNAME].
&lt;/code>&lt;/pre>
&lt;p>--os-user-id&lt;/p>
&lt;pre>&lt;code> Authentication user ID (Env: OS_USER_ID)
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-id&lt;/p>
&lt;pre>&lt;code> OpenStack user domain ID. Defaults to
env[OS_USER_DOMAIN_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-name&lt;/p>
&lt;pre>&lt;code> OpenStack user domain name. Defaults to
env[OS_USER_DOMAIN_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-id&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_PROJECT_DOMAIN_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-name&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_PROJECT_DOMAIN_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-cert&lt;/p>
&lt;pre>&lt;code> Path of certificate file to use in SSL connection.
This file can optionally be prepended with the private
key. Defaults to env[OS_CERT].
&lt;/code>&lt;/pre>
&lt;p>--os-cacert&lt;/p>
&lt;pre>&lt;code> Specify a CA bundle file to use in verifying a TLS
(https) server certificate. Defaults to
env[OS_CACERT].
&lt;/code>&lt;/pre>
&lt;p>--os-key Path of client key to use in SSL connection. This&lt;/p>
&lt;pre>&lt;code> option is not necessary if your key is prepended to
your certificate file. Defaults to env[OS_KEY].
&lt;/code>&lt;/pre>
&lt;p>--os-password&lt;/p>
&lt;pre>&lt;code> Authentication password, defaults to env[OS_PASSWORD].
&lt;/code>&lt;/pre>
&lt;p>--os-region-name&lt;/p>
&lt;pre>&lt;code> Authentication region name, defaults to
env[OS_REGION_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-token Authentication token, defaults to env[OS_TOKEN].&lt;/p>
&lt;p>--http-timeout&lt;/p>
&lt;pre>&lt;code> Timeout in seconds to wait for an HTTP response.
Defaults to env[OS_NETWORK_TIMEOUT] or None if not
specified.
&lt;/code>&lt;/pre>
&lt;p>--os-url Defaults to env[OS_URL].&lt;/p>
&lt;p>--insecure Explicitly allow neutronclient to perform &amp;ldquo;insecure&amp;rdquo;&lt;/p>
&lt;pre>&lt;code> SSL (https) requests. The server's certificate will
not be verified against any certificate authorities.
This option should be used with caution.
&lt;/code>&lt;/pre>
&lt;p>Commands for API v2.0:&lt;/p>
&lt;p>address-scope-create Create an address scope for a given tenant.&lt;/p>
&lt;p>address-scope-delete Delete an address scope.&lt;/p>
&lt;p>address-scope-list List address scopes that belong to a given tenant.&lt;/p>
&lt;p>address-scope-show Show information about an address scope.&lt;/p>
&lt;p>address-scope-update Update an address scope.&lt;/p>
&lt;p>agent-delete Delete a given agent.&lt;/p>
&lt;p>agent-list List agents.&lt;/p>
&lt;p>agent-show Show information of a given agent.&lt;/p>
&lt;p>agent-update Updates the admin status and description for a specified agent.&lt;/p>
&lt;p>auto-allocated-topology-delete Delete the auto-allocated topology of a given tenant.&lt;/p>
&lt;p>auto-allocated-topology-show Show the auto-allocated topology of a given tenant.&lt;/p>
&lt;p>availability-zone-list List availability zones.&lt;/p>
&lt;p>bash-completion Prints all of the commands and options for bash-completion.&lt;/p>
&lt;p>bgp-dragent-list-hosting-speaker List Dynamic Routing agents hosting a BGP speaker.&lt;/p>
&lt;p>bgp-dragent-speaker-add Add a BGP speaker to a Dynamic Routing agent.&lt;/p>
&lt;p>bgp-dragent-speaker-remove Removes a BGP speaker from a Dynamic Routing agent.&lt;/p>
&lt;p>bgp-peer-create Create a BGP Peer.&lt;/p>
&lt;p>bgp-peer-delete Delete a BGP peer.&lt;/p>
&lt;p>bgp-peer-list List BGP peers.&lt;/p>
&lt;p>bgp-peer-show Show information of a given BGP peer.&lt;/p>
&lt;p>bgp-peer-update Update BGP Peer&amp;rsquo;s information.&lt;/p>
&lt;p>bgp-speaker-advertiseroute-list List routes advertised by a given BGP speaker.&lt;/p>
&lt;p>bgp-speaker-create Create a BGP Speaker.&lt;/p>
&lt;p>bgp-speaker-delete Delete a BGP speaker.&lt;/p>
&lt;p>bgp-speaker-list List BGP speakers.&lt;/p>
&lt;p>bgp-speaker-list-on-dragent List BGP speakers hosted by a Dynamic Routing agent.&lt;/p>
&lt;p>bgp-speaker-network-add Add a network to the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-network-remove Remove a network from the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-peer-add Add a peer to the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-peer-remove Remove a peer from the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-show Show information of a given BGP speaker.&lt;/p>
&lt;p>bgp-speaker-update Update BGP Speaker&amp;rsquo;s information.&lt;/p>
&lt;p>dhcp-agent-list-hosting-net List DHCP agents hosting a network.&lt;/p>
&lt;p>dhcp-agent-network-add Add a network to a DHCP agent.&lt;/p>
&lt;p>dhcp-agent-network-remove Remove a network from a DHCP agent.&lt;/p>
&lt;p>ext-list List all extensions.&lt;/p>
&lt;p>ext-show Show information of a given resource.&lt;/p>
&lt;p>firewall-create Create a firewall.&lt;/p>
&lt;p>firewall-delete Delete a given firewall.&lt;/p>
&lt;p>firewall-list List firewalls that belong to a given tenant.&lt;/p>
&lt;p>firewall-policy-create Create a firewall policy.&lt;/p>
&lt;p>firewall-policy-delete Delete a given firewall policy.&lt;/p>
&lt;p>firewall-policy-insert-rule Insert a rule into a given firewall policy.&lt;/p>
&lt;p>firewall-policy-list List firewall policies that belong to a given tenant.&lt;/p>
&lt;p>firewall-policy-remove-rule Remove a rule from a given firewall policy.&lt;/p>
&lt;p>firewall-policy-show Show information of a given firewall policy.&lt;/p>
&lt;p>firewall-policy-update Update a given firewall policy.&lt;/p>
&lt;p>firewall-rule-create Create a firewall rule.&lt;/p>
&lt;p>firewall-rule-delete Delete a given firewall rule.&lt;/p>
&lt;p>firewall-rule-list List firewall rules that belong to a given tenant.&lt;/p>
&lt;p>firewall-rule-show Show information of a given firewall rule.&lt;/p>
&lt;p>firewall-rule-update Update a given firewall rule.&lt;/p>
&lt;p>firewall-show Show information of a given firewall.&lt;/p>
&lt;p>firewall-update Update a given firewall.&lt;/p>
&lt;p>flavor-associate Associate a Neutron service flavor with a flavor profile.&lt;/p>
&lt;p>flavor-create Create a Neutron service flavor.&lt;/p>
&lt;p>flavor-delete Delete a given Neutron service flavor.&lt;/p>
&lt;p>flavor-disassociate Disassociate a Neutron service flavor from a flavor profile.&lt;/p>
&lt;p>flavor-list List Neutron service flavors.&lt;/p>
&lt;p>flavor-profile-create Create a Neutron service flavor profile.&lt;/p>
&lt;p>flavor-profile-delete Delete a given Neutron service flavor profile.&lt;/p>
&lt;p>flavor-profile-list List Neutron service flavor profiles.&lt;/p>
&lt;p>flavor-profile-show Show information about a given Neutron service flavor profile.&lt;/p>
&lt;p>flavor-profile-update Update a given Neutron service flavor profile.&lt;/p>
&lt;p>flavor-show Show information about a given Neutron service flavor.&lt;/p>
&lt;p>flavor-update Update a Neutron service flavor.&lt;/p>
&lt;p>floatingip-associate Create a mapping between a floating IP and a fixed IP.&lt;/p>
&lt;p>floatingip-create Create a floating IP for a given tenant.&lt;/p>
&lt;p>floatingip-delete Delete a given floating IP.&lt;/p>
&lt;p>floatingip-disassociate Remove a mapping from a floating IP to a fixed IP.&lt;/p>
&lt;p>floatingip-list List floating IPs that belong to a given tenant.&lt;/p>
&lt;p>floatingip-show Show information of a given floating IP.&lt;/p>
&lt;p>help print detailed help for another command&lt;/p>
&lt;p>ipsec-site-connection-create Create an IPsec site connection.&lt;/p>
&lt;p>ipsec-site-connection-delete Delete a given IPsec site connection.&lt;/p>
&lt;p>ipsec-site-connection-list List IPsec site connections that belong to a given tenant.&lt;/p>
&lt;p>ipsec-site-connection-show Show information of a given IPsec site connection.&lt;/p>
&lt;p>ipsec-site-connection-update Update a given IPsec site connection.&lt;/p>
&lt;p>l3-agent-list-hosting-router List L3 agents hosting a router.&lt;/p>
&lt;p>l3-agent-router-add Add a router to a L3 agent.&lt;/p>
&lt;p>l3-agent-router-remove Remove a router from a L3 agent.&lt;/p>
&lt;p>lb-agent-hosting-pool Get loadbalancer agent hosting a pool.&lt;/p>
&lt;p>lb-healthmonitor-associate Create a mapping between a health monitor and a pool.&lt;/p>
&lt;p>lb-healthmonitor-create Create a health monitor.&lt;/p>
&lt;p>lb-healthmonitor-delete Delete a given health monitor.&lt;/p>
&lt;p>lb-healthmonitor-disassociate Remove a mapping from a health monitor to a pool.&lt;/p>
&lt;p>lb-healthmonitor-list List health monitors that belong to a given tenant.&lt;/p>
&lt;p>lb-healthmonitor-show Show information of a given health monitor.&lt;/p>
&lt;p>lb-healthmonitor-update Update a given health monitor.&lt;/p>
&lt;p>lb-member-create Create a member.&lt;/p>
&lt;p>lb-member-delete Delete a given member.&lt;/p>
&lt;p>lb-member-list List members that belong to a given tenant.&lt;/p>
&lt;p>lb-member-show Show information of a given member.&lt;/p>
&lt;p>lb-member-update Update a given member.&lt;/p>
&lt;p>lb-pool-create Create a pool.&lt;/p>
&lt;p>lb-pool-delete Delete a given pool.&lt;/p>
&lt;p>lb-pool-list List pools that belong to a given tenant.&lt;/p>
&lt;p>lb-pool-list-on-agent List the pools on a loadbalancer agent.&lt;/p>
&lt;p>lb-pool-show Show information of a given pool.&lt;/p>
&lt;p>lb-pool-stats Retrieve stats for a given pool.&lt;/p>
&lt;p>lb-pool-update Update a given pool.&lt;/p>
&lt;p>lb-vip-create Create a vip.&lt;/p>
&lt;p>lb-vip-delete Delete a given vip.&lt;/p>
&lt;p>lb-vip-list List vips that belong to a given tenant.&lt;/p>
&lt;p>lb-vip-show Show information of a given vip.&lt;/p>
&lt;p>lb-vip-update Update a given vip.&lt;/p>
&lt;p>lbaas-agent-hosting-loadbalancer Get lbaas v2 agent hosting a loadbalancer.&lt;/p>
&lt;p>lbaas-healthmonitor-create LBaaS v2 Create a healthmonitor.&lt;/p>
&lt;p>lbaas-healthmonitor-delete LBaaS v2 Delete a given healthmonitor.&lt;/p>
&lt;p>lbaas-healthmonitor-list LBaaS v2 List healthmonitors that belong to a given tenant.&lt;/p>
&lt;p>lbaas-healthmonitor-show LBaaS v2 Show information of a given healthmonitor.&lt;/p>
&lt;p>lbaas-healthmonitor-update LBaaS v2 Update a given healthmonitor.&lt;/p>
&lt;p>lbaas-l7policy-create LBaaS v2 Create L7 policy.&lt;/p>
&lt;p>lbaas-l7policy-delete LBaaS v2 Delete a given L7 policy.&lt;/p>
&lt;p>lbaas-l7policy-list LBaaS v2 List L7 policies that belong to a given listener.&lt;/p>
&lt;p>lbaas-l7policy-show LBaaS v2 Show information of a given L7 policy.&lt;/p>
&lt;p>lbaas-l7policy-update LBaaS v2 Update a given L7 policy.&lt;/p>
&lt;p>lbaas-l7rule-create LBaaS v2 Create L7 rule.&lt;/p>
&lt;p>lbaas-l7rule-delete LBaaS v2 Delete a given L7 rule.&lt;/p>
&lt;p>lbaas-l7rule-list LBaaS v2 List L7 rules that belong to a given L7 policy.&lt;/p>
&lt;p>lbaas-l7rule-show LBaaS v2 Show information of a given rule.&lt;/p>
&lt;p>lbaas-l7rule-update LBaaS v2 Update a given L7 rule.&lt;/p>
&lt;p>lbaas-listener-create LBaaS v2 Create a listener.&lt;/p>
&lt;p>lbaas-listener-delete LBaaS v2 Delete a given listener.&lt;/p>
&lt;p>lbaas-listener-list LBaaS v2 List listeners that belong to a given tenant.&lt;/p>
&lt;p>lbaas-listener-show LBaaS v2 Show information of a given listener.&lt;/p>
&lt;p>lbaas-listener-update LBaaS v2 Update a given listener.&lt;/p>
&lt;p>lbaas-loadbalancer-create LBaaS v2 Create a loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-delete LBaaS v2 Delete a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-list LBaaS v2 List loadbalancers that belong to a given tenant.&lt;/p>
&lt;p>lbaas-loadbalancer-list-on-agent List the loadbalancers on a loadbalancer v2 agent.&lt;/p>
&lt;p>lbaas-loadbalancer-show LBaaS v2 Show information of a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-stats Retrieve stats for a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-status Retrieve status for a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-update LBaaS v2 Update a given loadbalancer.&lt;/p>
&lt;p>lbaas-member-create LBaaS v2 Create a member.&lt;/p>
&lt;p>lbaas-member-delete LBaaS v2 Delete a given member.&lt;/p>
&lt;p>lbaas-member-list LBaaS v2 List members that belong to a given pool.&lt;/p>
&lt;p>lbaas-member-show LBaaS v2 Show information of a given member.&lt;/p>
&lt;p>lbaas-member-update LBaaS v2 Update a given member.&lt;/p>
&lt;p>lbaas-pool-create LBaaS v2 Create a pool.&lt;/p>
&lt;p>lbaas-pool-delete LBaaS v2 Delete a given pool.&lt;/p>
&lt;p>lbaas-pool-list LBaaS v2 List pools that belong to a given tenant.&lt;/p>
&lt;p>lbaas-pool-show LBaaS v2 Show information of a given pool.&lt;/p>
&lt;p>lbaas-pool-update LBaaS v2 Update a given pool.&lt;/p>
&lt;p>meter-label-create Create a metering label for a given tenant.&lt;/p>
&lt;p>meter-label-delete Delete a given metering label.&lt;/p>
&lt;p>meter-label-list List metering labels that belong to a given tenant.&lt;/p>
&lt;p>meter-label-rule-create Create a metering label rule for a given label.&lt;/p>
&lt;p>meter-label-rule-delete Delete a given metering label.&lt;/p>
&lt;p>meter-label-rule-list List metering labels that belong to a given label.&lt;/p>
&lt;p>meter-label-rule-show Show information of a given metering label rule.&lt;/p>
&lt;p>meter-label-show Show information of a given metering label.&lt;/p>
&lt;p>net-create Create a network for a given tenant.&lt;/p>
&lt;p>net-delete Delete a given network.&lt;/p>
&lt;p>net-external-list List external networks that belong to a given tenant.&lt;/p>
&lt;p>net-ip-availability-list List IP usage of networks&lt;/p>
&lt;p>net-ip-availability-show Show IP usage of specific network&lt;/p>
&lt;p>net-list List networks that belong to a given tenant.&lt;/p>
&lt;p>net-list-on-dhcp-agent List the networks on a DHCP agent.&lt;/p>
&lt;p>net-show Show information of a given network.&lt;/p>
&lt;p>net-update Update network&amp;rsquo;s information.&lt;/p>
&lt;p>port-create Create a port for a given tenant.&lt;/p>
&lt;p>port-delete Delete a given port.&lt;/p>
&lt;p>port-list List ports that belong to a given tenant.&lt;/p>
&lt;p>port-show Show information of a given port.&lt;/p>
&lt;p>port-update Update port&amp;rsquo;s information.&lt;/p>
&lt;p>purge Delete all resources that belong to a given tenant.&lt;/p>
&lt;p>qos-available-rule-types List available qos rule types.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-create Create a qos bandwidth limit rule.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-delete Delete a given qos bandwidth limit rule.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-list List all qos bandwidth limit rules belonging to the specified policy.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-show Show information about the given qos bandwidth limit rule.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-update Update the given qos bandwidth limit rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-create Create a QoS DSCP marking rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-delete Delete a given qos dscp marking rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-list List all QoS DSCP marking rules belonging to the specified policy.&lt;/p>
&lt;p>qos-dscp-marking-rule-show Show information about the given qos dscp marking rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-update Update the given QoS DSCP marking rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-create Create a qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-delete Delete a given qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-list List all qos minimum bandwidth rules belonging to the specified policy.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-show Show information about the given qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-update Update the given qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-policy-create Create a qos policy.&lt;/p>
&lt;p>qos-policy-delete Delete a given qos policy.&lt;/p>
&lt;p>qos-policy-list List QoS policies that belong to a given tenant connection.&lt;/p>
&lt;p>qos-policy-show Show information of a given qos policy.&lt;/p>
&lt;p>qos-policy-update Update a given qos policy.&lt;/p>
&lt;p>quota-default-show Show default quotas for a given tenant.&lt;/p>
&lt;p>quota-delete Delete defined quotas of a given tenant.&lt;/p>
&lt;p>quota-list List quotas of all tenants who have non-default quota values.&lt;/p>
&lt;p>quota-show Show quotas for a given tenant.&lt;/p>
&lt;p>quota-update Update a given tenant&amp;rsquo;s quotas.&lt;/p>
&lt;p>rbac-create Create a RBAC policy for a given tenant.&lt;/p>
&lt;p>rbac-delete Delete a RBAC policy.&lt;/p>
&lt;p>rbac-list List RBAC policies that belong to a given tenant.&lt;/p>
&lt;p>rbac-show Show information of a given RBAC policy.&lt;/p>
&lt;p>rbac-update Update RBAC policy for given tenant.&lt;/p>
&lt;p>router-create Create a router for a given tenant.&lt;/p>
&lt;p>router-delete Delete a given router.&lt;/p>
&lt;p>router-gateway-clear Remove an external network gateway from a router.&lt;/p>
&lt;p>router-gateway-set Set the external network gateway for a router.&lt;/p>
&lt;p>router-interface-add Add an internal network interface to a router.&lt;/p>
&lt;p>router-interface-delete Remove an internal network interface from a router.&lt;/p>
&lt;p>router-list List routers that belong to a given tenant.&lt;/p>
&lt;p>router-list-on-l3-agent List the routers on a L3 agent.&lt;/p>
&lt;p>router-port-list List ports that belong to a given tenant, with specified router.&lt;/p>
&lt;p>router-show Show information of a given router.&lt;/p>
&lt;p>router-update Update router&amp;rsquo;s information.&lt;/p>
&lt;p>security-group-create Create a security group.&lt;/p>
&lt;p>security-group-delete Delete a given security group.&lt;/p>
&lt;p>security-group-list List security groups that belong to a given tenant.&lt;/p>
&lt;p>security-group-rule-create Create a security group rule.&lt;/p>
&lt;p>security-group-rule-delete Delete a given security group rule.&lt;/p>
&lt;p>security-group-rule-list List security group rules that belong to a given tenant.&lt;/p>
&lt;p>security-group-rule-show Show information of a given security group rule.&lt;/p>
&lt;p>security-group-show Show information of a given security group.&lt;/p>
&lt;p>security-group-update Update a given security group.&lt;/p>
&lt;p>service-provider-list List service providers.&lt;/p>
&lt;p>subnet-create Create a subnet for a given tenant.&lt;/p>
&lt;p>subnet-delete Delete a given subnet.&lt;/p>
&lt;p>subnet-list List subnets that belong to a given tenant.&lt;/p>
&lt;p>subnet-show Show information of a given subnet.&lt;/p>
&lt;p>subnet-update Update subnet&amp;rsquo;s information.&lt;/p>
&lt;p>subnetpool-create Create a subnetpool for a given tenant.&lt;/p>
&lt;p>subnetpool-delete Delete a given subnetpool.&lt;/p>
&lt;p>subnetpool-list List subnetpools that belong to a given tenant.&lt;/p>
&lt;p>subnetpool-show Show information of a given subnetpool.&lt;/p>
&lt;p>subnetpool-update Update subnetpool&amp;rsquo;s information.&lt;/p>
&lt;p>tag-add Add a tag into the resource.&lt;/p>
&lt;p>tag-remove Remove a tag on the resource.&lt;/p>
&lt;p>tag-replace Replace all tags on the resource.&lt;/p>
&lt;p>vpn-endpoint-group-create Create a VPN endpoint group.&lt;/p>
&lt;p>vpn-endpoint-group-delete Delete a given VPN endpoint group.&lt;/p>
&lt;p>vpn-endpoint-group-list List VPN endpoint groups that belong to a given tenant.&lt;/p>
&lt;p>vpn-endpoint-group-show Show a specific VPN endpoint group.&lt;/p>
&lt;p>vpn-endpoint-group-update Update a given VPN endpoint group.&lt;/p>
&lt;p>vpn-ikepolicy-create Create an IKE policy.&lt;/p>
&lt;p>vpn-ikepolicy-delete Delete a given IKE policy.&lt;/p>
&lt;p>vpn-ikepolicy-list List IKE policies that belong to a tenant.&lt;/p>
&lt;p>vpn-ikepolicy-show Show information of a given IKE policy.&lt;/p>
&lt;p>vpn-ikepolicy-update Update a given IKE policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-create Create an IPsec policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-delete Delete a given IPsec policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-list List IPsec policies that belong to a given tenant connection.&lt;/p>
&lt;p>vpn-ipsecpolicy-show Show information of a given IPsec policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-update Update a given IPsec policy.&lt;/p>
&lt;p>vpn-service-create Create a VPN service.&lt;/p>
&lt;p>vpn-service-delete Delete a given VPN service.&lt;/p>
&lt;p>vpn-service-list List VPN service configurations that belong to a given tenant.&lt;/p>
&lt;p>vpn-service-show Show information of a given VPN service.&lt;/p>
&lt;p>vpn-service-update Update a given VPN service.&lt;/p></description></item><item><title>Docs: 3.1.RKE(Rancher Kubernetes Engine) 介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.1.rkerancher-kubernetes-engine-%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.1.rkerancher-kubernetes-engine-%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;h1 id="rke-介绍">RKE 介绍&lt;/h1>
&lt;p>官方文档：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://rancher.com/docs/rke/latest/en/">https://rancher.com/docs/rke/latest/en/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://rancher2.docs.rancher.cn/docs/rke/_index/">https://rancher2.docs.rancher.cn/docs/rke/_index/&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Rancher Kubernetes Engine(RKE)，是经过 CNCF 认证的 Kubernetes 发行版，完全在 Docker 容器内运行。它适用于裸机和虚拟机。RKE 解决了安装复杂性的问题，这是 Kubernetes 社区中的常见问题。借助 RKE，Kubernetes 的安装和操作既简单又自动化，并且完全独立于所运行的操作系统和平台。只要服务器可以运行受支持的 Docker 版本，就可以使用 RKE 部署和运行 Kubernetes。&lt;/p>
&lt;p>使用 rke 工具，仅需通过一个 yaml 的配置文件以及 docker 环境，即可启动一个功能完全的 kubernetes 集群。其中所有系统组件(包括 kubelet)都是以容器的方式运行的。通过 Rancher 创建的 kubernetes 集群，就是 RKE 集群。&lt;/p>
&lt;h2 id="rke-集群与原生-k8s-集群的区别">RKE 集群与原生 K8S 集群的区别&lt;/h2>
&lt;p>RKE 与 sealos 实现高可用的方式类似。不同点是 RKE 集群的 node 节点是通过 ngxin 来连接 API Server。&lt;/p>
&lt;h1 id="rke-集群部署">RKE 集群部署&lt;/h1>
&lt;p>参考：RKE 部署与清理&lt;/p>
&lt;ol>
&lt;li>
&lt;p>下载 rke 二进制文件。(在 github 上下载 rke 命令行工具)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建集群配置文件。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>RKE 默认使用名为 cluster.yml 的集群配置文件来确定集群中应该包含哪些节点以及如何部署 Kubernetes。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>下面是一个单节点 cluster.yml 文件示例，&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>cat &amp;gt; cluster.yml &amp;lt;&amp;lt;
# 指定要部署集群的节点信息
nodes:
# 指定该节点的IP
- address: 1.2.3.4
# 指定部署集群时，所使用的用户
user: ubuntu
# 指定该集群的角色，controlplane运行k8s主要组件，etcd运行etcd，worker运行用户创建的非k8s主要组件的pod。
role:
- controlplane # 对应 k8s master 节点
- etcd
- worker # 对应 k8s node 节点
EOF
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>在 cluster.yml 执行** rke up** 命令&lt;/li>
&lt;/ol>
&lt;h2 id="rke-清理">RKE 清理&lt;/h2>
&lt;p>参考：&lt;a href="https://rancher.com/docs/rke/latest/en/managing-clusters/">https://rancher.com/docs/rke/latest/en/managing-clusters/&lt;/a>。中文：&lt;a href="https://rancher2.docs.rancher.cn/docs/rke/managing-clusters/_index">https://rancher2.docs.rancher.cn/docs/rke/managing-clusters/_index&lt;/a>&lt;/p>
&lt;p>在 cluster.yaml 文件所在目录&lt;/p>
&lt;h1 id="rke-配置">RKE 配置&lt;/h1>
&lt;p>RKE 默认通过一个名为 cluster.yml 的文件配置集群参数。可以通过 &amp;ndash;config 选项来指定其他的 yaml 格式的文件&lt;/p>
&lt;p>cluster.yml #&lt;/p></description></item><item><title>Docs: 3.3.RKE 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.3.rke-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.3.rke-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description/></item><item><title>Docs: 3.4.RKE yaml 示例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.4.rke-yaml-%E7%A4%BA%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.4.rke-yaml-%E7%A4%BA%E4%BE%8B/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;pre>&lt;code># Cluster Config
#
answers: {}
docker_root_dir: /var/lib/docker
enable_cluster_alerting: false
enable_cluster_monitoring: false
enable_network_policy: false
local_cluster_auth_endpoint:
enabled: true
name: kubernetes-test
#
# Rancher Config
#
rancher_kubernetes_engine_config:
addon_job_timeout: 30
authentication:
strategy: x509|webhook
authorization: {}
bastion_host:
ssh_agent_auth: false
cloud_provider: {}
dns:
linear_autoscaler_params:
cores_per_replica: 128
max: 0
min: 1
nodes_per_replica: 4
prevent_single_point_failure: true
node_selector: null
nodelocal:
ip_address: ''
node_selector: null
update_strategy:
rolling_update: {}
reversecidrs: null
stubdomains: null
update_strategy:
rolling_update: {}
upstreamnameservers: null
ignore_docker_version: true
#
# # 当前仅支持nginx的ingress
# # 设置`provider: none`禁用ingress控制器
# # 通过node_selector可以指定在某些节点上运行ingress控制器，例如:
# provider: nginx
# node_selector:
# app: ingress
#
ingress:
provider: nginx
kubernetes_version: v1.18.6-rancher1-1
monitoring:
provider: metrics-server
replicas: 1
#
# # 如果您在AWS上使用calico
#
# network:
# plugin: calico
# calico_network_provider:
# cloud_provider: aws
#
# # 指定flannel网络接口
#
# network:
# plugin: flannel
# flannel_network_provider:
# iface: eth1
#
# # 指定canal网络插件的flannel网络接口
#
# network:
# plugin: canal
# canal_network_provider:
# iface: eth1
#
network:
mtu: 0
options:
flannel_backend_type: vxlan
plugin: flannel
restore:
restore: false
#
# # 自定义服务参数，仅适用于Linux环境
# services:
# kube-api:
# service_cluster_ip_range: 10.43.0.0/16
# extra_args:
# watch-cache: true
# kube-controller:
# cluster_cidr: 10.42.0.0/16
# service_cluster_ip_range: 10.43.0.0/16
# extra_args:
# # 修改每个节点子网大小(cidr掩码长度)，默认为24，可用IP为254个；23，可用IP为510个；22，可用IP为1022个；
# node-cidr-mask-size: 24
# # 控制器定时与节点通信以检查通信是否正常，周期默认5s
# node-monitor-period: '5s'
# # 当节点通信失败后，再等一段时间kubernetes判定节点为notready状态。这个时间段必须是kubelet的nodeStatusUpdateFrequency(默认10s)的N倍，其中N表示允许kubelet同步节点状态的重试次数，默认40s。
# node-monitor-grace-period: '20s'
# # 再持续通信失败一段时间后，kubernetes判定节点为unhealthy状态，默认1m0s。
# node-startup-grace-period: '30s'
# # 再持续失联一段时间，kubernetes开始迁移失联节点的Pod，默认5m0s。
# pod-eviction-timeout: '1m'
# kubelet:
# cluster_domain: cluster.local
# cluster_dns_server: 10.43.0.10
# # 扩展变量
# extra_args:
# # 与apiserver会话时的并发数，默认是10
# kube-api-burst: '30'
# # 与apiserver会话时的 QPS,默认是5
# kube-api-qps: '15'
# # 修改节点最大Pod数量
# max-pods: '250'
# # secrets和configmaps同步到Pod需要的时间，默认一分钟
# sync-frequency: '3s'
# # kubelet默认一次拉取一个镜像，设置为false可以同时拉取多个镜像，前提是存储驱动要为overlay2，对应的Docker也需要增加下载并发数
# serialize-image-pulls: false
# # 拉取镜像的最大并发数，registry-burst不能超过registry-qps ，仅当registry-qps大于0(零)时生效，(默认10)。如果registry-qps为0则不限制(默认5)。
# registry-burst: '10'
# registry-qps: '0'
# # 以下配置用于配置节点资源预留和限制
# cgroups-per-qos: 'true'
# cgroup-driver: cgroupfs
# # 以下两个参数指明为相关服务预留多少资源，仅用于调度，不做实际限制
# system-reserved: 'memory=300Mi'
# kube-reserved: 'memory=2Gi'
# enforce-node-allocatable: 'pods'
# # 硬驱逐阈值，当节点上的可用资源少于这个值时，就会触发强制驱逐。强制驱逐会强制kill掉POD，不会等POD自动退出。
# eviction-hard: 'memory.available&amp;lt;300Mi,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;15%,nodefs.inodesFree&amp;lt;5%'
# # 软驱逐阈值
# ## 以下四个参数配套使用，当节点上的可用资源少于这个值时但大于硬驱逐阈值时候，会等待eviction-soft-grace-period设置的时长；
# ## 等待中每10s检查一次，当最后一次检查还触发了软驱逐阈值就会开始驱逐，驱逐不会直接Kill POD，先发送停止信号给POD，然后等待eviction-max-pod-grace-period设置的时长；
# ## 在eviction-max-pod-grace-period时长之后，如果POD还未退出则发送强制kill POD
# eviction-soft: 'memory.available&amp;lt;500Mi,nodefs.available&amp;lt;50%,imagefs.available&amp;lt;50%,nodefs.inodesFree&amp;lt;10%'
# eviction-soft-grace-period: 'memory.available=1m30s'
# eviction-max-pod-grace-period: '30'
# ## 当处于驱逐状态的节点不可调度，当节点恢复正常状态后
# eviction-pressure-transition-period: '5m0s'
# extra_binds:
# - &amp;quot;/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins&amp;quot;
# - &amp;quot;/etc/iscsi:/etc/iscsi&amp;quot;
# - &amp;quot;/sbin/iscsiadm:/sbin/iscsiadm&amp;quot;
# etcd:
# # 修改空间配额为$((4*1024*1024*1024))，默认2G,最大8G
# extra_args:
# quota-backend-bytes: '4294967296'
# auto-compaction-retention: 240 #(单位小时)
# kubeproxy:
# extra_args:
# # 默认使用iptables进行数据转发
# proxy-mode: &amp;quot;&amp;quot; # 如果要启用ipvs，则此处设置为`ipvs`
#
services:
etcd:
backup_config:
enabled: true
interval_hours: 12
retention: 6
safe_timestamp: false
creation: 12h
extra_args:
election-timeout: '5000'
heartbeat-interval: '500'
listen-metrics-urls: 'http://0.0.0.0:2381'
gid: 0
retention: 72h
snapshot: false
uid: 0
kube-api:
always_pull_images: false
pod_security_policy: false
service_node_port_range: 30000-60000
kube-controller: {}
kubelet:
extra_args:
cgroup-driver: systemd
fail_swap_on: false
generate_serving_certificate: false
kubeproxy:
extra_args:
proxy-mode: ipvs
scheduler: {}
ssh_agent_auth: false
upgrade_strategy:
drain: false
max_unavailable_controlplane: '1'
max_unavailable_worker: 10%%
node_drain_input:
delete_local_data: false
force: false
grace_period: -1
ignore_daemon_sets: true
timeout: 120
scheduled_cluster_scan:
enabled: false
scan_config:
cis_scan_config:
debug_master: false
debug_worker: false
override_benchmark_version: rke-cis-1.4
profile: permissive
schedule_config:
cron_schedule: 0 0 * * *
retention: 24
&lt;/code>&lt;/pre></description></item><item><title>Docs: 3.5.RKE 部署时输出的信息</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.5.rke-%E9%83%A8%E7%BD%B2%E6%97%B6%E8%BE%93%E5%87%BA%E7%9A%84%E4%BF%A1%E6%81%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.5.rke-%E9%83%A8%E7%BD%B2%E6%97%B6%E8%BE%93%E5%87%BA%E7%9A%84%E4%BF%A1%E6%81%AF/</guid><description>
&lt;h1 id="rke-部署时输出的信息">RKE 部署时输出的信息&lt;/h1>
&lt;pre>&lt;code>[root@chinese-test rke]# rke up
INFO[0000] Running RKE version: v1.1.4
INFO[0000] Initiating Kubernetes cluster
INFO[0000] [dialer] Setup tunnel for host [172.38.40.214]
INFO[0000] Checking if container [cluster-state-deployer] is running on host [172.38.40.214], try #1
INFO[0000] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0002] Starting container [cluster-state-deployer] on host [172.38.40.214], try #1
INFO[0003] [state] Successfully started [cluster-state-deployer] container on host [172.38.40.214]
INFO[0003] [certificates] Generating CA kubernetes certificates
INFO[0004] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates
INFO[0004] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates
INFO[0004] [certificates] Generating Kubernetes API server certificates
INFO[0005] [certificates] Generating Service account token key
INFO[0005] [certificates] Generating Kube Controller certificates
INFO[0005] [certificates] Generating Kube Scheduler certificates
INFO[0005] [certificates] Generating Kube Proxy certificates
INFO[0006] [certificates] Generating Node certificate
INFO[0006] [certificates] Generating admin certificates and kubeconfig
INFO[0006] [certificates] Generating Kubernetes API server proxy client certificates
INFO[0006] [certificates] Generating kube-etcd-172-38-40-214 certificate and key
INFO[0006] Successfully Deployed state file at [./cluster.rkestate]
INFO[0006] Building Kubernetes cluster
INFO[0006] [dialer] Setup tunnel for host [172.38.40.214]
INFO[0006] [network] Deploying port listener containers
INFO[0006] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0008] Starting container [rke-etcd-port-listener] on host [172.38.40.214], try #1
INFO[0008] [network] Successfully started [rke-etcd-port-listener] container on host [172.38.40.214]
INFO[0008] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0009] Starting container [rke-cp-port-listener] on host [172.38.40.214], try #1
INFO[0009] [network] Successfully started [rke-cp-port-listener] container on host [172.38.40.214]
INFO[0009] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0010] Starting container [rke-worker-port-listener] on host [172.38.40.214], try #1
INFO[0011] [network] Successfully started [rke-worker-port-listener] container on host [172.38.40.214]
INFO[0011] [network] Port listener containers deployed successfully
INFO[0011] [network] Running control plane -&amp;gt; etcd port checks
INFO[0011] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0011] Starting container [rke-port-checker] on host [172.38.40.214], try #1
INFO[0012] [network] Successfully started [rke-port-checker] container on host [172.38.40.214]
INFO[0012] Removing container [rke-port-checker] on host [172.38.40.214], try #1
INFO[0012] [network] Running control plane -&amp;gt; worker port checks
INFO[0012] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0012] Starting container [rke-port-checker] on host [172.38.40.214], try #1
INFO[0013] [network] Successfully started [rke-port-checker] container on host [172.38.40.214]
INFO[0013] Removing container [rke-port-checker] on host [172.38.40.214], try #1
INFO[0013] [network] Running workers -&amp;gt; control plane port checks
INFO[0013] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0013] Starting container [rke-port-checker] on host [172.38.40.214], try #1
INFO[0014] [network] Successfully started [rke-port-checker] container on host [172.38.40.214]
INFO[0014] Removing container [rke-port-checker] on host [172.38.40.214], try #1
INFO[0014] [network] Checking KubeAPI port Control Plane hosts
INFO[0014] [network] Removing port listener containers
INFO[0014] Removing container [rke-etcd-port-listener] on host [172.38.40.214], try #1
INFO[0015] [remove/rke-etcd-port-listener] Successfully removed container on host [172.38.40.214]
INFO[0015] Removing container [rke-cp-port-listener] on host [172.38.40.214], try #1
INFO[0015] [remove/rke-cp-port-listener] Successfully removed container on host [172.38.40.214]
INFO[0015] Removing container [rke-worker-port-listener] on host [172.38.40.214], try #1
INFO[0016] [remove/rke-worker-port-listener] Successfully removed container on host [172.38.40.214]
INFO[0016] [network] Port listener containers removed successfully
INFO[0016] [certificates] Deploying kubernetes certificates to Cluster nodes
INFO[0016] Checking if container [cert-deployer] is running on host [172.38.40.214], try #1
INFO[0016] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0016] Starting container [cert-deployer] on host [172.38.40.214], try #1
INFO[0017] Checking if container [cert-deployer] is running on host [172.38.40.214], try #1
INFO[0022] Checking if container [cert-deployer] is running on host [172.38.40.214], try #1
INFO[0022] Removing container [cert-deployer] on host [172.38.40.214], try #1
INFO[0022] [reconcile] Rebuilding and updating local kube config
INFO[0022] Successfully Deployed local admin kubeconfig at [./kube_config_cluster.yml]
INFO[0022] [certificates] Successfully deployed kubernetes certificates to Cluster nodes
INFO[0022] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [172.38.40.214]
INFO[0022] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0022] Starting container [file-deployer] on host [172.38.40.214], try #1
INFO[0023] Successfully started [file-deployer] container on host [172.38.40.214]
INFO[0023] Waiting for [file-deployer] container to exit on host [172.38.40.214]
INFO[0023] Waiting for [file-deployer] container to exit on host [172.38.40.214]
INFO[0023] Removing container [file-deployer] on host [172.38.40.214], try #1
INFO[0023] [remove/file-deployer] Successfully removed container on host [172.38.40.214]
INFO[0023] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes
INFO[0023] [reconcile] Reconciling cluster state
INFO[0023] [reconcile] This is newly generated cluster
INFO[0023] Pre-pulling kubernetes images
INFO[0023] Image [rancher/hyperkube:v1.18.6-rancher1] exists on host [172.38.40.214]
INFO[0023] Kubernetes images pulled successfully
INFO[0023] [etcd] Building up etcd plane..
INFO[0023] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0024] Starting container [etcd-fix-perm] on host [172.38.40.214], try #1
INFO[0024] Successfully started [etcd-fix-perm] container on host [172.38.40.214]
INFO[0024] Waiting for [etcd-fix-perm] container to exit on host [172.38.40.214]
INFO[0024] Waiting for [etcd-fix-perm] container to exit on host [172.38.40.214]
INFO[0024] Container [etcd-fix-perm] is still running on host [172.38.40.214]
INFO[0025] Waiting for [etcd-fix-perm] container to exit on host [172.38.40.214]
INFO[0025] Removing container [etcd-fix-perm] on host [172.38.40.214], try #1
INFO[0025] [remove/etcd-fix-perm] Successfully removed container on host [172.38.40.214]
INFO[0025] Image [rancher/coreos-etcd:v3.4.3-rancher1] exists on host [172.38.40.214]
INFO[0026] Starting container [etcd] on host [172.38.40.214], try #1
INFO[0026] [etcd] Successfully started [etcd] container on host [172.38.40.214]
INFO[0026] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [172.38.40.214]
INFO[0026] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0027] Starting container [etcd-rolling-snapshots] on host [172.38.40.214], try #1
INFO[0027] [etcd] Successfully started [etcd-rolling-snapshots] container on host [172.38.40.214]
INFO[0032] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0033] Starting container [rke-bundle-cert] on host [172.38.40.214], try #1
INFO[0034] [certificates] Successfully started [rke-bundle-cert] container on host [172.38.40.214]
INFO[0034] Waiting for [rke-bundle-cert] container to exit on host [172.38.40.214]
INFO[0034] Container [rke-bundle-cert] is still running on host [172.38.40.214]
INFO[0035] Waiting for [rke-bundle-cert] container to exit on host [172.38.40.214]
INFO[0035] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [172.38.40.214]
INFO[0035] Removing container [rke-bundle-cert] on host [172.38.40.214], try #1
INFO[0035] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0035] Starting container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0036] [etcd] Successfully started [rke-log-linker] container on host [172.38.40.214]
INFO[0036] Removing container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0036] [remove/rke-log-linker] Successfully removed container on host [172.38.40.214]
INFO[0036] [etcd] Successfully started etcd plane.. Checking etcd cluster health
INFO[0036] [controlplane] Building up Controller Plane..
INFO[0036] Checking if container [service-sidekick] is running on host [172.38.40.214], try #1
INFO[0036] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0037] Image [rancher/hyperkube:v1.18.6-rancher1] exists on host [172.38.40.214]
INFO[0037] Starting container [kube-apiserver] on host [172.38.40.214], try #1
INFO[0038] [controlplane] Successfully started [kube-apiserver] container on host [172.38.40.214]
INFO[0038] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [172.38.40.214]
INFO[0048] [healthcheck] service [kube-apiserver] on host [172.38.40.214] is healthy
INFO[0048] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0049] Starting container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0050] [controlplane] Successfully started [rke-log-linker] container on host [172.38.40.214]
INFO[0050] Removing container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0050] [remove/rke-log-linker] Successfully removed container on host [172.38.40.214]
INFO[0050] Image [rancher/hyperkube:v1.18.6-rancher1] exists on host [172.38.40.214]
INFO[0050] Starting container [kube-controller-manager] on host [172.38.40.214], try #1
INFO[0050] [controlplane] Successfully started [kube-controller-manager] container on host [172.38.40.214]
INFO[0050] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [172.38.40.214]
INFO[0056] [healthcheck] service [kube-controller-manager] on host [172.38.40.214] is healthy
INFO[0056] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0056] Starting container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0057] [controlplane] Successfully started [rke-log-linker] container on host [172.38.40.214]
INFO[0057] Removing container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0058] [remove/rke-log-linker] Successfully removed container on host [172.38.40.214]
INFO[0058] Image [rancher/hyperkube:v1.18.6-rancher1] exists on host [172.38.40.214]
INFO[0058] Starting container [kube-scheduler] on host [172.38.40.214], try #1
INFO[0059] [controlplane] Successfully started [kube-scheduler] container on host [172.38.40.214]
INFO[0059] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [172.38.40.214]
INFO[0064] [healthcheck] service [kube-scheduler] on host [172.38.40.214] is healthy
INFO[0064] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0064] Starting container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0065] [controlplane] Successfully started [rke-log-linker] container on host [172.38.40.214]
INFO[0066] Removing container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0066] [remove/rke-log-linker] Successfully removed container on host [172.38.40.214]
INFO[0066] [controlplane] Successfully started Controller Plane..
INFO[0066] [authz] Creating rke-job-deployer ServiceAccount
INFO[0066] [authz] rke-job-deployer ServiceAccount created successfully
INFO[0066] [authz] Creating system:node ClusterRoleBinding
INFO[0066] [authz] system:node ClusterRoleBinding created successfully
INFO[0066] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding
INFO[0066] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully
INFO[0066] Successfully Deployed state file at [./cluster.rkestate]
INFO[0066] [state] Saving full cluster state to Kubernetes
INFO[0066] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state
INFO[0066] [worker] Building up Worker Plane..
INFO[0066] Checking if container [service-sidekick] is running on host [172.38.40.214], try #1
INFO[0066] [sidekick] Sidekick container already created on host [172.38.40.214]
INFO[0066] Image [rancher/hyperkube:v1.18.6-rancher1] exists on host [172.38.40.214]
INFO[0066] Starting container [kubelet] on host [172.38.40.214], try #1
INFO[0067] [worker] Successfully started [kubelet] container on host [172.38.40.214]
INFO[0067] [healthcheck] Start Healthcheck on service [kubelet] on host [172.38.40.214]
INFO[0077] [healthcheck] service [kubelet] on host [172.38.40.214] is healthy
INFO[0077] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0078] Starting container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0079] [worker] Successfully started [rke-log-linker] container on host [172.38.40.214]
INFO[0080] Removing container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0080] [remove/rke-log-linker] Successfully removed container on host [172.38.40.214]
INFO[0080] Image [rancher/hyperkube:v1.18.6-rancher1] exists on host [172.38.40.214]
INFO[0080] Starting container [kube-proxy] on host [172.38.40.214], try #1
INFO[0081] [worker] Successfully started [kube-proxy] container on host [172.38.40.214]
INFO[0081] [healthcheck] Start Healthcheck on service [kube-proxy] on host [172.38.40.214]
INFO[0086] [healthcheck] service [kube-proxy] on host [172.38.40.214] is healthy
INFO[0086] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0087] Starting container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0088] [worker] Successfully started [rke-log-linker] container on host [172.38.40.214]
INFO[0088] Removing container [rke-log-linker] on host [172.38.40.214], try #1
INFO[0088] [remove/rke-log-linker] Successfully removed container on host [172.38.40.214]
INFO[0088] [worker] Successfully started Worker Plane..
INFO[0088] Image [rancher/rke-tools:v0.1.59] exists on host [172.38.40.214]
INFO[0089] Starting container [rke-log-cleaner] on host [172.38.40.214], try #1
INFO[0090] [cleanup] Successfully started [rke-log-cleaner] container on host [172.38.40.214]
INFO[0090] Removing container [rke-log-cleaner] on host [172.38.40.214], try #1
INFO[0091] [remove/rke-log-cleaner] Successfully removed container on host [172.38.40.214]
INFO[0091] [sync] Syncing nodes Labels and Taints
INFO[0091] [sync] Successfully synced nodes Labels and Taints
INFO[0091] [network] Setting up network plugin: flannel
INFO[0091] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes
INFO[0091] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes
INFO[0091] [addons] Executing deploy job rke-network-plugin
INFO[0101] [addons] Setting up coredns
INFO[0101] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes
INFO[0101] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes
INFO[0101] [addons] Executing deploy job rke-coredns-addon
INFO[0112] [addons] CoreDNS deployed successfully
INFO[0112] [dns] DNS provider coredns deployed successfully
INFO[0112] [addons] Setting up Metrics Server
INFO[0112] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes
INFO[0112] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes
INFO[0112] [addons] Executing deploy job rke-metrics-addon
INFO[0122] [addons] Metrics Server deployed successfully
INFO[0122] [ingress] Setting up nginx ingress controller
INFO[0122] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes
INFO[0122] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes
INFO[0122] [addons] Executing deploy job rke-ingress-controller
INFO[0137] [ingress] ingress controller nginx deployed successfully
INFO[0137] [addons] Setting up user addons
INFO[0137] [addons] no user addons defined
INFO[0137] Finished building Kubernetes cluster successfully
&lt;/code>&lt;/pre></description></item><item><title>Docs: 3.Admission Controllers 准入控制器</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/3.admission-controllers-%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/3.admission-controllers-%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6%E5%99%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">官方文档，参考-API 访问控制-使用准入控制器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">官方文档，参考-API 访问控制-动态准入控制&lt;/a>&lt;/li>
&lt;li>理清 Kubernetes 中的准入控制（Admission Controller）&lt;a href="https://mp.weixin.qq.com/s/nwKO2dmfvXf6dFw-y-vU7A">https://mp.weixin.qq.com/s/nwKO2dmfvXf6dFw-y-vU7A&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/GdxSWFEyM1PYP30f3a-FCQ">公众号-运维开发故事，开发一个禁止删除 namespace 的控制器&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>准入控制器是&lt;strong>一段代码&lt;/strong>，它会在&lt;strong>请求通过 认证和授权 之后&lt;/strong>、&lt;strong>对象被持久化之前&lt;/strong>，拦截到达 API Server 的请求。&lt;/p>
&lt;p>由于准入控制器是拦截 API Server 中最后的持久化逻辑，所以现阶段 准入控制器在 kube-apiserver 自身中实现，一共由于两大类准入控制器&lt;/p>
&lt;ul>
&lt;li>&lt;strong>静态准入控制器&lt;/strong> # kube-apiserver 默认内置的准入控制器，可以从 &lt;a href="#Yd0ra">准入控制器列表&lt;/a> 查看。
&lt;ul>
&lt;li>比如 istio 为每个 Pod 注入 Sidecar 的功能，就是通过 Mutating 准入控制器实现的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>动态准入控制器&lt;/strong> # 以 Webhook 的形式运行，请求到达 kube-apiserver 后将会根据 &lt;code>ValidatingWebhookConfiguration&lt;/code> 资源的定义，将请求转发给自己编写的控制器来处理后再返回给 kube-apiserver。
&lt;ul>
&lt;li>比如我们编写了一个程序：如果请求是删除 namespace 资源的话，则进制删除。那么将这个程序部署到 k8s 时，再创建一个 ValidatingWebhookConfiguration 对面，以告诉 API Server 将请求转发给咱编写的程序。此时咱的程序处理请求后，会告诉 API Server 是否可以继续执行这个请求。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>准入控制器通常用以执行 &lt;strong>Validating(验证)&lt;/strong>、&lt;strong>Mutating(变更)&lt;/strong> 操作&lt;/p>
&lt;ul>
&lt;li>验证操作即验证该请求是否可以执行&lt;/li>
&lt;li>变更操作就是类似于 Istion，将创建的 Pod 中加入其他字段或减少某些字段。&lt;/li>
&lt;/ul>
&lt;h2 id="目前版本中默认启用的准入控制器">目前版本中，默认启用的准入控制器&lt;/h2>
&lt;p>CertificateApproval
CertificateSigning
CertificateSubjectRestriction
DefaultIngressClass
DefaultStorageClass
DefaultTolerationSeconds
LimitRanger
MutatingAdmissionWebhook
NamespaceLifecycle
PersistentVolumeClaimResize
PodSecurity
Priority
ResourceQuota
RuntimeClass
ServiceAccount
StorageObjectInUseProtection
TaintNodesByCondition
ValidatingAdmissionWebhook&lt;/p>
&lt;h1 id="准入控制器列表">准入控制器列表&lt;/h1>
&lt;h2 id="mutatingadmissionwebhook">MutatingAdmissionWebhook&lt;/h2>
&lt;p>当执行变更操作时，通过 Webhook 调用动态准入控制器&lt;/p>
&lt;h2 id="validatingadmissionwebhook">ValidatingAdmissionWebhook&lt;/h2>
&lt;p>当执行验证操作时，通过 Webhook 调用动态准入控制器&lt;/p></description></item><item><title>Docs: 3.Pod 集群最小的工作单元</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/3.pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/">官方文档,概念-工作负载-Pods&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Pod 是 Kubernetes 的&lt;strong>最小工作单元&lt;/strong>，是一个逻辑概念。Kubernetes 真正处理的，还是通过 CRI 在 HostOS 上的 Namespace 和 Cgroups。所谓的 Pod 只是一组共享了某些资源的 Container，这一组 Container 共享同一个 NetworkNamespace 并且可以声明共享同一个 Volume。&lt;/p>
&lt;p>&lt;strong>Infrastructure(基础设施，简称 Infra) 容器&lt;/strong>：为了保证多个 Container 在共享的时候是对等关系(一般情况可以先启动 ContainerA，再启动 ContainerB 并共享 ContainerA 的资源，但是这样 A 与 B 不对等，A 是必须先启动才能启动 B)，需要一个中间 Container，即 &lt;strong>Infra 容器&lt;/strong>，Infra 容器 永远是第一个被创建的 Container，想要共享某些资源的 Container 则通过加入 NetworkNamespce 的方式，与 Infra 容器 关联在一起。效果如图
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iogldt/1616119861463-06b2877d-519d-43a9-a6e4-6fc743d6ee30.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>Infra 类型的 Container 使用一个名为 pause 的镜像，就像它的名字一样，永远处于&amp;quot;暂停&amp;quot;状态&lt;/li>
&lt;li>Kubernetes 为每个 Pod 都附属了 k8s.gcr.ip/pause，这个 Container 只接管 Pod 的网络信息，业务 Container 通过加入网络 Container 的网络来实现网络共享。此容器随着 pod 创建而创建，随着 Pod 删除而删除。该容器是对业务 pod 的命名空间的解析。Note：如果想要更改该容器，则需要在 kubelet 中使用&amp;ndash;pod-infra-container-image 参数进行配置&lt;/li>
&lt;li>与 Infra 关联的 Container 的所有 NetworkNamespace 必然是完全一样的。&lt;/li>
&lt;li>该链接有一种详细的解释&lt;/li>
&lt;li>Note：对于 kubelet 来说，这种容器称为 Sandbox。每次 kubelet 创建 pod 时，首先创建的也是 sandbox(i.e.pause)容器&lt;/li>
&lt;/ol>
&lt;p>一组 Container 共享 Infra 的 NetworkNamespace 意味着：&lt;/p>
&lt;ol>
&lt;li>它们可以直接使用 localhost 进行通信&lt;/li>
&lt;li>它们看到的网络设备跟 Infra 容器中看到的完全一样&lt;/li>
&lt;li>一个 Pod 只能有有一个 IP 地址，就是这个 Pod 的 NetworkNamespace 对应的 IP 地址&lt;/li>
&lt;li>Pod 的生命周期只跟 Infra 容器一致，同与 Infra 关联的所有 Container 无关&lt;/li>
&lt;li>Pod 中的所有 Container 的进出流量都是通过 Infra 容器完成的，所以网络插件不必关心除 Infra 以外的容器的启动与否，只需关注如何配置 Pod(也就是 Infra 容器的 NetworkNamespace)即可&lt;/li>
&lt;/ol>
&lt;p>每个 Pod 包含一个或多个容器。Pod 中的容器会作为一个整体被 Master 调度到一个 Node 上运行。&lt;/p>
&lt;p>如果把 Pod 想象成一台&amp;quot;服务器&amp;quot;，把 Container 想象成运行在这台服务器中的&amp;quot;用户程序&amp;quot;&lt;/p>
&lt;ol>
&lt;li>凡是调度、网络、存储、以及安全相关的字段，基本都是 Pod 级别的，比如：&lt;/li>
&lt;li>配置这台&amp;quot;服务器&amp;quot;的网卡(Pod 的网络)、配置这台“服务器”的磁盘(Pod 的存储，Volume)、配置这台”服务器“的防火墙(Pod 中的安全)、配置这台”服务器“运行在哪个机房(Pod 的调度)&lt;/li>
&lt;li>凡是资源配额、所要使用的 port、探测该进程是否存活或就绪、需要使用&amp;quot;服务器&amp;quot;上的哪块 Volume 等等字段，都是 Container 级别的&lt;/li>
&lt;/ol>
&lt;h2 id="kubernetes-引入-pod-主要基于下面两个目的">Kubernetes 引入 Pod 主要基于下面两个目的&lt;/h2>
&lt;ol>
&lt;li>可管理性。
&lt;ol>
&lt;li>有些 Container 天生就是需要紧密联系，一起工作。Pod 提供了比容器更高层次的抽象，将它们封装到一个部署单元中。Kubernetes 以 Pod 为最小单位进行调度、扩展、共享资源、管理生命周期。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>通信和资源共享。
&lt;ol>
&lt;li>Pod 中的所有 Container 使用同一个网络 namespace，即相同的 IP 地址和 Port 空间。它们可以直接用 localhost 通信。同样的，这些 Container 可以共享存储，当 Kubernetes 挂载 volume 到 Pod，本质上是将 volume 挂载到 Pod 中的每一个 Container。user,mnt,pnt。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>使用户从传统虚拟机环境向容器环境迁移更加平滑，可以把 Pod 想象成 VM，Pod 中的 Container 是 VM 中的进程，甚至可以启动一个 systemd 的 Container&lt;/li>
&lt;li>还可以把 Pod 理解为传统环境中的物理主机&lt;/li>
&lt;/ol>
&lt;h2 id="container-设计模式">Container 设计模式&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iogldt/1616119861478-cf678269-344a-4932-8448-c9eee14a8438.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>sidecar&lt;/strong> #(该英文的解释“跨斗”：一辆小而低的车辆，安装在摩托车旁边，用于载客，就像右图中的样子)，所以该模式就类似于这个，指可以再一个 Pod 中启动一个辅助 Container，来完成一些独立于主进程(主 Container)之外的工作。
&lt;ul>
&lt;li>比如 Container 的日志收集：现在有一个 APP，需要不断把日志文件输出到 Container 的/var/log 目录中。这时，把一个 Pod 里的 Volume 挂载到应用 Container 的/var/log 目录上，然后在 Pod 中同时运行一个 sidecar 的 Container 也声明挂载同一个 Volume 到自己的/var/log 目录上，然后 sidecar 只需要不断得从自己的/var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来即可。&lt;/li>
&lt;li>Istio 项目也是使用 sidecar 模式的 Container 完成微服务治理的原理。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="pod-的类型">Pod 的类型&lt;/h2>
&lt;ul>
&lt;li>动态 Pod：由 k8s 管理，网络组件，监控，等等，这些在 使用 kubeadm 初始化集群后才创建的 Pod 为动态 Pod&lt;/li>
&lt;li>静态 Pod：由 kubelet 直接管理的，在 /etc/kubernetes/manifest/ 目录下的 yaml 文件&lt;/li>
&lt;/ul>
&lt;h1 id="pod-使用方式">Pod 使用方式&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">官方文档，概念-工作敷在-Pod-初始化容器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">官方文档，概念-工作负载-Pod-临时容器&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ul>
&lt;li>运行单一 Container。
&lt;ul>
&lt;li>one-container-per-Pod 是 Kubernetes 最常见的模型，这种情况下，只是将单个 Container 简单封装成 Pod。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>运行多个 Container。
&lt;ul>
&lt;li>这些 Container 联系必须非常紧密，而且需要直接共享资源的应该放到一个 Pod 中(注意：当使用多 Container 的时候，其中一个 Container 要加上 command 的参数，否则其中一个起不来。因为 container 如果不执行某些命令，则启动后会自动结束，详见 docker 说明 1.LXC 与 Docker 入门最佳实践.note 里《Dokcer 的工作模式》章节)&lt;/li>
&lt;li>比如：File Puller 会定期从外部的 Content Manager 中拉取最新的文件，将其存放在共享的 volume 中。Web Server 从 volume 读取文件，响应 Consumer 的请求。这两个容器是紧密协作的，它们一起为 Consumer 提供最新的数据；同时它们也通过 volume 共享数据。所以放到一个 Pod 是合适的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>在 Pod 中，可运行的容器分为三类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ephemeral_container(临时容器)&lt;/strong> # 与 1.23 版本进入 beta，用来调试集群&lt;/li>
&lt;li>&lt;strong>init_container(初始化容器)&lt;/strong> # 在应用容器启动前运行一次就结束的，常用来为容器运行初始化运行环境，比如设置权限等等&lt;/li>
&lt;li>&lt;strong>application_container(应用容器)&lt;/strong> # 真正运行业务的容器。&lt;/li>
&lt;/ul>
&lt;p>这三类容器，可以在 kubelet 代码中找到运行逻辑，详见 [《kubelet 源码解析-PodWorker 模块》](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/Kubernetes%20 开发/源码解析/Kubelet%20 源码/PodWorker%20 模块.md 开发/源码解析/Kubelet 源码/PodWorker 模块.md)&lt;/p>
&lt;h2 id="ephemeral_container临时容器">ephemeral_container(临时容器)&lt;/h2>
&lt;h2 id="init_container初始化容器">init_container(初始化容器)&lt;/h2>
&lt;p>Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。Init 容器在所有容器运行之前执行（run-to-completion），常用来初始化配置。&lt;/p>
&lt;p>如果为一个 Pod 指定了多个 Init 容器，那些容器会按顺序一次运行一个。 每个 Init 容器必须运行成功，下一个才能够运行。 当所有的 Init 容器运行完成时，Kubernetes 初始化 Pod 并像平常一样运行应用容器。&lt;/p>
&lt;p>下面是一个 Init 容器的示例：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">init-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workdir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/usr/share/nginx/html&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># These containers are run during pod initialization&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">initContainers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">install&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">busybox&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">wget&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;-O&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;/work-dir/index.html&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">http://kubernetes.io&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workdir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/work-dir&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">dnsPolicy&lt;/span>: &lt;span style="color:#ae81ff">Default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workdir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">emptyDir&lt;/span>: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>因为 Init 容器具有与应用容器分离的单独镜像，使用 init 容器启动相关代码具有如下优势：&lt;/p>
&lt;ol>
&lt;li>它们可以包含并运行实用工具，出于安全考虑，是不建议在应用容器镜像中包含这些实用工具的。&lt;/li>
&lt;li>它们可以包含使用工具和定制化代码来安装，但是不能出现在应用镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。&lt;/li>
&lt;li>应用镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。&lt;/li>
&lt;li>它们使用 Linux Namespace，所以对应用容器具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用容器不能够访问。&lt;/li>
&lt;li>它们在应用容器启动之前运行完成，然而应用容器并行运行，所以 Init 容器提供了一种简单的方式来阻塞或延迟应用容器的启动，直到满足了一组先决条件。&lt;/li>
&lt;/ol>
&lt;p>Init 容器的资源计算，选择一下两者的较大值：&lt;/p>
&lt;ol>
&lt;li>所有 Init 容器中的资源使用的最大值&lt;/li>
&lt;li>Pod 中所有容器资源使用的总和&lt;/li>
&lt;/ol>
&lt;p>Init 容器的重启策略：&lt;/p>
&lt;ol>
&lt;li>如果 Init 容器执行失败，Pod 设置的 restartPolicy 为 Never，则 pod 将处于 fail 状态。否则 Pod 将一直重新执行每一个 Init 容器直到所有的 Init 容器都成功。&lt;/li>
&lt;li>如果 Pod 异常退出，重新拉取 Pod 后，Init 容器也会被重新执行。所以在 Init 容器中执行的任务，需要保证是幂等的。&lt;/li>
&lt;/ol>
&lt;h2 id="container容器--也称为-application_container应用容器">container(容器) # 也称为 application_container(应用容器)&lt;/h2>
&lt;h1 id="pod-名字的命名规范">Pod 名字的命名规范&lt;/h1>
&lt;p>一般情况都不会直接使用 Pod，而是通过 Controller 来创建。通过 Controller 创建一个 POD 的流程为，以及 POD 名字的命名方式每个对象的命名方式是：子对象的名字 = 父对象名字 + 随机字符串或数字。如图所示，Controller 详见：2.0.Controller：控制器.note
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/iogldt/1616119861434-f2c4735b-c549-40a4-ab70-67217755ed3f.png" alt="">&lt;/p>
&lt;ol>
&lt;li>用户通过 kubectl 创建 Deployment。&lt;/li>
&lt;li>Deployment 创建 ReplicaSet。&lt;/li>
&lt;li>ReplicaSet 创建 Pod。&lt;/li>
&lt;/ol></description></item><item><title>Docs: 4.Controller(控制器)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/4.controller%E6%8E%A7%E5%88%B6%E5%99%A8/4.controller%E6%8E%A7%E5%88%B6%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/4.controller%E6%8E%A7%E5%88%B6%E5%99%A8/4.controller%E6%8E%A7%E5%88%B6%E5%99%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">官方文档,概念-集群架构-控制器&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Controller 是 Kubernetes 的大脑&lt;/p>
&lt;p>在机器人和自动化技术中，控制环是一个控制系统状态不终止的循环。&lt;/p>
&lt;p>比如：房间里的温度自动调节器，当我设置了温度，告诉温度调节器我的期望状态。房间的实际温度是当前状态。温度自动调节器就会让当前状态一直去接近期望状态。&lt;/p>
&lt;p>kubernetes 的 Controller 就是这样一种东西，通过 apiserver 监控集群的期望状态，并致力于将当前状态转变为期望状态。而 controller 是一个更高层次的抽象概念，指代多种具有 controller 功能的资源，比如 deployment、statefulset 等等。&lt;/p>
&lt;p>可以用一段 Go 语言风格的伪代码，来描述这个控制循环：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#960050;background-color:#1e0010">实际状态&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">获取集群中对象&lt;/span> &lt;span style="color:#a6e22e">X&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">的实际状态（&lt;/span>&lt;span style="color:#a6e22e">Actual&lt;/span> &lt;span style="color:#a6e22e">State&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#960050;background-color:#1e0010">期望状态&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">获取集群中对象&lt;/span> &lt;span style="color:#a6e22e">X&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">的期望状态（&lt;/span>&lt;span style="color:#a6e22e">Desired&lt;/span> &lt;span style="color:#a6e22e">State&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">实际状态&lt;/span> &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">期望状态&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#960050;background-color:#1e0010">什么都不做&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#66d9ef">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#960050;background-color:#1e0010">执行编排动作，将实际状态调整为期望状态&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在具体实现中，实际状态一般来自于 kubernetes 集群本身，e.g.kubelet 收集所在节点上容器状态和节点状态。而期望状态，一般来自于用户提交的 YAMl 文件。&lt;/p>
&lt;p>以 Deployment 这种控制器为例，简单描述一下它对控制器模型的实现：&lt;/p>
&lt;ul>
&lt;li>控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态；&lt;/li>
&lt;li>用户提交的 yaml 文件中 Replicas 字段的值就是期望状态(提交的 yaml 也会保存到 etcd 中)；&lt;/li>
&lt;li>控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod 。&lt;/li>
&lt;/ul>
&lt;p>这个对比的操作通常被叫作 &lt;strong>Reconcile(调和)&lt;/strong>。这个调谐的过程，则被称作 &lt;strong>Reconcile Loop(调和循环)&lt;/strong> 或者 &lt;strong>Sync Loop(同步循环)&lt;/strong>。这些词其实都代表一个东西：&lt;strong>控制循环&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>Reconcile 这词儿用的还挺有意思，Reconcile 是调和、使协调一致的概念，就是让具有矛盾的两方和解~这就好比控制器和被控制者，当被控制者不符合控制者预期状态时，这就相当于两者有矛盾了~~Reconcile 就会让矛盾消失
Reconcile 是一个动词，表示一个过程，在 k8s 中还有一个对应的名词 Reconciliation，用以表示“确保系统的实际状态与期望状态一致”&lt;/p>
&lt;/blockquote>
&lt;p>也可以这么说：控制器，使用一种 API 对象管理另一种 API 对象的工具。控制器这个对象本身负责定义被管理对象的期望状态(e.g.deployment 里的 replicas: 2 这个字段)；而被控制对象的定义，则来自于一个“模板”(e.g.deployment 里的 template 字段)。&lt;/p>
&lt;p>可以看到，deployment 资源中 template 字段里的内容跟一个标准的 pod 对象的定义丝毫不差。而所有被这个 deployment 管理的 pod 对象，都是根据 template 字段的内容创建出来的。&lt;/p>
&lt;p>像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。更有意思的是，我们还会看到其他类型的对象模板，比如 Volume 的模板。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/udaias/1617283643337-37faf2ac-fcaa-4f18-a119-c1099af0c765.png" alt="image.png">&lt;/p>
&lt;p>如上图所示，类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。&lt;/p>
&lt;p>这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。&lt;/p>
&lt;p>各种 Controller 是 kubernetes 的大脑，通过 apiserver 监控整个集群的状态，作为集群内部的管理控制中心，负责集群内的 Node、Pod 副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）等资源的管理。比如，当某个 Node 意外宕机时，Controller 会及时发现并执行自动化修复流程，comtroller 会确保集群始终处于预期的工作状态。&lt;/p>
&lt;p>Note：严格来说 Pod 并不是由 controller 直接管理的，而是通过 deployment 之类的 controller 来管理 Pod 或者由 kubelet 来管理 pod。kubelet 自身也具备控制循环的功能。&lt;/p>
&lt;h2 id="用白话来说">用白话来说：&lt;/h2>
&lt;p>kubernetes 有各种各样的资源，每个资源都有其自己的各种定义(比如一个 pod 资源里有 image、name、volumemount 等等)，那么这些定义所能实现的功能，又是谁来决定的呢？答案就是 controller&lt;/p>
&lt;p>资源的定义只是提供了 JSON 格式的数据，至于如何执行这些数据，则是 controller 这个程序的作用。&lt;/p>
&lt;p>例如 deployment 中定义了一个 replicas，该 replicas 有一个值，那么系统是怎么知道 replicas:VALUE 这一串字符串是干什么用的呢，这就是 controller 的作用，controller 会告诉 k8s 集群，这一段字符串的含义是该 deployment 下的 pod 应该具有多少个副本。除了描述该字段的含义以外，还会控制 deployment 下的 pod 向着期望的值来转变，当 pod 的副本数量多余或者少于设定的值时，controller 都会删除或者创建相应的 pod 来满足指定的值。&lt;/p>
&lt;p>而各种 controller 内各个字段的含义，则是靠 kube-controller-manager 这个程序来管理并定义其中各个字段的含义。&lt;/p>
&lt;p>总结：
这就是 kubernetes 的核心思想，编排也是依靠 controller 来实现的。&lt;/p>
&lt;h1 id="控制器的实现">控制器的实现&lt;/h1>
&lt;p>&lt;a href="https://www.yuque.com/go/doc/33166804">&lt;strong>kube-controller-manager&lt;/strong>&lt;/a> 是实现 Kubernetes 控制器功能的程序。&lt;/p>
&lt;p>可以在 &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller">Controller 代码&lt;/a>中，找到所有可用的控制器。所以，从逻辑上来说，每个控制器都应该是一个单独的进程，但是为了降低复杂性，这些控制器都被编译到同一个执行文件中，并在同一个进程中运行。所以，这些控制器的集合体，就称为 **Controller Manager(控制器管理器)，**所以实现控制器功能的组件的名称就叫 &lt;strong>kube-controller-manager&lt;/strong>。&lt;/p>
&lt;h1 id="各种控制器介绍">各种控制器介绍&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/">https://kubernetes.io/docs/concepts/workloads/controllers/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Controller 包括以下这些：不同的 controller 管理不同的资源。&lt;/p>
&lt;ul>
&lt;li>Replication Controller #副本控制器，主要用于保障 pod 的副本数量(副本就是复制)
&lt;ul>
&lt;li>Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Node Controller #节点控制器，用于控制节点相关信息，比如该节点的 cidr 信息等。&lt;/li>
&lt;li>CronJob Controller&lt;/li>
&lt;li>Daemon Controller&lt;/li>
&lt;li>Deployment Controller #部署控制器，管理 Deployment 资源&lt;/li>
&lt;li>Endpoint Controller #端点控制器&lt;/li>
&lt;li>Garbage Collector&lt;/li>
&lt;li>Namespace Controller # 管理 Namespace 资源。&lt;/li>
&lt;li>Job Controller&lt;/li>
&lt;li>Pod AutoScaler #i.e.HPA，用于 pod 自动伸缩&lt;/li>
&lt;li>RelicaSet&lt;/li>
&lt;li>Service Controller&lt;/li>
&lt;li>ServiceAccount Controller #服务账户控制器&lt;/li>
&lt;li>StatefulSet Controller&lt;/li>
&lt;li>Volume Controller # 卷控制器&lt;/li>
&lt;li>Resource quota Controller&lt;/li>
&lt;/ul>
&lt;h2 id="deployment--管理-pod-的多个副本并确保-pod-按照期望的状态运行">Deployment # 管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行&lt;/h2>
&lt;blockquote>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>比 ReplicaSet 还多了几个功能支持滚动更新和回滚，支持声明式配置的功能&lt;/p>
&lt;ol>
&lt;li>ReplicaSet 实现了 Pod 的多副本管理，一般情况 ReplicaSet 管理无状态的 Pod。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。 ReplicaSet 一般由下面三个组件组成，当启动 Pod 时，按照如下顺序进行创建 Pod
&lt;ol>
&lt;li>用户期望的副本数，即希望创建多少个与该 Pod 一样的副本，进行统一管理，创建在不同 Node 上以实现负载均衡以及高可用，这些 Pod 的功能与服务一模一样&lt;/li>
&lt;li>标签选择器，以便选定由自己管理和控制的 Pod，如果标签选择器选择的 pod 数量少于用户期望的副本数，则使用 Pod 资源模板&lt;/li>
&lt;li>Pod 资源模板，新建 Pod 资源&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Deployment 的滚动更新可以进行定制化配置，比如仅更新一个 pod 到新版，观察其稳定情况几天后决定是否更新现网其余 pod，使用的方式详见 kubectl set image 命令的内容&lt;/li>
&lt;/ol>
&lt;h2 id="statefulset-statefulset-表示对-pod-设定一致性身份consistent-identities">StatefulSet #StatefulSet 表示对 Pod 设定一致性身份(consistent identities)&lt;/h2>
&lt;blockquote>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Identities 的定义：&lt;/p>
&lt;ol>
&lt;li>Network:单一稳定的主机名和 DNS&lt;/li>
&lt;li>Storage: 具有同样的一些 VolumeClaims. StatefulSet 保证给定的网络标识将始终映射到相同的存储标识。&lt;/li>
&lt;/ol>
&lt;p>StatefulSet 能够保证 Pod 的每个副本在整个生命周期中名称是不变的，一般情况 StatefulSet 管理有状态的 Pod。而其他 Controller 不提供这个功能，当某个 Pod 发生故障需要删除并重新启动时，Pod 的名称会发生变化。同时 StatefulSet 会保证副本按照固定的顺序启动、更新或者删除。&lt;/p>
&lt;ol>
&lt;li>当需要保持 Pod 不变，比如数据库类型的服务，则使用该类型&lt;/li>
&lt;li>当一个有状态的应用失败需要重启的时候，比如主从结构的数据库，其中需要进行的操作时非常复杂的，这时候需要通过一个脚本来定义 statefulset 的功能，如果以后的研发人员可以基于 kubernetes 来开发有状态的应用(比如数据库等)，让新的应用在开发的时候就想着要放在云上运行，这种云原生的应用，则可以让 statefulset 更好的支持他&lt;/li>
&lt;/ol>
&lt;h2 id="daemonset--用于每个-node-最多只运行一个-pod-副本的场景正如其名称所揭示的daemonset-通常用于运行-daemon">DaemonSet # 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。&lt;/h2>
&lt;blockquote>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>当一个服务可以想象成守护进程的时候，使用该类型&lt;/p>
&lt;h2 id="cronjob-与-job--用于运行结束就删除的应用而其他-controller-中的-pod-通常是长期持续运行">CronJob 与 Job # 用于运行结束就删除的应用。而其他 Controller 中的 Pod 通常是长期持续运行。&lt;/h2>
&lt;blockquote>
&lt;p>官方文档：&lt;/p>
&lt;ul>
&lt;li>job 文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">https://kubernetes.io/docs/concepts/workloads/controllers/job/&lt;/a>&lt;/li>
&lt;li>cronjob 文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Job 对象用来创建一个或多个 pod，并确保指定数量的 pod 成功终止。pod 成功完成后，job 将跟踪完成的情况。当达到指定的成功完成次数时，Job 就完成了。删除 job 将清除其创建的 pod 。&lt;/p>
&lt;p>job 对象可以 通过 cronjob 对象来创建。cronjob 可以按照重复计划创建 job。cronjob 与 linux 中 crontab 的用法一样。可以根据指定的时间间隔定时运行任务。&lt;/p>
&lt;p>其实，cronjob 与 job 的关系，有点像 deployment 之类的控制器与 pod 之间的关系。在编写 yaml 时，deployment 需要指定 pod 的 template，而 cronjob 则需要指定 jobTemplate。&lt;/p>
&lt;p>CronJob 的行为可以通过其 yaml 中的 spec 字段中的相关字段来指定
官方文档：&lt;a href="https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#writing-a-cron-job-spec">https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#writing-a-cron-job-spec&lt;/a>&lt;/p>
&lt;ul>
&lt;li>concurrencyPolicy &lt;!-- raw HTML omitted --> # 指定如何处理作业的并发执行。有效值为：-“允许”（默认）：允许 CronJobs 同时运行； -“禁止”：禁止并行运行，如果前一个运行尚未完成，则跳过下一个运行； -“替换”：取消当前正在运行的作业，并将其替换为新作业&lt;/li>
&lt;li>failedJobsHistoryLimit &lt;!-- raw HTML omitted --> # 执行失败的 job(Completed 状态的 pod) 的保留数为 INTEGER。默认为 1&lt;/li>
&lt;li>jobTemplate &lt;!-- raw HTML omitted --> # -required- 指定在执行 CronJob 时将创建的作业。类似于 deployment 下的。&lt;/li>
&lt;li>schedule &lt;!-- raw HTML omitted --> # -required- Cron 格式的日程表，请参阅&lt;a href="https://en.wikipedia.org/wiki/Cron">https://en.wikipedia.org/wiki/Cron&lt;/a>。&lt;/li>
&lt;li>startingDeadlineSeconds &lt;!-- raw HTML omitted --> # 如果由于任何原因错过了计划的时间，则以秒为单位的可选截止日期，用于启动作业。错过的工作执行将被视为失败的工作。&lt;/li>
&lt;li>successfulJobsHistoryLimit &lt;!-- raw HTML omitted --> # 执行成功的 job(Completed 状态的 pod) 的保留数为 INTEGER。默认为 3&lt;/li>
&lt;li>suspend &lt;!-- raw HTML omitted --> # 此标志告诉控制器暂停随后的执行，不适用于已经开始的执行。默认为 false。&lt;/li>
&lt;/ul>
&lt;h2 id="garbage-collection垃圾收集器">Garbage Collection(垃圾收集器)&lt;/h2>
&lt;blockquote>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Kubernetes 垃圾收集器的作用是删除某些对象。曾经有一个所有者，但不再有一个所有者。&lt;/p>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33166462">Garbage Collection 垃圾收集器&lt;/a>&lt;/p>
&lt;h2 id="hpa--horizontal-pod-autoscaler">HPA # Horizontal Pod Autoscaler&lt;/h2>
&lt;blockquote>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: 5.Scheduling(调度)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.scheduling%E8%B0%83%E5%BA%A6/5.scheduling%E8%B0%83%E5%BA%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.scheduling%E8%B0%83%E5%BA%A6/5.scheduling%E8%B0%83%E5%BA%A6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/">官方文档,概念-调度、抢占与驱逐&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Scheduling(调度)&lt;/strong> 是一个行为，用来让 Pod 匹配到 Node，以便 Node 上的 Kubelet 可以运行这些 Pod。如果没有调度系统，Kubernetes 集群就不知道 Pod 应该运行在哪里。这种调度的概念，与 Linux 中调度任务来使用 CPU 是一个意思。可以看看 &lt;a href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8.%E9%80%9A%E7%94%A8%E6%8A%80%E6%9C%AF/%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%B2%BE%E8%A6%81.md">调度系统设计精要&lt;/a> 文章，调度是在 IT 行业中，很多程序都很重要的概念。&lt;/p>
&lt;p>与 Scheduling(调度) 伴生的，还有 &lt;strong>Preemption(抢占)&lt;/strong> 与 &lt;strong>Eviction(驱逐)&lt;/strong> 两个概念。顾名思义：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Preemption(抢占)&lt;/strong> 是指终止优先级较低的 Pod 的行为，以便优先级较高的 Pod 可以在节点上调度。
&lt;ul>
&lt;li>抢占行为通常发生在资源不足时，当一个新 Pod 需要调度，但是资源不足，那么就可能需要抢占优先级低的 Pod，这个低优先级的 Pod 将会被驱逐，以便让优先级高的 Pod 运行在节点上。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eviction(驱逐)&lt;/strong> 是指终止节点上一个或多个 Pod 的行为。&lt;/li>
&lt;/ul>
&lt;p>由 抢占 与 驱逐 两个行为，还引申出了 &lt;strong>Pod Disruption(中断)&lt;/strong> 的概念。&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod Disruption(中断)&lt;/a> 是指节点上的 Pod 自愿或者非资源终止运行的行为。&lt;/p>
&lt;ul>
&lt;li>自愿中断是由应用程序所有者或者集群管理故意启动的(比如.维护节点前手动驱逐 Pod)&lt;/li>
&lt;li>非自愿中断是无意的，可能由不可避免的问题触发(比如.节点资源耗尽或意外删除)&lt;/li>
&lt;/ul></description></item><item><title>Docs: 7.API 访问控制</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/security/controlling-access/">官方文档,概念-安全-Kubernetes API 的访问控制&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>认证用于身份验证，授权用于权限检查，准入控制机制用于补充授权机制&lt;/strong>&lt;/p>
&lt;p>客户端与服务端的概念：谁向谁发请求，前者就是客户端，所在在这里，客户端与服务端没有绝对。一个服务既可以是客户端也可以是服务端，kubectl 在控制集群需要给 apiservice 发送 get，creat，delete 等指令的时候，kubectl 就是 apiservice 的客户端；而 apiservice 需要往 etcd 写入数据的时候，apiservice 就是 etcd 的客户端。&lt;/p>
&lt;p>当客户端向服务端发起请求的时候，服务端需要对客户端进行认证以便确认客户端身份是否可以接入；接入后再进行授权检查，检查该身份的请求是否可以在服务端执行。所以后面介绍的 认证 与 授权 是相辅相成，不可分隔，创建完认证之后，需要为这个认证信息进行授权，才是一套完整的鉴权机制&lt;/p>
&lt;blockquote>
&lt;p>比如现在有这么一个场景，张三要去商场买酱油。当张三到达商场后，保安人员首先要对张三进行认证，确认张三这个人可以进入商场；然后张三到达货柜拿走酱油去结账，收银人员进行授权检查，核验张三是否有权力购买酱油。&lt;/p>
&lt;/blockquote>
&lt;p>在 kubernetes 集群中，就是类似张三买酱油的场景~~~各个组件与资源对象之间的互相访问，在大多数时候，都需要进行认证与授权的检查。&lt;/p>
&lt;p>API Server 是集群的入口，不管是对资源对象的增删改查，还是访问集群中的某些对象，不可避免得只能与 API Server 交互，虽然在访问某些管理组件的 https 端口时，也需要进行认证，但是这种访问是属于基本的 https 访问。所以，在与其说是 k8s 的认证与授权，不如说是 kubernetes API 的访问控制。因为不管是从外部(kubeclt 等)、还是内部(controller-manager、某个 pod 访问集群资源)，都逃不开与 kubernetes API，也就是 api-server 这个组件的交互。毕竟 kubernetes API 是集群的唯一入口&lt;del>就算是在集群内部署的 pod，如果想要访问集群内的资源，也逃不开 kubernetes API&lt;/del>&lt;/p>
&lt;p>当然，使用 curl 命令来访问 controller、scheduler 时、或者 etcd 互相交互，都属于 认证与授权 的概念范畴~只不过这种情况不占大多数，所以就不再单独讨论了。这些认证授权方式与 API 的认证授权类似。&lt;/p>
&lt;h1 id="kubernetes-api-访问控制">Kubernetes API 访问控制&lt;/h1>
&lt;p>我们可以通过 kubectl、客户端库、发送 REST 请求 这几种方法访问 &lt;a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">Kubernetes API&lt;/a>。 [人类用户(User Account)](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/7.API%20 访问控制/1.Authenticating(认证)/User%20Account%20 详解.md 访问控制/1.Authenticating(认证)/User Account 详解.md) 和 K[ubernetes 的 Service Account](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/7.API%20 访问控制/1.Authenticating(认证)/Service%20Account%20 详解.md 访问控制/1.Authenticating(认证)/Service Account 详解.md) 都可以被授权进行 API 访问。 请求到达 API Server 后会经过几个阶段，具体如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cvkvyz/1616118854890-e2e31942-d6ea-40a7-83d8-816abb4c136a.jpeg" alt="">&lt;/p>
&lt;h2 id="传输层安全">传输层安全&lt;/h2>
&lt;p>在典型的 Kubernetes 集群中，API 通过 443 端口提供服务。 API 服务器会提供一份证书。 该证书一般是私有 CA 自签名的，当然，也可以基于公信的 CA 公钥基础设施签名。&lt;/p>
&lt;p>如果集群使用私有证书颁发机构，需要在客户端的  &lt;code>~/.kube/config&lt;/code>  文件中提供该 CA 证书的副本， 以便在客户端使用程序访问 API 时，可以信任该连接并确认该连接没有被拦截。&lt;/p>
&lt;h2 id="认证">认证&lt;/h2>
&lt;p>一旦 TLS 连接建立，HTTP 请求就进入到了认证的步骤。即图中的步骤 1 。 集群创建脚本或集群管理员会为 API 服务器配置一个或多个认证模块。 更具体的认证相关的描述详见&lt;a href="https://kubernetes.io/docs/admin/authentication/">这里&lt;/a>。&lt;/p>
&lt;p>认证步骤的输入是整个 HTTP 请求，但这里通常只是检查请求头和 / 或客户端证书。&lt;/p>
&lt;p>认证模块支持客户端证书，密码和 Plain Tokens， Bootstrap Tokens，以及 JWT Tokens（用于服务账户）。&lt;/p>
&lt;p>（管理员）可以同时设置多种认证模块，在设置了多个认证模块的情况下，每个模块会依次尝试认证， 直到其中一个认证成功。&lt;/p>
&lt;p>在 GCE 平台中，客户端证书，密码和 Plain Tokens，Bootstrap Tokens，以及 JWT Tokens 同时被启用。&lt;/p>
&lt;p>如果请求认证失败，则请求被拒绝，返回 401 状态码。 如果认证成功，则被认证为具体的 username，该用户名可供随后的步骤中使用。一些认证模块还提供了用户的组成员关系，另一些则没有。&lt;/p>
&lt;p>尽管 Kubernetes 使用“用户名”来进行访问控制和请求记录，但它实际上并没有 user 对象，也不存储用户名称或其他相关信息。&lt;/p>
&lt;h2 id="授权">授权&lt;/h2>
&lt;p>当请求被认证为来自某个特定的用户后，该请求需要被授权。 即图中的步骤 2 。&lt;/p>
&lt;p>请求须包含请求者的用户名，请求动作，以及该动作影响的对象。 如果存在相应策略，声明该用户具有进行相应操作的权限，则该请求会被授权。&lt;/p>
&lt;p>例如，如果 Bob 有如下策略，那么他只能够读取 projectCaribou 命名空间下的 pod 资源：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;abac.authorization.kubernetes.io/v1beta1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Policy&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;spec&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;user&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;bob&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;projectCaribou&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;pods&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;readonly&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果 Bob 发起以下请求，那么请求能够通过授权，因为 Bob 被允许访问 projectCaribou 命名空间下的对象：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;authorization.k8s.io/v1beta1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;SubjectAccessReview&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;spec&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resourceAttributes&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;projectCaribou&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;get&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;unicorn.example.org&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;pods&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果 Bob 对 projectCaribou 命名空间下的对象发起一个写（create 或者 update）请求，那么它的授权会被拒绝。 如果 Bob 请求读取 （get）其他命名空间，例如 projectFish 下的对象，其授权也会被拒绝。&lt;/p>
&lt;p>Kubernetes 的授权要求使用通用的 REST 属性与现有的组织或云服务提供商的访问控制系统进行交互。 采用 REST 格式是必要的，因为除 Kubernetes 外，这些访问控制系统还可能与其他的 API 进行交互。&lt;/p>
&lt;p>Kubernetes 支持多种授权模块，例如 ABAC 模式，RBAC 模式和 Webhook 模式。 管理员创建集群时，会配置 API 服务器应用的授权模块。 如果多种授权模式同时被启用，Kubernetes 将检查所有模块，如果其中一种通过授权，则请求授权通过。 如果所有的模块全部拒绝，则请求被拒绝（HTTP 状态码 403）。&lt;/p>
&lt;p>要了解更多的 Kubernetes 授权相关信息，包括使用授权模块创建策略的具体说明等，可参考&lt;a href="https://kubernetes.io/docs/admin/authorization">授权概述&lt;/a>。&lt;/p>
&lt;h2 id="准入控制">准入控制&lt;/h2>
&lt;p>准入控制模块是能够修改或拒绝请求的软件模块。 作为授权模块的补充，准入控制模块会访问被创建或更新的对象的内容。 它们作用于对象的创建，删除，更新和连接（proxy）阶段，但不包括对象的读取。&lt;/p>
&lt;p>可以同时配置多个准入控制器，它们会按顺序依次被调用。&lt;/p>
&lt;p>即图中的步骤 3 。&lt;/p>
&lt;p>与认证和授权模块不同的是，如果任一个准入控制器拒绝请求，那么整个请求会立即被拒绝。&lt;/p>
&lt;p>除了拒绝请求外，准入控制器还可以为对象设置复杂的默认值。&lt;/p>
&lt;p>可用的准入控制模块描述 &lt;a href="https://kubernetes.io/docs/admin/admission-controllers/">如下&lt;/a>。&lt;/p>
&lt;p>一旦请求通过所有准入控制器，将使用对应 API 对象的验证流程对其进行验证，然后写入对象存储 （如步骤 4）。&lt;/p>
&lt;h1 id="api-的端口和-ip">API 的端口和 IP&lt;/h1>
&lt;p>上述讨论适用于发送请求到 API 服务器的安全端口（典型情况）。
实际上 API 服务器可以通过两个端口提供服务，默认情况下，API 服务器在 2 个端口上提供 HTTP 服务：&lt;/p>
&lt;ul>
&lt;li>Localhost Port:
&lt;ul>
&lt;li>用于测试和启动，以及管理节点的其他组件(scheduler, controller-manager)与 API 的交互&lt;/li>
&lt;li>没有 TLS&lt;/li>
&lt;li>默认值为 8080，可以通过 API Server 的 &lt;code>--insecure-port&lt;/code> 命令行标志来修改。&lt;/li>
&lt;li>默认的 IP 地址为 localhost，可以通过 API Server 的 &lt;code>--insecure-bind-address&lt;/code> 命令行标志来修改。&lt;/li>
&lt;li>请求会 &lt;strong>绕过&lt;/strong> 认证和鉴权模块。&lt;/li>
&lt;li>请求会被准入控制模块处理。&lt;/li>
&lt;li>其访问需要主机访问的权限。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Secure Port:
&lt;ul>
&lt;li>尽可能使用该端口访问&lt;/li>
&lt;li>应用 TLS。 可以通过 API Server 的 &lt;code>--tls-cert-file&lt;/code> 设置证书， &lt;code>--tls-private-key-file&lt;/code> 设置私钥。&lt;/li>
&lt;li>默认值为 6443，可以通过 API Server 的 &lt;code>--secure-port&lt;/code> 命令行标志来修改。&lt;/li>
&lt;li>默认 IP 是首个非本地的网络接口地址，可以通过 API Server 的 &lt;code>--bind-address&lt;/code> 命令行标志来修改。&lt;/li>
&lt;li>请求会经过认证和鉴权模块处理。&lt;/li>
&lt;li>请求会被准入控制模块处理。&lt;/li>
&lt;li>要求认证和授权模块正常运行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 8.Kubernetes 网络</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/8.kubernetes-%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/8.kubernetes-%E7%BD%91%E7%BB%9C/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">官方文档,概念-集群管理-集群网络&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>kubernetes 的整体网络分为以下三类&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Node IP(各节点网络) # &lt;/p>
&lt;/li>
&lt;li>
&lt;p>Cluster IP(Service 网络) # 虚拟的，在 Netfilter 结构上，就是主机上 iptables 规则中的地址&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Pod IP(Pod 网络) # &lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>网络是 Kubernetes 的核心部分，Kubernetes 中有下面几个点需要互相通信&lt;/p>
&lt;ol>
&lt;li>
&lt;p>同一个 Pod 内的多个容器间通信，通过各容器的 lo 通信&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Pod 之间的通信，Pod IP&amp;lt;&amp;ndash;&amp;gt;Pod IP&lt;/p>
&lt;ol>
&lt;li>overlay 叠加网络转发二层报文，通过隧道方式转发三层报文&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Pod 与 Service 之间的通信，Pod IP&amp;lt;&amp;ndash;&amp;gt;Cluster IP。&lt;a href="https://www.yuque.com/go/doc/33165265">详见 Service 章节&lt;/a>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Service 与集群外部客户端的通信。&lt;a href="https://www.yuque.com/go/doc/33165265">详见 Service 章节&lt;/a>。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Kubernetes 的宗旨就是在应用之间共享机器。 通常来说，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间 去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。&lt;/p>
&lt;p>动态分配端口也会给系统带来很多复杂度 - 每个应用都需要设置一个端口的参数， 而 API 服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。 与其去解决这些问题，Kubernetes 选择了其他不同的方法。&lt;/p>
&lt;h2 id="kubernetes-网络模型">Kubernetes 网络模型&lt;/h2>
&lt;p>每一个 Pod 都有它自己的 IP 地址，这就意味着你不需要显式地在每个 Pod 之间创建链接， 你几乎不需要处理容器端口到主机端口之间的映射。 这将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、 负载均衡、应用配置和迁移的角度来看，Pod 可以被视作虚拟机或者物理主机。&lt;/p>
&lt;p>Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信&lt;/p>
&lt;/li>
&lt;li>
&lt;p>节点上的代理（比如：系统守护进程、kubelet） 可以和节点上的所有 Pod 通信&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>备注：仅针对那些支持 Pods 在主机网络中运行的平台(比如：Linux) ：&lt;/p>
&lt;ul>
&lt;li>那些运行在节点的主机网络里的 Pod 可以不通过 NAT 和所有节点上的 Pod 通信&lt;/li>
&lt;/ul>
&lt;p>这个模型不仅不复杂，而且还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容， 如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP ， 这样就可以和其他的虚拟机进行通信，这是基本相同的模型。&lt;/p>
&lt;p>Kubernetes 的 IP 地址存在于 Pod 范围内 - 容器分享它们的网络命名空间 - 包括它们的 IP 地址。 这就意味着 Pod 内的容器都可以通过 localhost 到达各个端口。 这也意味着 Pod 内的容器都需要相互协调端口的使用，但是这和虚拟机中的进程似乎没有什么不同， 这也被称为“一个 Pod 一个 IP” 模型。&lt;/p>
&lt;p>如何实现这一点是正在使用的容器运行时的特定信息。&lt;/p>
&lt;p>也可以在 node 本身通过端口去请求你的 Pod （称之为主机端口）， 但这是一个很特殊的操作。转发方式如何实现也是容器运行时的细节。 Pod 自己并不知道这些主机端口是否存在。&lt;/p>
&lt;h1 id="network-plugin网络插件--实现-kubernetes-网络模型的方式">Network Plugin(网络插件) — 实现 Kubernetes 网络模型的方式&lt;/h1>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/&lt;/a>&lt;/p>
&lt;p>Kubernetes 中的网络插件有几种类型：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>CNI 插件： 遵守 appc/CNI 规约，为互操作性设计。详见：CNI 介绍&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubenet 插件：使用 bridge 和 host-local CNI 插件实现了基本的 cbr0&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 9.Kubernetes 存储</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/9.kubernetes-%E5%AD%98%E5%82%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/9.kubernetes-%E5%AD%98%E5%82%A8/9.kubernetes-%E5%AD%98%E5%82%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/">官方文档,概念-存储&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/hNR5XkMeZbDVInUOX_5MAg">公众号-CNCF，卷扩展现在是个稳定特性&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/EBghRVRQvnPSTf4YdCkp2w">公众号-CNCF，存储容量跟踪在 Kubernetes1.24 中正式 GA&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 Container 中的文件在磁盘上是临时存储的(这与 Docker 一样，容器删除后，容器内的文件也随着删除)，这给 Container 中运行的需要持久化存储的应用程序带来了很多问题。&lt;/p>
&lt;ul>
&lt;li>第一，当 Container 崩溃时，kubelet 会重启它，但是文件都将丢失并且 Container 以最干净的状态启动&lt;/li>
&lt;li>第二，当在 Pod 中运行多个 Container 的时候，这些 Container 需要共享文件以实现功能。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Volume(卷)&lt;/strong> 就是为了解决上面两种情况出现的。&lt;/p>
&lt;p>从本质上讲，Volume(卷) 只是一个包含一些数据目录，Pod 中 Container 可以访问这个目录。至于该目录是如何形成的是由所使用的 Volume 类型决定的。这个 Volume 的类型可以是：host 的内存，host 的文件，host 的目录，nfs、glusterfs、甚至是云厂商所提供的各种类型的存储&lt;/p>
&lt;p>&lt;strong>可以说，Kubernetes 存储功能的基础，就是 Volume(卷)。&lt;/strong>&lt;/p>
&lt;p>Volume 功能详解见&lt;a href="https://www.yuque.com/go/doc/33164035">单独章节&lt;/a>。&lt;/p>
&lt;h2 id="与-docker-中的-volume-的概念比较">与 Docker 中的 Volume 的概念比较&lt;/h2>
&lt;p>Kubernetse 为什么不直接复用 Docker 中的 Volume，而是要自己实现呢?~&lt;/p>
&lt;p>Kubernetes Volume 和 Docker Volume 概念相似，但是又有不同的地方，Kubernetes Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume 中的数据也不会丢失。当 Pod 被删除时，Volume 才会被清理。并且数据是否丢失取决于 Volume 的具体类型，比如 emptyDir 类型的 Volume 数据会丢失，而持久化类型的数据则不会丢失。另外 Kubernetes 提供了将近 20 种 Volume 类型。&lt;/p>
&lt;h1 id="volume-的实现-volume-plugins卷插件">Volume 的实现-Volume Plugins(卷插件)&lt;/h1>
&lt;p>&lt;strong>Volume Plugins(卷插件)&lt;/strong> 是实现 Kubernetes 存储功能的方式。说白了，&lt;strong>Volume Plugins 就是用来实现 Volume 功能的&lt;/strong>。一共可以分为两类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>In-Tree(树内)&lt;/strong> # 代码逻辑在 K8S 官方仓库中；表示源码是放在 Kubernetes 内部的(常见的 NFS、cephfs 等)，和 Kubernetes 一起发布、管理与迭代，缺点是迭代速度慢、灵活性差；&lt;/li>
&lt;li>&lt;strong>Out-of-Tree(树外)&lt;/strong> # 代码逻辑在 K8s 官方仓库之外，实现与 K8s 代码的解耦；表示代码独立于 Kubernetes，它是由存储提供商实现的，目前主要有 **Flexvolume **或 **CSI **两种实现机制，可以根据存储类型实现不同的存储插件&lt;/li>
&lt;/ul>
&lt;h2 id="in-tree树内">In-Tree(树内)&lt;/h2>
&lt;p>&lt;strong>In-Tree 类型的卷插件的代码 与 Kubernetes 代码 在一起&lt;/strong>。比如 emptyDir、hostPath、ConfigMap、PVC 等等类型的 Volume，凡是在&lt;a href="https://www.yuque.com/go/doc/33164035">官方文档的卷类型&lt;/a>中的都属于 In-Tree 类型的卷插件。所以 卷插件类型 也可以说是 卷类型。&lt;/p>
&lt;h2 id="out-of-tree树外">Out-of-Tree(树外)&lt;/h2>
&lt;p>&lt;strong>OUt-of-Tree 类型的卷插件代码 与 Kubernetes 代码 不在一起&lt;/strong>。这种类型的插件，可以让存储供应商创建自定义的存储插件而无需将他们添加到 Kubernetes 代码仓库。&lt;/p>
&lt;p>这类卷插件的实现方式分两位 **CSI **和 **FlexVolume **两类。都允许独立于 Kubernetes 代码库开发卷插件，并作为 Pod 部署在 Kubernetes 集群中。&lt;/p>
&lt;h3 id="csi--container-storage-interface容器存储接口">CSI # Container Storage Interface(容器存储接口)&lt;/h3>
&lt;p>CSI 为容器编排系统定义标准规范，以将任意存储系统暴露给它们的容器。
&lt;a href="https://www.yuque.com/go/doc/33164056">详见 CSI 章节&lt;/a>&lt;/p>
&lt;h3 id="flexvolume">FlexVolume&lt;/h3>
&lt;p>FlexVolume 是一个自 1.2 版本（在 CSI 之前）以来在 Kubernetes 中一直存在的树外插件接口。 它使用基于 exec 的模型来与驱动程序对接。 用户必须在每个节点（在某些情况下是主控节点）上的预定义卷插件路径中安装 FlexVolume 驱动程序可执行文件。
Pod 通过 &lt;code>flexvolume&lt;/code> 树内插件与 Flexvolume 驱动程序交互。 更多详情请参考 &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md">FlexVolume&lt;/a> 示例。&lt;/p>
&lt;h2 id="in-tree-向-csi-迁移">In-Tree 向 CSI 迁移&lt;/h2>
&lt;p>从 Kubernetes &lt;a href="https://kubernetes.io/zh-cn/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/">2021 年 12 月 10 日的博客&lt;/a>中报告了迁移工作的进展。从这里可以看到，Kubernetes 官方希望我们无论使用那哪种 In-Tree 模型的卷插件，都尽早迁移至使用 CSI 驱动的模型。&lt;/p>
&lt;p>容器存储接口旨在帮助 Kubernetes 取代其现有的树内存储驱动机制 ── 特别是供应商的特定插件。自 v1.13 起，Kubernetes 对&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">容器存储接口&lt;/a>的支持工作已达到&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">正式发布阶段&lt;/a>。引入对 CSI 驱动的支持，将使得 Kubernetes 和存储后端技术之间的集成工作更易建立和维护。使用 CSI 驱动可以实现更好的可维护性（驱动作者可以决定自己的发布周期和支持生命周期）、减少出现漏洞的机会（得益于更少的树内代码，出现错误的风险会降低。另外，集群操作员可以只选择集群需要的存储驱动）。&lt;/p>
&lt;p>随着更多的 CSI 驱动诞生并进入生产就绪阶段，Kubernetes 存储特别兴趣组希望所有 Kubernetes 用户都能从 CSI 模型中受益 ── 然而，我们不应破坏与现有存储 API 类型的 API 兼容性。对此，我们给出的解决方案是 CSI 迁移：该功能实现将树内存储 API 翻译成等效的 CSI API，并把操作委托给一个替换的 CSI 驱动来完成。&lt;/p>
&lt;p>CSI 迁移工作使存储后端现有的树内存储插件（如 kubernetes.io/gce-pd 或 kubernetes.io/aws-ebs）能够被相应的 &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI 驱动&lt;/a> 所取代。如果 CSI 迁移功能正确发挥作用，Kubernetes 终端用户应该不会注意到有什么变化。现有的 StorageClass、PersistentVolume 和 PersistentVolumeClaim 对象应继续工作。当 Kubernetes 集群管理员更新集群以启用 CSI 迁移功能时，利用到 PVCs&lt;a href="https://kubernetes.io/zh-cn/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#fn:1">1&lt;/a>（由树内存储插件支持）的现有工作负载将继续像以前一样运作 ── 不过在幕后，Kubernetes 将所有存储管理操作（以前面向树内存储驱动的）交给 CSI 驱动控制。&lt;/p>
&lt;p>举个例子。假设你是 kubernetes.io/gce-pd 用户，在启用 CSI 迁移功能后，你仍然可以使用 kubernetes.io/gce-pd 来配置新卷、挂载现有的 GCE-PD 卷或删除现有卷。所有现有的 API/接口 仍将正常工作 ── 只不过，底层功能调用都将通向 &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI 驱动&lt;/a>，而不是 Kubernetes 的树内存储功能。&lt;/p>
&lt;p>不过，这里面没提到那些 Kubernetes 内置资源类型的卷，比如 ConfigMap、Secret、等等。人们主要需要迁移的是那些类似 glusterfs、cephfs 之类第三方的 In-Tree 类型的卷插件。&lt;/p>
&lt;p>迁移进展文章&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/6nhv2zQIAOAfUJ661YmDsQ">https://mp.weixin.qq.com/s/6nhv2zQIAOAfUJ661YmDsQ&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="kubernetes-存储模型">Kubernetes 存储模型&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/aplkpr/1616117503767-42e19ed6-fbd6-4b5b-bc38-db7e7a699432.jpeg" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Volume Controller&lt;/strong> # K8S 的卷控制器。&lt;a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/volume">代码位置&lt;/a>
&lt;ul>
&lt;li>&lt;strong>PV Controller&lt;/strong> # 负责 PV/PVC 的绑定、生命周期管理，并根据需求进行数据卷的 &lt;strong>Provision/Delete&lt;/strong> 操作。&lt;a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/volume/persistentvolume">代码位置&lt;/a>&lt;/li>
&lt;li>&lt;strong>AD Controller&lt;/strong> # 负责存储设备的** Attach/Detach** 操作，将设备挂载到目标节点。&lt;a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/volume/attachdetach">代码位置&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>**Kubelet：**Kubelet 是在每个 Node 节点上运行的主要 “节点代理”，功能是 Pod 生命周期管理、容器健康检查、容器监控等；
&lt;ul>
&lt;li>&lt;strong>Volume Manager&lt;/strong> # 属于 kubelet 中的组件，管理卷的 **Mount/Unmount **操作、卷设备的格式化的操作.
&lt;ul>
&lt;li>注意：Volume Manager 也可以负责数据卷的 **Attach/Detach **操作，需要配置 kubelet 开启特性&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Volume Plugins&lt;/strong> # 它主要是对上面所有挂载功能的实现。PV Controller、AD Controller、Volume Manager 主要是进行操作的调用，而具体操作则是由 Volume Plugins 实现的。根据源码的位置可将 Volume Plugins 分为 In-Tree 和 Out-of-Tree 两类
&lt;ul>
&lt;li>In-Tree # 与 Kubernetes 代码强耦合的卷插件&lt;/li>
&lt;li>Out-of-Tree # 与 Kubernetes 代码无关的卷插件。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Scheduler&lt;/strong> # 实现对 Pod 的调度能力，会根据一些存储相关的的定义去做存储相关的调度&lt;/li>
&lt;li>其他
&lt;ul>
&lt;li>**External Provioner：**External Provioner 是一种 sidecar 容器，作用是调用 Volume Plugins 中的 CreateVolume 和 DeleteVolume 函数来执行 &lt;strong>Provision/Delet&lt;/strong>e 操作。因为 K8s 的 PV 控制器无法直接调用 Volume Plugins 的相关函数，故由 External Provioner 通过 gRPC 来调用&lt;/li>
&lt;li>**External Attacher：**External Attacher 是一种 sidecar 容器，作用是调用 Volume Plugins 中的 ControllerPublishVolume 和 ControllerUnpublishVolume 函数来执行 &lt;strong>Attach/Detach&lt;/strong> 操作。因为 K8s 的 AD 控制器无法直接调用 Volume Plugins 的相关函数，故由 External Attacher 通过 gRPC 来调用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Annotations 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/ingress/ingress-controller/nginx/annotations-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/ingress/ingress-controller/nginx/annotations-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">官方文档，用户指南-Annotations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>与 ConfigMap 实现配置 Nginx Ingress Controller 运行时行为类似，只不过，Annotations 的方式，是通过在 Ingress 对象的 &lt;code>.metadata.annotations&lt;/code> 字段下的内容实现的。&lt;/p>
&lt;p>同样，&lt;code>.metadata.annotations&lt;/code> 字段下的内容也是由无数的 &lt;strong>Key/Value Pairs(键/值对)&lt;/strong> 组成。很多 **Key **都会对应一个 Nginx 的 [&lt;strong>Directives(指令)&lt;/strong>]([&lt;strong>Directives(指令)&lt;/strong>](Nginx%20 配置详解.md 配置详解.md)ntication](&lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#authentication">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#authentication&lt;/a>) # 认证相关配置&lt;/p>
&lt;p>可以为 Nginx 所代理的后端配置一些简单的认证，比如 用户名/密码&lt;/p>
&lt;p>&lt;strong>nginx.ingress.kubernetes.io/auth-realm: &lt;!-- raw HTML omitted -->&lt;/strong> #&lt;/p>
&lt;p>&lt;strong>nginx.ingress.kubernetes.io/auth-secret: &lt;!-- raw HTML omitted -->&lt;/strong> #&lt;/p>
&lt;p>&lt;strong>nginx.ingress.kubernetes.io/auth-type: &lt;!-- raw HTML omitted -->&lt;/strong> #&lt;/p>
&lt;h2 id="custom-timeoutshttpskubernetesgithubioingress-nginxuser-guidenginx-configurationannotationscustom-timeouts--自定义超时时间">&lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-timeouts">Custom Timeouts&lt;/a> # 自定义超时时间&lt;/h2>
&lt;p>配置与 upstream 中定义的服务器的连接超时时间。
**nginx.ingress.kubernetes.io/proxy-connect-timeout: &amp;lt;&amp;gt; **# 对应 Nginx 的 proxy_connect_timeout 指令
**nginx.ingress.kubernetes.io/proxy-send-timeout: &amp;lt;&amp;gt; **#
&lt;strong>nginx.ingress.kubernetes.io/proxy-read-timeout: &amp;lt;&amp;gt;&lt;/strong> #
&lt;strong>nginx.ingress.kubernetes.io/proxy-next-upstream: &amp;lt;&amp;gt;&lt;/strong> #
&lt;strong>nginx.ingress.kubernetes.io/proxy-next-upstream-timeout: &amp;lt;&amp;gt;&lt;/strong> #
&lt;strong>nginx.ingress.kubernetes.io/proxy-next-upstream-tries: &amp;lt;&amp;gt;&lt;/strong> #
&lt;strong>nginx.ingress.kubernetes.io/proxy-request-buffering: &amp;lt;&amp;gt;&lt;/strong> #&lt;/p>
&lt;h2 id="canaryhttpskubernetesgithubioingress-nginxuser-guidenginx-configurationannotationscanary--金丝雀灰度发布相关配置">&lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary">Canary&lt;/a> # 金丝雀/灰度发布相关配置&lt;/h2>
&lt;p>通过 canary 相关的注释，我们可以实现金丝雀/灰度发布。即.相同的 host，根据不同的规则，将请求转发给不同的后端。
&lt;strong>nginx.ingress.kubernetes.io/canary: &amp;ldquo;&lt;!-- raw HTML omitted -->&amp;rdquo;&lt;/strong> # 是否启用 Canary 功能
其他功能详见 《&lt;a href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes%20%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.Kubernetes%20%E7%BD%91%E7%BB%9C/Ingress/Ingress%20Controller/Nginx/%E5%AE%9E%E7%8E%B0%E5%BA%94%E7%94%A8%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83.md">实现应用灰度发布&lt;/a>》&lt;/p></description></item><item><title>Docs: API Aggregation(聚合) Layer</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E6%89%A9%E5%B1%95/api-aggregation%E8%81%9A%E5%90%88-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E6%89%A9%E5%B1%95/api-aggregation%E8%81%9A%E5%90%88-layer/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考；&lt;/p>
&lt;ul>
&lt;li>官方文档参考：
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>api aggregation 称为 api 聚合。用于扩展 kubernetes 的 API 。如下所示。其中 v1beta1.metrics.k8s.io 是通过 prometheus-adapter 添加的新 API&lt;/p>
&lt;pre>&lt;code>[root@master-1 ~]# kubectl get apiservices.apiregistration.k8s.io
NAME SERVICE AVAILABLE AGE
v1. Local True 163d
.........
v1beta1.metrics.k8s.io monitoring/prometheus-adapter True 120m
v1beta1.networking.k8s.io Local True 163d
......
&lt;/code>&lt;/pre>
&lt;p>聚合出来的 API 会关联到一个指定的 service 上，所有对该 API 发起的请求，都会交由该 service 并转发到其后端的 pod 进行处理。&lt;/p>
&lt;p>下面是一个扩展 API 的样例，其中指定了该 API 所关联的 service&lt;/p>
&lt;pre>&lt;code>apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
name: v1beta1.metrics.k8s.io
spec:
group: metrics.k8s.io
groupPriorityMinimum: 100
insecureSkipTLSVerify: true
service:
name: prometheus-adapter
namespace: monitoring
version: v1beta1
versionPriority: 100
&lt;/code>&lt;/pre>
&lt;p>API Aggregation 的核心功能是动态注册、发现汇总、安全代理。&lt;/p>
&lt;h1 id="配置-api-aggregation">配置 API Aggregation&lt;/h1>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/">https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/&lt;/a>&lt;/p>
&lt;h1 id="应用实例">应用实例&lt;/h1>
&lt;p>可以参考 heapster 和 metrics-server 之间的过渡关系：kubectl top 命令解析.note 这里面谈到了 api 聚合 的用例。&lt;/p></description></item><item><title>Docs: API Server</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/api-server/api-server/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/api-server/api-server/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver">官方文档，概念-概述-Kubernetes 组件-kube-apiserver&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">官方文档，参考-通用组件-kube-apiserver&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**API Server 是实现 kubernetes API 的应用程序，它是 Kubernetes 控制平面的一个组件，用以对外暴露 Kubernetes API。**Kubernetes API Server 验证和配置 API 对象的数据，包括 pod、service、replicationcontroller 等。 API Server 为 REST 操作提供服务，并为集群的共享状态提供前端，所有其他组件通过该前端进行交互。&lt;/p>
&lt;p>如果是通过 kubeadm 安装的 k8s 集群，那么 API Server 的表现形式就是一个名为 **kube-apiserver 的静态 pod。**kube-apiserver 可以水平扩展，i.e.部署多个 kube-apiserver 以实现高可用，应对高并发请求，到达 kube-apiserver 的流量可以在这些实例之间平衡。&lt;/p>
&lt;p>API Server 启动后，默认监听在 6443 端口(http 默认监听在 8080 上)。API Server 是 Kubernetes 集群的前端接口 ，各种客户端工具（CLI 或 UI）以及 Kubernetes 其他组件可以通过它管理集群的各种资源。kubectl 就是 API Server 的客户端程序，实现对 k8s 各种资源的增删改查的功能。各个 node 节点的 kubelet 也通过 master 节点的 API Server 来上报本节点的 Pod 状态。&lt;/p>
&lt;ul>
&lt;li>提供集群管理的 REST 风格 API 接口，包括认证授权、数据校验以及集群状态变更等&lt;/li>
&lt;li>提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才可以直接操作 etcd）&lt;/li>
&lt;/ul>
&lt;h1 id="api-server-的访问方式">API Server 的访问方式：&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/">官方文档，任务-管理集群-使用 Kubernetes API 访问集群&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>注意：&lt;/p>
&lt;ol>
&lt;li>API Server 默认是安全的，在访问时，应使用 https 协议来操作。&lt;/li>
&lt;li>参考 [K8S 认证与授权介绍](7.API%20 访问控制.md 访问控制.md) 文章，学习在访问 API Server 时所遇到的验证问题。&lt;/li>
&lt;/ol>
&lt;h2 id="使用-kubectl-访问-api">使用 kubectl 访问 API&lt;/h2>
&lt;p>现阶段有 kubectl 工具可以实现对 API Server 的访问&lt;/p>
&lt;p>使用 kubectl get &amp;ndash;raw / 命令让 kubectl 不再输出标准格式的数据，而是直接向 api server 请求原始数据&lt;/p>
&lt;h2 id="直接访问-rest-apieg使用-curl浏览器-等方式访问-api">直接访问 REST API(e.g.使用 curl、浏览器 等方式访问 API)&lt;/h2>
&lt;p>kubectl 处理对 API 服务器的定位和身份验证。如果你想通过 http 客户端（如 curl 或 wget，或浏览器）直接访问 REST API，你可以通过多种方式对 API 服务器进行定位和身份验证：&lt;/p>
&lt;ol>
&lt;li>以代理模式运行 kubectl(推荐)。 推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。 使用这种方法无法进行中间人（MITM）攻击。&lt;/li>
&lt;li>另外，你可以直接为 HTTP 客户端提供位置和身份认证。 这适用于被代理混淆的客户端代码。 为防止中间人攻击，你需要将根证书导入浏览器。&lt;/li>
&lt;/ol>
&lt;p>比如 curl &amp;ndash;request DELETE -cacert ${CAPATH} -H &amp;ldquo;Authorization: Bearer ${TOKEN}&amp;rdquo; https://${IP}:6443/api/v1/namespaces/monitoring/pods/prometheus-k8s-0 -k 这样一个请求就可以将集群内 monitoring 空间下的 prometheus-k8s-0 这个 pod 删除&lt;/p>
&lt;h3 id="获取认证所需信息">获取认证所需信息&lt;/h3>
&lt;p>&lt;strong>方法一：使用 kubectl 的配置文件中的证书与私钥&lt;/strong>
想要访问 https 下的内容，首先需要准备证书与私钥或者 ca 与 token 等等。&lt;/p>
&lt;ol>
&lt;li>首先获取 kubeclt 工具配置文件中的证书与私钥
&lt;ol>
&lt;li>cat /etc/kubernetes/admin.conf | grep client-certificate-data | awk &amp;lsquo;{print $2}&amp;rsquo; | base64 -d &amp;gt; /root/certs/admin.crt&lt;/li>
&lt;li>cat /etc/kubernetes/admin.conf | grep client-key-data | awk &amp;lsquo;{print $2}&amp;rsquo; | base64 -d &amp;gt; /root/certs/admin.key&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>确定 CA 文件位置(文件一般在 /etc/kubernetes/pki/ca.crt)
&lt;ol>
&lt;li>CAPATH=/etc/kubernetes/pki/ca.crt&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>确定要访问组件的的 IP
&lt;ol>
&lt;li>IP=172.38.40.212&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>方法二：使用拥有最高权限 ServiceAccount 的 Token 访问 https&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>(可选)创建一个专门存放 SA 的名称空间
&lt;ul>
&lt;li>kubectl create namespace user-sa-manage&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>创建一个 ServiceAccount
&lt;ul>
&lt;li>kubectl create -n user-sa-manage serviceaccount test-admin&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将该 ServiceAccount 绑定到 cluster-admin 这个 clusterrole，以赋予最高权限
&lt;ul>
&lt;li>kubectl create clusterrolebinding test-admin &amp;ndash;clusterrole=cluster-admin &amp;ndash;serviceaccount=user-sa-manage:test-admin&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>将该 ServiceAccount 的 Token 的值注册到变量中
&lt;ul>
&lt;li>TOKEN=$(kubectl get -n user-sa-manage secrets -o jsonpath=&amp;quot;{.items[?(@.metadata.annotations[&amp;lsquo;kubernetes.io/service-account.name&amp;rsquo;]==&amp;lsquo;test-admin&amp;rsquo;)].data.token}&amp;quot;|base64 -d)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>确定 CA 文件位置(文件一般在 /etc/kubernetes/pki/ca.crt)
&lt;ul>
&lt;li>CAPATH=/etc/kubernetes/pki/ca.crt&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>确定要访问组件的的 IP
&lt;ul>
&lt;li>IP=172.38.40.212&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>使用令牌玩转 API
&lt;ul>
&lt;li>curl -k $IP/api -H &amp;ldquo;Authorization: Bearer $TOKEN&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Note：也可以从一个具有权限的 ServiceAccount 下的 secret 获取，可以使用现成的，也可以手动创建。比如下面用 promtheus 自带的 token。&lt;/p>
&lt;ol>
&lt;li>如果权限不足，那么访问的时候会报错，比如权限不够，或者认证不通过等等。报错信息有如下几种
&lt;ol>
&lt;li>no kind is registered for the type v1.Status in scheme &amp;ldquo;k8s.io/kubernetes/pkg/api/legacyscheme/scheme.go:30&amp;rdquo;&lt;/li>
&lt;li>Unauthorized&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>TOKEN=$(kubectl get secrets -n monitoring prometheus-k8s-token-q5hm4 &amp;ndash;template={{.data.token}} | base64 -d)&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>方法三：官方推荐，类似方法二&lt;/strong>
官方文档：&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/">https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 查看所有的集群，因为你的 .kubeconfig 文件中可能包含多个上下文&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl config view -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{&amp;#34;Cluster name\tServer\n&amp;#34;}{range .clusters[*]}{.name}{&amp;#34;\t&amp;#34;}{.cluster.server}{&amp;#34;\n&amp;#34;}{end}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 从上述命令输出中选择你要与之交互的集群的名称&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export CLUSTER_NAME&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;some_server_name&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 指向引用该集群名称的 API 服务器&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>APISERVER&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>kubectl config view -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;{.clusters[?(@.name==\&amp;#34;&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>CLUSTER_NAME&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">\&amp;#34;)].cluster.server}&amp;#34;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 获得令牌&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TOKEN&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>kubectl get secrets -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;{.items[?(@.metadata.annotations[&amp;#39;kubernetes\.io/service-account\.name&amp;#39;]==&amp;#39;default&amp;#39;)].data.token}&amp;#34;&lt;/span>|base64 -d&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 使用令牌玩转 API&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -X GET $APISERVER/api --header &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer &lt;/span>$TOKEN&lt;span style="color:#e6db74">&amp;#34;&lt;/span> --insecure
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="访问-api-server">访问 API Server&lt;/h3>
&lt;ol>
&lt;li>执行访问 https 前准备方法一
&lt;ol>
&lt;li>通过证书与私钥访问
&lt;ol>
&lt;li>curl &amp;ndash;cacert ${CAPATH} &amp;ndash;cert /root/certs/admin.crt &amp;ndash;key /root/certs/admin.key https://${IP}:6443/&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>执行访问 https 前准备方法二
&lt;ol>
&lt;li>通过 https 的方式访问 API
&lt;ol>
&lt;li>curl &amp;ndash;cacert ${CAPATH} -H &amp;ldquo;Authorization: Bearer ${TOKEN}&amp;rdquo; https://${IP}:6443/&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>kubeclt
&lt;ol>
&lt;li>kubectl get &amp;ndash;raw / #让 kubectl 不再输出标准格式的数据，而是直接向 api server 请求原始数据&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>kubectl proxy，一般监听在 6443 端口的 api server 使用该方式，监听在 8080 上的为 http，可直接访问
&lt;ol>
&lt;li>kubectl proxy &amp;ndash;port=8080 &amp;ndash;accept-hosts=&amp;rsquo;^localhost$,^127.0.0.1$,^[::1]$,10.10.100.151&amp;rsquo; &amp;ndash;address=&amp;lsquo;0.0.0.0&amp;rsquo; #在本地 8080 端口上启动 API Server 的一个代理网关，以便使用 curl 直接访问 api server 并使用命令 curl localhost:8080/获取数据
&lt;ol>
&lt;li>直接访问本地 8080 端口，即可通过 API Server 获取集群所有数据&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="编程方式访问-api">编程方式访问 API&lt;/h2>
&lt;p>Kubernetes 官方支持  &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#go-client">Go&lt;/a>、&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#python-client">Python&lt;/a>、&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#java-client">Java&lt;/a>、 &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#dotnet-client">dotnet&lt;/a>、&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#javascript-client">Javascript&lt;/a>  和  &lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#haskell-client">Haskell&lt;/a>  语言的客户端库。还有一些其他客户端库由对应作者而非 Kubernetes 团队提供并维护。 参考&lt;a href="https://kubernetes.io/zh/docs/reference/using-api/client-libraries/">客户端库&lt;/a>了解如何使用其他语言 来访问 API 以及如何执行身份认证。&lt;/p>
&lt;h3 id="go-客户端介绍">Go 客户端介绍&lt;/h3>
&lt;blockquote>
&lt;p>参考：官方文档：&lt;a href="https://github.com/kubernetes/client-go/#compatibility-matrix">https://github.com/kubernetes/client-go/#compatibility-matrix&lt;/a>
详见 &lt;a href="https://www.yuque.com/go/doc/33161293">Client Libraries&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>版本控制策略：k8s 版本 1.18.8 对应 client-go 版本 0.18.8，其他版本以此类推。&lt;/p>
&lt;p>使用前注意事项：
使用 client-go 之前，需要手动获取对应版本的的 client-go 库。根据版本控制策略，使用如下命令进行初始化&lt;/p>
&lt;pre>&lt;code>go mod init client-go-test
go get k8s.io/client-go@kubernetes-1.19.2
&lt;/code>&lt;/pre>
&lt;p>这是一个使用 client-go 访问 API 的基本示例&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">package&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;context&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;fmt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">v1&lt;/span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/kubernetes&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 根据指定的 kubeconfig 创建一个用于连接集群的配置，/root/.kube/config 为 kubectl 命令所用的 config 文件
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">clientcmd&lt;/span>.&lt;span style="color:#a6e22e">BuildConfigFromFlags&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;/root/.kube/config&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 根据 BuildConfigFromFlags 创建的配置，返回一个可以连接集群的指针
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">clientset&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">kubernetes&lt;/span>.&lt;span style="color:#a6e22e">NewForConfig&lt;/span>(&lt;span style="color:#a6e22e">config&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 根据 NewForConfig 所创建的连接集群的指针，来访问 API，并对集群进行操作
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">pods&lt;/span>, &lt;span style="color:#a6e22e">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">clientset&lt;/span>.&lt;span style="color:#a6e22e">CoreV1&lt;/span>().&lt;span style="color:#a6e22e">Pods&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>).&lt;span style="color:#a6e22e">List&lt;/span>(&lt;span style="color:#a6e22e">context&lt;/span>.&lt;span style="color:#a6e22e">TODO&lt;/span>(), &lt;span style="color:#a6e22e">v1&lt;/span>.&lt;span style="color:#a6e22e">ListOptions&lt;/span>{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Printf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;There are %d pods in the cluster\n&amp;#34;&lt;/span>, len(&lt;span style="color:#a6e22e">pods&lt;/span>.&lt;span style="color:#a6e22e">Items&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="从-pod-中访问-api">从 Pod 中访问 API&lt;/h2>
&lt;p>从 Pod 内部访问 API 时，定位 API 服务器和向服务器认证身份的操作 与上面描述的外部客户场景不同。&lt;/p>
&lt;p>从 Pod 使用 Kubernetes API 的最简单的方法就是使用官方的 客户端库。 这些库可以自动发现 API 服务器并进行身份验证。&lt;/p>
&lt;h3 id="使用官方客户端库">使用官方客户端库&lt;/h3>
&lt;p>从一个 Pod 内部连接到 Kubernetes API 的推荐方式为：&lt;/p>
&lt;ul>
&lt;li>对于 Go 语言客户端，使用官方的 Go 客户端库。 函数 &lt;code>rest.InClusterConfig()&lt;/code> 自动处理 API 主机发现和身份认证。 参见这里的一个例子。&lt;/li>
&lt;li>对于 Python 客户端，使用官方的 Python 客户端库。 函数 &lt;code>config.load_incluster_config()&lt;/code> 自动处理 API 主机的发现和身份认证。 参见这里的一个例子。&lt;/li>
&lt;li>还有一些其他可用的客户端库，请参阅客户端库页面。&lt;/li>
&lt;/ul>
&lt;p>在以上场景中，客户端库都使用 Pod 的服务账号凭据来与 API 服务器安全地通信。&lt;/p>
&lt;h3 id="直接访问-rest-api">直接访问 REST API&lt;/h3>
&lt;p>在运行在 Pod 中时，可以通过 &lt;code>default&lt;/code> 命名空间中的名为 &lt;code>kubernetes&lt;/code> 的服务访问 Kubernetes API 服务器。也就是说，Pod 可以使用 &lt;code>kubernetes.default.svc&lt;/code> 主机名 来查询 API 服务器。官方客户端库自动完成这个工作。&lt;/p>
&lt;p>向 API 服务器进行身份认证的推荐做法是使用 &lt;a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-service-account/">服务账号&lt;/a> 凭据。 默认情况下，每个 Pod 与一个服务账号关联，该服务账户的凭证（令牌）放置在此 Pod 中 每个容器的文件系统树中的 &lt;code>/var/run/secrets/kubernetes.io/serviceaccount/token&lt;/code> 处。&lt;/p>
&lt;p>如果由证书包可用，则凭证包被放入每个容器的文件系统树中的 &lt;code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&lt;/code> 处， 且将被用于验证 API 服务器的服务证书。&lt;/p>
&lt;p>最后，用于命名空间域 API 操作的默认命名空间放置在每个容器中的 &lt;code>/var/run/secrets/kubernetes.io/serviceaccount/namespace&lt;/code> 文件中。&lt;/p>
&lt;h3 id="使用-kubectl-proxy">使用 kubectl proxy&lt;/h3>
&lt;p>如果你希望不实用官方客户端库就完成 API 查询，可以将 &lt;code>kubectl proxy&lt;/code> 作为 command 在 Pod 启动一个边车（Sidecar）容器。&lt;/p>
&lt;p>这样，&lt;code>kubectl proxy&lt;/code> 自动完成对 API 的身份认证，并将其暴露到 Pod 的 &lt;code>localhost&lt;/code> 接口，从而 Pod 中的其他容器可以 直接使用 API。&lt;/p>
&lt;h3 id="不使用代理">不使用代理&lt;/h3>
&lt;p>通过将认证令牌直接发送到 API 服务器，也可以避免运行 kubectl proxy 命令。 内部的证书机制能够为链接提供保护。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 指向内部 API 服务器的主机名&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>APISERVER&lt;span style="color:#f92672">=&lt;/span>https://kubernetes.default.svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 服务账号令牌的路径&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SERVICEACCOUNT&lt;span style="color:#f92672">=&lt;/span>/var/run/secrets/kubernetes.io/serviceaccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 读取 Pod 的名字空间&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAMESPACE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cat &lt;span style="color:#e6db74">${&lt;/span>SERVICEACCOUNT&lt;span style="color:#e6db74">}&lt;/span>/namespace&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 读取服务账号的持有者令牌&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TOKEN&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cat &lt;span style="color:#e6db74">${&lt;/span>SERVICEACCOUNT&lt;span style="color:#e6db74">}&lt;/span>/token&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 引用内部整数机构（CA）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CACERT&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>SERVICEACCOUNT&lt;span style="color:#e6db74">}&lt;/span>/ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 使用令牌访问 API&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl --cacert &lt;span style="color:#e6db74">${&lt;/span>CACERT&lt;span style="color:#e6db74">}&lt;/span> --header &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer &lt;/span>&lt;span style="color:#e6db74">${&lt;/span>TOKEN&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> -X GET &lt;span style="color:#e6db74">${&lt;/span>APISERVER&lt;span style="color:#e6db74">}&lt;/span>/api
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>输出类似于：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;APIVersions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;versions&amp;#34;&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;serverAddressByClientCIDRs&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;clientCIDR&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0.0.0.0/0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;serverAddress&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10.0.1.149:443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="api-server-健康检查点">API Server 健康检查点&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://kubernetes.io/docs/reference/using-api/health-checks/">官方文档&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Kubernetes API 服务器 提供 API 端点以指示 API 服务器的当前状态。 本文描述了这些 API 端点，并说明如何使用。&lt;/p>
&lt;h3 id="api-健康检查点">API 健康检查点&lt;/h3>
&lt;p>Kubernetes API 服务器提供 3 个 API 端点（&lt;code>healthz&lt;/code>、&lt;code>livez&lt;/code> 和 &lt;code>readyz&lt;/code>）来表明 API 服务器的当前状态。 &lt;code>healthz&lt;/code> 端点已被弃用（自 Kubernetes v1.16 起），你应该使用更为明确的 &lt;code>livez&lt;/code> 和 &lt;code>readyz&lt;/code> 端点。 &lt;code>livez&lt;/code> 端点可与 &lt;code>--livez-grace-period&lt;/code> 标志一起使用，来指定启动持续时间。 为了正常关机，你可以使用 &lt;code>/readyz&lt;/code> 端点并指定 &lt;code>--shutdown-delay-duration&lt;/code> 标志。 检查 API 服务器的 &lt;code>health&lt;/code>/&lt;code>livez&lt;/code>/&lt;code>readyz&lt;/code> 端点的机器应依赖于 HTTP 状态代码。 状态码 &lt;code>200&lt;/code> 表示 API 服务器是 &lt;code>healthy&lt;/code>、&lt;code>live&lt;/code> 还是 &lt;code>ready&lt;/code>，具体取决于所调用的端点。 以下更详细的选项供操作人员使用，用来调试其集群或专门调试 API 服务器的状态。&lt;/p>
&lt;p>以下示例将显示如何与运行状况 API 端点进行交互。&lt;/p>
&lt;p>对于所有端点，都可以使用 &lt;code>verbose&lt;/code> 参数来打印检查项以及检查状态。 这对于操作人员调试 API 服务器的当前状态很有用，这些不打算给机器使用：&lt;/p>
&lt;pre>&lt;code>curl -k https://localhost:6443/livez?verbose
&lt;/code>&lt;/pre>
&lt;p>或从具有身份验证的远程主机：&lt;/p>
&lt;pre>&lt;code>kubectl get --raw='/readyz?verbose'
&lt;/code>&lt;/pre>
&lt;p>输出将如下所示：&lt;/p>
&lt;pre>&lt;code>[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check passed
&lt;/code>&lt;/pre>
&lt;p>Kubernetes API 服务器也支持排除特定的检查项。 查询参数也可以像以下示例一样进行组合：&lt;/p>
&lt;pre>&lt;code>curl -k 'https://localhost:6443/readyz?verbose&amp;amp;exclude=etcd'
&lt;/code>&lt;/pre>
&lt;p>输出显示排除了 &lt;code>etcd&lt;/code> 检查：&lt;/p>
&lt;pre>&lt;code>[+]ping ok
[+]log ok
[+]etcd excluded: ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]shutdown ok
healthz check passed
&lt;/code>&lt;/pre>
&lt;h3 id="独立健康检查">独立健康检查&lt;/h3>
&lt;p>&lt;strong>FEATURE STATE:&lt;/strong> &lt;code>Kubernetes v1.19 [alpha]&lt;/code>每个单独的健康检查都会公开一个 http 端点，并且可以单独检查。 单个运行状况检查的模式为 &lt;code>/livez/&amp;lt;healthcheck-name&amp;gt;&lt;/code>，其中 &lt;code>livez&lt;/code> 和 &lt;code>readyz&lt;/code> 表明你要检查的是 API 服务器是否存活或就绪。 &lt;code>&amp;lt;healthcheck-name&amp;gt;&lt;/code> 的路径可以通过上面的 &lt;code>verbose&lt;/code> 参数发现 ，并采用 &lt;code>[+]&lt;/code> 和 &lt;code>ok&lt;/code> 之间的路径。 这些单独的健康检查不应由机器使用，但对于操作人员调试系统而言，是有帮助的：&lt;/p>
&lt;pre>&lt;code>curl -k https://localhost:6443/livez/etcd
&lt;/code>&lt;/pre>
&lt;h1 id="api-server-与-etcd-的交互方式">API Server 与 Etcd 的交互方式&lt;/h1>
&lt;p>数据通过 API Server 时，一般是进行序列化后保存到 etcd 中的，可以使用参数 &amp;ndash;etcd-prefix 来指定数据保存在 etcd 中后的地址前缀，默认为 &lt;code>/registry&lt;/code>&lt;/p>
&lt;p>一般情况，保存到 etcd 中后，会省略 Group 与 Version，直接使用 Resource 来作为 etcd 中的路径。比如：URI 为 /api/v1/namespaces/kube-system/pods/kube-apiserver-master1 的 pod 资源，在 etcd 中的存储路径为 /registry/pods/kube-system/kube-apiserver-master1。&lt;/p>
&lt;p>而序列化的方式可以通过 &amp;ndash;storage-media-type 来指定，默认为 protobuf 。使用这种方式将数据序列化之后，得出来的将会有很多乱码，详见 &lt;a href="https://www.yuque.com/go/doc/33166015">Etcd 数据探秘章节&lt;/a> 中的说明&lt;/p>
&lt;h1 id="kube-apiserver-manifests-示例">kube-apiserver Manifests 示例&lt;/h1>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
annotations:
kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.19.42.231:6443
creationTimestamp: null
labels:
component: kube-apiserver
tier: control-plane
name: kube-apiserver
namespace: kube-system
spec:
containers:
- command:
- kube-apiserver
- --advertise-address=172.19.42.231
- --allow-privileged=true
- --authorization-mode=Node,RBAC
- --client-ca-file=/etc/kubernetes/pki/ca.crt
- --enable-admission-plugins=NodeRestriction
- --enable-bootstrap-token-auth=true
- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
- --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
- --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
- --etcd-servers=https://127.0.0.1:2379
- --insecure-port=0
- --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
- --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
- --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
- --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
- --requestheader-allowed-names=front-proxy-client
- --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
- --requestheader-extra-headers-prefix=X-Remote-Extra-
- --requestheader-group-headers=X-Remote-Group
- --requestheader-username-headers=X-Remote-User
- --secure-port=6443
- --service-account-key-file=/etc/kubernetes/pki/sa.pub
- --service-cluster-ip-range=10.96.0.0/12
- --service-node-port-range=30000-60000
- --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
- --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
image: registry.aliyuncs.com/k8sxio/kube-apiserver:v1.19.2
imagePullPolicy: IfNotPresent
livenessProbe:
failureThreshold: 8
httpGet:
host: 172.19.42.231
path: /livez
port: 6443
scheme: HTTPS
initialDelaySeconds: 10
periodSeconds: 10
timeoutSeconds: 15
name: kube-apiserver
readinessProbe:
failureThreshold: 3
httpGet:
host: 172.19.42.231
path: /readyz
port: 6443
scheme: HTTPS
periodSeconds: 1
timeoutSeconds: 15
resources:
requests:
cpu: 250m
startupProbe:
failureThreshold: 24
httpGet:
host: 172.19.42.231
path: /livez
port: 6443
scheme: HTTPS
initialDelaySeconds: 10
periodSeconds: 10
timeoutSeconds: 15
volumeMounts:
- mountPath: /etc/ssl/certs
name: ca-certs
readOnly: true
- mountPath: /etc/pki
name: etc-pki
readOnly: true
- mountPath: /etc/localtime
name: host-time
readOnly: true
- mountPath: /etc/kubernetes/pki
name: k8s-certs
readOnly: true
hostNetwork: true
priorityClassName: system-node-critical
volumes:
- hostPath:
path: /etc/ssl/certs
type: DirectoryOrCreate
name: ca-certs
- hostPath:
path: /etc/pki
type: DirectoryOrCreate
name: etc-pki
- hostPath:
path: /etc/localtime
type: &amp;quot;&amp;quot;
name: host-time
- hostPath:
path: /etc/kubernetes/pki
type: DirectoryOrCreate
name: k8s-certs
&lt;/code>&lt;/pre></description></item><item><title>Docs: API Server 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/api-server/api-server-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/api-server/api-server-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">官方文档,参考-组件工具-kube-apiserver&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>API Server 现阶段只能通过命令行标志才可以改变运行时行为。暂无配置文件可用。&lt;/p>
&lt;h1 id="kube-apiserver-命令行标志详解">kube-apiserver 命令行标志详解&lt;/h1>
&lt;p>&lt;strong>&amp;ndash;allow-privileged &lt;!-- raw HTML omitted --> # 是否允许有特权的容器。&lt;code>默认值：false&lt;/code>。&lt;/strong>
**
~~**&amp;ndash;basic-auth-file &lt;!-- raw HTML omitted --> # 配置 API Server 的基础认证。**~~
该标志已于 1.19 版本彻底弃用。详见 &lt;a href="https://github.com/kubernetes/kubernetes/pull/89069">PR #89069&lt;/a>&lt;/p>
&lt;p>&lt;strong>&amp;ndash;insecure-port &lt;!-- raw HTML omitted --> # 开启不安全的端口。&lt;code>默认值：0&lt;/code>，即不开启不安全的端口&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&amp;ndash;insecure-bind-address &lt;!-- raw HTML omitted --> # 不安全端口的监听地址。&lt;code>默认值：127.0.0.1&lt;/code>。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&amp;ndash;runtime-config &lt;!-- raw HTML omitted --> # 启用或禁用内置的 APIs。&lt;/strong>
OBJECT 是 key=value 的键值对格式。key 为 API 组名称，value 为 true 或 false。
比如&lt;/p>
&lt;ul>
&lt;li>要关闭 &lt;code>batch/v1&lt;/code> 组，则设置 &lt;code>--runtime-config=batch/v1=false&lt;/code>&lt;/li>
&lt;li>要开启 &lt;code>batch/v2alpha1&lt;/code> 组，则设置 &lt;code>--runtime-config=batch/v2alpha1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&amp;ndash;secure-port &lt;!-- raw HTML omitted --> # API Server 监听的安全端口。&lt;code>默认值：6443&lt;/code>。不能以 0 关闭。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&amp;ndash;service-node-port-range &lt;!-- raw HTML omitted --> # 指定 NodePort 类型的 service 资源可以使用的端口范围。&lt;/strong>
比如：&amp;lsquo;30000-32767&amp;rsquo;. Inclusive at both ends of the range。默认范围: 30000-32767&lt;/p>
&lt;p>&lt;strong>&amp;ndash;v NUM #指定日志级别&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>参考：&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-output-verbosity-and-debugging">https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-output-verbosity-and-debugging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md">https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md&lt;/a>&lt;/li>
&lt;li>--v=0 #通常对此有用，&lt;em>始终&lt;/em>对运维人员可见。&lt;/li>
&lt;li>--v=1 #如果您不想要详细程度，则为合理的默认日志级别。&lt;/li>
&lt;li>--v=2 #有关服务的有用稳定状态信息以及可能与系统中的重大更改相关的重要日志消息。这是大多数系统的建议默认日志级别。&lt;/li>
&lt;li>--v=3 #有关更改的扩展信息。&lt;/li>
&lt;li>--v=4 #Debug 级别。&lt;/li>
&lt;li>--v=6 #显示请求的资源。&lt;/li>
&lt;li>--v=7 #显示 HTTP 请求头。&lt;/li>
&lt;li>--v=8 #显示 HTTP 请求内容。&lt;/li>
&lt;li>--v=9 #显示 HTTP 请求内容而不截断内容。&lt;/li>
&lt;/ul></description></item><item><title>Docs: API Server 源码</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/api-server-%E6%BA%90%E7%A0%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/api-server-%E6%BA%90%E7%A0%81/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/HGJKYSfpIG3YxJTy0qVsIw">公众号,API Server service 的实现&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote></description></item><item><title>Docs: API 参考</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/api-%E5%8F%82%E8%80%83/api-%E5%8F%82%E8%80%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/api-%E5%8F%82%E8%80%83/api-%E5%8F%82%E8%80%83/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23">官方文档，参考-API 概述-API&lt;/a>(这里是通过单一页面显示 API 资源各字段详解)
&lt;ul>
&lt;li>链接里是 1.23 的，想查看其他版本 API，改变 URL 中的版本即可。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/">官方文档，参考-KubernetesAPI&lt;/a>(这里是通过多级页面显示 API 资源各字段详解)
&lt;ul>
&lt;li>这些连接的内容，其实是 &lt;code>kubectl explain&lt;/code> 命令的内容显示在浏览器中了。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/api/openapi-spec/swagger.json">OpeaAPI 格式文档&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 &lt;a href="https://www.yuque.com/go/doc/33168662">Kubernetes API&lt;/a> 章节，已经可以看到单一页面的详解中对 API 的分类，在本笔记后面的部分对各资源 Manifest 详解中，其实已经描述了 API 中各个字段的含义。所以本篇文章不会详解每个 API，而是记录一下如何通过 Kubernetes 官网来查找 API 详解，以及如何使用官方文档查看 API 详解。&lt;/p>
&lt;p>如果笔记中记录得不够详细，&lt;code>kubectl explain&lt;/code> 命令也看着不方便，那么通过这篇文章中介绍的官方文档中的 API 详解来查看，将会更加直观。&lt;/p>
&lt;p>&lt;strong>Kubernetes API 参考中将会描述每种资源的 Manifests 中每个字段(即.YAML 中的节点)的含义。&lt;/strong>&lt;/p>
&lt;p>下面是文档中占位符说明：
&lt;strong>[]TYPE&lt;/strong> # 表示该字段由数组组成，数组元素类型为 TYPE，比如 []STRING 格式应该就是下面这样&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">args&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">deletecr&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">ns&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">name&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>map[STRING]STRING&lt;/strong> # 表示多个键/值对。键 和 值 的数据类型都是 STRING。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">key1&lt;/span>: &lt;span style="color:#ae81ff">value1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">key2&lt;/span>: &lt;span style="color:#ae81ff">value2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>**OBJECT **# 表示复合结构的 map。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#ae81ff">2Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#ae81ff">500m&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#ae81ff">400Mi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>**[]OBJECT **# 表示该字段由数组组成，并且数组中的元素都是一个 OBJECT，比如格式应该像下面这样&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">args&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">AAA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">BBB&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">XXX&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">XXX&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">YYY&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">YYY&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>在每种资源的 Manifests 中，会有一些共用的部分称为&lt;/strong>[&lt;strong>通用定义&lt;/strong>](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/Common%20Definitions(通用定义).md 参考/Common Definitions(通用定义).md)&lt;strong>（也可以说是功能定义），比如常见的 &lt;strong>[&lt;strong>LabelSelector&lt;/strong>](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/Common%20Definitions(通用定义)/LabelSelector%20 详解.md 参考/Common Definitions(通用定义)/LabelSelector 详解.md)&lt;/strong>，这属于资源的 Manifests 的一部分。很多组件在解析 Manifests 中的通用定义时，都会遵循相同的规则。除了通用定义以外的，都属于 K8S 的资源定义，比如定义 &lt;strong>[&lt;strong>Pod&lt;/strong>](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/工作负载资源/Pod%20Manifest%20 详解.md 参考/工作负载资源/Pod Manifest 详解.md)&lt;/strong> 的 API 参考、定义 &lt;strong>[&lt;strong>Service&lt;/strong>](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/服务资源/Service%20Manifests%20 详解.md 参考/服务资源/Service Manifests 详解.md)&lt;/strong> 的 API 参考等等。&lt;/strong>&lt;/p>
&lt;p>这是单一页面的样子。左侧是根据对资源的分类而形成的目录，右侧是完整的页面
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dkxdpv/1616120193938-a171af16-575d-4de6-951a-99cdca271a50.png" alt="">
这是多级页面的样子，该 API 详解是内含在官方文档中的，并且对 API 进行了细致的分类
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dkxdpv/1616120193972-2c538ed5-7a6f-4aca-bf11-732240aa84d6.png" alt="">&lt;/p>
&lt;h2 id="kubernetes-api-删除和弃用流程">Kubernetes API 删除和弃用流程&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">官方文档，参考-API 概述-Kubernetes 弃用策略&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubernetes 项目有一个记录良好的特性弃用策略[1]。该策略规定，只有当同一 API 的更新的、稳定的版本可用时，才可以弃用稳定的 API，并且 API 对于每个稳定性级别都有一个最短的生存期。给弃用的 API，是在未来的 Kubernetes 版本中被标记为删除的 API；它将继续运行，直到给删除（从弃用至少一年），但使用将导致显示警告。删除的 API 在当前版本中不再可用，此时你必须迁移到使用替换的 API。&lt;/p>
&lt;ul>
&lt;li>GA（Generally available，普遍可用）或稳定的 API 版本可能会被标记为弃用，但不得在 Kubernetes 的主要版本中删除。&lt;/li>
&lt;li>测试版或预发布 API 版本弃用后，必须支持 3 个版本。&lt;/li>
&lt;li>Alpha 或实验 API 版本可能会在任何版本中被删除，恕不另行通知。&lt;/li>
&lt;/ul>
&lt;p>无论某个 API 是因为某个功能从测试版升级到稳定版而被删除，还是因为该 API 没有成功，所有的删除都遵循这个弃用策略。每当删除一个 API 时，迁移选项都会在文档中提供说明。&lt;/p>
&lt;h1 id="api-分类">API 分类&lt;/h1>
&lt;ul>
&lt;li>[Workloads Resources](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/工作负载资源.md 参考/工作负载资源.md)(工作负载资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/">Services Resources&lt;/a>(服务资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/">Config and Storage Resources&lt;/a>(配置与存储资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/">Authentication Resources&lt;/a>(认证资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/">Authorization Resources&lt;/a>(授权资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/policies-resources/">Policies Resources&lt;/a>(策略资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/">Extend Resources&lt;/a>(扩展资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/">Cluster Resources&lt;/a>(集群资源)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/">Common Definitions&lt;/a>(通用定义) # 在多种资源 API 中，可以使用的 API。比如 节点选择器、meta 字段 等等&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/common-parameters/common-parameters/">Common Parameters&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="config-and-storage-resources">Config and Storage Resources&lt;/h2>
&lt;h5 id="configmaphttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcesconfig-map-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/config-map-v1/">ConfigMap&lt;/a>&lt;/h5>
&lt;p>ConfigMap holds configuration data for pods to consume.&lt;/p>
&lt;h5 id="secrethttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcessecret-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">Secret&lt;/a>&lt;/h5>
&lt;p>Secret holds secret data of a certain type.&lt;/p>
&lt;h5 id="volumehttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcesvolume">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/volume/">Volume&lt;/a>&lt;/h5>
&lt;p>Volume represents a named volume in a pod that may be accessed by any container in the pod.&lt;/p>
&lt;h5 id="persistentvolumeclaimhttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcespersistent-volume-claim-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/">PersistentVolumeClaim&lt;/a>&lt;/h5>
&lt;p>PersistentVolumeClaim is a user&amp;rsquo;s request for and claim to a persistent volume.&lt;/p>
&lt;h5 id="persistentvolumehttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcespersistent-volume-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/">PersistentVolume&lt;/a>&lt;/h5>
&lt;p>PersistentVolume (PV) is a storage resource provisioned by an administrator.&lt;/p>
&lt;h5 id="storageclasshttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcesstorage-class-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/storage-class-v1/">StorageClass&lt;/a>&lt;/h5>
&lt;p>StorageClass describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned.&lt;/p>
&lt;h5 id="volumeattachmenthttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcesvolume-attachment-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/volume-attachment-v1/">VolumeAttachment&lt;/a>&lt;/h5>
&lt;p>VolumeAttachment captures the intent to attach or detach the specified volume to/from the specified node.&lt;/p>
&lt;h5 id="csidriverhttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcescsi-driver-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/">CSIDriver&lt;/a>&lt;/h5>
&lt;p>CSIDriver captures information about a Container Storage Interface (CSI) volume driver deployed on the cluster.&lt;/p>
&lt;h5 id="csinodehttpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcescsi-node-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/csi-node-v1/">CSINode&lt;/a>&lt;/h5>
&lt;p>CSINode holds information about all CSI drivers installed on a node.&lt;/p>
&lt;h5 id="csistoragecapacity-v1beta1httpskubernetesiodocsreferencekubernetes-apiconfig-and-storage-resourcescsi-storage-capacity-v1beta1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1beta1/">CSIStorageCapacity v1beta1&lt;/a>&lt;/h5>
&lt;p>CSIStorageCapacity stores the result of one CSI GetCapacity call.&lt;/p>
&lt;h2 id="authentication-resources">Authentication Resources&lt;/h2>
&lt;h5 id="serviceaccounthttpskubernetesiodocsreferencekubernetes-apiauthentication-resourcesservice-account-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/service-account-v1/">ServiceAccount&lt;/a>&lt;/h5>
&lt;p>ServiceAccount binds together: _ a name, understood by users, and perhaps by peripheral systems, for an identity _ a principal that can be authenticated and authorized * a set of secrets.&lt;/p>
&lt;h5 id="tokenrequesthttpskubernetesiodocsreferencekubernetes-apiauthentication-resourcestoken-request-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest&lt;/a>&lt;/h5>
&lt;p>TokenRequest requests a token for a given service account.&lt;/p>
&lt;h5 id="tokenreviewhttpskubernetesiodocsreferencekubernetes-apiauthentication-resourcestoken-review-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-review-v1/">TokenReview&lt;/a>&lt;/h5>
&lt;p>TokenReview attempts to authenticate a token to a known user.&lt;/p>
&lt;h5 id="certificatesigningrequesthttpskubernetesiodocsreferencekubernetes-apiauthentication-resourcescertificate-signing-request-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1/">CertificateSigningRequest&lt;/a>&lt;/h5>
&lt;p>CertificateSigningRequest objects provide a mechanism to obtain x509 certificates by submitting a certificate signing request, and having it asynchronously approved and issued.&lt;/p>
&lt;h2 id="authorization-resources">Authorization Resources&lt;/h2>
&lt;h5 id="localsubjectaccessreviewhttpskubernetesiodocsreferencekubernetes-apiauthorization-resourceslocal-subject-access-review-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/local-subject-access-review-v1/">LocalSubjectAccessReview&lt;/a>&lt;/h5>
&lt;p>LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace.&lt;/p>
&lt;h5 id="selfsubjectaccessreviewhttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcesself-subject-access-review-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/self-subject-access-review-v1/">SelfSubjectAccessReview&lt;/a>&lt;/h5>
&lt;p>SelfSubjectAccessReview checks whether or the current user can perform an action.&lt;/p>
&lt;h5 id="selfsubjectrulesreviewhttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcesself-subject-rules-review-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/self-subject-rules-review-v1/">SelfSubjectRulesReview&lt;/a>&lt;/h5>
&lt;p>SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace.&lt;/p>
&lt;h5 id="subjectaccessreviewhttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcessubject-access-review-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/subject-access-review-v1/">SubjectAccessReview&lt;/a>&lt;/h5>
&lt;p>SubjectAccessReview checks whether or not a user or group can perform an action.&lt;/p>
&lt;h5 id="clusterrolehttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcescluster-role-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/cluster-role-v1/">ClusterRole&lt;/a>&lt;/h5>
&lt;p>ClusterRole is a cluster level, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding or ClusterRoleBinding.&lt;/p>
&lt;h5 id="clusterrolebindinghttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcescluster-role-binding-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/cluster-role-binding-v1/">ClusterRoleBinding&lt;/a>&lt;/h5>
&lt;p>ClusterRoleBinding references a ClusterRole, but not contain it.&lt;/p>
&lt;h5 id="rolehttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcesrole-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/role-v1/">Role&lt;/a>&lt;/h5>
&lt;p>Role is a namespaced, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding.&lt;/p>
&lt;h5 id="rolebindinghttpskubernetesiodocsreferencekubernetes-apiauthorization-resourcesrole-binding-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/role-binding-v1/">RoleBinding&lt;/a>&lt;/h5>
&lt;p>RoleBinding references a role, but does not contain it.&lt;/p>
&lt;h2 id="policies-resources">Policies Resources&lt;/h2>
&lt;h5 id="limitrangehttpskubernetesiodocsreferencekubernetes-apipolicy-resourceslimit-range-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/limit-range-v1/">LimitRange&lt;/a>&lt;/h5>
&lt;p>LimitRange sets resource usage limits for each kind of resource in a Namespace.&lt;/p>
&lt;h5 id="resourcequotahttpskubernetesiodocsreferencekubernetes-apipolicy-resourcesresource-quota-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/">ResourceQuota&lt;/a>&lt;/h5>
&lt;p>ResourceQuota sets aggregate quota restrictions enforced per namespace.&lt;/p>
&lt;h5 id="networkpolicyhttpskubernetesiodocsreferencekubernetes-apipolicy-resourcesnetwork-policy-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/">NetworkPolicy&lt;/a>&lt;/h5>
&lt;p>NetworkPolicy describes what network traffic is allowed for a set of Pods.&lt;/p>
&lt;h5 id="poddisruptionbudgethttpskubernetesiodocsreferencekubernetes-apipolicy-resourcespod-disruption-budget-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/pod-disruption-budget-v1/">PodDisruptionBudget&lt;/a>&lt;/h5>
&lt;p>PodDisruptionBudget is an object to define the max disruption that can be caused to a collection of pods.&lt;/p>
&lt;h5 id="podsecuritypolicy-v1beta1httpskubernetesiodocsreferencekubernetes-apipolicy-resourcespod-security-policy-v1beta1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/pod-security-policy-v1beta1/">PodSecurityPolicy v1beta1&lt;/a>&lt;/h5>
&lt;p>PodSecurityPolicy governs the ability to make requests that affect the Security Context that will be applied to a pod and container.&lt;/p>
&lt;h2 id="extend-resources">Extend Resources&lt;/h2>
&lt;h5 id="customresourcedefinitionhttpskubernetesiodocsreferencekubernetes-apiextend-resourcescustom-resource-definition-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/">CustomResourceDefinition&lt;/a>&lt;/h5>
&lt;p>CustomResourceDefinition represents a resource that should be exposed on the API server.&lt;/p>
&lt;h5 id="mutatingwebhookconfigurationhttpskubernetesiodocsreferencekubernetes-apiextend-resourcesmutating-webhook-configuration-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/mutating-webhook-configuration-v1/">MutatingWebhookConfiguration&lt;/a>&lt;/h5>
&lt;p>MutatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and may change the object.&lt;/p>
&lt;h5 id="validatingwebhookconfigurationhttpskubernetesiodocsreferencekubernetes-apiextend-resourcesvalidating-webhook-configuration-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/validating-webhook-configuration-v1/">ValidatingWebhookConfiguration&lt;/a>&lt;/h5>
&lt;p>ValidatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and object without changing it.&lt;/p>
&lt;h2 id="cluster-resources">Cluster Resources&lt;/h2>
&lt;h5 id="nodehttpskubernetesiodocsreferencekubernetes-apicluster-resourcesnode-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/node-v1/">Node&lt;/a>&lt;/h5>
&lt;p>Node is a worker node in Kubernetes.&lt;/p>
&lt;h5 id="namespacehttpskubernetesiodocsreferencekubernetes-apicluster-resourcesnamespace-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/namespace-v1/">Namespace&lt;/a>&lt;/h5>
&lt;p>Namespace provides a scope for Names.&lt;/p>
&lt;h5 id="eventhttpskubernetesiodocsreferencekubernetes-apicluster-resourcesevent-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/">Event&lt;/a>&lt;/h5>
&lt;p>Event is a report of an event somewhere in the cluster.&lt;/p>
&lt;h5 id="apiservicehttpskubernetesiodocsreferencekubernetes-apicluster-resourcesapi-service-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService&lt;/a>&lt;/h5>
&lt;p>APIService represents a server for a particular GroupVersion.&lt;/p>
&lt;h5 id="leasehttpskubernetesiodocsreferencekubernetes-apicluster-resourceslease-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Lease&lt;/a>&lt;/h5>
&lt;p>Lease defines a lease concept.&lt;/p>
&lt;h5 id="runtimeclasshttpskubernetesiodocsreferencekubernetes-apicluster-resourcesruntime-class-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/runtime-class-v1/">RuntimeClass&lt;/a>&lt;/h5>
&lt;p>RuntimeClass defines a class of container runtime supported in the cluster.&lt;/p>
&lt;h5 id="flowschema-v1beta1httpskubernetesiodocsreferencekubernetes-apicluster-resourcesflow-schema-v1beta1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/flow-schema-v1beta1/">FlowSchema v1beta1&lt;/a>&lt;/h5>
&lt;p>FlowSchema defines the schema of a group of flows.&lt;/p>
&lt;h5 id="prioritylevelconfiguration-v1beta1httpskubernetesiodocsreferencekubernetes-apicluster-resourcespriority-level-configuration-v1beta1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/priority-level-configuration-v1beta1/">PriorityLevelConfiguration v1beta1&lt;/a>&lt;/h5>
&lt;p>PriorityLevelConfiguration represents the configuration of a priority level.&lt;/p>
&lt;h5 id="bindinghttpskubernetesiodocsreferencekubernetes-apicluster-resourcesbinding-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/binding-v1/">Binding&lt;/a>&lt;/h5>
&lt;p>Binding ties one object to another; for example, a pod is bound to a node by a scheduler.&lt;/p>
&lt;h5 id="componentstatushttpskubernetesiodocsreferencekubernetes-apicluster-resourcescomponent-status-v1">&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/component-status-v1/">ComponentStatus&lt;/a>&lt;/h5>
&lt;p>ComponentStatus (and ComponentStatusList) holds the cluster validation info.&lt;/p></description></item><item><title>Docs: API 扩展</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E6%89%A9%E5%B1%95/api-%E6%89%A9%E5%B1%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E6%89%A9%E5%B1%95/api-%E6%89%A9%E5%B1%95/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/">官方文档，概念-扩展 Kubernetes-扩展 Kubernetes API&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Resource(资源) 是 Kubernetes API 中的一个 endpoint(端点)， 其中存储的是某个类别的 API 对象 的一个集合。 例如内置的 pods 资源包含一组 Pod 对象。(这里面“点”的意思是这么一种语境。我说的几点记住了吗？知识点。等等)&lt;/p>
&lt;p>而扩展 Kubernetes API 实际上就是添加 &lt;strong>Custom Resource(自定义的资源)&lt;/strong>。&lt;/p>
&lt;h2 id="custom-resource-自定义资源">Custom Resource 自定义资源&lt;/h2>
&lt;p>&lt;strong>什么是自定义资源呢？&lt;/strong>
如 &lt;a href="https://www.yuque.com/go/doc/33169020">Kubernetes API 介绍&lt;/a> 中介绍的，Kubernetes 自身的一切都抽象为 Resource(资源)。顾名思义，Custom Resource(自定义资源) 就是非 Kubernetes 核心的资源。如果要类比的话，那么 Custom Resource 与 Kubernetes 的关系，类似于 Linux 中，Module(模块) 与 Kernel(内核) 的关系。其实，再准确的说法应该是下文将要提到的 Operator，Operator 与 Kubernetes 的关系，类似于 Linux 中，Module(模块) 与 Kernel(内核) 的关系。现在很多 Kubernetes 核心功能现在都用自定义资源来实现，这使得 Kubernetes 更加模块化。&lt;/p>
&lt;p>自定义资源可以像普通资源(比如.pod)一样被创建和销毁。一旦某个自定义资源被安装，就可以使用 kubectl 来创建和访问其中的对象，就像为 pods 这种内置资源所做的一样。&lt;/p>
&lt;p>添加 Custom Resource 方式有以下两种&lt;/p>
&lt;ol>
&lt;li>CustomResourceDefinitions(自定义资源)
&lt;ol>
&lt;li>使用 CustomResourceDefinition 对象来创建一个或多个 CRD 资源。&lt;/li>
&lt;li>相对简单，创建 CRD 可以不必编程。详见：Custom Resource Definitions(CRD)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>API Aggregation(API 聚合)
&lt;ol>
&lt;li>使用 APIService 对象来创建一个或多个 API 聚合 资源。&lt;/li>
&lt;li>需要编程，但支持对 API 行为进行更多的控制，例如数据如何存储以及在不同 API 版本间如何转换等。详见：API Aggregation(聚合) Layer&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>Kubernetes 提供的上述两种选项以满足不同用户的需求，这样就既不会牺牲易用性也不会牺牲灵活性。&lt;/p>
&lt;ul>
&lt;li>聚合 API 指的是一些下位的 API 服务器，运行在主 API 服务器后面；主 API 服务器以代理的方式工作。这种组织形式称作 API 聚合（API Aggregation，AA） 。 对用户而言，看起来仅仅是 Kubernetes API 被扩展了。&lt;/li>
&lt;li>CRD 允许用户创建新的资源类别同时又不必添加新的 API 服务器。 使用 CRD 时，你并不需要理解 API 聚合。&lt;/li>
&lt;/ul>
&lt;p>无论以哪种方式安装自定义资源，新的资源都会被当做自定义资源，以便与内置的 Kubernetes 资源(如 Pods) 和 内置的 API(如 v1.apps) 相区分。&lt;/p>
&lt;h2 id="custom-controller-自定义控制器">Custom Controller 自定义控制器&lt;/h2>
&lt;p>&lt;strong>为什么需要 Custom Controller 呢？&lt;/strong>
需要 Custom Controller 的原因，就要从 Kubernetes 的 声明式 API 的特点说起，基于这种特点，所有资源都基于 Kubernetes 的控制器模型来维护其自身的状态。那么当我们创建一个 Custom Resource 时，谁又来维护它的状态的呢？这就是 Custom Controller 的由来。Custom Resource 不能由集群的 controller-manager 来维护，必须要自定义一个 Controller 才可以。&lt;/p>
&lt;p>就 Custom Resource 本身而言，它只能用来存取结构化的数据。 当你将 Custom Resource 与 Custom Controller 相结合时，就能够提供真正的声明式 API（Declarative API）。&lt;/p>
&lt;p>使用&lt;a href="https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/">声明式 API&lt;/a>， 你可以声明或者设定你的资源的期望状态，并尝试让 Kubernetes 对象的当前状态 同步到其期望状态。控制器负责将结构化的数据解释为用户所期望状态的记录，并持续地维护该状态。&lt;/p>
&lt;p>Custom Controller 与 Custom Resource 不同，并不能通过原生对象来直接创建。Custom Controller 是一个逻辑上的概念。&lt;/p>
&lt;p>一般通过代码编写一个程序，以 Pod 方式运行在集群中，专门用来监听 Custom Resource 的状态，这种运作模型的程序，就是 Custom Controller。在一个运行中的集群上部署和更新自定义控制器，这类操作与集群的生命周期无关。 自定义控制器可以用于任何类别的资源，不过它们与自定义资源结合起来时最为有效。&lt;/p>
&lt;h2 id="operator-模式">Operator 模式&lt;/h2>
&lt;p>将 Custom Resource 与 Custom Controller 结合起来使用就是 &lt;a href="https://coreos.com/blog/introducing-operators.html">Operator 模式&lt;/a>。你可以使用 Custom Controller 来将特定于某应用的领域知识组织起来，以编码的形式构造对 Kubernetes API 的扩展。&lt;/p>
&lt;p>详见：Operator 模式介绍&lt;/p>
&lt;p>所以，Operator 也可以称为 Custom Controller。&lt;/p>
&lt;h1 id="应用示例">应用示例&lt;/h1>
&lt;p>kube-prometheus 项目就是一个典型的 API 扩展，其中包含 prometheus 这个 operater，还有多种 CRD ，还有名为 v1beta1.metrics.k8s.io 的聚合 API。这一整套扩展组合出来一个自动化的 prometheus 产品。prometheus-operator 会管理各种 CRD 创建出来的资源，以及 v1beta1.metrics.k8s.io 这个聚合 API 所关联的 service 的后端 pod 的状态。&lt;/p></description></item><item><title>Docs: API 源码</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/api-%E6%BA%90%E7%A0%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/api-%E6%BA%90%E7%A0%81/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/f31GkTs9j8V-OUYFtYzTLg">公众号-云原生实验室，深入 Kubernetes API 的源码实现&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubernetes API 代码在 &lt;a href="https://github.com/kubernetes/api">k8s.io/api &lt;/a>仓库中，该仓库的代码来源于 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api">https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api&lt;/a> 这个核心仓库的目录中。&lt;/p>
&lt;p>在 k8s.io/api 仓库定义的 kubernetes API 规范中，Pod 作为最基础的资源类型，一个典型的 YAML 形式的序列化 pod 对象如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">webserver&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">webserver&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">webserver&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>从编程的角度来看，序列化的 pod 对象最终会被发送到 API-Server 并解码为 Pod 类型的 Go 结构体，同时 YAML 中的各个字段会被赋值给该 Go 结构体。那么，Pod 类型在 Go 语言结构体中是怎么定义的呢？&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/api/blob/master/core/v1/types.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Pod&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 从TypeMeta字段名可以看出该字段定义Pod类型的元信息，类似于面向对象编程里面
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Class本身的元信息，类似于Pod类型的API分组、API版本等
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">TypeMeta&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// ObjectMeta字段定义单个Pod对象的元信息。每个kubernetes资源对象都有自己的元信息，
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 例如名字、命名空间、标签、注释等等，kuberentes把这些公共的属性提取出来就是
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// metav1.ObjectMeta，成为了API对象类型的父类
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ObjectMeta&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;metadata,omitempty&amp;#34; protobuf:&amp;#34;bytes,1,opt,name=metadata&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// PodSpec表示Pod类型的对象定义规范，最为代表性的就是CPU、内存的资源使用。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 这个字段和YAML中spec字段对应
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Spec&lt;/span> &lt;span style="color:#a6e22e">PodSpec&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;spec,omitempty&amp;#34; protobuf:&amp;#34;bytes,2,opt,name=spec&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// PodStatus表示Pod的状态，比如是运行还是挂起、Pod的IP等等。Kubernetes会根据pod在
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 集群中的实际状态来更新PodStatus字段
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Status&lt;/span> &lt;span style="color:#a6e22e">PodStatus&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;status,omitempty&amp;#34; protobuf:&amp;#34;bytes,3,opt,name=status&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>从上面 Pod 定义的结构体可以看出，它继承了 metav1.TypeMeta 和 metav1.ObjectMeta 两个类型，metav1.TypeMeta 对应 YAML 中的 kind 与 apiVersion 段，而 metav1.ObjectMeta 则对应 metadata 字段。这其实也可以从 Go 结构体的字段 json 标签看得出来。除了 metav1.TypeMeta 和 metav1.ObjectMeta 字段，Pod 结构体同时还定义了 Spec 和 Status 两个成员变量。如果去查看 k8s.io/api 仓库中其他 API 资源结构体的定义就会发现 kubernetes 绝大部分 API 资源类型都是这样的结构，这也就是说 kubernetes API 资源类型都继承 metav1.TypeMeta 和 metav1.ObjectMeta，前者用于定义资源类型的属性，后者用于定义资源对象的公共属性；Spec 用于定义 API 资源类型的私有属性，也是不同 API 资源类型之间的区别所在；Status 则是用于描述每个资源对象的状态，这和每个资源类型紧密相关的。
关于 metav1.TypeMeta 和 metav1.ObjectMeta 字段从语义上也很好理解，这两个类型作为所有 kubernetes API 资源对象的基类，每个 API 资源对象需要 metav1.TypeMeta 字段用于描述自己是什么类型，这样才能构造相应类型的对象，所以相同类型的所有资源对象的 metav1.TypeMeta 字段都是相同的，但是 metav1.ObjectMeta 则不同，它是定义资源对象实例的属性，即所有资源对象都应该具备的属性。这部分就是和对象本身相关，和类型无关，所以相同类型的资源对象的 metav1.ObjectMeta 可能是不同的。
在 kubernetes 的 API 资源对象中除了单体对象外，还有对象列表类型，用于描述一组相同类型的对象列表。对象列表的典型应用场景就是列举，对象列表就可以表达一组资源对象。可能有些读者会问为什么不用对象的 slice，例如[]Pod，伴随着笔者对对象列表的解释读者就会理解，此处以 PodList 为例进行分析：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/api/blob/master/core/v1/types.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">PodList&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// PodList也需要继承metav1.TypeMeta，毕竟对象列表也好、单体对象也好都需要类型属性。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// PodList比[]Pod类型在yaml或者json表达上多了类型描述，当需要根据YAML构建对象列表的时候，
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 就可以根据类型描述反序列成为PodList。而[]Pod则不可以，必须确保YAML就是[]Pod序列化的
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 结果，否则就会报错。这就无法实现一个通用的对象序列化/反序列化。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">TypeMeta&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 与Pod不同，PodList继承了metav1.ListMeta，metav1.ListMeta是所有资源对象列表类型的父类，
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// ListMeta定义了所有对象列表类型实例的公共属性。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ListMeta&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;metadata,omitempty&amp;#34; protobuf:&amp;#34;bytes,1,opt,name=metadata&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Items字段则是PodList定义的本质，表示Pod资源对象的列表，所以说PodList就是[]Pod基础上加了一些
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 跟类型和对象列表相关的元信息
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Items&lt;/span> []&lt;span style="color:#a6e22e">Pod&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;items&amp;#34; protobuf:&amp;#34;bytes,2,rep,name=items&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在开始下一节的内容之前，我们先做个小结：&lt;/p>
&lt;ol>
&lt;li>metav1.TypeMeta 和 metav1.ObjectMeta 是所有 API 单体资源对象的父类；&lt;/li>
&lt;li>metav1.TypeMeta 和 metav1.ListMeta 是所有 API 资源对象列表的父类；&lt;/li>
&lt;li>metav1.TypeMeta 是所有 API 资源对象的父类，因为所有的资源对象都要说明表示是什么类型；&lt;/li>
&lt;/ol>
&lt;h2 id="metav1">metav1&lt;/h2>
&lt;p>这里的 metav1 是包 k8s.io/apimachinery/pkg/apis/meta/v1 的别名，本文其他部分的将用 metav1 指代。&lt;/p>
&lt;h3 id="metav1typemeta">metav1.TypeMeta&lt;/h3>
&lt;p>metav1.TypeMeta 用来描述 kubernetes API 资源对象类型的元信息，包括资源类型的名字以及对应 API 的 schema。这里的 schema 指的是资源类型 API 分组以及版本。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/types.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// TypeMeta describes an individual object in an API response or request
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// with strings representing the type of the object and its API schema version.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Structures that are versioned or persisted should inline TypeMeta.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">TypeMeta&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Kind is a string value representing the REST resource this object represents.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Servers may infer this from the endpoint the client submits requests to.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Cannot be updated.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Kind&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;kind,omitempty&amp;#34; protobuf:&amp;#34;bytes,1,opt,name=kind&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// APIVersion defines the versioned schema of this representation of an object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Servers should convert recognized schemas to the latest internal value, and
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// may reject unrecognized values.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">APIVersion&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;apiVersion,omitempty&amp;#34; protobuf:&amp;#34;bytes,2,opt,name=apiVersion&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>细心的同学还会发现 metav1.TypeMeta 实现了 schema.ObjectKind 接口，schema.ObjectKind 接口了所有序列化对象怎么解码与编码资源类型信息的方法，它的完整定义如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/runtime/schema/interfaces.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// All objects that are serialized from a Scheme encode their type information. This interface is used
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// by serialization to set type information from the Scheme onto the serialized version of an object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// For objects that cannot be serialized or have unique requirements, this interface may be a no-op.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">ObjectKind&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// SetGroupVersionKind sets or clears the intended serialized kind of an object. Passing kind nil
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// should clear the current setting.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">SetGroupVersionKind&lt;/span>(&lt;span style="color:#a6e22e">kind&lt;/span> &lt;span style="color:#a6e22e">GroupVersionKind&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// GroupVersionKind returns the stored group, version, and kind of an object, or an empty struct
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// if the object does not expose or provide these fields.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">GroupVersionKind&lt;/span>() &lt;span style="color:#a6e22e">GroupVersionKind&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>从 metav1.TypeMeta 对象的实例（也就是任何 kubernetes API 资源对象）都可以通过 &lt;code>GetObjectKind()&lt;/code> 方法获取到 schema.ObjectKind 类型对象，而 TypeMeta 对象的实例本身也实现了 schema.ObjectKind 接口：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/types.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">obj&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">TypeMeta&lt;/span>) &lt;span style="color:#a6e22e">GetObjectKind&lt;/span>() &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">ObjectKind&lt;/span> { &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">obj&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// SetGroupVersionKind satisfies the ObjectKind interface for all objects that embed TypeMeta
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">obj&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">TypeMeta&lt;/span>) &lt;span style="color:#a6e22e">SetGroupVersionKind&lt;/span>(&lt;span style="color:#a6e22e">gvk&lt;/span> &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">GroupVersionKind&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">obj&lt;/span>.&lt;span style="color:#a6e22e">APIVersion&lt;/span>, &lt;span style="color:#a6e22e">obj&lt;/span>.&lt;span style="color:#a6e22e">Kind&lt;/span> = &lt;span style="color:#a6e22e">gvk&lt;/span>.&lt;span style="color:#a6e22e">ToAPIVersionAndKind&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// GroupVersionKind satisfies the ObjectKind interface for all objects that embed TypeMeta
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">obj&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">TypeMeta&lt;/span>) &lt;span style="color:#a6e22e">GroupVersionKind&lt;/span>() &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">GroupVersionKind&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">FromAPIVersionAndKind&lt;/span>(&lt;span style="color:#a6e22e">obj&lt;/span>.&lt;span style="color:#a6e22e">APIVersion&lt;/span>, &lt;span style="color:#a6e22e">obj&lt;/span>.&lt;span style="color:#a6e22e">Kind&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="metav1objectmeta">metav1.ObjectMeta&lt;/h3>
&lt;p>metav1.ObjectMeta 则用来定义资源对象实例的属性，即所有资源对象都应该具备的属性。这部分就是和对象本身相关，和类型无关，所以相同类型的资源对象的 metav1.ObjectMeta 可能是不同的。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/types.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// ObjectMeta is metadata that all persisted resources must have, which includes all objects
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// users must create.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">ObjectMeta&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Name must be unique within a namespace. Is required when creating resources, although
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// some resources may allow a client to request the generation of an appropriate name
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// automatically. Name is primarily intended for creation idempotence and configuration
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// definition.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Name&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;name,omitempty&amp;#34; protobuf:&amp;#34;bytes,1,opt,name=name&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// GenerateName is an optional prefix, used by the server, to generate a unique
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// name ONLY IF the Name field has not been provided.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system. Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">GenerateName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;generateName,omitempty&amp;#34; protobuf:&amp;#34;bytes,2,opt,name=generateName&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Namespace defines the space within which each name must be unique. An empty namespace is
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// equivalent to the &amp;#34;default&amp;#34; namespace, but &amp;#34;default&amp;#34; is the canonical representation.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Not all objects are required to be scoped to a namespace - the value of this field for
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// those objects will be empty.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Namespace&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;namespace,omitempty&amp;#34; protobuf:&amp;#34;bytes,3,opt,name=namespace&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// SelfLink is a URL representing this object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system. Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">SelfLink&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;selfLink,omitempty&amp;#34; protobuf:&amp;#34;bytes,4,opt,name=selfLink&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// UID is the unique in time and space value for this object. It is typically generated by
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// the server on successful creation of a resource and is not allowed to change on PUT
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// operations.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system. Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">UID&lt;/span> &lt;span style="color:#a6e22e">types&lt;/span>.&lt;span style="color:#a6e22e">UID&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;uid,omitempty&amp;#34; protobuf:&amp;#34;bytes,5,opt,name=uid,casttype=k8s.io/kubernetes/pkg/types.UID&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// An opaque value that represents the internal version of this object that can
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// be used by clients to determine when objects have changed. May be used for optimistic
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// concurrency, change detection, and the watch operation on a resource or set of resources.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Clients must treat these values as opaque and passed unmodified back to the server.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// They may only be valid for a particular resource or set of resources.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system. Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">ResourceVersion&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;resourceVersion,omitempty&amp;#34; protobuf:&amp;#34;bytes,6,opt,name=resourceVersion&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// A sequence number representing a specific generation of the desired state.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system. Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Generation&lt;/span> &lt;span style="color:#66d9ef">int64&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;generation,omitempty&amp;#34; protobuf:&amp;#34;varint,7,opt,name=generation&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// CreationTimestamp is a timestamp representing the server time when this object was
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// created. It is not guaranteed to be set in happens-before order across separate operations.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Clients may not set this value. It is represented in RFC3339 form and is in UTC.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system. Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">CreationTimestamp&lt;/span> &lt;span style="color:#a6e22e">Time&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;creationTimestamp,omitempty&amp;#34; protobuf:&amp;#34;bytes,8,opt,name=creationTimestamp&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// field is set by the server when a graceful deletion is requested by the user, and is not
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// directly settable by a client. The resource is expected to be deleted (no longer visible
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// from resource lists, and not reachable by name) after the time in this field, once the
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// finalizers list is empty. As long as the finalizers list contains items, deletion is blocked.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Once the deletionTimestamp is set, this value may not be unset or be set further into the
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// future, although it may be shortened or the resource may be deleted prior to this time.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// For example, a user may request that a pod is deleted in 30 seconds. The Kubelet will react
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// by sending a graceful termination signal to the containers in the pod. After that 30 seconds,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// the Kubelet will send a hard termination signal (SIGKILL) to the container and after cleanup,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// remove the pod from the API. In the presence of network partitions, this object may still
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// exist after this timestamp, until an administrator or automated process can determine the
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// resource is fully terminated.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// If not set, graceful deletion of the object has not been requested.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system when a graceful deletion is requested.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">DeletionTimestamp&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">Time&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;deletionTimestamp,omitempty&amp;#34; protobuf:&amp;#34;bytes,9,opt,name=deletionTimestamp&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Number of seconds allowed for this object to gracefully terminate before
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// it will be removed from the system. Only set when deletionTimestamp is also set.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// May only be shortened.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">DeletionGracePeriodSeconds&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">int64&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;deletionGracePeriodSeconds,omitempty&amp;#34; protobuf:&amp;#34;varint,10,opt,name=deletionGracePeriodSeconds&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Map of string keys and values that can be used to organize and categorize
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// (scope and select) objects. May match selectors of replication controllers
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// and services.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Labels&lt;/span> &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;labels,omitempty&amp;#34; protobuf:&amp;#34;bytes,11,rep,name=labels&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Annotations is an unstructured key value map stored with a resource that may be
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// set by external tools to store and retrieve arbitrary metadata. They are not
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// queryable and should be preserved when modifying objects.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Annotations&lt;/span> &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;annotations,omitempty&amp;#34; protobuf:&amp;#34;bytes,12,rep,name=annotations&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// List of objects depended by this object. If ALL objects in the list have
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// been deleted, this object will be garbage collected. If this object is managed by a controller,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// then an entry in this list will point to this controller, with the controller field set to true.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">OwnerReferences&lt;/span> []&lt;span style="color:#a6e22e">OwnerReference&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;ownerReferences,omitempty&amp;#34; patchStrategy:&amp;#34;merge&amp;#34; patchMergeKey:&amp;#34;uid&amp;#34; protobuf:&amp;#34;bytes,13,rep,name=ownerReferences&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Must be empty before the object is deleted from the registry. Each entry
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// is an identifier for the responsible component that will remove the entry
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// from the list. If the deletionTimestamp of the object is non-nil, entries
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// in this list can only be removed.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Finalizers may be processed and removed in any order. Order is NOT enforced
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// because it introduces significant risk of stuck finalizers.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// finalizers is a shared field, any actor with permission can reorder it.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// If the finalizer list is processed in order, then this can lead to a situation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// in which the component responsible for the first finalizer in the list is
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// waiting for a signal (field value, external system, or other) produced by a
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// component responsible for a finalizer later in the list, resulting in a deadlock.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Without enforced ordering finalizers are free to order amongst themselves and
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// are not vulnerable to ordering changes in the list.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Finalizers&lt;/span> []&lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;finalizers,omitempty&amp;#34; patchStrategy:&amp;#34;merge&amp;#34; protobuf:&amp;#34;bytes,14,rep,name=finalizers&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// The name of the cluster which the object belongs to.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// This is used to distinguish resources with same name and namespace in different clusters.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">ClusterName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;clusterName,omitempty&amp;#34; protobuf:&amp;#34;bytes,15,opt,name=clusterName&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// ManagedFields maps workflow-id and version to the set of fields
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// that are managed by that workflow. This is mostly for internal
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// housekeeping, and users typically shouldn&amp;#39;t need to set or
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// understand this field. A workflow can be the user&amp;#39;s name, a
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// controller&amp;#39;s name, or the name of a specific apply path like
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// &amp;#34;ci-cd&amp;#34;. The set of fields is always in the version that the
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// workflow used when modifying the object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">ManagedFields&lt;/span> []&lt;span style="color:#a6e22e">ManagedFieldsEntry&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;managedFields,omitempty&amp;#34; protobuf:&amp;#34;bytes,17,rep,name=managedFields&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>metav1.ObjectMeta 还实现了 metav1.Object 与 metav1.MetaAccessor 这两个接口，其中 metav1.Object 接口定义了获取单个资源对象各种元信息的 Get 与 Set 方法：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/meta.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Object lets you work with object metadata from any of the versioned or
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// internal API objects. Attempting to set or retrieve a field on an object that does
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// not support that field (Name, UID, Namespace on lists) will be a no-op and return
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// a default value.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Object&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetNamespace&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetNamespace&lt;/span>(&lt;span style="color:#a6e22e">namespace&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetName&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetName&lt;/span>(&lt;span style="color:#a6e22e">name&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetGenerateName&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetGenerateName&lt;/span>(&lt;span style="color:#a6e22e">name&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetUID&lt;/span>() &lt;span style="color:#a6e22e">types&lt;/span>.&lt;span style="color:#a6e22e">UID&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetUID&lt;/span>(&lt;span style="color:#a6e22e">uid&lt;/span> &lt;span style="color:#a6e22e">types&lt;/span>.&lt;span style="color:#a6e22e">UID&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetResourceVersion&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetResourceVersion&lt;/span>(&lt;span style="color:#a6e22e">version&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetGeneration&lt;/span>() &lt;span style="color:#66d9ef">int64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetGeneration&lt;/span>(&lt;span style="color:#a6e22e">generation&lt;/span> &lt;span style="color:#66d9ef">int64&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetSelfLink&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetSelfLink&lt;/span>(&lt;span style="color:#a6e22e">selfLink&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetCreationTimestamp&lt;/span>() &lt;span style="color:#a6e22e">Time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetCreationTimestamp&lt;/span>(&lt;span style="color:#a6e22e">timestamp&lt;/span> &lt;span style="color:#a6e22e">Time&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetDeletionTimestamp&lt;/span>() &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">Time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetDeletionTimestamp&lt;/span>(&lt;span style="color:#a6e22e">timestamp&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">Time&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetDeletionGracePeriodSeconds&lt;/span>() &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">int64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetDeletionGracePeriodSeconds&lt;/span>(&lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">int64&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetLabels&lt;/span>() &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetLabels&lt;/span>(&lt;span style="color:#a6e22e">labels&lt;/span> &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetAnnotations&lt;/span>() &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetAnnotations&lt;/span>(&lt;span style="color:#a6e22e">annotations&lt;/span> &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetFinalizers&lt;/span>() []&lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetFinalizers&lt;/span>(&lt;span style="color:#a6e22e">finalizers&lt;/span> []&lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetOwnerReferences&lt;/span>() []&lt;span style="color:#a6e22e">OwnerReference&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetOwnerReferences&lt;/span>([]&lt;span style="color:#a6e22e">OwnerReference&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetClusterName&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetClusterName&lt;/span>(&lt;span style="color:#a6e22e">clusterName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetManagedFields&lt;/span>() []&lt;span style="color:#a6e22e">ManagedFieldsEntry&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetManagedFields&lt;/span>(&lt;span style="color:#a6e22e">managedFields&lt;/span> []&lt;span style="color:#a6e22e">ManagedFieldsEntry&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>metav1.MetaAccessor 接口则定义了获取资源对象存取器的方法：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/meta.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">ObjectMetaAccessor&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetObjectMeta&lt;/span>() &lt;span style="color:#a6e22e">Object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>因为 kubernetes 所有单体资源对象都继承了 metav1.ObjectMeta，那么所有的 API 资源对象就都实现了 metav1.Object 和 metav1.MetaAccessor 接口。kubernetes 中有很多地方访问 API 资源对象的元信息并且不区分对象类型，只要是 metav1.Object 接口类型的对象都可以访问。&lt;/p>
&lt;h3 id="metav1listmeta">metav1.ListMeta&lt;/h3>
&lt;p>metav1.ListMeta 定义了所有对象列表类型实例的公共属性。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/types.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// ListMeta describes metadata that synthetic resources must have, including lists and
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// various status objects. A resource may have only one of {ObjectMeta, ListMeta}.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">ListMeta&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// selfLink is a URL representing this object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">SelfLink&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;selfLink,omitempty&amp;#34; protobuf:&amp;#34;bytes,1,opt,name=selfLink&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// String that identifies the server&amp;#39;s internal version of this object that
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// can be used by clients to determine when objects have changed.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Value must be treated as opaque by clients and passed unmodified back to the server.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Populated by the system.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Read-only.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">ResourceVersion&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;resourceVersion,omitempty&amp;#34; protobuf:&amp;#34;bytes,2,opt,name=resourceVersion&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// continue may be set if the user set a limit on the number of items returned, and indicates that
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// the server has more data available. The value is opaque and may be used to issue another request
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// to the endpoint that served this list to retrieve the next set of available objects. Continuing a
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// consistent list may not be possible if the server configuration has changed or more than a few
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// minutes have passed. The resourceVersion field returned when using this continue value will be
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// identical to the value in the first response, unless you have received this token from an error
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// message.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Continue&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;continue,omitempty&amp;#34; protobuf:&amp;#34;bytes,3,opt,name=continue&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// remainingItemCount is the number of subsequent items in the list which are not included in this
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// list response. If the list request contained label or field selectors, then the number of
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// remaining items is unknown and the field will be left unset and omitted during serialization.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// If the list is complete (either because it is not chunking or because this is the last chunk),
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// then there are no more remaining items and this field will be left unset and omitted during
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// serialization.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// Servers older than v1.15 do not set this field.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// The intended use of the remainingItemCount is *estimating* the size of a collection. Clients
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// should not rely on the remainingItemCount to be set or to be exact.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">RemainingItemCount&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">int64&lt;/span> &lt;span style="color:#e6db74">`json:&amp;#34;remainingItemCount,omitempty&amp;#34; protobuf:&amp;#34;bytes,4,opt,name=remainingItemCount&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>类似于与 metav1.ObjectMeta 结构体，metav1.ListMeta 还实现了 metav1.ListInterface 与 metav1.ListMetaAccessor 这两个接口，其中 metav1.ListInterface 接口定义了获取资源对象列表各种元信息的 Get 与 Set 方法：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/meta.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// ListInterface lets you work with list metadata from any of the versioned or
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// internal API objects. Attempting to set or retrieve a field on an object that does
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// not support that field will be a no-op and return a default value.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">ListInterface&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetResourceVersion&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetResourceVersion&lt;/span>(&lt;span style="color:#a6e22e">version&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetSelfLink&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetSelfLink&lt;/span>(&lt;span style="color:#a6e22e">selfLink&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetContinue&lt;/span>() &lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetContinue&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetRemainingItemCount&lt;/span>() &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">int64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">SetRemainingItemCount&lt;/span>(&lt;span style="color:#a6e22e">c&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">int64&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>metav1.ListMetaAccessor 接口则定义了获取资源对象列表存取器的方法：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/meta.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// ListMetaAccessor retrieves the list interface from an object
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">ListMetaAccessor&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetListMeta&lt;/span>() &lt;span style="color:#a6e22e">ListInterface&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="runtimeobject">runtime.Object&lt;/h3>
&lt;p>前面在介绍 metav1.TypeMeta 与 metav1.ObjectMeta 的时候我们发现 schema.ObjecKind 是所有 API 资源类型的抽象，metav1.Object 是所有 API 单体资源对象属性的抽象，那么同时实现这两个接口的类型对象不就可以访问任何 API 对象的公共属性了吗？是的，对于每一个特定的类型，如 Pod、Deployment 等，它们确实可以获取当前 API 对象的公共属性。有没有一种所有特定类型的统一父类，同时拥有 schema.ObjecKind 和 metav1.Object 两个接口，这样就可以表示任何特定类型的对象。这就是本节要讨论 runtime.Object 接口。
先来看看 runtime.Object 接口定义：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from: https://github.com/kubernetes/apimachinery/blob/master/pkg/runtime/interfaces.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Object interface must be supported by all API types registered with Scheme. Since objects in a scheme are
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// expected to be serialized to the wire, the interface an Object must provide to the Scheme allows
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// serializers to set the kind, version, and group the object is represented as. An Object may choose
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// to return a no-op ObjectKindAccessor in cases where it is not expected to be serialized.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Object&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// used to access type metadata(GVK)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">GetObjectKind&lt;/span>() &lt;span style="color:#a6e22e">schema&lt;/span>.&lt;span style="color:#a6e22e">ObjectKind&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// DeepCopyObject needed to implemented by each kubernetes API type definition,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// usually by automatically generated code.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">DeepCopyObject&lt;/span>() &lt;span style="color:#a6e22e">Object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>为什么 runtime.Object 接口只有这两个方法，不应该有 GetObjectMeta() 方法来获取 metav1.ObjectMeta 对象吗？仔细往下看的话会发现，这里使用了不一样的实现方式：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from: https://github.com/kubernetes/apimachinery/blob/master/pkg/api/meta/meta.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Accessor takes an arbitrary object pointer and returns meta.Interface.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// obj must be a pointer to an API type. An error is returned if the minimum
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// required fields are missing. Fields that are not required return the default
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// value and are a no-op if set.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">Accessor&lt;/span>(&lt;span style="color:#a6e22e">obj&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span>{}) (&lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">Object&lt;/span>, &lt;span style="color:#66d9ef">error&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">switch&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">obj&lt;/span>.(&lt;span style="color:#66d9ef">type&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">case&lt;/span> &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">Object&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>, &lt;span style="color:#66d9ef">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">case&lt;/span> &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ObjectMetaAccessor&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">m&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">t&lt;/span>.&lt;span style="color:#a6e22e">GetObjectMeta&lt;/span>(); &lt;span style="color:#a6e22e">m&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">m&lt;/span>, &lt;span style="color:#66d9ef">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span>, &lt;span style="color:#a6e22e">errNotObject&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">default&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span>, &lt;span style="color:#a6e22e">errNotObject&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Accessor 方法可以讲任何的类型 metav1.Object 或者返回错误信息，这样就避免了每个 API 资源类型都需要实现 GetObjectMeta() 方法了。
还有个问题是为什么没有看到 API 资源类型实现 runtime.Object.DeepCopyObject() 方法？那是因为深拷贝方法是具体 API 资源类型需要重载实现的，存在类型依赖，作为 API 资源类型的父类不能统一实现。一般来说，深拷贝方法是由工具自动生成的，定义在 &lt;code>zz_generated.deepcopy.go&lt;/code> 文件中，以 configMap 为例：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// source code from https://github.com/kubernetes/api/blob/master/core/v1/zz_generated.deepcopy.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> (&lt;span style="color:#a6e22e">in&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">ConfigMap&lt;/span>) &lt;span style="color:#a6e22e">DeepCopyObject&lt;/span>() &lt;span style="color:#a6e22e">runtime&lt;/span>.&lt;span style="color:#a6e22e">Object&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">in&lt;/span>.&lt;span style="color:#a6e22e">DeepCopy&lt;/span>(); &lt;span style="color:#a6e22e">c&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">c&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="metav1unstructured">metav1.Unstructured&lt;/h3>
&lt;p>metav1.Unstructured 与具体实现 rutime.Object 接口的类型（如 Pod、Deployment、Service 等等）不同，如果说，各个实现 rutime.Object 接口的类型主要用于 &lt;strong>client-go&lt;/strong>[1] 类型化的静态客户端，那么 metav1.Unstructured 则用于动态客户端。
在看 metav1.Unstructured 源码实现之前，我们先了解一下什么是结构化数据与非结构化数据。结构化数据，顾名思义，就是数据中的字段名与字段值都是固定的，例如一个 JSON 格式的字符串表示一个学生的信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;id&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">101&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Tom&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>定义这个学生的数据格式中的字段名与字段值都是固定的，我们很容易使用 Go 语言写出一个 struct 结构来表示这个学生的信息，各个字段意义明确：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Student&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ID&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Name&lt;/span> &lt;span style="color:#a6e22e">String&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>实际的情况是，一个格式化的字符串里面可能会包含很多编译时未知的信息，这些信息只有在运行时才能获取到。例如，上面的学生的数据中还包括第三个字段，该字段的类型和内容代码编译时未知，到运行时才可以获取具体的值。如何处理这种情况呢？熟悉反射的同学很快就应该想到，Go 语言可以依赖于反射机制在运行时动态获取各个字段，在编译阶段，我们将这些未知的类型统一为 &lt;code>interface{}&lt;/code>。正是基于此，metav1.Unstructured 的数据结构定义很简单：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// soure code from: https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/unstructured/unstructured.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Unstructured&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Object is a JSON compatible map with string, float, int, bool, []interface{}, or
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// map[string]interface{} children.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">Object&lt;/span> &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>事实上，metav1.Unstructured 是 apimachinery 中 runtime.Unstructured 接口的具体实现，runtime.Unstructured 接口定义了非结构化数据的操作接口方法列表，它提供程序来处理资源的通用属性，例如 metadata.namespace 等。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// soure code from: https://github.com/kubernetes/apimachinery/blob/master/pkg/runtime/interfaces.go
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Unstructured objects store values as map[string]interface{}, with only values that can be serialized
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// to JSON allowed.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Unstructured&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// NewEmptyInstance returns a new instance of the concrete type containing only kind/apiVersion and no other data.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// This should be called instead of reflect.New() for unstructured types because the go type alone does not preserve kind/apiVersion info.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">NewEmptyInstance&lt;/span>() &lt;span style="color:#a6e22e">Unstructured&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// UnstructuredContent returns a non-nil map with this object&amp;#39;s contents. Values may be
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// []interface{}, map[string]interface{}, or any primitive type. Contents are typically serialized to
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// and from JSON. SetUnstructuredContent should be used to mutate the contents.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">UnstructuredContent&lt;/span>() &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// SetUnstructuredContent updates the object content to match the provided map.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">SetUnstructuredContent&lt;/span>(&lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// IsList returns true if this type is a list or matches the list convention - has an array called &amp;#34;items&amp;#34;.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">IsList&lt;/span>() &lt;span style="color:#66d9ef">bool&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// EachListItem should pass a single item out of the list as an Object to the provided function. Any
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// error should terminate the iteration. If IsList() returns false, this method should return an error
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// instead of calling the provided function.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">EachListItem&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">Object&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>只有 metav1.Unstructured 的定义并不能发挥什么作用，真正重要的是其实现的方法，借助这些方法可以灵活的处理非结构化数据。metav1.Unstructured 实现了存取类型元信息与对象元信息的方法，除此之外，它也实现了 runtime.Unstructured 接口中的所有方法。
基于这些方法，我们可以构建操作 kubernetes 资源的动态客户端，不需要使用 k8s.io/api 中定义的 Go 类型，使用 metav1.Unstructured 非结构化直接解码是 YAML/JSON 对象表示形式；非结构化数据编码时生成的 JSON/YAML 外也不会添加额外的字段。
以下示例演示了如何将 YAML 清单读为非结构化，非结构化并将其编码回 JSON：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;encoding/json&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;fmt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;os&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1/unstructured&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;k8s.io/apimachinery/pkg/runtime/serializer/yaml&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">dsManifest&lt;/span> = &lt;span style="color:#e6db74">`
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">kind: DaemonSet
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> name: example
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> namespace: default
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> matchLabels:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> name: nginx-ds
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> template:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> labels:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> name: nginx-ds
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> containers:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - name: nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> image: nginx:latest
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">obj&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">unstructured&lt;/span>.&lt;span style="color:#a6e22e">Unstructured&lt;/span>{}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// decode YAML into unstructured.Unstructured
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">dec&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">yaml&lt;/span>.&lt;span style="color:#a6e22e">NewDecodingSerializer&lt;/span>(&lt;span style="color:#a6e22e">unstructured&lt;/span>.&lt;span style="color:#a6e22e">UnstructuredJSONScheme&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">_&lt;/span>, &lt;span style="color:#a6e22e">gvk&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">dec&lt;/span>.&lt;span style="color:#a6e22e">Decode&lt;/span>([]byte(&lt;span style="color:#a6e22e">dsManifest&lt;/span>), &lt;span style="color:#66d9ef">nil&lt;/span>, &lt;span style="color:#a6e22e">obj&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// Get the common metadata, and show GVK
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Println&lt;/span>(&lt;span style="color:#a6e22e">obj&lt;/span>.&lt;span style="color:#a6e22e">GetName&lt;/span>(), &lt;span style="color:#a6e22e">gvk&lt;/span>.&lt;span style="color:#a6e22e">String&lt;/span>())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// encode back to JSON
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">enc&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">NewEncoder&lt;/span>(&lt;span style="color:#a6e22e">os&lt;/span>.&lt;span style="color:#a6e22e">Stdout&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">enc&lt;/span>.&lt;span style="color:#a6e22e">SetIndent&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34; &amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">enc&lt;/span>.&lt;span style="color:#a6e22e">Encode&lt;/span>(&lt;span style="color:#a6e22e">obj&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>程序的输出如下：&lt;/p>
&lt;pre>&lt;code>example apps/v1, Kind=DaemonSet
{
&amp;quot;apiVersion&amp;quot;: &amp;quot;apps/v1&amp;quot;,
&amp;quot;kind&amp;quot;: &amp;quot;DaemonSet&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;example&amp;quot;,
&amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;
},
&amp;quot;spec&amp;quot;: {
&amp;quot;selector&amp;quot;: {
&amp;quot;matchLabels&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;nginx-ds&amp;quot;
}
},
&amp;quot;template&amp;quot;: {
&amp;quot;metadata&amp;quot;: {
&amp;quot;labels&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;nginx-ds&amp;quot;
}
},
&amp;quot;spec&amp;quot;: {
&amp;quot;containers&amp;quot;: [
{
&amp;quot;image&amp;quot;: &amp;quot;nginx:latest&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;nginx&amp;quot;
}
]
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>此外，通过 Go 语言的反射机制可以实现 metav1.Unstructured 对象与具体资源对象的相互转换，runtime.unstructuredConverter 接口定义了 metav1.Unstructured 对象与具体资源对象的相互转换方法，并且内置了 runtime.DefaultUnstructuredConverter 实现了 runtime.unstructuredConverter 接口。&lt;/p>
&lt;pre>&lt;code>// source code from: https://github.com/kubernetes/apimachinery/blob/master/pkg/runtime/converter.go
//
// UnstructuredConverter is an interface for converting between interface{}
// and map[string]interface representation.
type UnstructuredConverter interface {
ToUnstructured(obj interface{}) (map[string]interface{}, error)
FromUnstructured(u map[string]interface{}, obj interface{}) error
}
&lt;/code>&lt;/pre>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>为了便于记忆，现在对前面介绍的各种接口以及实现做一个小结：&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tbi8lr/1616564773044-5ecb4b10-4253-489f-a556-a867533823f6.webp" alt="">&lt;/p>
&lt;ol>
&lt;li>runtime.Object 接口是所有 API 单体资源对象的根类，各个 API 对象的编码与解码依赖于该接口类型；&lt;/li>
&lt;li>schema.ObjectKind 接口是对 API 资源对象类型的抽象，可以用来获取或者设置 GVK；&lt;/li>
&lt;li>metav1.Object 接口是 API 资源对象属性的抽象，用来存取资源对象的属性；&lt;/li>
&lt;li>metav1.ListInterface 接口是 API 对象列表属性的抽象，用来存取资源对象列表的属性；&lt;/li>
&lt;li>metav1.TypeMeta 结构体实现了 schema.ObjectKind 接口，所有的 API 资源类型继承它；&lt;/li>
&lt;li>metav1.ObjectMeta 结构体实现了 metav1.Object 接口，所有的 API 资源类型继承它；&lt;/li>
&lt;li>metav1.ListMeta 结构体实现了 metav1.ListInterface 接口，所有的 API 资源对象列表类型继承它；&lt;/li>
&lt;li>metav1.Unstructured 结构体实现了 runtime.Unstructured 接口，可以用于构建动态客户端，从 metav1.Unstructured 的实例中可以获取资源类型元信息与资源对象元信息，还可以获取到对象的 map[string]interface{} 的通用内容表示；&lt;/li>
&lt;li>metav1.Unstructured 与实现了 metav1.Object 接口具体 API 类型进行相互转化，转换依赖于 runtime.UnstructuredConverter 的接口方法。&lt;/li>
&lt;/ol>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;p>[1]
client-go: &lt;a href="https://github.com/kubernetes/client-go">&lt;em>https://github.com/kubernetes/client-go&lt;/em>&lt;/a>&lt;/p></description></item><item><title>Docs: Argo CD</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/devops/argocd/argo-cd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/devops/argocd/argo-cd/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/argoproj/argo-cd">GitHub 项目，argoproj/argo-cd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/r1DnnHptOTaS_Gp8tpPWdg">Argo CD 保姆级入门教程&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在上一篇『👉&lt;a href="http://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&amp;amp;mid=2247509873&amp;amp;idx=1&amp;amp;sn=dd6daee66f39a965e4680ecd91f884d7&amp;amp;chksm=fbede7bccc9a6eaa6ddf5d082ed20a31d1956425eb8129428b2e2701fb216ef331a706f6d008&amp;amp;scene=21#wechat_redirect">GitOps 介绍[1]&lt;/a>』中，我介绍了什么是 GitOps，包括 GitOps 的原则和优势，以及 GitOps 与 DevOps 的区别。本文将介绍用于实施 GitOps 的工具 Argo CD。&lt;/p>
&lt;p>Argo CD 是以 Kubernetes 作为基础设施，遵循声明式 GitOps 理念的持续交付（continuous delivery, CD）工具，支持多种配置管理工具，包括 ksonnet/jsonnet、kustomize 和 Helm 等。它的配置和使用非常简单，并且自带一个简单易用的可视化界面。&lt;/p>
&lt;p>按照官方定义，Argo CD 被实现为一个 Kubernetes 控制器，它会持续监控正在运行的应用，并将当前的实际状态与 Git 仓库中声明的期望状态进行比较，如果实际状态不符合期望状态，就会更新应用的实际状态以匹配期望状态。&lt;/p>
&lt;p>在正式开始解读和使用 Argo CD 之前，我们需要先搞清楚为什么需要 Argo CD？它能给我们带来什么价值？&lt;/p>
&lt;h2 id="传统-cd-工作流">传统 CD 工作流&lt;/h2>
&lt;p>从上篇文章『👉&lt;a href="http://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&amp;amp;mid=2247509873&amp;amp;idx=1&amp;amp;sn=dd6daee66f39a965e4680ecd91f884d7&amp;amp;chksm=fbede7bccc9a6eaa6ddf5d082ed20a31d1956425eb8129428b2e2701fb216ef331a706f6d008&amp;amp;scene=21#wechat_redirect">GitOps 介绍[2]&lt;/a>』可以知道，目前大多数 CI/CD 工具都使用基于 Push 的部署模式，例如 Jenkins、CircleCI 等。这种模式一般都会在 CI 流水线运行完成后执行一个命令（比如 kubectl）将应用部署到目标环境中。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076442944-8d47843e-5af5-4d31-ba22-3e8e6194bc88.jpeg" alt="">&lt;/p>
&lt;p>这种 CD 模式的缺陷很明显：&lt;/p>
&lt;ul>
&lt;li>需要安装配置额外工具（比如 kubectl）；&lt;/li>
&lt;li>需要 Kubernetes 对其进行授权；&lt;/li>
&lt;li>需要云平台授权；&lt;/li>
&lt;li>无法感知部署状态。也就无法感知期望状态与实际状态的偏差，需要借助额外的方案来保障一致性。&lt;/li>
&lt;/ul>
&lt;p>下面以 Argo CD 为例，来看看遵循声明式 GitOps 理念的 CD 工具是怎么实现的。&lt;/p>
&lt;h2 id="使用-argo-cd-的-cd-工作流">使用 Argo CD 的 CD 工作流&lt;/h2>
&lt;p>和传统 CI/CD 工具一样，CI 部分并没有什么区别，无非就是测试、构建镜像、推送镜像、修改部署清单等等。重点在于 CD 部分。&lt;/p>
&lt;p>Argo CD 会被部署在 Kubernetes 集群中，使用的是基于 Pull 的部署模式，它会周期性地监控应用的实际状态，也会周期性地拉取 Git 仓库中的配置清单，并将实际状态与期望状态进行比较，如果实际状态不符合期望状态，就会更新应用的实际状态以匹配期望状态。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443010-40f73633-ab15-4158-8e71-f792a2b8fe11.png" alt="">&lt;/p>
&lt;p>无论是通过 CI 流水线触发更新 K8s 编排文件，还是 DevOps 工程师直接修改 K8s 编排文件，Argo CD 都会自动拉取最新的配置并应用到 K8s 集群中。&lt;/p>
&lt;p>最终会得到一个相互隔离的 CI 与 CD 流水线，CI 流水线通常由研发人员（或者 DevOps 团队）控制，CD 流水线通常由集群管理员（或者 DevOps 团队）控制。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076442866-9f57ee65-a081-468a-8013-cb6319c73a51.png" alt="">&lt;/p>
&lt;h2 id="argo-cd-的优势">Argo CD 的优势&lt;/h2>
&lt;p>下面我们来看看 Argo CD 相较于传统 CD 工具有哪些比较明显的优势。&lt;/p>
&lt;h3 id="git-作为应用的唯一真实来源">Git 作为应用的唯一真实来源&lt;/h3>
&lt;p>所有 K8s 的声明式配置都保存在 Git 中，并把 Git 作为应用的唯一事实来源，我们不再需要手动更新应用（比如执行脚本，执行 kubectl apply 或者 helm install 命令），只需要通过统一的接口（Git）来更新应用。&lt;/p>
&lt;p>此外，Argo CD 不仅会监控 Git 仓库中声明的期望状态，还会监控集群中应用的实际状态，并将两种状态进行对比，只要实际状态不符合期望状态，实际状态就会被修正与期望状态一致。所以即使有人修改了集群中应用的状态（比如修改了副本数量），Argo CD 还是会将其恢复到之前的状态。&lt;strong>这就真正确保了 Git 仓库中的编排文件可以作为集群状态的唯一真实来源。&lt;/strong>&lt;/p>
&lt;p>当然，有时候我们需要快速更新应用并进行调试，通过 Git 来触发更新还是慢了点，这也不是没有办法，我们可以修改 Argo CD 的配置，使其不对手动修改的部分进行覆盖或者回退，而是直接发送告警，提醒管理员不要忘了将更新提交到 Git 仓库中。&lt;/p>
&lt;h3 id="快速回滚">快速回滚&lt;/h3>
&lt;p>Argo CD 会定期拉取最新配置并应用到集群中，一旦最新的配置导致应用出现了故障（比如应用启动失败），我们可以通过 Git History 将应用状态快速恢复到上一个可用的状态。&lt;/p>
&lt;p>如果你有多个 Kubernetes 集群使用同一个 Git 仓库，这个优势会更明显，因为你不需要分别在不同的集群中通过 &lt;code>kubectl delete&lt;/code> 或者 &lt;code>helm uninstall&lt;/code> 等手动方式进行回滚，只需要将 Git 仓库回滚到上一个可用的版本，Argo CD 便会自动同步。&lt;/p>
&lt;h3 id="集群灾备">集群灾备&lt;/h3>
&lt;p>如果你在青云[3]北京 3 区中的 KubeSphere[4] 集群出现故障，且短期内不可恢复，可以直接创建一个新集群，然后将 Argo CD 连接到 Git 仓库，这个仓库包含了整个集群的所有配置声明。最终新集群的状态会与之前旧集群的状态一致，完全不需要人工干预。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076442877-2851ab43-7392-4cf9-8328-2403d8d2c586.png" alt="">&lt;/p>
&lt;h3 id="使用-git-实现访问控制">使用 Git 实现访问控制&lt;/h3>
&lt;p>通常在生产环境中是不允许所有人访问 Kubernetes 集群的，如果直接在 Kubernetes 集群中控制访问权限，必须要使用复杂的 RBAC 规则。在 Git 仓库中控制权限就比较简单了，例如所有人（DevOps 团队，运维团队，研发团队，等等）都可以向仓库中提交 Pull Request，但只有高级工程师可以合并 Pull Request。&lt;/p>
&lt;p>这样做的好处是，除了集群管理员和少数人员之外，其他人不再需要直接访问 Kubernetes 集群，只需访问 Git 仓库即可。对于程序而言也是如此，类似于 Jenkins 这样的 CI 工具也不再需要访问 Kubernetes 的权限，因为只有 Argo CD 才可以 apply 配置清单，而且 Argo CD 已经部署在 Kubernetes 集群中，必要的访问权限已经配置妥当，这样就不需要给集群外的任意人或工具提供访问的证书，可以提供更强大的安全保障。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443389-772dc87a-8c2c-4ba9-aad2-310ec93fcb50.png" alt="">&lt;/p>
&lt;h3 id="扩展-kubernetes">扩展 Kubernetes&lt;/h3>
&lt;p>虽然 Argo CD 可以部署在 Kubernetes 集群中，享受 Kubernetes 带来的好处，但这不是 Argo CD 专属的呀！Jenkins 不是也可以部署在 Kubernetes 中吗？Argo CD 有啥特殊的吗？&lt;/p>
&lt;p>那当然有了，没这金刚钻也不敢揽这瓷器活啊，Argo CD 巧妙地利用了 Kubernetes 集群中的很多功能来实现自己的目的，例如所有的资源都存储在 Etcd 集群中，利用 Kubernetes 的控制器来监控应用的实际状态并与期望状态进行对比，等等。&lt;/p>
&lt;p>这样做最直观的好处就是&lt;strong>可以实时感知应用的部署状态&lt;/strong>。例如，当你在 Git 仓库中更新配置清单中的镜像版本后，Argo CD 会将集群中的应用更新到最新版本，你可以在 Argo CD 的可视化界面中实时查看更新状态（比如 Pod 创建成功，应用成功运行并且处于健康状态，或者应用运行失败需要进行回滚操作）。&lt;/p>
&lt;h2 id="argo-cd-架构">Argo CD 架构&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443403-3ee90747-ee9b-44c4-9990-0ec25bff6e51.png" alt="">&lt;/p>
&lt;p>从功能架构来看，Argo CD 主要有三个组件：API Server、Repository Server 和 Application Controller。从 GitOps 工作流的角度来看，总共分为 3 个阶段：检索、调谐和呈现。&lt;/p>
&lt;h3 id="检索----repository-server">检索 &amp;ndash; Repository Server&lt;/h3>
&lt;p>检索阶段会克隆应用声明式配置清单所在的 Git 仓库，并将其缓存到本地存储。包含 Kubernetes 原生的配置清单、Helm Chart 以及 Kustomize 配置清单。履行这些职责的组件就是 &lt;strong>Repository Server&lt;/strong>。&lt;/p>
&lt;h3 id="调谐----application-controller">调谐 &amp;ndash; Application Controller&lt;/h3>
&lt;p>调谐（Reconcile）阶段是最复杂的，这个阶段会将 &lt;strong>Repository Server&lt;/strong> 获得的配置清单与反映集群当前状态的实时配置清单进行对比，一旦检测到应用处于 &lt;code>OutOfSync&lt;/code> 状态，&lt;strong>Application Controller&lt;/strong> 就会采取修正措施，使集群的实际状态与期望状态保持一致。&lt;/p>
&lt;h3 id="呈现----api-server">呈现 &amp;ndash; API Server&lt;/h3>
&lt;p>最后一个阶段是呈现阶段，由 Argo CD 的 &lt;strong>API Server&lt;/strong> 负责，它本质上是一个 gRPC/REST Server，提供了一个无状态的可视化界面，用于展示调谐阶段的结果。同时还提供了以下这些功能：&lt;/p>
&lt;ul>
&lt;li>应用管理和状态报告；&lt;/li>
&lt;li>调用与应用相关的操作（例如同步、回滚、以及用户自定义的操作）；&lt;/li>
&lt;li>Git 仓库与集群凭证管理（以 Kubernetes Secret 的形式存储）；&lt;/li>
&lt;li>为外部身份验证组件提供身份验证和授权委托；&lt;/li>
&lt;li>RBAC 增强；&lt;/li>
&lt;li>Git Webhook 事件的监听器/转发器。&lt;/li>
&lt;/ul>
&lt;h2 id="部署-argo-cd">部署 Argo CD&lt;/h2>
&lt;p>Argo CD 有两种不同的部署模式：&lt;/p>
&lt;h3 id="多租户">多租户&lt;/h3>
&lt;p>Argo CD 最常用的部署模式是多租户，一般如果组织内部包含多个应用研发团队，就会采用这种部署模式。用户可以使用可视化界面或者 argocd CLI 来访问 Argo CD。argocd CLI 必须先通过 &lt;code>argocd login &amp;lt;server-host&amp;gt;&lt;/code> 来获取 Argo CD 的访问授权。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> argocd login SERVER [flags]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## Login to Argo CD using a username and password
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> argocd login cd.argoproj.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## Login to Argo CD using SSO
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> argocd login cd.argoproj.io &lt;span style="color:#f92672">--&lt;/span>sso
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## Configure direct access using Kubernetes API server
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> argocd login cd.argoproj.io &lt;span style="color:#f92672">--&lt;/span>core
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>多租户模式提供了两种不同的配置清单：&lt;/p>
&lt;h4 id="非高可用">非高可用&lt;/h4>
&lt;p>推荐用于测试和演示环境，不推荐在生产环境下使用。有两种部署清单可供选择：&lt;/p>
&lt;ul>
&lt;li>install.yaml[5] - 标准的 Argo CD 部署清单，拥有集群管理员权限。可以使用 Argo CD 在其运行的集群内部署应用程序，也可以通过接入外部集群的凭证将应用部署到外部集群中。&lt;/li>
&lt;li>namespace-install.yaml[6] - 这个部署清单只需要 namespace 级别的权限。如果你不需要在 Argo CD 运行的集群中部署应用，只需通过接入外部集群的凭证将应用部署到外部集群中，推荐使用此部署清单。还有一种花式玩法，你可以为每个团队分别部署单独的 Argo CD 实例，但是每个 Argo CD 实例都可以使用特殊的凭证（例如 &lt;code>argocd cluster add &amp;lt;CONTEXT&amp;gt; --in-cluster --namespace &amp;lt;YOUR NAMESPACE&amp;gt;&lt;/code>）将应用部署到同一个集群中（即 &lt;code>kubernetes.svc.default&lt;/code>，也就是内部集群）。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>⚠️ 注意：namespace-install.yaml 配置清单中并不包含 Argo CD 的 CRD，需要自己提前单独部署：&lt;code>kubectl apply -k https://github.com/argoproj/argo-cd/manifests/crds\?ref\=stable&lt;/code>。&lt;/p>
&lt;/blockquote>
&lt;h4 id="高可用">高可用&lt;/h4>
&lt;p>与非高可用部署清单包含的组件相同，但增强了高可用能力和弹性能力，推荐在生产环境中使用。&lt;/p>
&lt;ul>
&lt;li>ha/install.yaml[7] - 与上文提到的 install.yaml 的内容相同，但配置了相关组件的多个副本。&lt;/li>
&lt;li>ha/namespace-install.yaml[8] - 与上文提到的 namespace-install.yaml 相同，但配置了相关组件的多个副本。&lt;/li>
&lt;/ul>
&lt;h3 id="core">Core&lt;/h3>
&lt;p>Core 模式也就是最精简的部署模式，不包含 API Server 和可视化界面，只部署了每个组件的轻量级（非高可用）版本。&lt;/p>
&lt;p>用户需要 Kubernetes 访问权限来管理 Argo CD，因此必须使用下面的命令来配置 argocd CLI：&lt;/p>
&lt;p>&lt;code>$ kubectl config set-context --current --namespace=argocd # change current kube context to argocd namespace $ argocd login --core&lt;/code>&lt;/p>
&lt;p>也可以使用命令 &lt;code>argocd admin dashboard&lt;/code> 手动启用可视化界面。&lt;/p>
&lt;p>具体的配置清单位于 Git 仓库中的 core-install.yaml[9]。&lt;/p>
&lt;hr>
&lt;p>除了直接通过原生的配置清单进行部署，Argo CD 还支持额外的配置清单管理工具。&lt;/p>
&lt;h3 id="kustomize">Kustomize&lt;/h3>
&lt;p>Argo CD 配置清单也可以使用 Kustomize 来部署，建议通过远程的 URL 来调用配置清单，使用 patch 来配置自定义选项。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kustomize.config.k8s.io/v1beta1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Kustomization&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">argocd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">https://raw.githubusercontent.com/argoproj/argo-cd/v2.0.4/manifests/ha/install.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="helm">Helm&lt;/h3>
&lt;p>Argo CD 的 Helm Chart 目前由社区维护，地址：https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd[10]。
接下来开始部署 Argo CD：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl create namespace argocd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>查看部署结果：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl -n argocd get pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-applicationset-controller-69879c47c-pcbkg 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-notifications-controller-6b4b74d8d8-s7mrz 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-redis-65596bf87-2hzcv 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-dex-server-78c9764884-6lcww 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-repo-server-657d46f8b-87rzq 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-application-controller-0 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-server-6b48df79dd-b7bkw 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 26m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="访问-argo-cd">访问 Argo CD&lt;/h2>
&lt;p>部署完成后，可以通过 Service &lt;code>argocd-server&lt;/code> 来访问可视化界面。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl -n argocd get svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT&lt;span style="color:#f92672">(&lt;/span>S&lt;span style="color:#f92672">)&lt;/span> AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-applicationset-controller ClusterIP 10.105.250.212 &amp;lt;none&amp;gt; 7000/TCP,8080/TCP 5m10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-dex-server ClusterIP 10.108.88.97 &amp;lt;none&amp;gt; 5556/TCP,5557/TCP,5558/TCP 5m10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-metrics ClusterIP 10.103.11.245 &amp;lt;none&amp;gt; 8082/TCP 5m10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-notifications-controller-metrics ClusterIP 10.98.136.200 &amp;lt;none&amp;gt; 9001/TCP 5m9s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-redis ClusterIP 10.110.151.108 &amp;lt;none&amp;gt; 6379/TCP 5m9s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-repo-server ClusterIP 10.109.131.197 &amp;lt;none&amp;gt; 8081/TCP,8084/TCP 5m9s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-server ClusterIP 10.98.23.255 &amp;lt;none&amp;gt; 80/TCP,443/TCP 5m9s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>argocd-server-metrics ClusterIP 10.103.184.121 &amp;lt;none&amp;gt; 8083/TCP 5m8s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果你的客户端可以直连 Service IP，那就直接可以通过 argocd-server 的 Cluster IP 来访问。或者可以直接通过本地端口转发来访问：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl port-forward svc/argocd-server -n argocd 8080:443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Forwarding from 127.0.0.1:8080 -&amp;gt; &lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Forwarding from &lt;span style="color:#f92672">[&lt;/span>::1&lt;span style="color:#f92672">]&lt;/span>:8080 -&amp;gt; &lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>初始密码以明文形式存储在 Secret &lt;code>argocd-initial-admin-secret&lt;/code> 中，可以通过以下命令获取：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;{.data.password}&amp;#34;&lt;/span> | base64 -d; echo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>也可以通过以下命令来修改登录密码：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ argocd account update-password --account admin --current-password xxxx --new-password xxxx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>登录后的界面：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443531-40c539e0-8797-4c46-b641-f433feeb83b8.png" alt="">&lt;/p>
&lt;h2 id="argo-cd-核心概念">Argo CD 核心概念&lt;/h2>
&lt;p>在正式开始使用 Argo CD 之前，需要先了解两个基本概念。&lt;/p>
&lt;h3 id="argo-cd-application">Argo CD Application&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443778-4339e877-616e-464e-a6ef-b2f02ff400e6.png" alt="">&lt;/p>
&lt;p>Argo CD 中的 Application 定义了 Kubernetes 资源的&lt;strong>来源&lt;/strong>（Source）和&lt;strong>目标&lt;/strong>（Destination）。来源指的是 Git 仓库中 Kubernetes 资源配置清单所在的位置，而目标是指资源在 Kubernetes 集群中的部署位置。&lt;/p>
&lt;p>来源可以是原生的 Kubernetes 配置清单，也可以是 Helm Chart 或者 Kustomize 部署清单。&lt;/p>
&lt;p>目标指定了 Kubernetes 集群中 API Server 的 URL 和相关的 namespace，这样 Argo CD 就知道将应用部署到哪个集群的哪个 namespace 中。&lt;/p>
&lt;p>简而言之，&lt;strong>Application 的职责就是将目标 Kubernetes 集群中的 namespace 与 Git 仓库中声明的期望状态连接起来&lt;/strong>。&lt;/p>
&lt;p>Application 的配置清单示例：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443806-967865f5-cc3a-4f10-b687-16c0c479a069.jpeg" alt="">&lt;/p>
&lt;p>如果有多个团队，每个团队都要维护大量的应用，就需要用到 Argo CD 的另一个概念：&lt;strong>项目&lt;/strong>（Project）。&lt;/p>
&lt;h3 id="argo-cd-project">Argo CD Project&lt;/h3>
&lt;p>Argo CD 中的项目（Project）可以用来对 Application 进行分组，不同的团队使用不同的项目，这样就实现了多租户环境。项目还支持更细粒度的访问权限控制：&lt;/p>
&lt;ul>
&lt;li>限制部署内容（受信任的 Git 仓库）；&lt;/li>
&lt;li>限制目标部署环境（目标集群和 namespace）；&lt;/li>
&lt;li>限制部署的资源类型（例如 RBAC、CRD、DaemonSets、NetworkPolicy 等）；&lt;/li>
&lt;li>定义项目角色，为 Application 提供 RBAC（与 OIDC group 或者 JWT 令牌绑定）。&lt;/li>
&lt;/ul>
&lt;h2 id="demo-演示">Demo 演示&lt;/h2>
&lt;p>最后通过一个简单的示例来展示 Argo CD 的工作流程。&lt;/p>
&lt;h3 id="准备-git-仓库">准备 Git 仓库&lt;/h3>
&lt;p>在 GitHub 上创建一个项目，取名为 argocd-lab[13]，为了方便实验将仓库设置为公共仓库。在仓库中新建 dev 目录，在目录中创建两个 YAML 配置清单，分别是 &lt;code>deployment.yaml&lt;/code> 和 &lt;code>service.yaml&lt;/code>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076443997-1e374e40-cdf9-49dc-b9a1-68269c22df40.png" alt="">
配置清单内容如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># deployment.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">myapp&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">myapp&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">myapp&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">myapp&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx:latest&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># service.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">myapp-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">myapp&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">protocol&lt;/span>: &lt;span style="color:#ae81ff">TCP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">targetPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>接下来在仓库根目录中创建一个 Application 的配置清单：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># application.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">argoproj.io/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Application&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">myapp-argo-application&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">argocd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">project&lt;/span>: &lt;span style="color:#ae81ff">default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">source&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">repoURL&lt;/span>: &lt;span style="color:#ae81ff">https://github.com/yangchuansheng/argocd-lab.git&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">targetRevision&lt;/span>: &lt;span style="color:#ae81ff">HEAD&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">dev&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">destination&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">server&lt;/span>: &lt;span style="color:#ae81ff">https://kubernetes.default.svc&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">myapp&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">syncPolicy&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">syncOptions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">CreateNamespace=true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">automated&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selfHeal&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">prune&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>参数解释：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>syncPolicy&lt;/strong> : 指定自动同步策略和频率，不配置时需要手动触发同步。&lt;/li>
&lt;li>&lt;strong>syncOptions&lt;/strong> : 定义同步方式。&lt;/li>
&lt;li>&lt;strong>CreateNamespace=true&lt;/strong> : 如果不存在这个 namespace，就会自动创建它。&lt;/li>
&lt;li>&lt;strong>automated&lt;/strong> : 检测到实际状态与期望状态不一致时，采取的同步措施。&lt;/li>
&lt;li>&lt;strong>selfHeal&lt;/strong> : 当集群世纪状态不符合期望状态时，自动同步。&lt;/li>
&lt;li>&lt;strong>prune&lt;/strong> : 自动同步时，删除 Git 中不存在的资源。&lt;/li>
&lt;/ul>
&lt;p>Argo CD 默认情况下&lt;strong>每 3 分钟&lt;/strong>会检测 Git 仓库一次，用于判断应用实际状态是否和 Git 中声明的期望状态一致，如果不一致，状态就转换为 &lt;code>OutOfSync&lt;/code>。默认情况下并不会触发更新，除非通过 &lt;code>syncPolicy&lt;/code> 配置了自动同步。&lt;/p>
&lt;p>如果嫌周期性同步太慢了，也可以通过设置 Webhook 来使 Git 仓库更新时立即触发同步。具体的使用方式会放到后续的教程中，本文不再赘述。&lt;/p>
&lt;h3 id="创建-application">创建 Application&lt;/h3>
&lt;p>现在万事具备，只需要通过 application.yaml 创建 Application 即可。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl apply -f application.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>application.argoproj.io/myapp-argo-application created
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 Argo CD 可视化界面中可以看到应用已经创建成功了。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076444145-4526c86e-f5b5-43c3-8737-ec29479adfdf.png" alt="">
点进去可以看到应用的同步详情和各个资源的健康状况。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/cogrm6/1662076444129-228cbe84-33c0-4d98-ad77-d0e76e9292b5.png" alt="">
&lt;strong>如果你更新了 deployment.yaml 中的镜像，Argo CD 会自动检测到 Git 仓库中的更新，并且将集群中 Deployment 的镜像更新为 Git 仓库中最新设置的镜像版本。&lt;/strong>&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文介绍了 Argo CD 的优势、架构和工作原理，并通过一个简单的示例对其功能进行演示，比如修改 Git 仓库内容后，可以自动触发更新。还可以通过 Event Source 和 Trigger 实现更多自动化部署的需求。&lt;/p>
&lt;p>在部署 Kubernetes 资源时，Argo CD 还支持 Kustomize、Helm、Ksonnet 等资源描述方式，包括其他更高级的使用方式都会在后续的教程中为大家一一道来，敬请期待。&lt;/p>
&lt;h3 id="引用链接">引用链接&lt;/h3>
&lt;p>[1]&lt;/p>
&lt;p>GitOps 介绍: &lt;a href="https://icloudnative.io/">&lt;em>https://icloudnative.io/&lt;/em>&lt;/a>&lt;/p>
&lt;p>[2]&lt;/p>
&lt;p>GitOps 介绍: &lt;a href="https://icloudnative.io/">&lt;em>https://icloudnative.io/&lt;/em>&lt;/a>&lt;/p>
&lt;p>[3]&lt;/p>
&lt;p>青云: &lt;a href="https://www.qingcloud.com/">&lt;em>https://www.qingcloud.com/&lt;/em>&lt;/a>&lt;/p>
&lt;p>[4]&lt;/p>
&lt;p>KubeSphere: &lt;a href="https://kubesphere.com.cn">&lt;em>https://kubesphere.com.cn&lt;/em>&lt;/a>&lt;/p>
&lt;p>[5]&lt;/p>
&lt;p>install.yaml: &lt;a href="https://github.com/argoproj/argo-cd/blob/master/manifests/install.yaml">&lt;em>https://github.com/argoproj/argo-cd/blob/master/manifests/install.yaml&lt;/em>&lt;/a>&lt;/p>
&lt;p>[6]&lt;/p>
&lt;p>namespace-install.yaml: &lt;a href="https://github.com/argoproj/argo-cd/blob/master/manifests/namespace-install.yaml">&lt;em>https://github.com/argoproj/argo-cd/blob/master/manifests/namespace-install.yaml&lt;/em>&lt;/a>&lt;/p>
&lt;p>[7]&lt;/p>
&lt;p>ha/install.yaml: &lt;a href="https://github.com/argoproj/argo-cd/blob/master/manifests/ha/install.yaml">&lt;em>https://github.com/argoproj/argo-cd/blob/master/manifests/ha/install.yaml&lt;/em>&lt;/a>&lt;/p>
&lt;p>[8]&lt;/p>
&lt;p>ha/namespace-install.yaml: &lt;a href="https://github.com/argoproj/argo-cd/blob/master/manifests/ha/namespace-install.yaml">&lt;em>https://github.com/argoproj/argo-cd/blob/master/manifests/ha/namespace-install.yaml&lt;/em>&lt;/a>&lt;/p>
&lt;p>[9]&lt;/p>
&lt;p>core-install.yaml: &lt;a href="https://github.com/argoproj/argo-cd/blob/master/manifests/core-install.yaml">&lt;em>https://github.com/argoproj/argo-cd/blob/master/manifests/core-install.yaml&lt;/em>&lt;/a>&lt;/p>
&lt;p>[10]&lt;/p>
&lt;p>&lt;a href="https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd:">https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd:&lt;/a> &lt;a href="https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd">&lt;em>https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd&lt;/em>&lt;/a>&lt;/p>
&lt;p>[11]&lt;/p>
&lt;p>KubeSphere Cloud 托管集群服务: &lt;a href="https://kubesphere.cloud/console/resource/">&lt;em>https://kubesphere.cloud/console/resource/&lt;/em>&lt;/a>&lt;/p>
&lt;p>[12]&lt;/p>
&lt;p>&lt;a href="https://kubesphere.cloud">https://kubesphere.cloud&lt;/a>: &lt;a href="https://kubesphere.cloud/">&lt;em>https://kubesphere.cloud/&lt;/em>&lt;/a>&lt;/p>
&lt;p>[13]&lt;/p>
&lt;p>argocd-lab: &lt;a href="https://github.com/yangchuansheng/argocd-lab">&lt;em>https://github.com/yangchuansheng/argocd-lab&lt;/em>&lt;/a>&lt;/p>
&lt;p>[14]&lt;/p>
&lt;p>KubeSphere 从 3.3.0 开始: &lt;a href="https://kubesphere.com.cn/news/kubesphere-3.3.0-ga-announcement/">&lt;em>https://kubesphere.com.cn/news/kubesphere-3.3.0-ga-announcement/&lt;/em>&lt;/a>&lt;/p></description></item><item><title>Docs: BUG</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/bug/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/bug/</guid><description>
&lt;h2 id="orphaned-pod-xx-found-but-volume-paths-are-still-present-on-disk">orphaned pod &amp;ldquo;XX&amp;rdquo; found, but volume paths are still present on disk&lt;/h2>
&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/60987">issue #60987&lt;/a>&lt;/p>
&lt;p>kubelet 执行逻辑：&lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.19/pkg/kubelet/kubelet_volumes.go#L173">https://github.com/kubernetes/kubernetes/blob/release-1.19/pkg/kubelet/kubelet_volumes.go#L173&lt;/a>&lt;/p>
&lt;p>解决方式：&lt;/p>
&lt;ul>
&lt;li>更新至 1.19.8 版本及以上，&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1198">ChangeLog 中提到，在 #95301 Merged&lt;/a> 中已解决&lt;/li>
&lt;li>未更新的话，通过 &lt;a href="https://raw.githubusercontent.com/AliyunContainerService/kubernetes-issues-solution/master/kubelet/kubelet.sh">ali 提供的脚本&lt;/a>，进行一些修改，该脚本会手动 umount 和 rm 目录&lt;/li>
&lt;/ul>
&lt;h1 id="已修复">已修复&lt;/h1>
&lt;h1 id="heading">&lt;/h1>
&lt;h2 id="aggregator_unavailable_apiservice">aggregator_unavailable_apiservice&lt;/h2>
&lt;p>问题描述：聚合 API 删除之后，依然存在于 kube-apiserver 的 metrics 中，这会导致频繁告警&lt;/p>
&lt;p>跟踪连接：&lt;a href="https://github.com/kubernetes/kubernetes/issues/92671">https://github.com/kubernetes/kubernetes/issues/92671&lt;/a>&lt;/p>
&lt;p>解决方式：&lt;a href="https://github.com/kubernetes/kubernetes/pull/96421">https://github.com/kubernetes/kubernetes/pull/96421&lt;/a>&lt;/p>
&lt;p>&lt;strong>将在 1.20 版本解决&lt;/strong>&lt;/p>
&lt;h2 id="scope-libcontainer-21733-systemd-test-default-dependenciesscope-has-no-pids-refusing">Scope libcontainer-21733-systemd-test-default-dependencies.scope has no PIDs. Refusing.&lt;/h2>
&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/71887">https://github.com/kubernetes/kubernetes/issues/71887&lt;/a>&lt;/p>
&lt;p>解决方式：（1.16 及以后的版本中，无该问题。主要是 18+版本 docker 无该问题）&lt;/p>
&lt;p>忽略该告警：&lt;a href="https://www-01.ibm.com/support">https://www-01.ibm.com/support&lt;/a> &amp;hellip; .wss?uid=ibm10883724&lt;/p>
&lt;h2 id="error-while-processing-event-sysfscgroupdeviceslibcontainer_2434_systemd_test_defaultslice-0x40000100--in_createin_isdir-open-sysfscgroupdeviceslibcontainer_2434_systemd_test_defaultslice-no-such-file-or-directory">Error while processing event (&amp;quot;/sys/fs/cgroup/devices/libcontainer_2434_systemd_test_default.slice&amp;quot;: 0x40000100 == IN_CREATE|IN_ISDIR): open /sys/fs/cgroup/devices/libcontainer_2434_systemd_test_default.slice: no such file or directory&lt;/h2>
&lt;p>解决方式：1.16 及以后版本解决该问题&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/issues/76531#issuecomment-548230839">https://github.com/kubernetes/kubernetes/issues/76531#issuecomment-548230839&lt;/a>&lt;/p>
&lt;h2 id="setting-volume-ownership-for-xxx-and-fsgroup-set-if-the-volume-has-a-lot-of-files-then-setting-volume-ownership-could-be-slow">Setting volume ownership for XXX and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow,&lt;/h2>
&lt;pre>&lt;code>Apr 20 11:03:37 lxkubenode01 kubelet[9103]: W0420 11:03:37.275020 9103 volume_linux.go:49] Setting volume ownership for /var/lib/kubelet/pods/63c4a49f-bf23-4b87-989e-102f5fcdb315/volumes/kubernetes.io~secret/seq-token-whpfk and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
Apr 20 11:03:46 lxkubenode01 kubelet[9103]: W0420 11:03:46.198559 9103 volume_linux.go:49] Setting volume ownership for /var/lib/kubelet/pods/cdb79fa0-4942-4211-9261-a4928f872bd6/volumes/kubernetes.io~secret/prometheus-operator-prometheus-node-exporter-token-snbtf and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
Apr 20 11:03:47 lxkubenode01 kubelet[9103]: W0420 11:03:47.201212 9103 volume_linux.go:49] Setting volume ownership for /var/lib/kubelet/pods/8cafe45f-2d14-4eb1-8c38-71a54b34f83c/volumes/kubernetes.io~secret/default-token-9hq84 and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
&lt;/code>&lt;/pre>
&lt;p>该报警发生于 1.17 及其以后的版本，由于 kubelet 代码更改后，导致频繁得大量刷新类似的日志信息&lt;/p>
&lt;p>问题跟踪：
&lt;a href="https://github.com/kubernetes/kubernetes/issues/90293">https://github.com/kubernetes/kubernetes/issues/90293&lt;/a>&lt;/p>
&lt;p>解决方式：
治标不治本方法：配置 rayslog 忽略&lt;/p>
&lt;pre>&lt;code>cat &amp;gt; /etc/rsyslog.d/ignore-kubelet-volume.conf &amp;lt;&amp;lt; EOF
if (\$programname == &amp;quot;kubelet&amp;quot;) and (\$msg contains &amp;quot;Setting volume ownership&amp;quot;) then {
stop
}
EOF
&lt;/code>&lt;/pre>
&lt;p>根治方法：升级集群
&lt;a href="https://github.com/kubernetes/kubernetes/pull/92878">https://github.com/kubernetes/kubernetes/pull/92878&lt;/a>，pr 已被 merged，在 1.18.8 及 1.19.0 之后得版本已修复，参见：
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#other-cleanup-or-flake">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#other-cleanup-or-flake&lt;/a>
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#other-cleanup-or-flake-1">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#other-cleanup-or-flake-1&lt;/a>&lt;/p>
&lt;h2 id="aggregatedapidown-报警">AggregatedAPIDown 报警&lt;/h2>
&lt;p>问题描述：添加聚合 API 再删除后，kube-apiserver 中的 metircs 并不会删除，导致一致产生报警。报警规则：&lt;/p>
&lt;pre>&lt;code>aggregator_unavailable_apiservice
&lt;/code>&lt;/pre>
&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/92671">https://github.com/kubernetes/kubernetes/issues/92671&lt;/a>&lt;/p>
&lt;p>解决方式：v1.20 有解决 PR&lt;/p>
&lt;h2 id="kubectl-get-cs-获取信息-unknown">kubectl get cs 获取信息 unknown&lt;/h2>
&lt;p>相关报警信息：&lt;/p>
&lt;pre>&lt;code>watch chan error: etcdserver: mvcc: required revision has been compacted
&lt;/code>&lt;/pre>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>[root@master-1 manifests]# kubectl get cs
NAME AGE
controller-manager &amp;lt;unknown&amp;gt;
scheduler &amp;lt;unknown&amp;gt;
etcd-0 &amp;lt;unknown&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>问题连接：&lt;a href="https://github.com/kubernetes/kubernetes/issues/83024#issuecomment-559538245">https://github.com/kubernetes/kubernetes/issues/83024#issuecomment-559538245&lt;/a>&lt;/p>
&lt;p>大意是在 1.17 中得到解决，1.19 及其以后版本不再支持 cs&lt;/p>
&lt;h2 id="一次-kube-apiserver-响应超时的问题记录">一次 kube-apiserver 响应超时的问题记录&lt;/h2>
&lt;p>首先是收到 apiserver 响应时间过长的告警，查看日志，发现频繁出现如下内容&lt;/p>
&lt;pre>&lt;code>I0331 01:40:30.953289 1 trace.go:116] Trace[2133477734]: &amp;quot;Get&amp;quot; url:/api/v1/namespaces/kube-system (started: 2020-03-31 01:40:21.623714299 +0000 UTC m=+338766.344413381) (total time: 9.329480544s):
Trace[2133477734]: [9.329404093s] [9.329362028s] About to write a response
I0331 01:40:36.528652 1 trace.go:116] Trace[1431450424]: &amp;quot;Get&amp;quot; url:/api/v1/namespaces/default (started: 2020-03-31 01:40:28.063278623 +0000 UTC m=+338772.783977705) (total time: 8.465254793s):
Trace[1431450424]: [8.465073901s] [8.465027207s] About to write a response
I0331 01:40:37.333718 1 trace.go:116] Trace[1319947973]: &amp;quot;Get&amp;quot; url:/api/v1/namespaces/kube-public (started: 2020-03-31 01:40:30.954280125 +0000 UTC m=+338775.674979196) (total time: 6.379382999s):
Trace[1319947973]: [6.37929667s] [6.379253238s] About to write a response
&lt;/code>&lt;/pre>
&lt;p>查看 etcd 日志，发现很多 took too long 的信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>etcdserver: read-only range request ..... took too long to execute
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>怀疑有可能是磁盘性能问题，使用 dd 测试，发现只有 10+M/s，查看 Raid 卡发现模式是直写，更换 Raid 卡，修改模式为强制回写。问题解决&lt;/p></description></item><item><title>Docs: BuildKit 构建工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/%E6%9E%84%E5%BB%BA-oci-image/buildkit-%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/%E6%9E%84%E5%BB%BA-oci-image/buildkit-%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/moby/buildkit">GitHub 项目，moby/buildkit&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/develop/develop-images/build_enhancements/">官方文档&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>BuildKit 是 Docker 上游社区(Moby)推出的下一代镜像构建神器，可以更加快速，有效，安全地构建容器镜像。Docker v18.06 已经集成了该组件。BuildKit 可用于多种导出格式（例如 OCI 或 Docker）以及前端支持（Dockerfile），并提供高效缓存和运行并行构建操作等功能。BuildKit 仅需要容器运行时就能执行，当前受支持的运行时包括 Containerd 和 Runc。&lt;/p>
&lt;h2 id="构建步骤优化">构建步骤优化&lt;/h2>
&lt;p>Docker 提供的原始构建最令人沮丧的问题之一是 Dockerfile 指令执行构建步骤的顺序性。在引入多阶段构建之后，可以将构建步骤分组为单独的逻辑构建任务在同一个 Dockerfile 中。&lt;/p>
&lt;p>有时，这些构建阶段是彼此完全独立的，这意味着它们可以并行执行-或根本不需要执行。遗憾的是，传统的 Docker 镜像构建无法满足这种灵活性。这意味着构建时间通常会比绝对必要的时间更长。&lt;/p>
&lt;p>相比之下，BuildKit 会创建一个构建步骤之间的依赖关系图，并使用该图来确定可以忽略构建的哪些元素;可以并行执行的元素;需要顺序执行的元素。这可以更有效地执行构建，这对开发人员来说很有价值，因为他们可以迭代其应用程序的镜像构建。&lt;/p>
&lt;h3 id="高效灵活的缓存">高效灵活的缓存&lt;/h3>
&lt;p>虽然在旧版 Docker 镜像构建中缓存构建步骤非常有用，但效率却不如预期。作为对构建后端的重写，BuildKit 在此方面进行了改进，并提供了更快，更准确的缓存机制。使用为构建生成的依赖关系图，并且基于指令定义和构建步骤内容。&lt;/p>
&lt;p>BuildKit 提供的另一个巨大好处是以构建缓存导入和导出的形式出现，正如 Kaniko 和 Makisu 允许将构建缓存推送到远程注册表一样，BuildKit 也是如此，但是 BuildKit 使您可以灵活地将缓存嵌入到内部注册表中。镜像（内联）并将它们放在一起（虽然不是每个注册表都支持），或者将它们分开导入。也可以将缓存导出到本地目录以供以后使用。&lt;/p>
&lt;p>当从头开始建立构建环境而没有任何先前的构建历史时，导入构建缓存的能力就发挥了自己的作用：导入“预热”缓存，对于临时 CI/CD 环境特别有用。&lt;/p>
&lt;h3 id="工件">工件&lt;/h3>
&lt;p>当使用旧版 Docker 镜像构建器构建镜像时，将生成的镜像添加到 Docker 守护进程管理的本地镜像的缓存中。需要单独的&lt;code>docker push&lt;/code>将该镜像上载到远程容器镜像注册表。新的工件构建工具通过允许您在构建调用时指定镜像推送来增强体验，BuildKit 也不例外，它还允许以几种不同格式输出镜像；本地目录中的文件，本地 tarball，一个本地 OCI 镜像 tarball，一个 Docker 镜像 tarball，一个存储在本地缓存中的 Docker 镜像以及一个推送到注册表的 Docker 镜像，有很多格式！&lt;/p>
&lt;h3 id="扩展语法">扩展语法&lt;/h3>
&lt;p>对于 docker 构建体验而言，经常重复出现的众多功能请求之一就是安全处理镜像构建过程中所需的机密信息。Moby 项目抵制了这一要求很多年了，但是，借助 BuildKit 灵活的“前端”定义，为 Buildkit 提供了一个实验性前端，它扩展了 Dockerfile 语法。扩展后的语法为 RUN Dockerfile 指令提供了有用的补充，其中包括安全性功能。&lt;/p>
&lt;pre>&lt;code>RUN --mount=type=secret,id=top-secret-passwd my_command
&lt;/code>&lt;/pre>
&lt;p>引用实验性前端的 Dockerfile 可以为 RUN 指令临时挂载秘钥。使用 &lt;code>--secret&lt;/code> 标志将秘钥提供给构建，用于 docker build。使用 ssh mount 类型可以转发 SSH 代理连接以实现安全 SSH 身份验证。&lt;/p>
&lt;h3 id="buildkit-使用场景">BuildKit 使用场景&lt;/h3>
&lt;p>BuildKit 还有许多其他功能，可以极大地改善构建容器镜像的技巧。如果它是适用于许多不同环境的通用工具，那么如何使用它呢？&lt;/p>
&lt;p>根据您工作的环境，这个问题的答案是多种多样的。让我们来看看。&lt;/p>
&lt;p>Docker&lt;/p>
&lt;p>尽管目前 BuildKit 不是 Docker 的默认构建工具，但是完全可以考虑将其作为 Docker（v18.09 +）的首选构建工具。当然目前在 windows 平台是不支持的。&lt;/p>
&lt;p>临时方案是设置环境变量&lt;code>DOCKER_BUILDKIT=1&lt;/code>。如果是想永久生效的话，将&lt;code>&amp;quot;features&amp;quot;:{&amp;quot;buildkit&amp;quot;: true}&lt;/code> 添加到 docker 守护进程的配置文件中。在此配置中，由于 Docker 守护程序中的当前限制，Docker 并未充分展现 BuildKit 的全部功能。因此，Docker 客户端 CLI 已扩展为提供插件框架，该框架允许使用插件扩展提供了可用的 CLI 功能。一个名为&lt;code>Buildx&lt;/code>的实验性插件会绕过守护程序中的旧版构建函数，并使用 BuildKit 后端进行所有构建，它提供所有熟悉的镜像构建命令和功能，但通过一些特定于 BuildKit 的附加功能对其进行了扩充。&lt;/p>
&lt;p>BuildKit 以及 Buildx 的都支持多个构建器实例，这是一项重要功能，这实际上意味着可以共享一个构建器实例场以进行构建;也许是一个项目被分配了一组构建器实例。&lt;/p>
&lt;pre>&lt;code>$ docker buildx ls
NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS
default * docker
default default running linux/amd64, linux/386
&lt;/code>&lt;/pre>
&lt;p>默认情况下，Buildx 插件以 docker 驱动程序为目标，该驱动程序使用 Docker 守护程序提供的 BuildKit 库具有其固有的局限性。另一个驱动程序是 docker-container，它可以透明地在容器内启动 BuildKit 以执行构建。 BuildKit 中提供的功能 CLI：这是否是理想的工作流程，完全取决于个人或公司的选择。&lt;/p>
&lt;p>Kubernetes&lt;/p>
&lt;p>越来越多的组织将构建放到 Kubernetes 当中，通常将容器镜像构建作为 CI/CD 工作流的一部分出现在 pod 中。在 Kubernetes 中运行 BuildKit 实例时，有一个每种部署策略都有其优缺点，每种策略都适合不同的目的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gg5yyl/1616121909927-de523c03-e2e7-4541-b2af-0bde7004e81d.png" alt="">&lt;/p>
&lt;p>除了使用 Docker CLI 为 BuildKit 启动面向开发人员的构建之外，构建还可以通过多种 CI/CD 工具触发。使用 BuildKit 进行的容器镜像构建可以作为 Tekton Pipeline Task 执行。&lt;/p>
&lt;h3 id="结论">结论&lt;/h3>
&lt;p>本文主要讲了 BuildKit 诸多特性和使用场景。&lt;/p>
&lt;p>目前类似的工具不少，如 Redhat 的 Buildah，Google 的 Kaniko 或 Docker 的 BuildKit。&lt;/p>
&lt;p>不过 BuildKit 是官方提供，和 docker 本身结合比较好。&lt;/p>
&lt;h1 id="部署-buildkit">部署 Buildkit&lt;/h1>
&lt;h2 id="docker-启用-buildkitk">Docker 启用 Buildkitk&lt;/h2>
&lt;p>可以通过两种方式启用 Buildkit 功能&lt;/p>
&lt;ol>
&lt;li>构建之前添加环境变量 &lt;code>export DOCKER_BUILDKIT=1&lt;/code>&lt;/li>
&lt;li>在 /etc/docker/deamon.json 配置中，添加 &lt;code>&amp;quot;features&amp;quot;: { &amp;quot;buildkit&amp;quot;: true }&lt;/code>。&lt;/li>
&lt;/ol></description></item><item><title>Docs: Calico</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/cni/calico/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/cni/calico/</guid><description>
&lt;h1 id="calico-基本概念">Calico 基本概念&lt;/h1>
&lt;h1 id="imagepnghttpsnotes-learningoss-cn-beijingaliyuncscombgb09a1616118512894-684263a6-9d81-4a8a-8be3-088fd6aedef1png">&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bgb09a/1616118512894-684263a6-9d81-4a8a-8be3-088fd6aedef1.png" alt="image.png">&lt;/h1>
&lt;p>基于以 BGP 协议构建网络，主要由三个部分组成&lt;/p>
&lt;p>第一部分：Calico 的 CNI 插件。这是 Calico 与 Kubernetes 对接的部分&lt;/p>
&lt;p>第二部分：Felix，一个 DaemonSet。负责在 Host 上插入路由规则(即：写入 Linux 内核的 FIB(转发信息库 Forwarding information base)，以及维护 Calico 所需的网络设备等工作&lt;/p>
&lt;p>第三部分：BIRD，BGP Client。专门负责在集群内分发路由规则信息&lt;/p>
&lt;p>Calico 利用 Linux 内核原生的路由和 iptables 防火墙功能。进出各个容器，虚拟机和主机的所有流量都会在路由到目标之前遍历这些内核规则。&lt;/p>
&lt;ol>
&lt;li>calicoctl：允许您从简单的命令行界面实现高级策略和网络。&lt;/li>
&lt;li>orchestrator plugins：提供与各种流行协调器的紧密集成和同步。&lt;/li>
&lt;li>key/value store：保存 Calico 的策略和网络配置状态。比如 etcd&lt;/li>
&lt;li>calico/node：在每个主机上运行，从 key/value store 中读取相关的策略和网络配置信息，并在 Linux 内核中实现它。&lt;/li>
&lt;li>Dikastes/Envoy：可选的 Kubernetes sidecar，通过相互 TLS 身份验证保护工作负载到工作负载的通信，并实施应用层策略。&lt;/li>
&lt;/ol>
&lt;h2 id="calico-bgp-工作原理">Calico BGP 工作原理&lt;/h2>
&lt;p>实际上，Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。也就是说，Calico 也会在每台宿主机上，添加一个格式如下所示的路由规则：&lt;/p>
&lt;pre>&lt;code>&amp;lt; 目的容器 IP 地址段 &amp;gt; via &amp;lt; 网关的 IP 地址 &amp;gt; dev eth0
&lt;/code>&lt;/pre>
&lt;p>其中，网关的 IP 地址，正是目的容器所在宿主机的 IP 地址。&lt;/p>
&lt;p>而正如前所述，这个三层网络方案得以正常工作的核心，是为每个容器的 IP 地址，找到它所对应的、“下一跳”的网关。不过，不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了一个“重型武器”来自动地在整个集群中分发路由信息。这个“重型武器”，就是 BGP。详见：BGP 协议&lt;/p>
&lt;p>Calico 项目的架构由三个部分组成：&lt;/p>
&lt;ol>
&lt;li>Calico 的 CNI 插件。这是 Calico 与 Kubernetes 对接的部分。&lt;/li>
&lt;li>Felix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。&lt;/li>
&lt;li>BIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。&lt;/li>
&lt;/ol>
&lt;p>除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。这时候，Calico 的工作方式，可以用一幅示意图来描述，如下所示（在接下来的讲述中，我会统一用“BGP 示意图”来指代它）
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bgb09a/1617285084820-0455ddc1-9808-4eb4-955f-2c37a3697372.png" alt="image.png">&lt;/p>
&lt;p>其中的绿色实线标出的路径，就是一个 IP 包从 Node 1 上的 Container 1，到达 Node 2 上的 Container 4 的完整路径。可以看到，Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置&lt;/p>
&lt;p>在宿主机上（它的名字以 cali 前缀开头）。此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示&lt;/p>
&lt;pre>&lt;code># 发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备
10.233.2.3 dev cali5863f3 scope link
&lt;/code>&lt;/pre>
&lt;p>Note：基于上述原因，Calico 项目在宿主机上设置的路由规则，肯定要比 Flannel 项目多得多(因为没有单独的网桥来连接这些 veth 设备，也就没法通过网桥来自动将数据包交给对应的容器，只能为每一个 veth 单独配置路由信息)。不过，Flannel host-gw 模式使用 CNI 网桥的主要原因，其实是为了跟 VXLAN 模式保持一致。否则的话，Flannel 就需要维护两套 CNI 插件了。&lt;/p>
&lt;p>有了这样的 Veth Pair 设备之后，容器发出的 IP 包就会经过 Veth Pair 设备出现在宿主机上。&lt;/p>
&lt;p>然后，宿主机网络栈就会根据路由规则的下一跳 IP 地址，把它们转发给正确的网关。接下来的流程就跟 Flannel host-gw 模式完全一致了。&lt;/p>
&lt;p>其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。&lt;/p>
&lt;p>而这些通过 BGP 协议传输的消息，你可以简单地理解为如下格式：&lt;/p>
&lt;pre>&lt;code>[BGP 消息]
我是宿主机 192.168.1.2
10.233.2.0/24 网段的容器都在我这里
这些容器的下一跳地址是我
&lt;/code>&lt;/pre>
&lt;p>不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。&lt;/p>
&lt;h3 id="route-reflector-的出现">Route Reflector 的出现&lt;/h3>
&lt;p>需要注意的是，Calico 维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。这时候，每台宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息。但是，随着节点数量 N 的增加，这些连接的数量就会以 N² 的规模快速增长，从而给集群本身的网络带来巨大的压力。&lt;/p>
&lt;p>所以，Node-to-Node Mesh 模式一般推荐用在少于 100 个节点的集群里。而在更大规模的集群中，你需要用到的是一个叫作 Route Reflector 的模式。&lt;/p>
&lt;p>在这种模式下，Calico 会指定一个或者几个专门的节点，来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。&lt;/p>
&lt;p>这些专门的节点，就是所谓的 Route Reflector 节点，它们实际上扮演了“中间代理”的角色，从而把 BGP 连接的规模控制在 N 的数量级上。&lt;/p>
&lt;h3 id="k8s-节点跨网段的解决方案">k8s 节点跨网段的解决方案&lt;/h3>
&lt;p>此外，我在前面提到过，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。&lt;/p>
&lt;p>举个例子，假如我们有两台处于不同子网的宿主机 Node 1 和 Node 2，对应的 IP 地址分别是 192.168.1.2 和 192.168.2.2。需要注意的是，这两台机器通过路由器实现了三层转发，所以这两个 IP 地址之间是可以相互通信的。而我们现在的需求，还是 Container 1 要访问 Container 4。按照我们前面的讲述，Calico 会尝试在 Node 1 上添加如下所示的一条路由规则：&lt;/p>
&lt;pre>&lt;code>10.233.2.0/16 via 192.168.2.2 eth0
&lt;/code>&lt;/pre>
&lt;p>但是，这时候问题就来了。&lt;/p>
&lt;p>上面这条规则里的下一跳地址是 192.168.2.2，可是它对应的 Node 2 跟 Node 1 却根本不在一个子网里，没办法通过二层网络把 IP 包发送到下一跳地址。&lt;/p>
&lt;p>在这种情况下，你就需要为 Calico 打开 IPIP 模式。我把这个模式下容器通信的原理，总结成了一副示意图，如下所示（接下来我会称之为：IPIP 示意图）：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bgb09a/1617285101992-492a004e-7a36-4cd2-8d66-a2ec3772827b.jpeg" alt="image.jpeg">&lt;/p>
&lt;p>在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示：&lt;/p>
&lt;pre>&lt;code>10.233.2.0/24 via 192.168.2.2 tunl0
&lt;/code>&lt;/pre>
&lt;p>可以看到，尽管这条规则的下一跳地址仍然是 Node 2 的 IP 地址，但这一次，要负责将 IP 包发出去的设备，变成了 tunl0。注意，是 T-U-N-L-0，而不是 Flannel UDP 模式使用的 T-UN-0（tun0），这两种设备的功能是完全不一样的。&lt;/p>
&lt;p>Calico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。在上面的例子中，&lt;/p>
&lt;ol>
&lt;li>IP 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。&lt;/li>
&lt;li>IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，类似 flannel 中 vxlan 的封装过程。其中，经过封装后的新的 IP 包的目的地址正是原 IP 包的下一跳地址，即 Node 2 的 IP 地址：192.168.2.2。而原 IP 包本身，则会被直接封装成新 IP 包的 Payload。&lt;/li>
&lt;li>这样，原先从容器到 Node 2 的 IP 包，就被伪装成了一个从 Node 1 到 Node 2 的 IP 包。&lt;/li>
&lt;li>由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个 IP 包在离开 Node 1 之后，就可以经过路由器，最终“跳”到 Node 2 上。&lt;/li>
&lt;li>这时，Node 2 的网络内核栈会使用 IPIP 驱动进行解包，从而拿到原始的 IP 包。然后，原始 IP 包就会经过路由规则和 Veth Pair 设备到达目的容器内部。&lt;/li>
&lt;/ol>
&lt;h3 id="如何让外部真正的数通设备路由器三层交换机也加入到集群的-bgp-中">如何让外部真正的数通设备(路由器、三层交换机)也加入到集群的 BGP 中&lt;/h3>
&lt;p>以上，就是 Calico 项目主要的工作原理了。&lt;/p>
&lt;p>不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。所以，在实际使用时，如非硬性需求，我建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。&lt;/p>
&lt;p>不过，通过上面对 Calico 工作原理的讲述，你应该能发现这样一个事实：如果 Calico 项目能够让宿主机之间的路由设备（也就是网关），也通过 BGP 协议“学习”到 Calico 网络里的路由规则，那么从容器发出的 IP 包，不就可以通过这些设备路由到目的宿主机了么？&lt;/p>
&lt;p>比如，只要在上面“IPIP 示意图”中的 Node 1 上，添加如下所示的一条路由规则：&lt;/p>
&lt;pre>&lt;code>10.233.2.0/24 via 192.168.1.1 eth0
&lt;/code>&lt;/pre>
&lt;p>然后，在 Router 1 上（192.168.1.1），添加如下所示的一条路由规则：&lt;/p>
&lt;pre>&lt;code>10.233.2.0/24 via 192.168.2.1 eth0
&lt;/code>&lt;/pre>
&lt;p>那么 Container 1 发出的 IP 包，就可以通过两次“下一跳”，到达 Router 2（192.168.2.1）了。以此类推，我们可以继续在 Router 2 上添加“下一条”路由，最终把 IP 包转发到 Node 2&lt;/p>
&lt;p>上。&lt;/p>
&lt;p>遗憾的是，上述流程虽然简单明了，但是在 Kubernetes 被广泛使用的公有云场景里，却完全不可行。&lt;/p>
&lt;p>这里的原因在于：公有云环境下，宿主机之间的网关，肯定不会允许用户进行干预和设置。&lt;/p>
&lt;p>不过，在私有部署的环境下，宿主机属于不同子网（VLAN）反而是更加常见的部署状态。这时候，想办法将宿主机网关也加入到 BGP Mesh 里从而避免使用 IPIP，就成了一个非常迫切的需求。&lt;/p>
&lt;p>而在 Calico 项目中，它已经为你提供了两种将宿主机网关设置成 BGP Peer 的解决方案。&lt;/p>
&lt;ol>
&lt;li>第一种方案，就是所有宿主机都跟宿主机网关建立 BGP Peer 关系。
&lt;ol>
&lt;li>这种方案下，Node 1 和 Node 2 就需要主动跟宿主机网关 Router 1 和 Router 2 建立 BGP 连接。从而将类似于 10.233.2.0/24 这样的路由信息同步到网关上去。需要注意的是，这种方式下，Calico 要求宿主机网关必须支持一种叫作 Dynamic Neighbors 的 BGP 配置方式。这是因为，在常规的路由器 BGP 配置里，运维人员必须明确给出所有 BGP Peer 的 IP 地址。考虑到 Kubernetes 集群可能会有成百上千个宿主机，而且还会动态地添加和删除节点，这时候再手动管理路由器的 BGP 配置就非常麻烦了。而 Dynamic Neighbors 则允许你给路由器配置一个网段，然后路由器就会自动跟该网段里的主机建立起 BGP Peer 关系。&lt;/li>
&lt;li>不过，相比之下，我更愿意推荐第二种方案。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>第二种方案，是使用一个或多个独立组件负责搜集整个集群里的所有路由信息，然后通过 BGP 协议同步给网关。
&lt;ol>
&lt;li>而我们前面提到，在大规模集群中，Calico 本身就推荐使用 Route Reflector 节点的方式进行组网。所以，这里负责跟宿主机网关进行沟通的独立组件，直接由 Route Reflector 兼任即可。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>更重要的是，这种情况下网关的 BGP Peer 个数是有限并且固定的。所以我们就可以直接把这些独立组件配置成路由器的 BGP Peer，而无需 Dynamic Neighbors 的支持。&lt;/li>
&lt;li>当然，这些独立组件的工作原理也很简单：它们只需要 WATCH Etcd 里的宿主机和对应网段的变化信息，然后把这些信息通过 BGP 协议分发给网关即可。&lt;/li>
&lt;/ol>
&lt;h1 id="calico-配置">Calico 配置&lt;/h1>
&lt;h1 id="calicoctl-命令行工具是使用说明">Calicoctl 命令行工具是使用说明&lt;/h1>
&lt;p>calicoctl 命令行工具用于管理 Calico 网络和安全策略，查看和管理后端配置以及管理 Calico 节点实例&lt;/p>
&lt;p>安装方式：&lt;/p>
&lt;ol>
&lt;li>kubectl apply -f \ &lt;a href="https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/calicoctl.yaml">https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/calicoctl.yaml&lt;/a>&lt;/li>
&lt;li>alias calicoctl=&amp;ldquo;kubectl exec -i -n kube-system calicoctl /calicoctl &amp;ndash; &amp;quot;&lt;/li>
&lt;li>然后可以直接使用别名来使用该命令，calico 对应的 etcd 后端 ip 以及 etcd 证书已经在 calicoctl 的 pod 中定义完成&lt;/li>
&lt;/ol>
&lt;p>配置文件路径：/etc/calico/calicoctl.cfg #该路径一般是对于使用二进制方式把 calicoctl 命令文件放在 linux 的$PATH 中来使用&lt;/p>
&lt;p>calicoctl [OPTIONS] &lt;!-- raw HTML omitted --> [&lt;!-- raw HTML omitted -->&amp;hellip;]&lt;/p>
&lt;p>OPTIONS&lt;/p>
&lt;ol>
&lt;li>-h &amp;ndash;help #Show this screen.&lt;/li>
&lt;li>-l &amp;ndash;log-level=&lt;!-- raw HTML omitted --> #Set the log level (one of panic, fatal, error,warn, info, debug) [default: panic]&lt;/li>
&lt;/ol>
&lt;p>COMMAND&lt;/p>
&lt;p>create Create a resource by filename or stdin.&lt;/p>
&lt;p>calicoctl create &amp;ndash;filename=&lt;!-- raw HTML omitted --> [&amp;ndash;skip-exists] [&amp;ndash;config=&lt;!-- raw HTML omitted -->] [&amp;ndash;namespace=&lt;!-- raw HTML omitted -->]&lt;/p>
&lt;p>replace Replace a resource by filename or stdin.&lt;/p>
&lt;p>apply Apply a resource by filename or stdin. This creates a resource if it does not exist, and replaces a resource if it does exists.&lt;/p>
&lt;p>delete Delete a resource identified by file, stdin or resource type and name.&lt;/p>
&lt;p>get Get a resource identified by file, stdin or resource type and name.&lt;/p>
&lt;ol>
&lt;li>EXAMPLE&lt;/li>
&lt;/ol>
&lt;p>convert Convert config files between different API versions.&lt;/p>
&lt;p>ipam IP address management.&lt;/p>
&lt;p>node Calico node management.&lt;/p>
&lt;p>version Display the version of calicoctl.&lt;/p></description></item><item><title>Docs: CGroup FS</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/cgroup-fs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/cgroup-fs/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote>
&lt;h1 id="sysfscgroup">/sys/fs/cgroup/*&lt;/h1>
&lt;h3 id="cgroupv1">CGroupV1&lt;/h3>
&lt;p>CGroupV1 根目录下的每个目录的名称都是一个子系统的名称，每个子系统都有其自己独立的资源控制配置文件。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls -l /sys/fs/cgroup/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 blkio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">11&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpu -&amp;gt; cpu,cpuacct
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">11&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpuacct -&amp;gt; cpu,cpuacct
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpu,cpuacct
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 cpuset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 devices
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">4&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 freezer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 hugetlb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 memory
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">16&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 net_cls -&amp;gt; net_cls,net_prio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 net_cls,net_prio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">16&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 net_prio -&amp;gt; net_cls,net_prio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 perf_event
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 pids
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">2&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 rdma
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 systemd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dr-xr-xr-x &lt;span style="color:#ae81ff">5&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Jan &lt;span style="color:#ae81ff">26&lt;/span> 21:46 unified
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="cpu--cpu-子系统">./cpu # CPU 子系统&lt;/h4>
&lt;ul>
&lt;li>./cpu.cfs_quota_us 与 ./cpu.cfs_period_us # 用来限制进程每运行 cfs_period_us 一段时间，只能被分配到的总量为 cfs_quota_us 的 CPU 时间
&lt;ul>
&lt;li>cfs_quota_us 默认值为-1，不做任何限制，如果修改为 20000(20ms)则表示 CPU 只能使用到 20%的&lt;/li>
&lt;li>cfs_period_us 默认值为 100000(100ms)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>./cpu.shares #&lt;/li>
&lt;li>./cpu.stat #
&lt;ul>
&lt;li>nr_periods #&lt;/li>
&lt;li>nr_throttled #&lt;/li>
&lt;li>throttled_time #&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="cgroupv2">CGroupV2&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls -l /sys/fs/cgroup/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>total &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cgroup.controllers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.max.depth
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.max.descendants
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cgroup.procs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.stat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cgroup.subtree_control
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cgroup.threads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 cpu.pressure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cpuset.cpus.effective
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-r--r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 cpuset.mems.effective
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>drwxr-xr-x &lt;span style="color:#ae81ff">2&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:52 init.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 io.cost.model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 io.cost.qos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 io.pressure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-rw-r--r-- &lt;span style="color:#ae81ff">1&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:54 memory.pressure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>drwxr-xr-x &lt;span style="color:#ae81ff">44&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:53 system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>drwxr-xr-x &lt;span style="color:#ae81ff">3&lt;/span> root root &lt;span style="color:#ae81ff">0&lt;/span> Feb &lt;span style="color:#ae81ff">18&lt;/span> 10:53 user.slice
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Chart Hooks</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86/helm/charts/chart-hooks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86/helm/charts/chart-hooks/</guid><description>
&lt;h1 id="chart-hooks-概述">Chart Hooks 概述&lt;/h1>
&lt;p>参考：&lt;a href="https://helm.sh/docs/topics/charts_hooks/">&lt;strong>官方文档&lt;/strong>&lt;/a>&lt;/p>
&lt;p>Helm 提供了一种** Hook(钩子) **机制可以在一个 release 的生命周期内进行干预，比如：&lt;/p>
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>安装任何资源之前，提前先安装 ConfigMap 或者 Secret&lt;/p>
&lt;ol start="2">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>在安装一个新的 Chart 执行，执行一个 Job 以备份数据库，然后在升级后执行第二个 Job 以还原数据。&lt;/p>
&lt;ol start="3">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>在删除一个 Release 之前，运行一个 Job，以便在删除服务之前优雅得停止服务。&lt;/p>
&lt;ol start="4">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>等等等&lt;/p>
&lt;p>说白了：就是让我们在操作 Chart 中的资源时，可以运行一个 Job 或某些资源(比如删除 operator 之前，运行一个 job 先删除所有 CRD 资源)。有点类似与就类似 &lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5fae78274cc5830001b9bbd6?scroll-to-block=5fae7842875a1b0329e90945">&lt;strong>crds 目录&lt;/strong>&lt;/a> 的作用一样，但并不完全一样。&lt;/p>
&lt;h2 id="hooks-种类">Hooks 种类&lt;/h2>
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>pre-install # 在渲染模板之后、创建资源之前，执行安装&lt;/p>
&lt;ol start="2">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>post-install # 在 Chart 中所有资源创建之后(并不用等待 running)，执行安装&lt;/p>
&lt;ol start="3">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>pre-delete # 在从 Kubernetes 删除任何资源之前，执行安装&lt;/p>
&lt;ol start="4">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>post-delete # 删除所有 releases 资源后，执行安装&lt;/p>
&lt;ol start="5">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>pre-upgrade # 在渲染模板之后，在任何资源更新之前，执行安装&lt;/p>
&lt;ol start="6">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>post-upgrade # 在所有资源都升级后，执行安装&lt;/p>
&lt;ol start="7">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>pre-rollback # Executes on a rollback request after templates are rendered, but before any resources are rolled back&lt;/p>
&lt;ol start="8">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>post-rollback # Executes on a rollback request after all resources have been modified&lt;/p>
&lt;ol start="9">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>test Executes when the Helm test subcommand is invoked ( view test docs)&lt;/p>
&lt;h1 id="release-的生命周期">Release 的生命周期&lt;/h1>
&lt;ol>
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>运行 helm install myapp&lt;/p>
&lt;ol start="2">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>The Helm library install API is called&lt;/p>
&lt;ol start="3">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>经过一些验证后，开始渲染 myapp 的模板&lt;/p>
&lt;ol start="4">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>渲染后生成的资源加载到 Kubernetes 中&lt;/p>
&lt;ol start="5">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>library 将 release 的数据返回给客户端&lt;/p>
&lt;ol start="6">
&lt;li>&lt;/li>
&lt;/ol>
&lt;p>客户端退出&lt;/p>
&lt;p>等待钩子准备好意味着什么？ 这取决于挂钩中声明的资源。 如果资源是 Job 或 Pod 类型，Helm 将等待直到成功运行完成为止。 如果挂钩失败，释放将失败。 这是一项阻止操作，因此 Helm 客户端将在 Job 运行时暂停。&lt;/p>
&lt;h1 id="使用-hooks">使用 Hooks&lt;/h1>
&lt;p>Helm 会读取 manifest 文件中的 &lt;code>.annotations.&amp;quot;helm.sh/hook&amp;quot;&lt;/code>、&lt;code>annotations.&amp;quot;helm.sh/hook-weight&amp;quot;&lt;/code>、&lt;code>annotations.&amp;quot;helm.sh/hook-delete-policy&amp;quot;&lt;/code>这三个字段，来为具有这三个字段的资源执行 Hooks 功能。&lt;/p></description></item><item><title>Docs: Charts</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86/helm/charts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86/helm/charts/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://helm.sh/docs/topics/charts/">官方文档，主题-charts&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Helm 管理的安装包称为 &lt;strong>Charts(图表)&lt;/strong>。就好比 Cento 的安装包是 rpm、Windows 的安装包是 exe、Ubuntu 的安装包是 deb。&lt;/p>
&lt;p>Charts 是描述 Kubernete 资源的一组 manifests 集合，被有规则得放在特定的目录树中。这些 Charts 可以打包成 &lt;strong>archives&lt;/strong>。&lt;/p>
&lt;p>Chart 也有&lt;strong>海图&lt;/strong>的概念，就好像 Helm 代表舵柄一样，当人们手握 Helm 在大海中航行时，可以查看 Charts，来观察地图，以便决定我们如何航行。&lt;/p>
&lt;h1 id="chart-file-structure图表文件结构">Chart File Structure(图表文件结构)&lt;/h1>
&lt;blockquote>
&lt;p>官方文档：&lt;a href="https://helm.sh/docs/topics/charts/">&lt;strong>https://helm.sh/docs/topics/charts/&lt;/strong>&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>一个 Chart 保存在一个目录中，目录名就是 Chart 的名称(没有版本信息)。比如 myapp 这个 chart 就放在 ./mapp/ 这个目录中&lt;/p>
&lt;p>在这个目录中，一般由以下内容组成：&lt;/p>
&lt;blockquote>
&lt;p>带有 OPTIONAL 表示不是必须的，可选的内容就算不存在，该 chart 也可正常使用&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Chart.yaml&lt;/strong> # 用来做 Chart 的初始化的文件，记录该 Chart 的名称、版本、维护者等元数据信息&lt;/li>
&lt;li>&lt;strong>LICENSE&lt;/strong> # (OPTIONAL)一个 chart 许可证的纯文本文件。&lt;/li>
&lt;li>&lt;strong>README.md&lt;/strong> # (OPTIONAL)一个易于阅读的自述文件。&lt;/li>
&lt;li>&lt;strong>values.yaml&lt;/strong> # 用于给 templates 目录下的各个 manifests 模板设定默认值。values.yaml 文件的说明详见 &lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9a633937398300016bed65?scroll-to-block=5f9a6348246f30cbbdf35c5a">&lt;strong>Helm Template 章节&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>values.yaml.json&lt;/strong> # (OPTIONAL)用于在 values.yaml 文件上强加结构的 JSON 模式&lt;/li>
&lt;li>&lt;strong>charts/&lt;/strong> # (OPTIONAL)包含该 Chart 所依赖的其他 Chart (这种被依赖的其他 Chart 称为 &lt;a href="https://thoughts.teambition.com/workspaces/603b04c9f83f2a00428f7321/docs/5fae78274cc5830001b9bbd6?scroll-to-block=6040f2c3a4b1ca00462d7837">&lt;strong>Subcharts 子图表&lt;/strong>&lt;/a>)。&lt;/li>
&lt;li>&lt;strong>crds/&lt;/strong> # (OPTIONAL)CRD 文件。该目录下的资源将会首先创建，其他资源等待 crds 资源 running 后，再创建。&lt;/li>
&lt;li>&lt;strong>templates/&lt;/strong> # 模板目录，与 values.yaml 相结合将生成有效的 kubernetes manifest 文件。
&lt;ul>
&lt;li>该目录下包含支撑 chart 的 manifests 文件，是各种 yaml 文件，只不过这些 yaml 文件是以模板形式存在的。&lt;/li>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9a633937398300016bed65">&lt;strong>模板详解见此处&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>templates/NOTES.txt&lt;/strong> # (OPTIONAL)生成 release 后给用户的提示信息&lt;/li>
&lt;li>&lt;strong>ci/&lt;/strong> # 存放自定义的 values.yaml。这是非官方推荐的目录，只不过大家都这么用。&lt;/li>
&lt;/ul>
&lt;p>当使用 helm create mychart 命令创建一个本地 chart 目录是，helm 会默认自动生成下列信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master helm&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># helm create mychart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Creating mychart
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master helm&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># tree mychart/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mychart/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── Chart.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── templates
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── deployment.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── _helpers.tpl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── hpa.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── ingress.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── NOTES.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── serviceaccount.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── service.yaml.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └── tests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └── test-connection.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── values.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="chartyaml-文件">Chart.yaml 文件&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://helm.sh/docs/topics/charts/#the-chartyaml-file">&lt;strong>官方文档&lt;/strong>&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#75715e"># (必须的)Chart 的 API 版本，有 v1、v2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#75715e"># (必须的)Chart 的名字&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#75715e"># (必须的)Chart 的版本号，必须符合 SemVer2 标准。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kubeVersion&lt;/span>: &lt;span style="color:#75715e"># (可选的)Chart 兼容的 Kubernetes 版本号，必须符合 SemVer 标准。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">description&lt;/span>: &lt;span style="color:#75715e"># (可选的)Chart 的简要描述&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">(可选的)The type of the chart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">keywords&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">(可选的)A list of keywords about this project&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">home&lt;/span>: &lt;span style="color:#ae81ff">(可选的)The URL of this projects home page&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">sources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">(可选的)A list of URLs to source code for this project&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">dependencies&lt;/span>: &lt;span style="color:#75715e"># (可选的)Chart 所依赖的其他 Charts 列表。也就是 SubCharts&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#75715e"># SubChart 的名字&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#75715e"># SubChart 的版本号&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">repository&lt;/span>: &lt;span style="color:#75715e"># The repository URL (&amp;#34;https://example.com/charts&amp;#34;) or alias (&amp;#34;@repo-name&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">condition&lt;/span>: &lt;span style="color:#75715e"># (可选的)根据条件控制这个 Chart 是否与上层 Chart 一起被安装。这个条件可以在 values.yaml 中定义(e.g. subchart1.enabled )&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: &lt;span style="color:#75715e"># (可选的)该字段可以用来将 SubCharts 分组，以便统一启用或禁用&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">XXXXX&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enabled&lt;/span>: &lt;span style="color:#75715e"># (可选的)控制这个 Chart 是否与上层 Chart 一起被安装&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">import-values&lt;/span>: &lt;span style="color:#75715e"># (可选的)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">alias&lt;/span>: &lt;span style="color:#ae81ff">(可选的) Alias to be used for the chart. Useful when you have to add the same chart multiple times&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">maintainers&lt;/span>: &lt;span style="color:#75715e"># (可选的)Chart 维护者的信息。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">The maintainers name (required for each maintainer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">email&lt;/span>: &lt;span style="color:#ae81ff">The maintainers email (optional for each maintainer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">url&lt;/span>: &lt;span style="color:#ae81ff">A URL for the maintainer (optional for each maintainer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">icon&lt;/span>: &lt;span style="color:#75715e"># (可选的)Chart 的 Logo，值必须是 URL。需要 helm 自动从 URL 中获取图片&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">appVersion&lt;/span>: &lt;span style="color:#75715e"># (可选的)Chart 中包含的应用程序的版本。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">deprecated&lt;/span>: &lt;span style="color:#75715e"># (可选的)标识该图表是否已弃用。可用的值是 true 和 false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">annotations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">example&lt;/span>: &lt;span style="color:#ae81ff">(可选的)A list of annotations keyed by name.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="各种-version-字段">各种 version 字段&lt;/h2>
&lt;h3 id="apiversion">apiVersion&lt;/h3>
&lt;p>这个其实就像 kubernetes 中的 apiVersion 概念，用来定义如何解析 Charts 文件的。不同的版本，Charts 中包含的内容不同，Chart.yaml 文件中的字段也不同。&lt;/p>
&lt;h3 id="version">version&lt;/h3>
&lt;p>这个就是 Chart 本身的版本。只不过这个版本号的格式必须符合 &lt;a href="https://semver.org/">&lt;strong>SemVer2&lt;/strong>&lt;/a>。&lt;/p>
&lt;p>SemVer2 格式大体是这样的：&lt;code>X.Y.Z&lt;/code>&lt;/p>
&lt;h3 id="kubeversion">kubeVersion&lt;/h3>
&lt;p>待整理&lt;/p>
&lt;h3 id="appversion">appVersion&lt;/h3>
&lt;p>用来定义 Chart 中包含的应用程序的版本，如果有多个应用程序，就自己选择用哪个，版本号格式随意。&lt;/p>
&lt;h2 id="dependencies-字段">dependencies 字段&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://helm.sh/zh/docs/topics/charts/#chart-dependency">官方文档，主题-charts-chart 依赖&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Helm 中，Chart 可以依赖其他任意数量的 Chart，这些可以被依赖的 Chart 可以通过 Chart.yaml 文件中的 dependencies 字段来控制。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>name: STRING&lt;/strong> # SubChart 的名字&lt;/li>
&lt;li>&lt;strong>version: STRING&lt;/strong> # SubChart 的版本号&lt;/li>
&lt;li>&lt;strong>repository: STRING&lt;/strong> # The repository URL (&amp;ldquo;&lt;a href="https://example.com/charts%22">https://example.com/charts&amp;quot;&lt;/a>) or alias (&amp;quot;@repo-name&amp;rdquo;)&lt;/li>
&lt;li>&lt;strong>condition: STRING&lt;/strong> # (可选的)根据条件控制这个 Chart 是否与上层 Chart 一起被安装。这个条件可以在 values.yaml 中定义(e.g. subchart1.enabled)
&lt;ul>
&lt;li>该字段非常重要与常见，假如我们定义 condition：&lt;code>condition: abc.enabled&lt;/code>&lt;/li>
&lt;li>然后我们可以在父 Chart 的 values.yaml 中定义字段 &lt;code>abc.enabled&lt;/code>，若 &lt;code>abc.enabled&lt;/code> 为 true 则该 Chart 将会与 父 Chart 一起被安装。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>tags: []STRING&lt;/strong> # (可选的)该字段可以用来将 SubCharts 分组，以便统一启用或禁用&lt;/li>
&lt;li>&lt;strong>enabled: true|false&lt;/strong> # (可选的)控制这个 Chart 是否与上层 Chart 一起被安装&lt;/li>
&lt;li>&lt;strong>import-values: []&lt;/strong> # (可选的)ImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items.&lt;/li>
&lt;li>&lt;strong>alias: STRING&lt;/strong> # (可选的)为该 Chart 起一个别名。若一个 Chart 需要被多次依赖时非常有用&lt;/li>
&lt;/ul>
&lt;h3 id="为什么需要-chart-dependencies">为什么需要 Chart Dependencies&lt;/h3>
&lt;blockquote>
&lt;p>首先需要明确一点，官方用 Dependencies 这个词不太准确，用 &lt;strong>SubCharts(子图表)&lt;/strong> 这个词更准确，因为依赖不是绝对的。&lt;/p>
&lt;/blockquote>
&lt;p>比如我想安装三个 Chart，分别为 A、B、C，如果要逐一安装是非常麻烦也不便于管理的，所以，我们需要把这些 Charts 整合起来，而整合的前提是，必须要存在一个 Chart。所以，我们可以这么做&lt;/p>
&lt;ul>
&lt;li>&lt;code>helm create mychart&lt;/code> 首先创建一个 Chart&lt;/li>
&lt;li>&lt;code>cd mychart/charts&lt;/code> 进入刚创建的 Chart 目录，逐一创建其他 Chart。&lt;code>for i in A B C; do helm create subchart${i}; done&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>此时该 Chart 目录结构如下&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>root@lichenhao:~/testDir# tree -L &lt;span style="color:#ae81ff">2&lt;/span> mychart/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mychart/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── subchartA
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── subchartB
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── subchartC
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── Chart.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── templates
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── deployment.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── _helpers.tpl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── hpa.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── ingress.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── NOTES.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── serviceaccount.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   ├── service.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── tests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── values.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这些当我安装 mychart 时，subchartA、subchartB、subchartC 这三个 Charts 也就一起被安装了。&lt;/p>
&lt;p>这时，我又有新的需求了，由于某些原因，我不想 subchartC 跟随 mychart 一起安装，而是根据某些规则来启动。所以，这些 Charts 就可以根据 Chart.yaml 文件中的 &lt;code>dependencies.enabled&lt;/code> 或 &lt;code>dependencies.condition&lt;/code> 字段来控制。&lt;/p>
&lt;p>由于这种不是强依赖的关系，所以用 &lt;strong>SubCharts(子图表)&lt;/strong> 这个词描述这个功能更为准确，而 mychart 这种就称为 &lt;strong>ParentChart(父图表)&lt;/strong>。而跟随父图表一起安装的行为称为 &lt;strong>Enabling(启用图表)&lt;/strong>，反之则称为 &lt;strong>Disabling(禁用图表)&lt;/strong>。&lt;/p>
&lt;h3 id="subcharts-的启用时机">SubCharts 的启用时机&lt;/h3>
&lt;p>SubCharts 与 Charts 关于 values.yaml 文件的使用还有一些注意事项，详见 &lt;a href="https://www.yuque.com/go/doc/33981940">Subcharts 与 Global Values 章节&lt;/a>&lt;/p>
&lt;p>Chart.yaml 文件中的 &lt;code>dependencies.condition&lt;/code> 与 &lt;code>dependencies.tags&lt;/code> 字段可以控制子图表安装的时机。&lt;/p>
&lt;ul>
&lt;li>condition # 该字段包含一个或多个 YAML 路径（用逗号分隔）。 如果这个路径在上层 values 中已存在并解析为布尔值，chart 会基于布尔值启用或禁用 chart。 只会使用列表中找到的第一个有效路径，如果路径为未找到则条件无效。&lt;/li>
&lt;li>tags - 该字段是与 chart 关联的 YAML 格式的标签列表。在顶层 value 中，通过指定 tag 和布尔值，可以启用或禁用所有的带 tag 的 chart。&lt;/li>
&lt;/ul>
&lt;p>假如现在有这么一个 Chart.yaml 文件：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">mychart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">description&lt;/span>: &lt;span style="color:#ae81ff">A Helm chart for Kubernetes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">application&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">0.1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">appVersion&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;1.16.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">dependencies&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">subchart1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">repository&lt;/span>: &lt;span style="color:#ae81ff">http://localhost:10191&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">0.1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">condition&lt;/span>: &lt;span style="color:#ae81ff">subchart1.enabled, global.subchart1.enabled&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">front-end&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">subchart1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">subchart2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">repository&lt;/span>: &lt;span style="color:#ae81ff">http://localhost:10191&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">0.1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">condition&lt;/span>: &lt;span style="color:#ae81ff">subchart2.enabled,global.subchart2.enabled&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">back-end&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">subchart2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>下面是 values.yaml 文件&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">subchart1&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enabled&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">tags&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">front-end&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">back-end&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在上面的例子中，所有带 &lt;code>front-end&lt;/code>tag 的 chart 都会被禁用，但只要上层的 value 中 &lt;code>subchart1.enabled&lt;/code> 路径被设置为 &amp;rsquo;true&amp;rsquo;，该条件会覆盖 &lt;code>front-end&lt;/code>标签且 &lt;code>subchart1&lt;/code> 会被启用。&lt;/p>
&lt;p>一旦 &lt;code>subchart2&lt;/code>使用了&lt;code>back-end&lt;/code>标签并被设置为了 &lt;code>true&lt;/code>，&lt;code>subchart2&lt;/code>就会被启用。 也要注意尽管&lt;code>subchart2&lt;/code> 指定了一个条件字段， 但是上层 value 没有相应的路径和 value，因此这个条件不会生效。&lt;/p>
&lt;h1 id="crds-目录">crds 目录&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://helm.sh/docs/topics/charts/#limitations-on-crds">&lt;strong>https://helm.sh/docs/topics/charts/#limitations-on-crds&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>crds 目录下的资源将会在其他资源安装之前，进行安装。并且无法在卸载 release 时，卸载 crds 目录下的资源。&lt;/p>
&lt;p>&lt;strong>crds 目录下的文件不能是模板&lt;/strong>，必须是普通的 YAML 文件。&lt;/p>
&lt;p>当 Helm 安装一个新 Chart 时，首先会安装 crds 目录下的资源，直到 API 服务器可以正常提供 crd，然后再启动模板引擎开始渲染模板，并安装 Chart 中的其余资源。&lt;/p>
&lt;p>由于 CRD 资源属于全局的，不受 namespace 限制，所以 Helm 在管理 CRD 时非常谨慎&lt;/p>
&lt;ul>
&lt;li>更新 Chart 时，无论如何都不会更新 crds 目录下的资源。只有当 crds 目录下的资源不存在时， Helm 才会创建它们&lt;/li>
&lt;li>卸载 Chart 时，不会删除 crds 目录下的资源。也就是说，只要第一次安装 Chart 时，创建了 crds 目录下的资源，则后续都不会&lt;/li>
&lt;li>也就是说，crds 目录下的资源永远不会被删除&lt;/li>
&lt;/ul>
&lt;p>删除 CRD 会自动删除集群中所有命名空间中 CRD 的所有内容。Helm 鼓励想要升级或删除 CRD 的维护人员手动操作，并格外注意&lt;/p>
&lt;h1 id="ci-目录">ci 目录&lt;/h1>
&lt;p>这是一个非官方的目录，我们在使用 Charts 时，经常会需要一些自定义的 values.yaml 文件，大家通常都将这些自定义的值文件放在 Charts 根目录下的 ci 目录中。&lt;/p></description></item><item><title>Docs: Cilium</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/cni/cilium/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/8.kubernetes-%E7%BD%91%E7%BB%9C/cni/cilium/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cilium/cilium">GitHub 项目，cilium/cilium&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cilium.io/">官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.cilium.io/en/latest/">官方文档&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;a href="https://docs.google.com/presentation/d/1cZJ-pcwB9WG88wzhDm2jxQY4Sh8adYg0-N3qWQ8593I/edit#slide=id.g7608b8c2de_0_0">https://docs.google.com/presentation/d/1cZJ-pcwB9WG88wzhDm2jxQY4Sh8adYg0-N3qWQ8593I/edit#slide=id.g7608b8c2de_0_0&lt;/a>
&lt;a href="https://www.youtube.com/watch?v=bIRwSIwNHC0">https://www.youtube.com/watch?v=bIRwSIwNHC0&lt;/a>
&lt;a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/">http://arthurchiao.art/blog/ebpf-and-k8s-zh/&lt;/a>&lt;/p>
&lt;h1 id="heading">&lt;/h1></description></item><item><title>Docs: Client Libraries(客户端库)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/client-libraries%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BA%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/client-libraries%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BA%93/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/client-libraries/">官方文档&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Client Libraries(客户端库)&lt;/strong> 是各种编程语言的&lt;strong>第三方库的统称&lt;/strong>。这些库可以用来让各种编程语言通过代码的方式访问 Kubernetes API。在使用这些库编写代码时，并不需要自己实现对 Kubernetes API 的调用和 处理 Request/Response，这些处理逻辑都在 Client Libraries 中包括了。客户端库还会处理诸如身份验证之类的行为。&lt;/p>
&lt;p>如果代码在 Kubernetes 集群中运行，代码中的 Client Libraires 可以发现并使用 Kubernetes 的 ServiceAccount 进行身份验证。&lt;/p>
&lt;p>如果代码在 Kubernetes 集群外运行，代码中的 Client Libraires 能够理解 &lt;a href="https://kubernetes.io/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">kubeconfig 文件&lt;/a> 格式来读取凭据和 API 服务器地址。&lt;/p>
&lt;p>Kubernetes 现阶段官方支持 Go、Python、Java、 dotnet、Javascript 和 Haskell 语言的客户端库。还有一些其他客户端库由对应作者而非 Kubernetes 团队提供并维护。 参考客户端库了解如何使用其他语言 来访问 API 以及如何执行身份认证。&lt;/p></description></item><item><title>Docs: client-go连接K8s集群进行pod的增删改查</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/client-libraries%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BA%93/client-go%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8Cpod%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E5%BC%80%E5%8F%91/client-libraries%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BA%93/client-go%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4%E8%BF%9B%E8%A1%8Cpod%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/</guid><description>
&lt;h1 id="背景">背景&lt;/h1>
&lt;p>最近在看&lt;code>client-go&lt;/code>源码最基础的部分，&lt;code>client-go&lt;/code>的四类客户端，&lt;code>RestClient、ClientSet、DynamicClient、DiscoveryClient&lt;/code>。其中&lt;code>RestClient&lt;/code>是最基础的客户端，它对&lt;code>Http&lt;/code>进行了封装，支持&lt;code>JSON&lt;/code>和&lt;code>protobuf&lt;/code>格式数据。其它三类客户端都是通过在&lt;code>RestClient&lt;/code>基础上再次封装而得来。不过我对&lt;code>ClientSet&lt;/code>和&lt;code>DynamicClient&lt;/code>傻傻分不清，虽然很多资料上说它两最大区别是，&lt;code>ClientSet&lt;/code>能够使用预先生成的&lt;code>Api&lt;/code>和&lt;code>ApiServer&lt;/code>进行通信；而&lt;code>DynamicClient&lt;/code>更加强大，不仅仅能够调用预先生成的&lt;code>Api&lt;/code>，还能够对一些&lt;code>CRD&lt;/code>资源通过结构化嵌套类型跟&lt;code>ApiServer&lt;/code>进行通信。意思大致明白前者能够调用&lt;code>Kubernetes&lt;/code>本地资源类型，后者还可以调用一些自定资源，那么他们究竟是如何跟&lt;code>ApiServer&lt;/code>进行交互、&lt;code>Pod&lt;/code>的增删改查呢？本文通过分析&lt;code>ClientSet&lt;/code>代码和&lt;code>client-go&lt;/code>客户端调用&lt;code>Kubernetes&lt;/code>集群的方式来演示下整个交互过程。&lt;/p>
&lt;h1 id="准备工作">准备工作&lt;/h1>
&lt;p>已经安装&lt;code>Kubernetes&lt;/code>集群和配置本地&lt;code>IDE&lt;/code>环境&lt;/p>
&lt;ol>
&lt;li>根据&lt;code>kubernetes&lt;/code>集群版本选择&lt;code>clone client-go&lt;/code>到本地：&lt;code>https://github.com/kubernetes/client-go/tree/release-14.0&lt;/code>。&lt;/li>
&lt;li>导入到&lt;code>IDE&lt;/code>。&lt;/li>
&lt;li>运行 &lt;code>examples/create-update-delete-deployment/main.go&lt;/code> 正常情况下会提示如下错误：&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>panic: CreateFile C:\Users\shj\.kube\config: The system cannot find the path spe
cified.
&lt;/code>&lt;/pre>
&lt;p>错误信息提示很清楚，没有找到本地文件夹下的&lt;code>config&lt;/code>文件，处理方式也很简单，只需要把你&lt;code>Kubernetes&lt;/code>集群中&lt;code>$HOME/.kube/config&lt;/code>复制到本地即可；仔细阅读代码可以发现，也可以通过自行配置客户端连接信息（生产环境慎用）。&lt;/p>
&lt;p>4、运行 main 函数即可进行 Pod 增删改查操作。&lt;/p>
&lt;h1 id="client-go-连接-apiserver-进行-pod-的增删改查">client-go 连接 ApiServer 进行 Pod 的增删改查&lt;/h1>
&lt;ol>
&lt;li>获取&lt;code>APIserver&lt;/code>连接地址、认证配置等信息&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">kubeconfig&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#66d9ef">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//获取当前用户home文件夹，并获取kubeconfig配置
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">home&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">homedir&lt;/span>.&lt;span style="color:#a6e22e">HomeDir&lt;/span>(); &lt;span style="color:#a6e22e">home&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">kubeconfig&lt;/span> = &lt;span style="color:#a6e22e">flag&lt;/span>.&lt;span style="color:#a6e22e">String&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;kubeconfig&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">filepath&lt;/span>.&lt;span style="color:#a6e22e">Join&lt;/span>(&lt;span style="color:#a6e22e">home&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;.kube&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;config&amp;#34;&lt;/span>), &lt;span style="color:#e6db74">&amp;#34;(optional) absolute path to the kubeconfig file&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#66d9ef">else&lt;/span> {&lt;span style="color:#75715e">//如果没有获取到，则需要自行配置kubeconfig
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">kubeconfig&lt;/span> = &lt;span style="color:#a6e22e">flag&lt;/span>.&lt;span style="color:#a6e22e">String&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;kubeconfig&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;absolute path to the kubeconfig file&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//把用户传递的命令行参数，解析为响应变量的值
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">flag&lt;/span>.&lt;span style="color:#a6e22e">Parse&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//加载kubeconfig中的apiserver地址、证书配置等信息
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">config&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">clientcmd&lt;/span>.&lt;span style="color:#a6e22e">BuildConfigFromFlags&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">kubeconfig&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tbu177/311z1fa79ef320e339b2cf84c6924ecaf224" alt="">&lt;/p>
&lt;pre>&lt;code>debug信息
&lt;/code>&lt;/pre>
&lt;p>2、创建&lt;code>Clientset&lt;/code>客户端&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//NewForConfig为给定的配置创建一个新的Clientset（如下图所示包含所有的api-versions，这样做的目的是便于其它
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//资源类型对这个Pod进行管理和控制？）。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">clientset&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">kubernetes&lt;/span>.&lt;span style="color:#a6e22e">NewForConfig&lt;/span>(&lt;span style="color:#a6e22e">config&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tbu177/311z40ea6463a53d2e1fbffae83e513a8852" alt="">&lt;/p>
&lt;pre>&lt;code>debug信息
&lt;/code>&lt;/pre>
&lt;p>3、创建&lt;code>deployment&lt;/code>客户端&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//这个过程中会把包含RESTClient配置信息、命名空间信息赋值到deploymentsClient中，具体如下图信息。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">deploymentsClient&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">clientset&lt;/span>.&lt;span style="color:#a6e22e">AppsV1&lt;/span>().&lt;span style="color:#a6e22e">Deployments&lt;/span>(&lt;span style="color:#a6e22e">apiv1&lt;/span>.&lt;span style="color:#a6e22e">NamespaceDefault&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//构造deployment
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">deployment&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">appsv1&lt;/span>.&lt;span style="color:#a6e22e">Deployment&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ObjectMeta&lt;/span>: &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ObjectMeta&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Name&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;demo-deployment&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Spec&lt;/span>: &lt;span style="color:#a6e22e">appsv1&lt;/span>.&lt;span style="color:#a6e22e">DeploymentSpec&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Replicas&lt;/span>: &lt;span style="color:#a6e22e">int32Ptr&lt;/span>(&lt;span style="color:#ae81ff">2&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Selector&lt;/span>: &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">LabelSelector&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">MatchLabels&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;app&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;demo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Template&lt;/span>: &lt;span style="color:#a6e22e">apiv1&lt;/span>.&lt;span style="color:#a6e22e">PodTemplateSpec&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ObjectMeta&lt;/span>: &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ObjectMeta&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Labels&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">string&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;app&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;demo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Spec&lt;/span>: &lt;span style="color:#a6e22e">apiv1&lt;/span>.&lt;span style="color:#a6e22e">PodSpec&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Containers&lt;/span>: []&lt;span style="color:#a6e22e">apiv1&lt;/span>.&lt;span style="color:#a6e22e">Container&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Name&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;web&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Image&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx:1.12&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Ports&lt;/span>: []&lt;span style="color:#a6e22e">apiv1&lt;/span>.&lt;span style="color:#a6e22e">ContainerPort&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Name&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;http&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Protocol&lt;/span>: &lt;span style="color:#a6e22e">apiv1&lt;/span>.&lt;span style="color:#a6e22e">ProtocolTCP&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ContainerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tbu177/311zfe33d3d2814176bc1ef2cc4f72eeb2e7" alt="">&lt;/p>
&lt;p>看到这里之后，&lt;code>ClientSet&lt;/code>之所以只能处理预先声明的原生资源类型，是因为对象都是使用的内置元数据类型，不存在的自然没有办法使用了，这时我们在看下&lt;code>DynamicClient&lt;/code>，部分代码如下所示，它使用&lt;code>unstructured.Unstructured&lt;/code>表示来自 &lt;code>API Server&lt;/code>的所有对象值。&lt;code>Unstructured&lt;/code>类型是一个嵌套的&lt;code>map[string]inferface{}&lt;/code> 值的集合来创建一个内部结构，通过这种方式，可以表示自定义资源&lt;code>CRD&lt;/code>资源对象。具体示例，请参考：&lt;code>examples/dynamic-create-update-delete-deployment/main.go&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">deployment&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">unstructured&lt;/span>.&lt;span style="color:#a6e22e">Unstructured&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Object&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;apps/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Deployment&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;metadata&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;demo-deployment&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;spec&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;replicas&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;selector&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;matchLabels&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;app&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;demo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;template&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;metadata&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;labels&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;app&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;demo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;spec&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;containers&amp;#34;&lt;/span>: []&lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;web&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;image&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx:1.12&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ports&amp;#34;&lt;/span>: []&lt;span style="color:#66d9ef">map&lt;/span>[&lt;span style="color:#66d9ef">string&lt;/span>]&lt;span style="color:#66d9ef">interface&lt;/span>{}{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;http&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;TCP&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;containerPort&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>4、发送&lt;code>Post&lt;/code>请求&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//发送Post请求到Kubernetes集群
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">result&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">deploymentsClient&lt;/span>.&lt;span style="color:#a6e22e">Create&lt;/span>(&lt;span style="color:#a6e22e">deployment&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>执行下&lt;code>kubectl get pod&lt;/code>发现 Kubernetes 集群中 Pod 已经创建。5、更新&lt;code>Pod&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//尝试更新资源
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">retryErr&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">retry&lt;/span>.&lt;span style="color:#a6e22e">RetryOnConflict&lt;/span>(&lt;span style="color:#a6e22e">retry&lt;/span>.&lt;span style="color:#a6e22e">DefaultRetry&lt;/span>, &lt;span style="color:#66d9ef">func&lt;/span>() &lt;span style="color:#66d9ef">error&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//获取Get（）返回的“result”
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">result&lt;/span>, &lt;span style="color:#a6e22e">getErr&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">deploymentsClient&lt;/span>.&lt;span style="color:#a6e22e">Get&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;demo-deployment&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">GetOptions&lt;/span>{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">getErr&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> panic(&lt;span style="color:#a6e22e">fmt&lt;/span>.&lt;span style="color:#a6e22e">Errorf&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Failed to get latest version of Deployment: %v&amp;#34;&lt;/span>, &lt;span style="color:#a6e22e">getErr&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//replica数量降低到1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">result&lt;/span>.&lt;span style="color:#a6e22e">Spec&lt;/span>.&lt;span style="color:#a6e22e">Replicas&lt;/span> = &lt;span style="color:#a6e22e">int32Ptr&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//修改nginx镜像
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">result&lt;/span>.&lt;span style="color:#a6e22e">Spec&lt;/span>.&lt;span style="color:#a6e22e">Template&lt;/span>.&lt;span style="color:#a6e22e">Spec&lt;/span>.&lt;span style="color:#a6e22e">Containers&lt;/span>[&lt;span style="color:#ae81ff">0&lt;/span>].&lt;span style="color:#a6e22e">Image&lt;/span> = &lt;span style="color:#e6db74">&amp;#34;nginx:1.13&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//更新（result）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">_&lt;/span>, &lt;span style="color:#a6e22e">updateErr&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">deploymentsClient&lt;/span>.&lt;span style="color:#a6e22e">Update&lt;/span>(&lt;span style="color:#a6e22e">result&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">updateErr&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tbu177/311ze17817d585f6936fa2f18f17f2678de6" alt="">&lt;/p>
&lt;pre>&lt;code>更新模板信息
&lt;/code>&lt;/pre>
&lt;p>&lt;code>RetryOnConflict&lt;/code>用于需要考虑更新冲突的情况下对资源进行更新，出现这种场景，大多因为存在其它客户端但或者代码同一时间内操作该资源对象。如果&lt;code>update&lt;/code>函数返回冲突错误，&lt;code>RetryOnConflict&lt;/code>将按指定策略等待一段时间退后，再次尝试更新。&lt;/p>
&lt;p>6、查询操作&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//发送http get请求获取pod列表
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">list&lt;/span>, &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">deploymentsClient&lt;/span>.&lt;span style="color:#a6e22e">List&lt;/span>(&lt;span style="color:#a6e22e">metav1&lt;/span>.&lt;span style="color:#a6e22e">ListOptions&lt;/span>{})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>其内部查询接口如下图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/tbu177/311zdad11d29dbefd5810a31c0e3a02fa976" alt="">&lt;/p>
&lt;p>其中&lt;code>c.client&lt;/code>读取配置实例化&lt;code>RESTClient&lt;/code>对象和&lt;code>ns&lt;/code>,其中&lt;code>deployments&lt;/code>这个对象是在这行&lt;code>deploymentsClient := clientset.AppsV1().Deployments(apiv1.NamespaceDefault)&lt;/code>进行初始化; &lt;code>Get()&lt;/code>通过&lt;code>Get&lt;/code>请求，同样支持&lt;code>POST PUT DELETE PATCH&lt;/code>;&lt;code>Resource&lt;/code>设置请求的资源名称;&lt;code>VersionedParams&lt;/code> 设置查询选项，例如：&lt;code>TimeoutSeconds&lt;/code>;&lt;code>Do()&lt;/code>执行请求，结果&lt;code>Into&lt;/code>到&lt;code>Result&lt;/code>。&lt;/p>
&lt;p>7、删除操作&lt;/p>
&lt;pre>&lt;code>//指定删除策略
deletePolicy := metav1.DeletePropagationForeground
//针对特定deployment进行删除操作
if err := deploymentsClient.Delete(&amp;quot;demo-deployment&amp;quot;, &amp;amp;metav1.DeleteOptions{
PropagationPolicy: &amp;amp;deletePolicy,
});
&lt;/code>&lt;/pre>
&lt;p>&lt;code>Kubernetes&lt;/code>控制器的删除有 3 种模式：&lt;/p>
&lt;ul>
&lt;li>&lt;code>Foreground&lt;/code>: 删除控制器之前，先删除控制器所管理的资源对象删除。&lt;/li>
&lt;li>&lt;code>Background&lt;/code>：删除控制器后，控制器所管理的资源对象由&lt;code>GC&lt;/code>在后台进行删除。&lt;/li>
&lt;li>&lt;code>Orphan&lt;/code>：只删除控制器，不删除控制器所管理的资源对象(举个例子，比如你删除了&lt;code>deployment&lt;/code>，那么对应的&lt;code>Pod&lt;/code>不会被删除)。&lt;/li>
&lt;/ul>
&lt;p>8、观察&lt;code>Pod&lt;/code>变化&lt;/p>
&lt;pre>&lt;code>[root@k8s-m1 ~]# kubectl get pod --watch
NAME READY STATUS RESTARTS AGE
demo-deployment-5fc8ffdb68-8xdcx 1/1 Running 0 26s
demo-deployment-5fc8ffdb68-w555g 1/1 Running 0 26s
demo-deployment-5fc8ffdb68-w555g 1/1 Terminating 0 42s
demo-deployment-5cb6f65f77-tn5bn 0/1 Pending 0 0s
demo-deployment-5cb6f65f77-tn5bn 0/1 Pending 0 0s
demo-deployment-5cb6f65f77-tn5bn 0/1 ContainerCreating 0 0s
demo-deployment-5fc8ffdb68-w555g 0/1 Terminating 0 43s
demo-deployment-5cb6f65f77-tn5bn 1/1 Running 0 2s
demo-deployment-5fc8ffdb68-8xdcx 1/1 Terminating 0 44s
demo-deployment-5fc8ffdb68-8xdcx 0/1 Terminating 0 45s
demo-deployment-5fc8ffdb68-8xdcx 0/1 Terminating 0 48s
demo-deployment-5fc8ffdb68-8xdcx 0/1 Terminating 0 48s
demo-deployment-5fc8ffdb68-w555g 0/1 Terminating 0 51s
demo-deployment-5fc8ffdb68-w555g 0/1 Terminating 0 51s
demo-deployment-5cb6f65f77-tn5bn 1/1 Terminating 0 52s
demo-deployment-5cb6f65f77-tn5bn 0/1 Terminating 0 53s
demo-deployment-5cb6f65f77-tn5bn 0/1 Terminating 0 56s
demo-deployment-5cb6f65f77-tn5bn 0/1 Terminating 0 56s
&lt;/code>&lt;/pre>
&lt;h1 id="总结">总结&lt;/h1>
&lt;p>本文主要通过在本地运行&lt;code>client-go/ClientSet&lt;/code>客户端对&lt;code>Pod&lt;/code>的增删改查，并解释了代码的执行过程。同时加深了对&lt;code>ClientSet&lt;/code>客户端的理解。&lt;/p></description></item></channel></rss>