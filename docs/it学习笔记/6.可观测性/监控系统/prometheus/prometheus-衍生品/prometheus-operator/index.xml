<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Prometheus Operator</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/</link><description>Recent content in Prometheus Operator on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: CR 详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/cr-%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/cr-%E8%AF%A6%E8%A7%A3/</guid><description/></item><item><title>Docs: kube-prometheus 项目</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/kube-prometheus-%E9%A1%B9%E7%9B%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/kube-prometheus-%E9%A1%B9%E7%9B%AE/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/prometheus-operator/kube-prometheus">GitHub 项目，prometheus-operator/kube-prometheus&lt;/a>
&lt;ul>
&lt;li>部署文件
&lt;ul>
&lt;li>&lt;a href="https://github.com/coreos/kube-prometheus/tree/master/manifests">https://github.com/coreos/kube-prometheus/tree/master/manifests&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus-operator/kube-prometheus/tree/main/manifests">https://github.com/prometheus-operator/kube-prometheus/tree/main/manifests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus-community/helm-charts">GitHub 项目，prometheus-community/helm-charts&lt;/a>（kube-prometheus 项目的 Helm Chart）&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>该项目曾经属于 prometheus operator 项目的一部分，后来挪到 coreos 社区中，再后来又挪回 prometheus operator 社区中，并作为一个单独的 repo 存在。&lt;/p>
&lt;p>kube-prometheus 在 prometheus-operator 基础上，给用户提供了一套完整的 yaml 文件，这样就不用让用户在创建完 operator 之后，还要自己写一大堆 prometheus 相关的 yaml 才能把监控系统用起来。&lt;/p>
&lt;p>这套完整的 yaml 文件就在上面所写的‘部署文件’链接中,其中包括 prometheus 部署所用的各种 yaml 文件以及配置生成文件、RBAC、告警文件、grafana 还有 grafna 模板等等&lt;/p>
&lt;h2 id="兼容矩阵">兼容矩阵&lt;/h2>
&lt;h1 id="部署">部署&lt;/h1></description></item><item><title>Docs: Operator API</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/operator-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/operator-api/</guid><description>
&lt;p>参考：&lt;a href="https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md">GitHub 文档&lt;/a>&lt;/p></description></item><item><title>Docs: Prometheus Operator</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/prometheus-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/prometheus-operator/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://prometheus-operator.dev/">官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus-operator/prometheus-operator">GitHub 项目&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>该项目曾经在 &lt;a href="https://github.com/coreos/prometheus-operator">coreos/prometheus-operator&lt;/a> 仓库中，后来移动到 prometheus-operator/prometheus-operator&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>为什么会需要 prometheus-operator(后文简称 operator)&lt;/p>
&lt;p>当 prometheus 需要监控 kubernetes 集群时，要手动修改配置文件中的 scrape 配置段是非常复杂且繁琐的。每启动一个新 pod 就要新加配置，并手动更新 prometheus 配置文件，有没有一种办法可以在新增 pod 时，让 prometheus 自动更新其配置文件呢？这就是 operator 的作用。&lt;/p>
&lt;p>Prometheus Operator 通过数个 CRD 资源来控制 Prometheus 监控套件的运行，并作为这几个 CRD 的 controller(类似于 kube-controller-manager，只不过这个 Controller 只维护几个自定义的资源)来维护其正常运行，这些 CRD 就可以实现这样的功能：自动添加配置文件中 scrape 配置段的 job，并且自动执行热更新来加载配置文件等等。下面是这几个 CRD 的简介&lt;/p>
&lt;h2 id="crd-介绍">CRD 介绍&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design">官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md">https://github.com/coreos/prometheus-operator/blob/master/Documentation/design.md&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Prometheus Operator 现阶段引入了如下几种自定义资源：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#prometheus">Prometheus&lt;/a> # 它定义了所需的 Prometheus 主程序。Operator 始终确保正在运行与资源定义匹配的 prometheus 主程序。&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#alertmanager">Alertmanager&lt;/a> # 它定义了所需的 Alertmanager 主程序。Operator 始终确保正在运行与资源定义匹配的 Alertmanager 主程序。&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#thanosruler">ThanosRuler&lt;/a> #&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#servicemonitor">ServiceMonitor&lt;/a> # 为 Prometheus Server 配置文件中的 scrape_config 配置段生成配置内容。以声明方式指定应如何监控服务组。&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#podmonitor">PodMonitor&lt;/a> # 为 Prome theus Server 配置文件中的 scrape_config 配置段生成配置内容。与 ServiceMonitor 类型类似，只不过是从指定的 pod 中，发现待抓去的目标。&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#probe">Probe&lt;/a> # 为 Prometheus Server 配置文件中的 scrape_config 配置段生成配置内容。只会生成 blackbox-exporter 程序所需的配置。&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#prometheusrule">PrometheusRule&lt;/a> # 它定义了一个所需的 Prometheus 规则文件，该文件可以由包含 Prometheus 警报和记录规则的 Prometheus 实例加载。&lt;/li>
&lt;li>&lt;a href="https://prometheus-operator.dev/docs/operator/design/#alertmanagerconfig">AlertmanagerConfig&lt;/a> #&lt;/li>
&lt;/ul>
&lt;p>随着发展，也许还会有其他的 CR 产生&lt;/p>
&lt;p>其中 ServiceMonitor、PodMonitor、Probe、PrometheusRule 这几个资源，会被 Operator 监听，并通知配置换换程序将其转换为 Prometheus Server 的配置文件中的内容&lt;/p>
&lt;h3 id="prometheus">Prometheus&lt;/h3>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33146451">Prometheus CRD 详解&lt;/a>&lt;/p>
&lt;h3 id="alertmanager">Alertmanager&lt;/h3>
&lt;h3 id="thanosruler">ThanosRuler&lt;/h3>
&lt;h3 id="servicemonitor简称-sm囧">ServiceMonitor(简称 SM。。。囧)&lt;/h3>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33146422">Service Monitor CRD 详解&lt;/a>&lt;/p>
&lt;h3 id="podmonitor">PodMonitor&lt;/h3>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33146411">Pod Monitor CRD 详解&lt;/a>&lt;/p>
&lt;h3 id="probe">Probe&lt;/h3>
&lt;p>Probe CRD 定义应如何监视分组和静态目标。除目标外，该&lt;code>Probe&lt;/code>对象还需要一个&lt;code>prober&lt;/code>服务，该服务可监视目标并提供 Prometheus 进行刮擦的度量。例如，可以使用 &lt;a href="https://github.com/prometheus/blackbox_exporter/">blackbox-exporter&lt;/a> 来实现。&lt;/p>
&lt;h3 id="prometheusrule">PrometheusRule&lt;/h3>
&lt;p>它定义了一个所需的 Prometheus 规则文件，该文件可以由包含 Prometheus 警报和记录规则的 Prometheus 实例加载。&lt;/p>
&lt;h3 id="alertmanager-1">Alertmanager&lt;/h3>
&lt;p>它定义了所需的 Alertmanager 部署。operator 始终确保正在运行与资源定义匹配的部署。&lt;/p>
&lt;p>PrometheusRule:对于 Prometheus 而言，在原生的管理方式上，我们需要手动创建 Prometheus 的告警文件，并且通过在 Prometheus 配置中声明式的加载。而在 Prometheus Operator 模式中，告警规则也编程一个通过 Kubernetes API 声明式创建的一个资源.告警规则创建成功后，通过在 Prometheus 中使用想 servicemonitor 那样用 ruleSelector 通过 label 匹配选择需要关联的 PrometheusRule 即可&lt;/p>
&lt;h1 id="prometheus-operator-部署">Prometheus Operator 部署&lt;/h1>
&lt;pre>&lt;code>curl -LO https://raw.githubusercontent.com/coreos/prometheus-operator/master/bundle.yaml
&lt;/code>&lt;/pre>
&lt;p>该文件会在 default 名称空间里创建 operator。如果要放在其他 namespace 中，需要修改一下 bundle.yaml 文件中 namespace 的值，并修改 clusterrolebinding 中引用的 ServiceAccount 的名称空间。&lt;/p>
&lt;pre>&lt;code>[root@master-1 prometheus-operator]# kubectl apply -f bundle.yaml
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
serviceaccount/prometheus-operator created
service/prometheus-operator created
&lt;/code>&lt;/pre>
&lt;p>部署成功后会有一个名为 prometheus-operator 的 deployment、相关的 RBAC(ServiceAccount、ClusterRole、ClusterRoleBinding)、一个 service。还有几个 CRD。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-1 prometheus-operator&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># kubectl get -n monitor all&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/prometheus-operator-6cdb7d79fb-mgv97 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 35s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT&lt;span style="color:#f92672">(&lt;/span>S&lt;span style="color:#f92672">)&lt;/span> AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service/prometheus-operator ClusterIP None &amp;lt;none&amp;gt; 8080/TCP 36s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment.apps/prometheus-operator 1/1 &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> 36s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>replicaset.apps/prometheus-operator-6cdb7d79fb &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> 36s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@master-1 prometheus-operator&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># kubectl get clusterrole,clusterrolebindings | grep prometheus&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrole.rbac.authorization.k8s.io/prometheus-operator 2021-01-22T15:43:34Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator ClusterRole/prometheus-operator 58s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="使用-helm-快速部署一个-prometheus-operator-套件">使用 helm 快速部署一个 prometheus operator 套件&lt;/h2>
&lt;p>在 &lt;a href="https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack">Artifact Hub 上有官方发布的 chart 包&lt;/a>&lt;/p>
&lt;p>为适应 eHualu 生成部署，添加了一个名为 custom 的 subchart 。具体详见 GitHub&lt;/p>
&lt;p>&lt;strong>其他&lt;/strong>
其他的在安装时使用 -f 参数使用自定义的值文件覆盖即可。&lt;/p></description></item><item><title>Docs: Prometheus-adapter</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/prometheus-adapter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/prometheus/prometheus-%E8%A1%8D%E7%94%9F%E5%93%81/prometheus-operator/prometheus-adapter/</guid><description>
&lt;h1 id="介绍">介绍&lt;/h1>
&lt;blockquote>
&lt;p>参考:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/DirectXMan12/k8s-prometheus-adapter">GitHub 项目，kubernetes-sigs/prometheus-adapter&lt;/a>
&lt;ul>
&lt;li>该项目从 DirectXMan12/k8s-prometheus-adapter 移动到 kubernetes-sigs/prometheus-adapter&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>主要特性：&lt;/strong>
&lt;strong>一、adapter 在成功注册 API 之后，可以通过 Prometheus 实现 custom.metrics.k8s.io API 和 metrics.k8s.io API 的功能&lt;/strong>
adaper 可以替换掉 metrics server 来实现其功能。adapter 要想实现 kubectl top node/pod 命令的功能，则需要 adapter 通过查询 Prometheus 来获取数据完成，这需要 prometheus 提前获取某些数据来支撑 adapter 得查询，而查询语句则是根据 adapter 的配置文件中 resourceRules 配置环境中的规则来指定。&lt;/p>
&lt;ul>
&lt;li>其中 kubectl top node 如果查询语句查询结果为空，则在执行命令查询时会报错：error: metrics not available yet&lt;/li>
&lt;li>其中 kubectl top pod 如果查询语句查询结果为空，则在执行命令查询时会报错：No resources found&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>二、adapter 可以根据 prometheus 提供的核心 metrics 数据(比如 CPU 使用率等)或者自定义 metrics 数据，来自动实现&lt;/strong>[&lt;strong>HPA&lt;/strong>](4.Controller(控制器).md 容器编排系统/4.Controller(控制器).md)&lt;strong>功能。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>HPA 的概念详见《Controller 控制器》章节中的 HPA 控制器介绍&lt;/p>
&lt;/blockquote>
&lt;h2 id="adapter-工作流程">adapter 工作流程&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rd9psx/1616068726920-7b7a454e-7853-49a7-b56e-f8fcc594ad0e.png" alt="">
一、注册指标。adpater 启动后根据配置中的 seriesQuery 和 name 字段的规则，将匹配到的 metrics 注册到 Metrics API 中
二、查询 metrics 的值并返回。当 hpa 或者 kubectl top 命令想要获取信息时，会对 Metrics API 发起请求，请求交给 adapter 进行处理。adapter 根据 resources 和 metricsQuery 字段的规则，向 prometheus 发起查询请求，并将结果返回给 hap 或者 kubectl top 命令。&lt;/p>
&lt;p>adapter 不像 metrics server 直接与 kubelet 交互，然后从 kubelet 的 cAdvisor 中获取 Core Metrics 和其中的值。而是与 prometheus 交互，通过 PromQL 查询语句来获取想要的 Metrics 和 Metircs 的值。通过配置文件中的规则，发现可以使用的 Core Metrics 或者 Custom Metrics，并将其注册到 Metrics API 中，而不像 metrics server 可以直接将 cpu 和 memory 的 metrics 注册到 Metrics API 。&lt;/p>
&lt;h2 id="apapter-处理-metricsapi-接收到的请求的方式">apapter 处理 MetricsAPI 接收到的请求的方式&lt;/h2>
&lt;p>比如一个 MetricsAPI 请求是这样的：/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/grafana-5c55845445-q2p9l/http_request_per_second，adapter 会提取其中的字段，将其分为三个部分 MetricsName、Resource、Resource 的 Objects。Resource 概念详见：Kubernetes API 介绍。Object 概念详见：Kubernets Object 对象。&lt;/p>
&lt;ul>
&lt;li>MetricsName 值为 http_request_per_second&lt;/li>
&lt;li>Resource 值为 namesapces 和 pods&lt;/li>
&lt;li>Resource 的 Object 值分为两部分
&lt;ul>
&lt;li>namespaces 资源的 object 值为 monitoring&lt;/li>
&lt;li>pods 资源的 object 值为 grafana-5c55845445-q2p9l&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>adapter 将这三个内容填充到配置文件中 metricsQuery 关键字定义的 Go 模板中，生成 PromQL，并向 prometheus 发起查询。&lt;/p>
&lt;h1 id="adapter-配置">Adapter 配置&lt;/h1>
&lt;blockquote>
&lt;ul>
&lt;li>官方文档：
&lt;ul>
&lt;li>配置文件说明：&lt;a href="https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config.md">https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config.md&lt;/a>&lt;/li>
&lt;li>配置文件示例：&lt;a href="https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config-walkthrough.md">https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config-walkthrough.md&lt;/a>&lt;/li>
&lt;li>官方默认的配置文件：&lt;a href="https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/deploy/manifests/custom-metrics-config-map.yaml">https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/deploy/manifests/custom-metrics-config-map.yaml&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Adapter 的配置文件，是用来通过其中定义的 rules 来确定公开哪些 metrics ，以及如何公开它们。还有通过对 prometheus 查询获取样本值&lt;/p>
&lt;p>Note：
如果配置文件有问题，adapter 无法获取到 metrics 并注册到 api，则该 api 会报错：FailedDiscoveryCheck
在配置文件里会经常看到 series 这个单词，表示序列的意思，应该时 time-series 的简称，也有 metrics name 的意思&lt;/p>
&lt;pre>&lt;code>[root@master adapter]# kubectl get apiservices.apiregistration.k8s.io
NAME SERVICE AVAILABLE AGE
......
v1beta1.custom.metrics.k8s.io monitoring/prometheus-adapter False (FailedDiscoveryCheck) 3s
......
&lt;/code>&lt;/pre>
&lt;p>配置文件有两大部分&lt;/p>
&lt;ol>
&lt;li>rules 配置环境 # 用于 Custom Metrics&lt;/li>
&lt;li>resourceRules 配置环境 # 用于 Core Metrics&lt;/li>
&lt;/ol>
&lt;h2 id="rules-配置环境">rules 配置环境&lt;/h2>
&lt;pre>&lt;code>rules:
# 指定 PromQL 从 Prometheus 查找时间序列，然后 adapter 会剥离获取到的时间序列中的 label ，只保留 metrics name ，然后根据配置文件中 name 字段重命名之后，将新命名的 metrics name 注册到 Metrics API 中
# 对 seriesQuery 中获取到的时间序列进行过滤，可以使用 is 或者 isNot 关键字
- seriesQuery: 'PromQL'
seriesFilters:
- is: &amp;lt;RegEx&amp;gt; #保留 RegEx 中匹配到的时间序列
isNot: &amp;lt;RegEx&amp;gt; #丢弃 RegEx 中匹配到的时间序列
# 关联 metrics 中的 label 与 k8s 资源
resources:
overrides:
# LabelName为填到 metricsQuery 中 PromQL 里的标签名
# ResourceName为 k8s 中的资源，根据对Metrics API 请求的url，找到对应的资源，并将资源具体的对象填入 metricsQuery 中 PromQL 的标签值
LabelName: {[group: GroupName,]resource: &amp;quot;ResourceName&amp;quot;} # 核心组可以省略组名
template: TEMPLATE
# 根据 matches 关键字中的正则对发现中获取的 metrics 重命名(如果不需要重命名，可以省略该配置段)。
name:
matches: &amp;quot;RegEx&amp;quot;
as: &amp;quot;$1&amp;quot; # 可省略，默认值为 $1 。i.e.保留 metches 字段的正则中第一个 () 中的内容。也可以显式得添加 as 字段，来指定重命名结果
# 该字段使用 Go模板，adapter 处理 MetricsAPI 的请求，(处理方式详见上文)
# 处理结果的 MetricsName、Resource、Objects 这三个内容，会填充到模板中，生成 PromQL，然后向 prometheus 发起查询。
metricsQuery: &amp;quot;sum(rate(&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=&amp;quot;POD&amp;quot;}[2m])) by (&amp;lt;.GroupBy&amp;gt;)&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>rules 大致可以分为四个部分，下面对四个部分的关键字进行说明：&lt;/p>
&lt;p>一、Discovery(发现) # 根据 rules 中定义的 Prometheus QL 查找 metircs 。&lt;/p>
&lt;ol>
&lt;li>seriesQuery: &amp;lt;PromQL&amp;gt; # 查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA&lt;/li>
&lt;li>seriesFilters [] # 查询到的指标可能会存在不需要的，可以通过它过滤掉。&lt;/li>
&lt;li>is: &amp;lt;RegEx&amp;gt; #&lt;/li>
&lt;li>isNot: &amp;lt;RegEx&amp;gt; #&lt;/li>
&lt;/ol>
&lt;p>二、Association(关联) # 关联 metrics 的 label 与 k8s resource。adapter 向 prometheus 发起查询时，会将关联规则中指定的名字作为标签名。adapter 接收到 MetricsAPI 请求中的 k8s object 作为标签值，将两者填充到 PromQL 中&lt;/p>
&lt;ol>
&lt;li>resources # 通过 seriesQuery 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询， resources 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 overrides，另一种是 template。&lt;/li>
&lt;li>overrides # 它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 nginx:{group:&amp;ldquo;apps&amp;rdquo;,resource:&amp;ldquo;deployment&amp;rdquo;} 这么写表示的就是将指标中 nginx 这个标签和 apps 这个 api 组中的 deployment 资源关联起来；
&lt;ol>
&lt;li>LabelName: {[group: Group,]resource: &amp;ldquo;RESOURCE&amp;rdquo;}&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>template # 通过 go 模板的形式。比如 template:&amp;ldquo;kube_&amp;laquo;.Group&amp;raquo;_&amp;laquo;.Resource&amp;raquo;&amp;rdquo; 这么写表示，假如 &amp;laquo;.Group&amp;raquo; 为 apps， &amp;laquo;.Resource&amp;raquo; 为 deployment，那么它就是将指标中 kube_apps_deployment 标签和 deployment 资源关联起来。&lt;/li>
&lt;/ol>
&lt;p>三、Naming(命名) #(如果不需要重命名配置可省略)根据表达式匹配规则，把发现配置中查找到的 metrics 重命名。
比如某些以 total 结尾的指标，这些指标拿来做 HPA 是没有意义的，我们需要对这些指标进行速率计算，比如这种语句 sum(rate(http_requests_total{}[2m])) by (pod,namespace) ，在进行计算后，使用 total 来命名没意义了，需要赋予一个新的名字来表示。&lt;/p>
&lt;ol>
&lt;li>name # 用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。&lt;/li>
&lt;li>matches: &amp;lt;RegEx&amp;gt; # 通过正则表达式来匹配指标名，可以进行分组&lt;/li>
&lt;li>as: &amp;lt;STRING&amp;gt; # 默认值为$1。也就是第一个分组。 as 为空就是使用默认值的意思。&lt;/li>
&lt;/ol>
&lt;p>四、Querying(查询) # adapter 在向 prometheus 查询数据时，根据该规则发送 PromQL 。&lt;/p>
&lt;ol>
&lt;li>metricsQuery: &amp;lt;GoTemplate&amp;gt; # 模板中，adapter 处理 MetricsAPI 的请求转换后的三个部分，在模板中变为以下字段。(这就是 Prometheus 的查询语句了，前面的 seriesQuery 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。)
&lt;ol>
&lt;li>&amp;laquo;.Series&amp;raquo; # PromQL 的指标名。根据 Discovery 和 Naming 配置部分结合获取&lt;/li>
&lt;li>&amp;laquo;.LabelMatchers&amp;raquo; # PromQL 的标签集合。根据 Association 配置部分获取。Name 与 Value 的对应关系是根据 Association 配置部分的规则实现的。Name 为 Association 配置段中的 LabelName；Value 为 Resource 的 Object。&lt;/li>
&lt;li>&amp;laquo;.GroupBy&amp;raquo; # 以逗号分隔的标签名的集合，该值是 .LabelMatchers 中的标签名集合&lt;/li>
&lt;li>这三个字段的内容，需要根据 MetircsAPI 收到的请求，以及上面三个配置部分的规则，综合起来获取&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>比如我现在做了如下配置
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rd9psx/1616068726798-f8716c85-b38c-4bc8-acd3-10d934933312.png" alt="">
当对 Metrics API 的请求为 kubectl get &amp;ndash;raw &amp;ldquo;/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/http_request_per_second&amp;rdquo; 时，这个请求会被 adapter 拆分成如下三部分&lt;/p>
&lt;ol>
&lt;li>MetricsName：http_requests_total&lt;/li>
&lt;li>Resource：namesapces,pods&lt;/li>
&lt;li>Object 分两部分
&lt;ol>
&lt;li>namespaces 的 Object：monitoring&lt;/li>
&lt;li>pods 的 Object：grafana 和 prometheus #(Note:这里假定该时间序列的 monitoring 名称空间里两个 pods，prometheus 和 grafana)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>这时，如果想要生成 PromQL 语句向 prometheus 发起查询，需要根据配置文件的 Association 和 Naming 配置段的内容来向 metricsQuery 中的模板填入信息&lt;/p>
&lt;ol>
&lt;li>&amp;laquo;.Series&amp;raquo;：http_requests_total # 因为 MetricsAPI 请求中的指标名(http_request_per_second)是重命名之后的，所以 adapter 会根据这个指标名，找到配置文件中 name 字段中匹配到这个指标名的 rule 配置环境，将其中 seriesQuery 字段中的原始指标名填入模板 .Series 中&lt;/li>
&lt;li>&amp;laquo;.LabelMatchers&amp;raquo;：namespace=&amp;ldquo;monitoring&amp;rdquo;,pod=~&amp;ldquo;grafana|prometheus&amp;rdquo;
&lt;ol>
&lt;li>LabelMatchers 中的内容是根据 Association 配置段的内容取得的，namespace 和 pod 就是的&amp;rsquo;标签名&amp;rsquo;。&lt;/li>
&lt;li>根据绑定规则，‘namesapce 和 pod 标签’分别绑定了‘namespaces 和 pods 资源’，那么根据 MetricsAPI 中的 url，获取到指定资源的 object，会作为对应标签的值。&lt;/li>
&lt;li>namespace 标签对应的 k8s 资源是 namesapces ，MetricsAPI 请求中该资源的 object 是 monitoring ，则 monitoring 就是 LabelValue&lt;/li>
&lt;li>pod 标签对应的 k8s 资源是 pods ，MetricsAPI 请求中该资源的 object 是全部对象，也就是说 grafana 和 prometheus ，则 grafana|prometheus 就是 LabelValue&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&amp;laquo;.GroupBy&amp;raquo;：pod&lt;/li>
&lt;/ol>
&lt;p>那么，该模板会被转换为这样的 PromQL：sum(rate(http_requests_total{namespace=&amp;ldquo;monitoring&amp;rdquo;, pod=~&amp;ldquo;grafana|prometheus&amp;rdquo;}[2m])) by (namesapce,pod)&lt;/p>
&lt;p>Note:如果在关联时，没有关联 namespace ，那么在向 Metrics API 发起上述相同请求时会报错 Error from server (NotFound): the server could not find the metric http_request_per_second for pods。因为没有关联，所以 adapter 也就无法处理 url 中 namesapce 的信息。&lt;/p>
&lt;p>配置文件中 Naming 加 Querying 的组合有点类似于 prometheus 的 Recording Rules ，Naming 是 record 字段，Querying 是 expr 字段。等于是通过一个表达式，生成了一个新的 Metrics 名称&lt;/p>
&lt;h3 id="配置示例">配置示例&lt;/h3>
&lt;pre>&lt;code>rules:
- seriesQuery: 'http_request_total{namespace!=&amp;quot;&amp;quot;,pod!=&amp;quot;&amp;quot;}'
resources:
overrides:
namespace: {resource: &amp;quot;namespace&amp;quot;}
pod: {resource: &amp;quot;pod&amp;quot;}
name:
matches: &amp;quot;^(.*)_total&amp;quot;
as: &amp;quot;${1}_per_second&amp;quot;
metricsQuery: 'sum(rate(http_request_total{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[2m])) by (&amp;lt;.GroupBy&amp;gt;)'
&lt;/code>&lt;/pre>
&lt;p>通过该示例配置， adapter 会向 Metrics API 注册如下两个 metrics，namespaces/http_request_per_second 和 pods/http_request_per_second&lt;/p>
&lt;pre>&lt;code>[root@master-1 custom]# kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/&amp;quot; | jq .
{
&amp;quot;kind&amp;quot;: &amp;quot;APIResourceList&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
&amp;quot;groupVersion&amp;quot;: &amp;quot;custom.metrics.k8s.io/v1beta1&amp;quot;,
&amp;quot;resources&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;namespaces/http_request_per_second&amp;quot;,
&amp;quot;singularName&amp;quot;: &amp;quot;&amp;quot;,
# 此字段指明该 metric 是否需要在发起url请求时指定具体的namespace
# 因为这里是namespaces资源，所以可能会有误解，如果此处node(i.e.&amp;quot;name&amp;quot;: &amp;quot;nodes/http_request_per_second&amp;quot;)资源，则很好理解了，node是不需要namesacpe的
&amp;quot;namespaced&amp;quot;: false,
&amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
&amp;quot;verbs&amp;quot;: [
&amp;quot;get&amp;quot;
]
},
{
&amp;quot;name&amp;quot;: &amp;quot;pods/http_request_per_second&amp;quot;,
&amp;quot;singularName&amp;quot;: &amp;quot;&amp;quot;,
&amp;quot;namespaced&amp;quot;: true,
&amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
&amp;quot;verbs&amp;quot;: [
&amp;quot;get&amp;quot;
]
}
]
}
&lt;/code>&lt;/pre>
&lt;p>通过 MetricsAPI 获取指标 http_request_per_second 值时，会返回如下内容&lt;/p>
&lt;pre>&lt;code>[root@master-1 custom]# kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/http_request_per_second | jq .
{
&amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;custom.metrics.k8s.io/v1beta1&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;selfLink&amp;quot;: &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/%2A/http_request_per_second&amp;quot;
},
&amp;quot;items&amp;quot;: [
{
&amp;quot;describedObject&amp;quot;: {
&amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
&amp;quot;namespace&amp;quot;: &amp;quot;monitoring&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;grafana-5c55845445-q2p9l&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;/v1&amp;quot;
},
&amp;quot;metricName&amp;quot;: &amp;quot;http_request_per_second&amp;quot;,
&amp;quot;timestamp&amp;quot;: &amp;quot;2020-06-18T07:47:26Z&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;9m&amp;quot;,
&amp;quot;selector&amp;quot;: null
}
]
}
&lt;/code>&lt;/pre>
&lt;h2 id="resourcerules-配置环境">resourceRules 配置环境&lt;/h2>
&lt;p>示例如下：&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
data:
config.yaml: |-
resourceRules:
cpu:
# 该 PromQL 用于 kubectl top pod 命令
containerQuery: sum(rate(container_cpu_usage_seconds_total{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;.GroupBy&amp;gt;)
# 该 PromQL 用于 kubectl top node 命令
nodeQuery: sum(rate(container_cpu_usage_seconds_total{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;, id='/'}[1m])) by (&amp;lt;.GroupBy&amp;gt;)
resources:
overrides:
node:
resource: node
namespace:
resource: namespace
pod:
resource: pod
containerLabel: container
memory:
containerQuery: sum(container_memory_working_set_bytes{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;.GroupBy&amp;gt;)
nodeQuery: sum(container_memory_working_set_bytes{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,id='/'}) by (&amp;lt;.GroupBy&amp;gt;)
resources:
overrides:
node:
resource: node
namespace:
resource: namespace
pod:
resource: pod
containerLabel: container
window: 1m
kind: ConfigMap
metadata:
name: adapter-config
namespace: monitoring
&lt;/code>&lt;/pre>
&lt;h1 id="问题示例">问题示例&lt;/h1>
&lt;p>如果部署完成后，无法通过 Metrics API 获取指标，则可能的原因有以下几点&lt;/p>
&lt;ol>
&lt;li>api 无法关联到后端 adapter&lt;/li>
&lt;li>adapter 的 config 文件配置有问题&lt;/li>
&lt;/ol></description></item></channel></rss>