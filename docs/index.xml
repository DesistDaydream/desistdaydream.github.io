<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 文档</title><link>https://desistdaydream.github.io/docs/</link><description>Recent content in 文档 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs:</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/hugo/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/hugo/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gohugo.io/commands/hugo/">官方文档，命令-hugo&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在网站的根目录下使用 &lt;code>hugo&lt;/code> 命令，可以为网站构建静态文件，并保存到 &lt;code>publish/&lt;/code> 目录中。&lt;/p>
&lt;h2 id="sytnax语法">Sytnax(语法)&lt;/h2>
&lt;p>&lt;strong>hugo [COMMAND] [FLAGS]&lt;/strong>&lt;/p>
&lt;p>FLAGS&lt;/p>
&lt;ul>
&lt;li>&lt;strong>-D, &amp;ndash;buildDrafts&lt;/strong> # 包含标记为 draft 的内容&lt;/li>
&lt;/ul>
&lt;h1 id="hugo-server">hugo server&lt;/h1>
&lt;h2 id="syntax语法">Syntax(语法)&lt;/h2>
&lt;p>&lt;strong>hugo server [FLAGS]&lt;/strong>&lt;/p>
&lt;p>FLAGS&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&amp;ndash;bind STRING&lt;/strong> # 监听地址。&lt;code>默认值: 127.0.0.1&lt;/code>&lt;/li>
&lt;li>&lt;strong>-p, &amp;ndash;port INT&lt;/strong> # 监听端口。&lt;code>默认值: 1313&lt;/code>&lt;/li>
&lt;li>&lt;strong>-w, &amp;ndash;watch&lt;/strong> # 监听文件的改变，文件改变时重新应用，以便结果可以实时显示。&lt;code>默认值: true&lt;/code>&lt;/li>
&lt;li>&lt;strong>&amp;ndash;cacheDir STRING&lt;/strong> # filesystem path to cache directory。&lt;code>默认值: $TMPDIR/hugo_cache/&lt;/code>&lt;/li>
&lt;/ul></description></item><item><title>Docs: (弃用)Graph 类型面板详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/grafana/panel%E9%9D%A2%E6%9D%BF-%E4%B8%8E-dashboard%E4%BB%AA%E8%A1%A8%E7%9B%98/time-series-%E7%B1%BB%E5%9E%8B%E9%9D%A2%E6%9D%BF/%E5%BC%83%E7%94%A8graph-%E7%B1%BB%E5%9E%8B%E9%9D%A2%E6%9D%BF%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/grafana/panel%E9%9D%A2%E6%9D%BF-%E4%B8%8E-dashboard%E4%BB%AA%E8%A1%A8%E7%9B%98/time-series-%E7%B1%BB%E5%9E%8B%E9%9D%A2%E6%9D%BF/%E5%BC%83%E7%94%A8graph-%E7%B1%BB%E5%9E%8B%E9%9D%A2%E6%9D%BF%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;/blockquote>
&lt;p>这是一个初始的 Graph 面板，有两条查询语句，更改了序列的名称。&lt;/p>
&lt;pre>&lt;code>sum(node_memory_MemTotal_bytes)
(sum(node_memory_MemTotal_bytes{} - node_memory_MemAvailable_bytes{}) / sum(node_memory_MemTotal_bytes{}))*100
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957167-730a2679-0ad0-488a-9c4c-8f3ba5ace79d.png" alt="">&lt;/p>
&lt;p>Graph 是一个二维的，具有 x/y Axes(轴) 的面板。x 轴(横轴) 以时间分布、y 轴(纵轴) 以样本值分布&lt;/p>
&lt;p>下面的文章将只介绍 Graph 面板的独有配置，有很多共有配置详见&lt;a href="https://www.yuque.com/go/doc/33145831">此处&lt;/a>&lt;/p>
&lt;h1 id="panel--面板配置">Panel # 面板配置&lt;/h1>
&lt;h2 id="display--显示设置值的显示样式柱状线条圆点三种">Display # 显示。设置值的显示样式(柱状、线条、圆点三种)&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957211-044eecd5-5b98-425a-8de8-3799545d50f6.png" alt="">&lt;/p>
&lt;p>在 Display 标签里，有 5 个开关，Bars、Lines、Staircase、Points 用于指定值的显示样式。Alert thresholds 用于指定是否显示告警阈值。&lt;/p>
&lt;blockquote>
&lt;p>其他的配置选项，都是在开启某个样式后，才会显示对应样式专用的选项。
Min step 设置时间长一点，Bars 与 Points 样式才可以看出来效果。否则都挤到一坨去了~&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Bars&lt;/strong> # 柱状图样式。当 X 轴的模式变为 Series、Historgram 时，自动开启
&lt;strong>Lines&lt;/strong> # 线条样式。默认样式&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Staircase&lt;/strong> # 开启线条样式时，才可以使用这种样式。&lt;/li>
&lt;li>&lt;strong>Line width&lt;/strong> # 线条宽度。&lt;/li>
&lt;li>&lt;strong>Area fill&lt;/strong> # 线条到底部这一区域的填充度。默认 1。&lt;/li>
&lt;li>&lt;strong>Fill gradient&lt;/strong> # 填充渐变。默认 0。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Points&lt;/strong> # 圆点样式。
&lt;strong>Point radius&lt;/strong> # 每个圆点的半径
&lt;strong>Alert thresholds&lt;/strong> # 在面板上显示报警阈值和区域&lt;/p>
&lt;p>除了设置面板中值的显示样式，还可以设置一些其他的设置
&lt;strong>Stacking and null value(叠加与空值)&lt;/strong>
用于在面板上叠加所有 series 的值&lt;/p>
&lt;p>**Hover tooltip(悬停提示) # **开启后，鼠标悬停在面板上，会出现一些关于 series 的信息
Mode # 模式。&lt;/p>
&lt;ul>
&lt;li>All series # 鼠标悬停到面板时，显示所有 series 的信息&lt;/li>
&lt;li>Single # 鼠标选定到面板时，只显示鼠标所在的 series 的信息。&lt;/li>
&lt;/ul>
&lt;p>Sort order # 排序。有三种排序方式：None(不排序)、Increasing(由上到下逐渐增大)、Decreasing(由上到下逐渐减小)&lt;/p>
&lt;h2 id="series-overrides--序列替换用于个性化每个序列的配置">Series overrides # 序列替换。用于个性化每个序列的配置&lt;/h2>
&lt;p>顾名思义，就是用来替换序列样式的。当一个面板上，配置了多个查询语句，这时就会产生多条 Series。而右侧的配置，是统一的，所有 Series 的配置内容都一样，这样不利于数据展示。所以通过 Series overrides 可以个性化得配置每一条 Series，让不同的 Series 展示出不同的效果(比如多条 Series 可以具有不同的单位、不同的线条宽度不同、不同的显示方式)&lt;/p>
&lt;p>点击 &lt;code>+ Add series override&lt;/code> 即可为指定的 series 进行配置&lt;/p>
&lt;p>在 &lt;code>Alias or regex&lt;/code> 选择要配置的序列。这里也可以使用正则表达式进行多个 series 的匹配。然后点击 &lt;code>+&lt;/code> 符号，即可为选定的 series 进行单独的配置。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957244-03bc347c-faa5-4145-8a6b-fe3138242f0b.png" alt="">&lt;/p>
&lt;h3 id="应用实例">应用实例&lt;/h3>
&lt;p>上面的例子，一个语句是内存用量，一个语句是内存使用率，单位是不一样(一个 KiB、一个百分比)。这时候，就需要使用 Series overrides，为每个 Serie 单独配置。不但单位可以分别配置，还可以将 Serie 移动到右侧的 Y 轴。还可以为不同的 series 配置不用的显示方式(比如有的用圆点、有点用线条、有的用柱状图，都可以在同一个面板显示出来)&lt;/p>
&lt;p>比如我现在为 总平均使用率 序列进行单独配置，面板就会变成下面这种效果：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957204-1cae00cd-e82c-4042-9d6b-11daed4f9b8f.png" alt="">&lt;/p>
&lt;h2 id="axis--设定坐标轴的显示内容">Axis # 设定坐标轴的显示内容&lt;/h2>
&lt;p>在这里可以更改 x 轴 和 y 轴 的显示内容。常用于配置 metrics 值的 unit(单位)、Decimals(小数位数)。&lt;/p>
&lt;p>&lt;strong>Left Y/Right Y&lt;/strong> # 更改 Y-axes(Y 轴) 的信息&lt;/p>
&lt;ul>
&lt;li>Show # 是否显示这个轴&lt;/li>
&lt;li>Unit # 配置 Y 轴 的单位&lt;/li>
&lt;li>Decimals # 配置 Y 轴显示的小数位数。&lt;/li>
&lt;li>Label # 配置 Y 轴 的标签(标签会显示在 Y 轴 的旁边)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Y-Axes&lt;/strong> # Y 轴 配置。一些对齐方式
&lt;strong>X-Axis&lt;/strong> # X 轴 配置。可以更改 X 轴的 Mode(模式)&lt;/p>
&lt;ul>
&lt;li>Mode # 模式。用于改变 X 轴的 显示模式。
&lt;ul>
&lt;li>Time # 时间模式。默认模式。X 轴 表示 时间，数据按时间分组（例如，按小时或分钟）。&lt;/li>
&lt;li>Series # 序列模式。X 轴 表示 series，数据按照序列分组。Y 轴 仍然代表该序列的值
&lt;ul>
&lt;li>注意：当 X 轴 切换到 Series 模式时，Display 配置中的将自动使用柱状图的方式&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Histogram # 直方图模式。X 轴 表示 序列的值，Y 轴 表示 该值的计数。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="x-轴的-series-模式-示例">X 轴的 Series 模式 示例&lt;/h3>
&lt;p>当 X 轴 变为 series 模式 时，由于没有时间这种维度，所以一般都使用 当前值。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957169-8ff35969-aa1d-4259-8144-1a88bb33a486.png" alt="">&lt;/p>
&lt;h2 id="legend--用于配置面板内的-legendit-学习笔记6可观测性grafanapanel面板20-与20dashboard仪表盘panel面板20-配置详解md-配置详解md">Legend # 用于配置面板内的 [Legend](✏IT 学习笔记/👀6.可观测性/Grafana/Panel(面板)%20 与%20Dashboard(仪表盘)/Panel(面板)%20 配置详解.md 配置详解.md)&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957204-998bd4db-f185-4f42-9138-9edb69081d84.png" alt="">&lt;/p>
&lt;h2 id="time-regions--时间区域graph-类型面板不常用">Time regions # 时间区域。Graph 类型面板不常用&lt;/h2>
&lt;h1 id="field--字段配置">Field # 字段配置&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qzbd5e/1616067957229-733ae428-0442-42e4-b76e-d36f89196a3d.png" alt="">&lt;/p>
&lt;h1 id="overrides--字段替换配置graph-类型面板不常用">Overrides # 字段替换配置，Graph 类型面板不常用&lt;/h1>
&lt;p>详见：[Overrides](✏IT 学习笔记/👀6.可观测性/Grafana/Panel(面板)%20 与%20Dashboard(仪表盘)/Panel(面板)%20 配置详解.md 配置详解.md)&lt;/p></description></item><item><title>Docs: 「你天天关注这些新闻有什么用？」</title><link>https://desistdaydream.github.io/docs/%E7%A5%9E%E5%A5%87/%E8%AF%AD%E8%A8%80/%E4%BD%A0%E5%A4%A9%E5%A4%A9%E5%85%B3%E6%B3%A8%E8%BF%99%E4%BA%9B%E6%96%B0%E9%97%BB%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/%E7%A5%9E%E5%A5%87/%E8%AF%AD%E8%A8%80/%E4%BD%A0%E5%A4%A9%E5%A4%A9%E5%85%B3%E6%B3%A8%E8%BF%99%E4%BA%9B%E6%96%B0%E9%97%BB%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/</guid><description>
&lt;p>原文：&lt;a href="https://mp.weixin.qq.com/s/Lnw3wdJ0CQ88QV8bHsvWzw">公众号-唐一水&lt;/a>
这是我前段时间，后台收到的一句质问。
具体信息我已翻找不到，只记得发这句质问的，是个从头像到昵称，都在彰显岁月静好正能量的中年网友。
我看到这个质问的那一刻，内心那个埋藏许久的声音也瞬间响起：是啊，我关注那么多新闻，我和读者们愤怒、发声、传播了那么多次，其实，又有什么用？
惨剧依旧在重复，不公依旧在上演，疫情、战争、自然灾害、性别暴力，无数不可抗拒的力量依旧将我们碾压，我们关注的唯一用处，似乎就是让自己和社会，更加陷入到「政治性抑郁」。
我们其实完全有另外一个选择——
不听不看，不烦不忧。
我也可以岁月静好，拍拍抖音卡点，发发情感文案，去做一个更稳当美好的自媒体人。各位也可以积极向上，看看正能量，转转暖心事，去做一个每天等待「反转」的理性网友。
但为什么不愿意呢？为什么做不到呢？
明明「政治性抑郁」已令你痛苦疲惫，需要通过停止阅读社交媒体来自我修复，可为什么第二天当再看到那些不公和惨剧，你依然会关注、会愤怒、会发声、会传播？
这个世界少一个「政治性抑郁」的人，多一个「政治性冷漠」的人，又能怎么样，可为什么，你就是不愿意快乐地冷漠着？
也许是因为我们明白，此刻的冷漠，只会换来未来更大的抑郁及荒谬，而那些使我们抑郁的存在，最乐于看到我们的冷漠，好成全一切使其获利的荒谬。
也许是因为我们明白，世界的本质就是荒谬——正义能被轻易打破、善恶并不遵循因果、法律和道德随时准备双标，人只能以「作为」抵抗荒谬。本身就因「无作为」而暴露的荒谬，如果我们仍以「无作为」对待，世界所剩下的，也就只有荒谬叠加荒谬。
当我们向「政治性抑郁」投降时，我们也就只配输给荒谬。
也许是因为我们明白，「政治性抑郁」本质就是一种妄自菲薄，我们以为面对坏消息，自己只能「无作为」。
可是「抑郁」本身就是一种「作为」。情绪，就是改变的开始。
社会历史的改变、进步、对灾难的预防，恰恰发源于一代又一代人的政治性抑郁，先哲前辈们自封建时代一路演进，正是依托一次次情绪的浪潮，而此刻的抑郁，不过是时代责任落于双肩，必然产生的痛楚。
也许是因为我们明白，正如徐贲所说，知识分子没有沉默的权利。当我们的学识，能够让我们理解什么是「政治性冷漠」时，我们就已没有资格去选择「政治性冷漠」。
当下观点市场中，大谈所谓消除或缓解「政治性抑郁」，太容易落入犬儒主义的圈套。当避免共情和社会责任变成一种理所当然的选择，冷漠必将成为主流。
你所受的教育和所学的知识，既是你发声的能力所在，更是你发声的义务所在。如果连能理解这一切荒谬的你都不再发声，你还能指望谁来替你发声，你又凭什么指望谁来替你发声。
这个社会最不缺「政治性冷漠」的人，不关心公共议题，更无视道德自律，他们蒙住双眼，他们面朝大海，甚至会嫌弃发声者聒噪：
「你天天关注这些新闻有什么用？」
「我也不知道。我只是不想成为你们。」&lt;/p></description></item><item><title>Docs: 【BPF网络篇系列-2】容器网络延时之 ipvs 定时器篇 | 深入浅出 eBPF</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/bpf/bpf-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/bpf%E7%BD%91%E7%BB%9C%E7%AF%87%E7%B3%BB%E5%88%97-2%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E4%B9%8B-ipvs-%E5%AE%9A%E6%97%B6%E5%99%A8%E7%AF%87-_-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-ebpf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/bpf/bpf-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/bpf%E7%BD%91%E7%BB%9C%E7%AF%87%E7%B3%BB%E5%88%97-2%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E4%B9%8B-ipvs-%E5%AE%9A%E6%97%B6%E5%99%A8%E7%AF%87-_-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-ebpf/</guid><description>
&lt;h2 id="1-前言">1. 前言&lt;/h2>
&lt;p>趣头条的容器化已经开展了一年有余，累计完成了近 1000 个服务的容器化工作，微服务集群的规模也达到了千台以上的规模。随着容器化服务数量和集群规模的不断增大，除了常规的 API Server 参数优化、Scheduler 优化等常规优化外，近期我们还碰到了 kubernetes 底层负载均衡 ipvs 模块导致的网络抖动问题，在此把整个问题的分析、排查和解决的思路进行总结，希望能为有类似问题场景解决提供一种思路。&lt;/p>
&lt;p>涉及到的 k8s 集群和机器操作系统版本如下：&lt;/p>
&lt;ul>
&lt;li>k8s 阿里云 ACK 14.8 版本，网络模型为 CNI 插件 &lt;a href="https://github.com/AliyunContainerService/terway">terway&lt;/a> 中的 terway-eniip 模式；&lt;/li>
&lt;li>操作系统为 CentOS 7.7.1908，内核版本为 3.10.0-1062.9.1.el7.x86_64；&lt;/li>
&lt;/ul>
&lt;h2 id="2-网络抖动问题">2. 网络抖动问题&lt;/h2>
&lt;p>在容器集群中新部署的服务 A，在测试初期发现通过服务注册发现访问下游服务 B（在同一个容器集群） 调用延时 999 线偶发抖动，测试 QPS 比较小，从业务监控上看起来比较明显，最大的延时可以达到 200 ms。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/service_latency_high.png" alt="">&lt;/p>
&lt;p>图 2-1 服务调用延时&lt;/p>
&lt;p>服务间的访问通过 gRPC 接口访问，节点发现基于 consul 的服务注册发现。通过在服务 A 容器内的抓包分析和排查，经过了以下分析和排查：&lt;/p>
&lt;ul>
&lt;li>服务 B 部分异常注册节点，排除异常节点后抖动情况依然存在；&lt;/li>
&lt;li>HTTP 接口延时测试， 抖动情况没有改善；&lt;/li>
&lt;li>服务 A 在 VM（ECS）上部署测试，抖动情况没有改善；&lt;/li>
&lt;/ul>
&lt;p>经过上述的对比测试，我们逐步把范围缩小至服务 B 所在的主机上的底层网络抖动。&lt;/p>
&lt;p>经过多次 ping 包测试，我们寻找到了某台主机 A 与 主机 B 两者之间的 ping 延时抖动与服务调用延时抖动规律比较一致，由于 ping 包 的分析比 gRPC 的分析更加简单直接，因此我们将目标转移至底层网络的 ping 包测试的轨道上。&lt;/p>
&lt;p>能够稳定复现的主机环境如下图，通过主机 A ping 主机 B 中的容器实例 172.23.14.144 实例存在 ping 延时抖动。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 主机 B 中的 Pod IP 地址&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ip route|grep 172.23.14.144&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>172.23.14.144 dev cali95f3fd83a87 scope link
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>![ping_host_container] (imgs/ping_host_container.png)&lt;/p>
&lt;p>图 2-2 ping 测试涉及到的主机和容器拓扑图&lt;/p>
&lt;p>基于主机 B 网络 eth1 和容器网络 cali-xxx 进行 ping 的对比结果如图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/ping_host_container_detail.png" alt="">&lt;/p>
&lt;p>图 2-3 ping 主机与容器网络详情&lt;/p>
&lt;p>通过多次测试我们发现至 Node 主机 B 主机网络的 ping 未有抖动，容器网络 cali-xx 存在比较大的抖动，最高达到 133 ms。&lt;/p>
&lt;p>在 ping 测试过程中分别在主机 A 和主机 B 上使用 tcpdump 抓包分析，发现在主机 B 上的 eth1 与网卡 cali95f3fd83a87 之间的延时达 133 ms。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/ping_server_pcap.png" alt="">&lt;/p>
&lt;p>图 2-4 主机 B 上的 ping 包延时&lt;/p>
&lt;p>到此为止问题已经逐步明确，在主机 B 上接收到 ping 包在转发过程中有 100 多 ms 的延时，那么是什么原因导致的 ping 数据包在主机 B 转发的延时呢？&lt;/p>
&lt;h2 id="3-问题分析">3. 问题分析&lt;/h2>
&lt;p>在分析 ping 数据包转发延时的情况之前，我们首先简单回顾一下网络数据包在内核中工作机制和数据流转路径。&lt;/p>
&lt;h3 id="31-网络数据包内核中的处理流程">3.1 网络数据包内核中的处理流程&lt;/h3>
&lt;p>在内核中，网络设备驱动是通过中断的方式来接受和处理数据包。当网卡设备上有数据到达的时候，会触发一个硬件中断来通知 CPU 来处理数据，此类处理中断的程序一般称作 ISR (Interrupt Service Routines)。ISR 程序不宜处理过多逻辑，否则会它设备的中断处理无法及时响应。因此 Linux 中将中断处理函数分为上半部和下半部。上半部是只进行最简单的工作，快速处理然后释放 CPU。剩下将绝大部分的工作都放到下半部中，下半部中逻辑有内核线程选择合适时机进行处理。&lt;/p>
&lt;p>Linux 2.4 以后内核版本采用的下半部实现方式是软中断，由 ksoftirqd 内核线程全权处理， 正常情况下每个 CPU 核上都有自己的软中断处理数队列和 &lt;code>ksoftirqd&lt;/code> 内核线程。软中断实现只是通过给内存中设置一个对应的二进制值来标识，软中断处理的时机主要为以下 2 种：&lt;/p>
&lt;ul>
&lt;li>硬件中断 &lt;code>irq_exit&lt;/code>退出时；&lt;/li>
&lt;li>被唤醒 &lt;code>ksoftirqd&lt;/code> 内核线程进行处理软中断；&lt;/li>
&lt;/ul>
&lt;p>常见的软中断类型如下：&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
5
6
7
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">enum&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> HI_SOFTIRQ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> TIMER_SOFTIRQ,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> NET_TX_SOFTIRQ, &lt;span style="color:#75715e">// 网络数据包发送软中断
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> NET_RX_SOFTIRQ, &lt;span style="color:#75715e">// 网络数据包接受软中断
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">//...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-1 Linux 软中断类型&lt;/p>
&lt;p>优先级自上而下，HI_SOFTIRQ 的优先级最高。其中 &lt;code>NET_TX_SOFTIRQ&lt;/code> 对应于网络数据包的发送， &lt;code>NET_RX_SOFTIRQ&lt;/code> 对应于网络数据包接受，两者共同完成网络数据包的发送和接收。&lt;/p>
&lt;p>网络相关的中断程序在网络子系统初始化的时候进行注册， &lt;code>NET_RX_SOFTIRQ&lt;/code> 的对应函数为 &lt;code>net_rx_action()&lt;/code> ，在 &lt;code>net_rx_action()&lt;/code> 函数中会调用网卡设备设置的 &lt;code>poll&lt;/code> 函数，批量收取网络数据包并调用上层注册的协议函数进行处理，如果是为 ip 协议，则会调用 &lt;code>ip_rcv&lt;/code>，上层协议为 icmp 的话，继续调用 &lt;code>icmp_rcv&lt;/code> 函数进行后续的处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/netcard_dev_softirq.png" alt="">&lt;/p>
&lt;p>图 3-1 网卡设备数据包接收示意图&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code> 1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//net/core/dev.c
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> __init &lt;span style="color:#a6e22e">net_dev_init&lt;/span>(&lt;span style="color:#66d9ef">void&lt;/span>){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">for_each_possible_cpu&lt;/span>(i) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> softnet_data &lt;span style="color:#f92672">*&lt;/span>sd &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">per_cpu&lt;/span>(softnet_data, i);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">memset&lt;/span>(sd, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#66d9ef">sizeof&lt;/span>(&lt;span style="color:#f92672">*&lt;/span>sd));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">skb_queue_head_init&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>sd&lt;span style="color:#f92672">-&amp;gt;&lt;/span>input_pkt_queue);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">skb_queue_head_init&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>sd&lt;span style="color:#f92672">-&amp;gt;&lt;/span>process_queue);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sd&lt;span style="color:#f92672">-&amp;gt;&lt;/span>completion_queue &lt;span style="color:#f92672">=&lt;/span> NULL;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">INIT_LIST_HEAD&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>sd&lt;span style="color:#f92672">-&amp;gt;&lt;/span>poll_list); &lt;span style="color:#75715e">// 软中断的处理中的 poll 函数列表
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// ......
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ......
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">open_softirq&lt;/span>(NET_TX_SOFTIRQ, net_tx_action); &lt;span style="color:#75715e">// 注册网络数据包发送的软中断
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">open_softirq&lt;/span>(NET_RX_SOFTIRQ, net_rx_action); &lt;span style="color:#75715e">// 注册网络数据包接受的软中断
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">subsys_initcall&lt;/span>(net_dev_init);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-2 软中断数据及网络软中断注册&lt;/p>
&lt;p>网络数据的收发的延时，多数场景下都会和系统软中断处理相关，这里我们将重点分析 ping 包抖动时的软中断情况。这里我们采用基于 &lt;a href="https://github.com/iovisor/bcc">BCC&lt;/a> 的 &lt;a href="https://gist.github.com/DavadDi/62ee75228f03631c845c51af292c2b17">&lt;strong>traceicmpsoftirq.py&lt;/strong>&lt;/a> 来协助定位 ping 包处理的内核情况。&lt;/p>
&lt;blockquote>
&lt;p>BCC 为 Linux 内核 BPF 技术的前端程序，主要提供 Python 语言的绑定，&lt;code>traceicmpsoftirq.py&lt;/code> 脚本依赖于 BCC 库，需要先安装 BCC 项目，各操作系统安装参见 &lt;a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">INSTALL.md&lt;/a>。
&lt;code>traceicmpsoftirq.py&lt;/code> 脚本在 Linux 3.10 内核与 Linux 4.x 内核上的读写方式有差异，需要根据内核略有调整。&lt;/p>
&lt;/blockquote>
&lt;p>使用 &lt;code>traceicmpsoftirq.py&lt;/code> 在主机 B 上运行，我们发现出现抖动延时的时内核运行的内核线程都为 &lt;code>ksoftirqd/0&lt;/code>。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code> 1
2
3
4
5
6
7
8
9
10
11
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#主机 主机 A#A&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ping -c 150 -i 0.01 172.23.14.144 |grep -E &amp;#34;[0-9]{2,}[\.0-9]+ ms&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 主机 B&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ./traceicmpsoftirq.py&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tgid pid comm icmp_seq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> swapper/0 &lt;span style="color:#ae81ff">128&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">6&lt;/span> &lt;span style="color:#ae81ff">6&lt;/span> ksoftirqd/0 &lt;span style="color:#ae81ff">129&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">6&lt;/span> &lt;span style="color:#ae81ff">6&lt;/span> ksoftirqd/0 &lt;span style="color:#ae81ff">130&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-3 &lt;code>traceicmpsoftirq.py&lt;/code> ping 主机 B 容器 IP 抖动时的详情&lt;/p>
&lt;p>&lt;code>[ksoftirqd/0]&lt;/code> 这个给了我们两个重要的信息：&lt;/p>
&lt;ul>
&lt;li>从主机 A ping 主机 B 中容器 IP 的地址，每次处理包的处理都会固定落到 CPU#0 上；&lt;/li>
&lt;li>出现延时的时候该 CPU#0 都在运行软中断处理内核线程 &lt;code>ksoftirqd/0&lt;/code>，即在处理软中断的过程中调用的数据包处理，软中断另外一种处理时机如上所述 &lt;code>irq_exit&lt;/code> 硬中断退出时；&lt;/li>
&lt;/ul>
&lt;p>如果 ping 主机 B 中的容器 IP 地址落在 CPU#0 核上，那么按照我们的测试过程， ping 主机 B 的宿主机 IP 地址没有抖动，那么处理的 CPU 一定不在 #0 号上，才能符合测试场景，我们继续使用主机 B 主机 IP 地址进行测试：&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code> 1
2
3
4
5
6
7
8
9
10
11
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 主机 A&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ping -c 150 -i 0.01 172.23.14.144 |grep -E &amp;#34;[0-9]{2,}[\.0-9]+ ms&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 主机 B&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ./traceicmpsoftirq.py&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tgid pid comm icmp_seq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> swapper/19 &lt;span style="color:#ae81ff">55&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> swapper/19 &lt;span style="color:#ae81ff">56&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> swapper/19 &lt;span style="color:#ae81ff">57&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-4 &lt;code>traceicmpsoftirq.py&lt;/code> ping 主机 B 主机 IP 详情&lt;/p>
&lt;p>通过实际的测试验证，ping 主机 B 宿主机 IP 地址时候，全部都落在了 CPU#19 上。问题排查至此处，我们可以断定是 CPU#0 与 CPU#19 在软中断处理的负载上存在差异，但是此处我们有带来另外一个疑问，为什么我们的 ping 包的处理总是固定落到同一个 CPU 核上呢？ 通过查询资料和主机配置确认，主机上默认启用了 RPS 的技术。RPS 全称是 Receive Packet Steering，这是 Google 工程师 Tom Herbert 提交的内核补丁, 在 2.6.35 进入 Linux 内核，采用软件模拟的方式，实现了多队列网卡所提供的功能，分散了在多 CPU 系统上数据接收时的负载，把软中断分到各个 CPU 处理，而不需要硬件支持，大大提高了网络性能。简单点讲，就是在软中断的处理函数 &lt;code>net_rx_action()&lt;/code> 中依据 RPS 的配置，使用接收到的数据包头部（比如源 IP 地址端口等信息）信息进行作为 key 进行 Hash 到对应的 CPU 核上去处理，算法具体参见 &lt;a href="https://elixir.bootlin.com/linux/v5.8/source/net/core/dev.c#L4305">get_rps_cpu&lt;/a> 函数。&lt;/p>
&lt;p>Linux 环境下的 RPS 配置，可以通过下面的命令检查：&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># cat /sys/class/net/*/queues/rx-*/rps_cpus&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>通过对上述情况的综合分析，我们把问题定位在 CPU#0 在内核线程中对于软中断处理的问题上。&lt;/p>
&lt;h3 id="32-cpu-软中断处理排查">3.2 CPU 软中断处理排查&lt;/h3>
&lt;p>问题排查到这里，我们将重点开始排查 CPU#0 上的 CPU 内核态的性能指标，看看是否有运行的函数导致了软中断处理的延期。&lt;/p>
&lt;p>首先我们使用 &lt;code>perf&lt;/code> 命令对于 CPU#0 进行内核态使用情况进行分析。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/perf_kernel_cpu0.png" alt="">&lt;/p>
&lt;p>图 3-2 perf top CPU#0 内核性能数据&lt;/p>
&lt;p>通过 &lt;code>perf top&lt;/code> 命令我们注意到 CPU#0 的内核态中，&lt;code>estimation_timer&lt;/code> 这个函数的使用率一直占用比较高，同样我们通过对于 CPU#0 上的火焰图分析，也基本与 &lt;code>perf top&lt;/code> 的结果一致。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/estimation_timer_flamgraph.png" alt="">&lt;/p>
&lt;p>图 3-3 &lt;code>estimation_timer&lt;/code> 在内核 CPU#0 上的火焰图&lt;/p>
&lt;p>为了弄清楚 &lt;code>estimation_timer&lt;/code> 的内核占用情况，我们继续使用 开源项目 &lt;a href="https://github.com/brendangregg/perf-tools">perf-tools&lt;/a>（作者为 Brendan Gregg）中的 &lt;a href="https://github.com/brendangregg/perf-tools/blob/master/bin/funcgraph">funcgraph&lt;/a> 工具分析函数 &lt;code>estimation_timer&lt;/code> 在内核中的调用关系图和占用延时。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -m 1最大堆栈为 1 层，-a 显示全部信息 -d 6 跟踪 6秒&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#./funcgraph -m 1 -a -d 6 estimation_timer&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/estimation_timer_funcgraph.png" alt="">&lt;/p>
&lt;p>图 3-4 &lt;code>estimation_timer&lt;/code> 函数在内核函数调用&lt;/p>
&lt;p>同时我们注意到 &lt;code>estimation_timer&lt;/code> 函数在 CPU#0 内核中的遍历一次遍历时间为 119 ms，在内核处理软中断的情况中占用过长的时间，这一定会影响到其他软中断的处理。&lt;/p>
&lt;p>为了进一步确认 CPU#0 上的软中断处理情况，我们基于 BCC 项目中的 &lt;a href="https://github.com/iovisor/bcc/blob/master/tools/softirqs.py">softirqs.py&lt;/a> 脚本（本地略有修改），观察 CPU#0 上的软中断数量变化和整体耗时分布，发现 CPU#0 上的软中断数量增长并不是太快，但是 timer 的直方图却又异常点数据， 通过 timer 在持续 10s 内的 timer 数据分析，我们发现执行的时长分布在 [65 - 130] ms 区间的记录有 5 条。这个结论完全与通过 &lt;code>funcgraph&lt;/code> 工具抓取到的 &lt;code>estimation_timer&lt;/code> 在 CPU#0 上的延时一致。。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># -d 采用直方图 10 表示 10s 做一次聚合， 1 显示一次 -C 0 为我们自己修改的功能，用于过滤 CPU#0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /usr/share/bcc/tools/softirqs -d 10 1 -C 0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/a705eb8d-74c1-4b40-bb71-60999d58cfc8/timer_softirq_hist.png" alt="">&lt;/p>
&lt;p>图 3-5 CPU#0 软中断之 timer 的执行时长直方图&lt;/p>
&lt;p>通过上述分析我们得知 &lt;code>estimation_timer&lt;/code> 来自于 ipvs 模块（参见图 3-4），kubernets 中 kube-proxy 组件负载均衡器正是基于 ipvs 模块，那么问题基本上出现在 kube-proxy 进程上。&lt;/p>
&lt;p>我们在主机 B 上仅保留测试的容器实例，在停止 kubelet 服务后，手工停止 kube-proxy 容器进程，经过重新测试，ping 延时抖动的问题果然消失了。&lt;/p>
&lt;p>到此问题的根源我们可以确定是 kube-proxy 中使用的 ipvs 内核模块中的 &lt;code>estimation_timer&lt;/code> 函数执行时间过长，导致网络软中断处理延迟，从而使 ping 包的出现抖动，那么 &lt;code>estimation_timer[ipvs]&lt;/code> 的作用是什么？ 什么情况下导致的该函数执行如此之长呢？&lt;/p>
&lt;h3 id="33-ipvs-estimation_timer-定时器">3.3 ipvs estimation_timer 定时器&lt;/h3>
&lt;p>谜底终将揭晓！&lt;/p>
&lt;p>我们通过阅读 ipvs 相关的源码，发现 &lt;code>estimation_timer()[ipvs]&lt;/code> 函数针对每个 Network Namespace 创建时候的通过 &lt;a href="https://elixir.bootlin.com/linux/v5.8/source/net/netfilter/ipvs/ip_vs_core.c#L2469">ip_vs_core.c&lt;/a> 中的 &lt;code>__ip_vs_init&lt;/code> 初始化的，&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code> 1
2
3
4
5
6
7
8
9
10
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">/*
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> * Initialize IP Virtual Server netns mem.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> __net_init &lt;span style="color:#a6e22e">__ip_vs_init&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> net &lt;span style="color:#f92672">*&lt;/span>net)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> netns_ipvs &lt;span style="color:#f92672">*&lt;/span>ipvs;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">if&lt;/span> (&lt;span style="color:#a6e22e">ip_vs_estimator_net_init&lt;/span>(ipvs) &lt;span style="color:#f92672">&amp;lt;&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>) &lt;span style="color:#75715e">// 初始化
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">goto&lt;/span> estimator_fail;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-5 ipvs 初始化函数&lt;/p>
&lt;p>&lt;code>ip_vs_estimator_net_init&lt;/code> 函数在文件 &lt;a href="https://elixir.bootlin.com/linux/v5.8/source/net/netfilter/ipvs/ip_vs_est.c#L187">ip_vs_est.c&lt;/a> 中，定义如下：&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
5
6
7
8
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">int&lt;/span> __net_init &lt;span style="color:#a6e22e">ip_vs_estimator_net_init&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> netns_ipvs &lt;span style="color:#f92672">*&lt;/span>ipvs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">INIT_LIST_HEAD&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_list);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">spin_lock_init&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_lock);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">timer_setup&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_timer, estimation_timer, &lt;span style="color:#ae81ff">0&lt;/span>); &lt;span style="color:#75715e">// 设置定时器函数 estimation_timer
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">mod_timer&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_timer, jiffies &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">*&lt;/span> HZ); &lt;span style="color:#75715e">// 启动第一次计时器，2秒启动
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-6 ipvs estimator 初始化函数&lt;/p>
&lt;p>&lt;code>estimation_timer&lt;/code> 也定义在 &lt;a href="https://elixir.bootlin.com/linux/v5.8/source/net/netfilter/ipvs/ip_vs_est.c#L96">ip_vs_est.c&lt;/a> 文件中。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code> 1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">estimation_timer&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> timer_list &lt;span style="color:#f92672">*&lt;/span>t)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">spin_lock&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_lock);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">list_for_each_entry&lt;/span>(e, &lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_list, list) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">container_of&lt;/span>(e, &lt;span style="color:#66d9ef">struct&lt;/span> ip_vs_stats, est);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">spin_lock&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>s&lt;span style="color:#f92672">-&amp;gt;&lt;/span>lock);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ip_vs_read_cpu_stats&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>s&lt;span style="color:#f92672">-&amp;gt;&lt;/span>kstats, s&lt;span style="color:#f92672">-&amp;gt;&lt;/span>cpustats);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">/* scaled by 2^10, but divided 2 seconds */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rate &lt;span style="color:#f92672">=&lt;/span> (s&lt;span style="color:#f92672">-&amp;gt;&lt;/span>kstats.conns &lt;span style="color:#f92672">-&lt;/span> e&lt;span style="color:#f92672">-&amp;gt;&lt;/span>last_conns) &lt;span style="color:#f92672">&amp;lt;&amp;lt;&lt;/span> &lt;span style="color:#ae81ff">9&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> e&lt;span style="color:#f92672">-&amp;gt;&lt;/span>last_conns &lt;span style="color:#f92672">=&lt;/span> s&lt;span style="color:#f92672">-&amp;gt;&lt;/span>kstats.conns;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> e&lt;span style="color:#f92672">-&amp;gt;&lt;/span>cps &lt;span style="color:#f92672">+=&lt;/span> ((s64)rate &lt;span style="color:#f92672">-&lt;/span> (s64)e&lt;span style="color:#f92672">-&amp;gt;&lt;/span>cps) &lt;span style="color:#f92672">&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">spin_unlock&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_lock);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">mod_timer&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ipvs&lt;span style="color:#f92672">-&amp;gt;&lt;/span>est_timer, jiffies &lt;span style="color:#f92672">+&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#f92672">*&lt;/span>HZ); &lt;span style="color:#75715e">// 2 秒后启动新的一轮统计
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>代码 3-7 ipvs estimation_timer 函数&lt;/p>
&lt;p>从 &lt;code>estimation_timer&lt;/code> 的函数实现来看，会首先调用 spin_lock 进行锁的操作，然后遍历当前 Network Namespace 下的全部 ipvs 规则。由于我们集群的某些历史原因导致生产集群中的 Service 比较多，因此导致一次遍历的时候会占用比较长的时间。&lt;/p>
&lt;p>该函数的统计最终体现在 &lt;code>ipvsadm --stat&lt;/code> 的结果中（Conns InPkts OutPkts InBytes OutBytes）：&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
5
6
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ipvsadm -Ln --stats&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP Virtual Server version 1.2.1 &lt;span style="color:#f92672">(&lt;/span>size&lt;span style="color:#f92672">=&lt;/span>4096&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Prot LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes &lt;span style="color:#75715e"># 相关统计&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -&amp;gt; RemoteAddress:Port
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TCP 10.85.0.10:9153 &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -&amp;gt; 172.22.34.187:9153 &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>对于我们集群中的 &lt;code>ipvs&lt;/code> 规则进行统计，我们发现大概在 30000 左右。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ipvsadm -Ln --stats|wc -l&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>既然每个 Network Namespace 下都会有 &lt;code>estimation_timer&lt;/code> 的遍历，为什么只有 CPU#0 上的规则如此多呢？&lt;/p>
&lt;p>这是因为只有主机的 Host Network Namespace 中才会有全部的 ipvs 规则，这个我们也可以通过 &lt;code>ipvsadm -Ln&lt;/code> (执行在 Host Network Namespace 下) 验证。从现象来看，CPU#0 是 ipvs 模块加载的时候用于处理宿主机 Host Network Namespace 中的 ipvs 规则，当然这个核的加载完全是随机的。&lt;/p>
&lt;h2 id="4-问题解决">4. 问题解决&lt;/h2>
&lt;h3 id="41-解决方案">4.1 解决方案&lt;/h3>
&lt;p>到此，问题已经彻底定位，由于我们服务早期部署的历史原因，短期内调整 Service 的数目会导致大量的迁移工作，中间还有云厂商 SLB 产生的大量规则，也没有办法彻底根除，单从技术上解决的话，我们可以采用的方式有以下 3 种：&lt;/p>
&lt;ol>
&lt;li>动态感知到宿主机 Network Namespace 中 ipvs &lt;code>estimation_timer&lt;/code> 函数的函数，在 RPS 中设置关闭该 CPU 映射；
该方式需要调整 RPS 的配置，而且 ipvs 处理主机 Network Namespace 的核数不固定，需要识别并调整配置，还需要处理重启时候的 ipvs 主机 Network Namespace 的变动；&lt;/li>
&lt;li>由于我们不需要 ipvs 这种统计的功能，可以通过修改 ipvs 驱动的方式来规避该问题；
修改 ipvs 的驱动模块，需要重新加载该内核模块，也会导致主机服务上的短暂中断；&lt;/li>
&lt;li>ipvs 模块将内核遍历统计调整成一个独立的内核线程进行统计；&lt;/li>
&lt;/ol>
&lt;p>ipvs 规则在内核 timer 中遍历是 ipvs 移植到 k8s 上场景未适配的问题，社区应该需要把在 timer 中的遍历独立出去，但是这个方案需要社区的推动解决，远水解不了近渴。&lt;/p>
&lt;p>通过上述 3 种方案的对比，解决我们当前抖动的问题都不太容易实施，为了保证生产环境的稳定和实施的难易程度，最终我们把眼光定位在 Linux Kernel 热修的 &lt;a href="https://github.com/dynup/kpatch">kpatch&lt;/a> 方案上， kpath 实现的 livepatch 功能可以实时为正在运行的内核提供功能增强，无需重新启动系统。&lt;/p>
&lt;h3 id="42-kpatch-livepatch">4.2 kpatch livepatch&lt;/h3>
&lt;p>Kpatch 是给 Linux 内核 livepatch 的工具，由 Redhat 公司出品。最早出现的打热补丁工具是 Ksplice。但是 Ksplice 被 Oracle 收购后，一些发行版生产商就不得不开发自己的热补丁工具，分别是 Redhat 的 Kpatch 和 Suse 的 KGraft。同时，在这两家厂商的推进下，kernel 4.0 开始，开始集成了 livepatch 技术。 Kpatch 虽然是 Redhat 研发，但其也支持 Ubuntu、Debian、Oracle Linux 等的发行版。&lt;/p>
&lt;p>这里我们简单同步一下实施的步骤，更多的文档可以从 kpath 项目中获取。&lt;/p>
&lt;h4 id="421-获取-kpath-编译和安装">4.2.1 获取 kpath 编译和安装&lt;/h4>
&lt;p>|&lt;/p>
&lt;pre>&lt;code> 1
2
3
4
5
6
7
8
9
10
11
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ git clone https://github.com/dynup/kpatch.git
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ source test/integration/lib.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 中间会使用 yum 安装相关的依赖包，安装时间视网络情况而定，在阿里云的环境下需要的时间比较长&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo kpatch_dependencies
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cd kpatch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 进行编译&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ make
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 默认安装到 /usr/local，需要注意 kpatch-build 在目录 /usr/local/bin/ 下，而 kpatch 在 /usr/local/sbin/ 目录&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo make install
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;h4 id="422-生成内核源码-patch">4.2.2 生成内核源码 patch&lt;/h4>
&lt;p>在 kpatch 的使用过程中，需要使用到内核的源码，源码拉取的方式可以参考这里&lt;a href="https://wiki.centos.org/zh/HowTos/I_need_the_Kernel_Source?highlight=(kernel)%7C(src)">我需要内核的源代码&lt;/a>。&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ rpm2cpio kernel-3.10.0-1062.9.1.el7.src.rpm |cpio -div
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ xz -d linux-3.10.0-1062.9.1.el7.tar.xz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ tar -xvf linux-3.10.0-1062.9.1.el7.tar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cp -ra linux-3.10.0-1062.9.1.el7/ linux-3.10.0-1062.9.1.el7-patch
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>此处我们将 &lt;code>estimation_timer&lt;/code> 函数的实现设置为空&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
5
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">estimation_timer&lt;/span>(&lt;span style="color:#66d9ef">unsigned&lt;/span> &lt;span style="color:#66d9ef">long&lt;/span> arg)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">printk&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;hotfix estimation_timer patched&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>并生成对应的 patch 文件&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># diff -u linux-3.10.0-1062.9.1.el7/net/netfilter/ipvs/ip_vs_est.c linux-3.10.0-1062.9.1.el7-patch/net/netfilter/ipvs/ip_vs_est.c &amp;gt; ip_vs_timer_v1.patch&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;h4 id="423-生产内核补丁并-livepatch">4.2.3 生产内核补丁并 livepatch&lt;/h4>
&lt;p>然后生成相关的 patch ko 文件并应用到内核：&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
5
6
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /usr/local/bin/kpatch-build ip_vs_timer_v1.patch --skip-gcc-check --skip-cleanup -r /root/kernel-3.10.0-1062.9.1.el7.src.rpm&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 编译成功后会在当前目录生成 livepatch-ip_vs_timer_v1.ko 文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 应用到内核中.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /usr/local/sbin/kpatch load livepatch-ip_vs_timer_v1.ko&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>通过内核日志查看确认&lt;/p>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
3
4
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ dmesg -T
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Thu Dec &lt;span style="color:#ae81ff">3&lt;/span> 19:50:50 2020&lt;span style="color:#f92672">]&lt;/span> livepatch: enabling patch &lt;span style="color:#e6db74">&amp;#39;livepatch_ip_vs_timer_v1&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Thu Dec &lt;span style="color:#ae81ff">3&lt;/span> 19:50:50 2020&lt;span style="color:#f92672">]&lt;/span> livepatch: &lt;span style="color:#e6db74">&amp;#39;livepatch_ip_vs_timer_v1&amp;#39;&lt;/span>: starting patching transition
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>Thu Dec &lt;span style="color:#ae81ff">3&lt;/span> 19:50:50 2020&lt;span style="color:#f92672">]&lt;/span> hotfix estimation_timer patched
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;p>至此通过我们的 livepatch 成功修订了 &lt;code>estimation_timer&lt;/code> 的调用，一切看起来很成功。然后通过 &lt;code>funcgraph&lt;/code> 工具查看 &lt;code>estimation_timer&lt;/code> 函数不再出现在调用关系中。&lt;/p>
&lt;blockquote>
&lt;p>如果仅仅把函数设置为空的实现，等于是关闭了 &lt;code>estimation_timer&lt;/code> 的调用，即使通过命令 unload 掉 livepatch，该函数的也不会恢复，因此在生产环境中建议将函数的 2s 调用设置成个可以接受的时间范围内，比如 5 分钟，这样在 unload 以后，可以在 5 分钟以后恢复 &lt;code>estimation_timer&lt;/code> 的继续调用。&lt;/p>
&lt;/blockquote>
&lt;h3 id="43-使用-kpatch-注意事项">4.3 使用 kpatch 注意事项&lt;/h3>
&lt;ul>
&lt;li>kpatch 是基于内核版本生成的 ko 内核模块，必须保证后续 livepatch 的内核版本与编译机器的内核完全一致。&lt;/li>
&lt;li>通过手工 livepatch 的方式修复，如果保证机器在重启以后仍然生效需要通过 &lt;code>install&lt;/code> 来启用 kpathc 服务进行保证。&lt;/li>
&lt;/ul>
&lt;p>|&lt;/p>
&lt;pre>&lt;code>1
2
&lt;/code>&lt;/pre>
&lt;p>|&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /usr/local/sbin/kpatch install livepatch-ip_vs_timer_v1.ko&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># systemctl start kpatch&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>|&lt;/p>
&lt;ul>
&lt;li>在其他的机器上进行 livepatch 需要文件&lt;code>kpatch&lt;/code>、&lt;code>livepatch-ip_vs_timer_v1.ko&lt;/code> 和 &lt;code>kpatch.service&lt;/code>（用于 install 后重启生效） 3 个文件即可。&lt;/li>
&lt;/ul>
&lt;h2 id="5-总结">5. 总结&lt;/h2>
&lt;p>网络抖动问题的排查，涉及应用层、网络协议栈和内核中运作机制等多方面的协调，排查过程中需要逐层排查、逐步缩小范围，在整个过程中，合适的工具至关重要，在我们本次问题的排查过程中， BPF 技术为我们排查的方向起到了至关重要的作用。BPF 技术的出现为我们观测和跟踪内核中的事件，提供了更加灵活的数据采集和数据分析的能力，在生产环境中我们已经将其广泛用于了监控网络底层的重传和抖动等维度，极大提升提升我们在偶发场景下的问题排查效率，希望更多的人能够从 BPF 技术中受益。
&lt;a href="https://www.ebpf.top/post/ebpf_network_kpath_ipvs/">https://www.ebpf.top/post/ebpf_network_kpath_ipvs/&lt;/a>&lt;/p></description></item><item><title>Docs: /boot目录被清空下物理机无法开机的一次救援 · zhangguanzhang's Blog</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/x.linux-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/boot%E7%9B%AE%E5%BD%95%E8%A2%AB%E6%B8%85%E7%A9%BA%E4%B8%8B%E7%89%A9%E7%90%86%E6%9C%BA%E6%97%A0%E6%B3%95%E5%BC%80%E6%9C%BA%E7%9A%84%E4%B8%80%E6%AC%A1%E6%95%91%E6%8F%B4-zhangguanzhangs-blog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/x.linux-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/boot%E7%9B%AE%E5%BD%95%E8%A2%AB%E6%B8%85%E7%A9%BA%E4%B8%8B%E7%89%A9%E7%90%86%E6%9C%BA%E6%97%A0%E6%B3%95%E5%BC%80%E6%9C%BA%E7%9A%84%E4%B8%80%E6%AC%A1%E6%95%91%E6%8F%B4-zhangguanzhangs-blog/</guid><description>
&lt;p>今天下午到公司被通知苏州一个节点的客户的裸金属无法开机，14:00 上去到 16:50 终于给整好了，这里记录下笔记分享下&lt;/p>
&lt;h2 id="故障现象">故障现象&lt;/h2>
&lt;p>物理机裸金属，连上跳板机通过带外连上去 (等同于现场接了一个显示屏 + 键盘一样) 错误为&lt;/p>
&lt;pre>&lt;code>errorL file `/grub2/i386-pc/normal.mod' not found.
Entering rescue mode...
grub rescue&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>这个物理机是 grub2，这个错误和类似的&lt;code>/grub/i386-pc/normal.mod&lt;/code>本质上都是文件丢失，但是也分情况，网上的一些恢复步骤都是在丢失部分文件的情况下恢复的
查看分区&lt;/p>
&lt;pre>&lt;code>grub rescue&amp;gt;ls
(hd0) (hd0,msdos2) (hd0,msdos1)
grub rescue&amp;gt;ls (hd0,msdos1)/
./ ../
grub rescue&amp;gt;ls (hd0,msdos2)/
error: unknown filesystem
&lt;/code>&lt;/pre>
&lt;p>这里是等同于你实际的分区，我们这基本是一个 / boot 和一个根，看到&lt;code>(hd0,msdos1)&lt;/code>是 / boot 分区，文件是完全丢失的，&lt;code>(hd0,msdos2)/&lt;/code>报错未知文件系统是因为这个是 lvm，正常乐观下来讲只是丢失部分文件的话，可以参考下面步骤去恢复&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=RqRm1bEXO9M">https://www.youtube.com/watch?v=RqRm1bEXO9M&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.csdn.net/qq_20545159/article/details/50810089">https://blog.csdn.net/qq_20545159/article/details/50810089&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="救援">救援&lt;/h2>
&lt;h3 id="livecd-进入-rescue-救援">livecd 进入 rescue 救援&lt;/h3>
&lt;p>这里我是完全丢失，我利用带外远程挂载了一个 centos7.6 的 iso(最好和目标系统版本一样)，重启物理机进入 cdrom，选择&lt;code>Troubleshooting&lt;/code> –&amp;gt; &lt;code>Rescue a CentOS Linux system&lt;/code>&lt;/p>
&lt;p>下面我引用下别人的图，如果图被拦截了请看文字吧
&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/dd979e87-2201-4df4-890e-d122c4681296/troubleshooting-option-boot-RHEL-CentOS-7-into-rescue-mode.png">
&lt;/a>
&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/dd979e87-2201-4df4-890e-d122c4681296/rescue-CentOS-RHEL-7-system.png">
&lt;/a>
&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/dd979e87-2201-4df4-890e-d122c4681296/find-linux-installation-for-rescue-mode-RHEL-7-reinstall-GRUB2.png">
&lt;/a>
选择 1 后然后回车会得到一个交互式 shell，查看下分区信息&lt;/p>
&lt;pre>&lt;code>sh-4.2# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sda 8:0 1 558G 0 disk
├─sda1 8:1 1 1G 0 part /mnt/sysimage/boot
└─sda2 8:2 1 557G 0 part
├─centos-root 253:0 0 550G 0 lvm /mnt/sysimage
└─centos-swap 253:1 0 4G 0 lvm [SWAP]
sr0 11:0 1 4.3G 0 rom /run/install/repo
sr1 11:1 1 107.2M 0 rom
loop0 7:1 0 432.4M 1 loop
loop1 7:1 0 2G 1 loop
├─live-rw 253:0 0 2G 0 dm /
└─live-base 253:1 0 2G 1 dm
loop2 7:2 0 512M 1 loop
└─live-rw 253:0 0 2G 0 dm /
&lt;/code>&lt;/pre>
&lt;p>根被挂载到&lt;code>/mnt/sysimage&lt;/code>,boot 被挂载到&lt;code>/mnt/sysimage/boot&lt;/code>，iso 被挂载到&lt;code>/run/install/repo&lt;/code>
最开始我是 chroot /mnt/sysimage 后 grub2-install /dev/sda，然后重启后进入&lt;/p>
&lt;pre>&lt;code> Minimal BASH_like line editing is supported. For the first word,
...
..
grub&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>出现这个一般是缺少&lt;code>grub.cfg&lt;/code>，然后再进去光盘的 rescue 里去 chroot 进去&lt;code>grub2-mkconfig -o /boot/grub2/grub.cfg&lt;/code>还是不行。因为实际上 / boot 都被清空了，grub2-install 和 mkconfig 只是生成了&lt;code>/boot/grub2&lt;/code>下面一些文件，因为开机需要的 vmlinuz 和 kernel 都没有.&lt;/p>
&lt;h3 id="复制同样机器同样系统的--boot">复制同样机器同样系统的 / boot&lt;/h3>
&lt;p>正确姿势来整的话，考虑到 / boot 下面的相关文件被清理了，叫同事找同样物理机和系统的裸金属上去看看&lt;code>/boot/&lt;/code>目录，然后叫他把 / boot 打包成 iso，我在带外挂载上去。
         中间他直接复制到 windows 的，权限信息丢失了。我叫他直接 Linux 上 tar 打包了后再在 win 上打包 iso。
可以先在 rescue 模式里看&lt;code>/mnt/sysimage/etc/redhat-release&lt;/code>查看系统信息，然后正常物理机的同样系统上的 / boot 信息为&lt;/p>
&lt;pre>&lt;code>$ ll
total 110920
-rw-r--r--. 1 root root 151918 Nov 9 2018 config-3.10.0-957.el7.x86_64
drwxr-xr-x. 3 root root 17 Feb 26 2019 efi
drwxr-xr-x. 2 root root 27 Feb 26 2019 grub
drwx------. 5 root root 97 Feb 26 2019 grub2
-rw-------. 1 root root 59891363 Feb 26 2019 initramfs-0-rescue-cd270b115cc741328f7812c0be97041d.img
-rw-------. 1 root root 22834950 Feb 26 2019 initramfs-3.10.0-957.el7.x86_64.img
-rw-------. 1 root root 13548147 Oct 11 16:32 initramfs-3.10.0-957.el7.x86_64kdump.img
-rw-r--r--. 1 root root 314036 Nov 9 2018 symvers-3.10.0-957.el7.x86_64.gz
-rw-------. 1 root root 3543471 Nov 9 2018 System.map-3.10.0-957.el7.x86_64
-rwxr-xr-x. 1 root root 6639904 Feb 26 2019 vmlinuz-0-rescue-cd270b115cc741328f7812c0be97041d
-rwxr-xr-x. 1 root root 6639904 Nov 9 2018 vmlinuz-3.10.0-957.el7.x86_64
&lt;/code>&lt;/pre>
&lt;p>现在步骤开始是实际有效的步骤，前提是挂载了 centos 的 iso 和 boot 文件的 iso
下面我第一个光驱是 iso，第二个是 boot，所以是&lt;code>/dev/sr1&lt;/code>&lt;/p>
&lt;pre>&lt;code>sh-4.2# chroot /mnt/sysimage
bash-4.2# alias ll='ls -l'
bash-4.2# mkdir -p /media/tmp
bash-4.2# mount /dev/sr1 /media/tmp/
mount: /dev/sr1 is write-protected, mounting read-only
bash-4.2# cd /media
bash-4.2# tar zxf /media/tmp/boot.tar.gz
bash-4.2# cp -a boot/* /boot/
&lt;/code>&lt;/pre>
&lt;p>这里有一个点不确定，但是为了保险起见我是操作了，有条件的人可以自己去测下看看下面步骤不执行有影响不, 删除 uuid 文件 (我对比了下实际上 MD5 是一样的，有条件可以测下下面这几个步骤不执行看看正常不)&lt;/p>
&lt;pre>&lt;code>bash-4.2# cd /boot
bash-4.2# ll /media/boot/*cd270b11*
-rw-------. 1 root root 59891363 Feb 26 2019 /media/boot/initramfs-0-rescue-cd270b115cc741328f7812c0be97041d.img
-rwxr-xr-x. 1 root root 6639904 Feb 26 2019 /media/boot/vmlinuz-0-rescue-cd270b115cc741328f7812c0be97041d
bash-4.2# rm -f *cd270b11*
bash-4.2# /etc/kernel/postinst.d/51-dracut-rescue-postinst.sh $(uname -r) /boot/vmlinuz-$(uname -r)
&lt;/code>&lt;/pre>
&lt;p>grub 配置文件里有硬盘分区的 uuid，这里需要重新生成&lt;code>grub.cfg&lt;/code>&lt;/p>
&lt;pre>&lt;code>bash-4.2# mv /boot/grub2/grub.cfg{,.bak}
bash-4.2# grub2-mkconfig -o /boot/grub2/grub.cfg
&lt;/code>&lt;/pre>
&lt;p>如果报错&lt;code>grub-probe: error: cannot find a device for / (is /dev mounted?)&lt;/code>
则在 chroot 之前用 bind mount 相关目录&lt;/p>
&lt;pre>&lt;code>mount -o bind /dev /mnt/sysimage/dev
mount -o bind /proc /mnt/sysimage/proc
mount -o bind /run /mnt/sysimage/run
mount -o bind /sys /mnt/sysimage/sys
&lt;/code>&lt;/pre>
&lt;h3 id="重启">重启&lt;/h3>
&lt;p>重启测试&lt;/p>
&lt;pre>&lt;code>bash-4.2# exit
sh-4.2# init 6
&lt;/code>&lt;/pre>
&lt;p>结果是进入了&lt;code>emergency mode&lt;/code>
&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/dd979e87-2201-4df4-890e-d122c4681296/emergencyMode.png">
&lt;/a>
这个模式进来了大多数原因是有个非系统的分区无法挂载，输入 root 密码进去后，先查看下&lt;code>systemctl failed&lt;/code>发现&lt;code>/home&lt;/code>无法被挂载&lt;/p>
&lt;pre>&lt;code>[root@cn19 ~]# systemctl --failed
UNIT LOAD ACTIVE SUB DESCRIPTION
● home.mount loaded failed failed /home
● auditd.service loaded failed failed Security Auditing Service
LOAD = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB = The low-level unit activation state, values depend on unit type.
2 loaded units listed. Pass --all to see loaded but inactive units, too.
To show all installed unit files use 'systemctl list-unit-files'.
[root@cn19 ~]# grep -Pv '^#|^$' /etc/fstab
/dev/mapper/centos-root / xfs defaults 0 0
UUID=71b43bbc-819c-4420-9ba8-9c85110999dd /boot xfs defaults 0 0
/dev/mapper/centos-swap swap swap defaults 0 0
[root@cn19 ~]# lvs
LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert
home centos -wi-a----- 2.00g
root centos -wi-ao---- 550.00g
swap centos -wi-ao---- 4.00g
&lt;/code>&lt;/pre>
&lt;p>尝试修复无果&lt;/p>
&lt;pre>&lt;code>[root@cn19 ~]# xfs_repair /dev/mapper/centos-home
Phase 1 - find and verify superblock...
bad primary superblock - bad magic number !!!
attempting to find secondary superblock...
......................Sorry, could not find valid secondary superblock
Exiting now.
&lt;/code>&lt;/pre>
&lt;p>询问了用户&lt;code>/home&lt;/code>目录不重要，直接取消 fstab 取消 / home 的挂载然后 reboot 恢复正常
&lt;a href="https://notes-learning.oss-cn-beijing.aliyuncs.com/dd979e87-2201-4df4-890e-d122c4681296/right.png">
&lt;/a>&lt;/p>
&lt;h2 id="参考">参考：&lt;/h2>
&lt;p>&lt;a href="https://www.tecmint.com/recover-or-rescue-corrupted-grub-boot-loader-in-centos-7/">https://www.tecmint.com/recover-or-rescue-corrupted-grub-boot-loader-in-centos-7/&lt;/a>
rescue mode 安装 kernel: &lt;a href="https://www.thegeekdiary.com/centos-rhel-7-how-to-install-kernel-from-rescue-mode/">https://www.thegeekdiary.com/centos-rhel-7-how-to-install-kernel-from-rescue-mode/&lt;/a>
rescue mode 生成 vmlinuz 和 initramfs: &lt;a href="https://www.thegeekdiary.com/how-to-re-generate-initramfs-and-vmlinuz-for-rescue-kernel-with-current-kernel-in-centos-rhel-7/">https://www.thegeekdiary.com/how-to-re-generate-initramfs-and-vmlinuz-for-rescue-kernel-with-current-kernel-in-centos-rhel-7/&lt;/a>
&lt;a href="https://zhangguanzhang.github.io/2019/10/12/boot-grub-rescue/">https://zhangguanzhang.github.io/2019/10/12/boot-grub-rescue/&lt;/a>&lt;/p></description></item><item><title>Docs: /etc/kubernetes 目录误删恢复</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/etc_kubernetes-%E7%9B%AE%E5%BD%95%E8%AF%AF%E5%88%A0%E6%81%A2%E5%A4%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%B8%8E-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/etc_kubernetes-%E7%9B%AE%E5%BD%95%E8%AF%AF%E5%88%A0%E6%81%A2%E5%A4%8D/</guid><description>
&lt;h1 id="故障现象">故障现象&lt;/h1>
&lt;p>参考：&lt;a href="https://mp.weixin.qq.com/s/O3fJF5aZuxPOKa7lIjrHnQ">阳明公众号原文&lt;/a>&lt;/p>
&lt;p>Kubernetes 是一个很牛很牛的平台，Kubernetes 的架构可以让你轻松应对各种故障，今天我们将来破坏我们的集群、删除证书，然后再想办法恢复我们的集群，进行这些危险的操作而不会对已经运行的服务造成宕机。&lt;/p>
&lt;blockquote>
&lt;p>如果你真的想要执行接下来的操作，还是建议别在生产环境去折腾，虽然理论上不会造成服务宕机，但是如果出现了问题，&lt;strong>可千万别骂我~~~&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>我们知道 Kubernetes 的控制平面是由几个组件组成的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>etcd：作为整个集群的数据库使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-apiserver：集群的 API 服务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-controller-manager：整个集群资源的控制操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kube-scheduler：核心调度器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubelet：是运行在节点上用来真正管理容器的组件&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这些组件都由一套针对客户端和服务端的 TLS 证书保护，用于组件之间的认证和授权，大部分情况下它们并不是直接存储在 Kubernetes 的数据库中的，而是以普通文件的形式存在。&lt;/p>
&lt;pre>&lt;code># tree /etc/kubernetes/pki/
/etc/kubernetes/pki/
├── apiserver.crt
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
├── apiserver.key
├── apiserver-kubelet-client.crt
├── apiserver-kubelet-client.key
├── ca.crt
├── ca.key
├── CTNCA.pem
├── etcd
│ ├── ca.crt
│ ├── ca.key
│ ├── healthcheck-client.crt
│ ├── healthcheck-client.key
│ ├── peer.crt
│ ├── peer.key
│ ├── server.crt
│ └── server.key
├── front-proxy-ca.crt
├── front-proxy-ca.key
├── front-proxy-client.crt
├── front-proxy-client.key
├── sa.key
└── sa.pub
&lt;/code>&lt;/pre>
&lt;p>控制面板的组件以静态 Pod (我这里用 kubeadm 搭建的集群)的形式运行在 master 节点上，默认资源清单位于 &lt;code>/etc/kubernetes/manifests&lt;/code> 目录下。通常来说这些组件之间会进行互相通信，基本流程如下所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ghm4g3/1616115588241-7c7f556a-1526-43e4-847a-d78a70821f6b.png" alt="">&lt;/p>
&lt;p>组件之间为了通信，他们需要使用到 TLS 证书。假设我们已经有了一个部署好的集群，接下来让我们开始我们的破坏行为。&lt;/p>
&lt;pre>&lt;code>rm -rf /etc/kubernetes/
&lt;/code>&lt;/pre>
&lt;p>在 master 节点上，这个目录包含：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>etcd 的一组证书和 CA（在 &lt;code>/etc/kubernetes/pki/etcd&lt;/code> 目录下）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一组 kubernetes 的证书和 CA（在 &lt;code>/etc/kubernetes/pki&lt;/code> 目录下）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>还有 kube-controller-manager、kube-scheduler、cluster-admin 以及 kubelet 这些使用的 kubeconfig 文件&lt;/p>
&lt;/li>
&lt;li>
&lt;p>etcd、kube-apiserver、kube-scheduler 和 kube-controller-manager 的静态 Pod 资源清单文件（位于 &lt;code>/etc/kubernetes/manifests&lt;/code> 目录）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>现在我们就上面这些全都删除了，如果是在生产环境做了这样的操作，可能你现在正瑟瑟发抖吧~&lt;/p>
&lt;p>修复控制平面&lt;/p>
&lt;p>首先我也确保下我们的所有控制平面 Pod 已经停止了。&lt;/p>
&lt;pre>&lt;code># 如果你用 docker 也是可以的
crictl rm `crictl ps -aq`
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>注意：kubeadm 默认不会覆盖现有的证书和 kubeconfigs，为了重新颁发证书，你必须先手动删除旧的证书。&lt;/p>
&lt;/blockquote>
&lt;p>接下来我们首先恢复 etcd，在**一个 master **节点上执行下面的命令生成 etcd 集群的证书：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs etcd-ca --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令将为我们的 etcd 集群生成一个新的 CA，由于所有其他证书都必须由它来签署，我们也将把它和私钥复制到其他 master 节点(如果你是多 master)。&lt;/p>
&lt;pre>&lt;code>/etc/kubernetes/pki/etcd/ca.{key,crt}
&lt;/code>&lt;/pre>
&lt;p>接下来让我们在&lt;strong>所有 master&lt;/strong> 节点上为它重新生成其余的 etcd 证书和静态资源清单。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs etcd-healthcheck-client --config=kubeadm-config.yaml
kubeadm init phase certs etcd-peer --config=kubeadm-config.yaml
kubeadm init phase certs etcd-server --config=kubeadm-config.yaml
kubeadm init phase etcd local --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>上面的命令执行后，你应该已经有了一个正常工作的 etcd 集群了。&lt;/p>
&lt;pre>&lt;code># crictl ps
CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID
ac82b4ed5d83a 0369cf4303ffd 2 seconds ago Running etcd 0 bc8b4d568751b
&lt;/code>&lt;/pre>
&lt;p>接下来我们对 Kubernetes 服务做同样的操作，在其中&lt;strong>一个 master&lt;/strong> 节点上执行如下的命令：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase certs all --config=kubeadm-config.yaml
kubeadm init phase kubeconfig all --config=kubeadm-config.yaml
kubeadm init phase control-plane all --config=kubeadm-config.yaml
rm -rf /root/.kube/*
cp -f /etc/kubernetes/admin.conf ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>上面的命令将生成 Kubernetes 的所有 SSL 证书，以及 Kubernetes 服务的静态 Pods 清单和 kubeconfigs 文件。&lt;/p>
&lt;p>如果你使用 kubeadm 加入 kubelet，你还需要更新 &lt;code>kube-public&lt;/code> 命名空间中的 cluster-info 配置，因为它仍然包含你的旧 CA 的哈希值。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase bootstrap-token --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>由于其他 master 节点上的所有证书也必须由单一 CA 签署，所以我们将其复制到其他控制面节点，并在每个节点上重复上述命令。&lt;/p>
&lt;pre>&lt;code>/etc/kubernetes/pki/{ca,front-proxy-ca}.{key,crt}
/etc/kubernetes/pki/sa.{key,pub}
&lt;/code>&lt;/pre>
&lt;p>顺便说一下，作为手动复制证书的替代方法，你也可以使用 Kubernetes API，如下所示的命令：&lt;/p>
&lt;pre>&lt;code>kubeadm init phase upload-certs --upload-certs --config=kubeadm-config.yaml
# 上一条命令输出的 certificate-key 替换 ${MasterJoinKey}
kubeadm token create --ttl=2h --certificate-key=${MasterJoinKey} --print-join-command
&lt;/code>&lt;/pre>
&lt;p>该命令将加密并上传证书到 Kubernetes，时间为 2 小时，所以你可以按以下方式注册 master 节点：&lt;/p>
&lt;pre>&lt;code># 注意替换上面命令输出的 join 命令的内容
kubeadm join phase control-plane-prepare all kubernetes-apiserver:6443 --control-plane --token cs0etm.ua7fbmwuf1jz946l --discovery-token-ca-cert-hash sha256:555f6ececd4721fed0269d27a5c7f1c6d7ef4614157a18e56ed9a1fd031a3ab8 --certificate-key 385655ee0ab98d2441ba8038b4e8d03184df1806733eac131511891d1096be73
kubeadm join phase control-plane-join all
&lt;/code>&lt;/pre>
&lt;p>需要注意的是，Kubernetes API 还有一个配置，它为 &lt;code>front-proxy&lt;/code> 客户端持有 CA 证书，它用于验证从 apiserver 到 webhooks 和聚合层服务的请求。不过 kube-apiserver 会自动更新它。到在这个阶段，我们已经有了一个完整的控制平面了。&lt;/p>
&lt;h2 id="修复-kubelet">修复 kubelet&lt;/h2>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/
kubeadm init phase kubeconfig kubelet --config=kubeadm-config.yaml
kubeadm init phase kubelet-start --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;h2 id="修复工作节点">修复工作节点&lt;/h2>
&lt;p>现在我们可以使用下面的命令列出集群的所有节点：&lt;/p>
&lt;pre>&lt;code>kubectl get nodes
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>若报错：Unable to connect to the server: x509: certificate signed by unknown authority
删除 /root/.kube/config 文件，并重新拷贝一遍&lt;/p>
&lt;/blockquote>
&lt;p>当然正常现在所有节点的状态都是 NotReady，这是因为他们仍然还使用的是旧的证书，为了解决这个问题，我们将使用 kubeadm 来执行重新加入集群节点。&lt;/p>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/ /etc/kubernetes/kubelet.conf
kubeadm init phase kubeconfig kubelet --config=kubeadm-config.yaml
kubeadm init phase kubelet-start --config=kubeadm-config.yaml
&lt;/code>&lt;/pre>
&lt;p>但要加入工作节点，我们必须生成一个新的 token。&lt;/p>
&lt;pre>&lt;code>kubeadm token create --print-join-command
&lt;/code>&lt;/pre>
&lt;p>然后在工作节点分别执行下面的命令：&lt;/p>
&lt;pre>&lt;code>systemctl stop kubelet
rm -rf /var/lib/kubelet/pki/ /etc/kubernetes/pki/ /etc/kubernetes/kubelet.conf
kubeadm join phase kubelet-start kubernetes-apiserver:6443 --token cs0etm.ua7fbmwuf1jz946l --discovery-token-ca-cert-hash sha256:555f6ececd4721fed0269d27a5c7f1c6d7ef4614157a18e56ed9a1fd031a3ab8
&lt;/code>&lt;/pre>
&lt;p>上面的操作会把你所有的 kubelet 重新加入到集群中，它并不会影响任何已经运行在上面的容器，但是，如果集群中有多个节点并且不同时进行，则可能会遇到一种情况，即 kube-controller-mananger 开始从 NotReady 节点重新创建容器，并尝试在活动节点上重新调度它们。&lt;/p>
&lt;p>为了防止这种情况，我们可以暂时停掉 master 节点上的 controller-manager。&lt;/p>
&lt;pre>&lt;code>rm /etc/kubernetes/manifests/kube-controller-manager.yaml
crictl rmp `crictl ps --name kube-controller-manager -q`
&lt;/code>&lt;/pre>
&lt;p>一旦集群中的所有节点都被加入，你就可以为 controller-manager 生成一个静态资源清单，在所有 master 节点上运行下面的命令。&lt;/p>
&lt;pre>&lt;code>kubeadm init phase control-plane controller-manager
&lt;/code>&lt;/pre>
&lt;p>如果 kubelet 被配置为请求由你的 CA 签署的证书(选项 serverTLSBootstrap: true)，你还需要批准来自 kubelet 的 CSR：&lt;/p>
&lt;pre>&lt;code>kubectl get csrkubectl certificate approve &amp;lt;csr&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="修复-serviceaccounts">修复 ServiceAccounts&lt;/h2>
&lt;p>因为我们丢失了 &lt;code>/etc/kubernetes/pki/sa.key&lt;/code> ，这个 key 用于为集群中所有 &lt;code>ServiceAccounts&lt;/code> 签署 &lt;code>jwt tokens&lt;/code>，因此，我们必须为每个 sa 重新创建 tokens。这可以通过类型为 &lt;code>kubernetes.io/service-account-token&lt;/code> 的 Secret 中删除 token 字段来完成。&lt;/p>
&lt;pre>&lt;code>kubectl get secret --all-namespaces | awk '/kubernetes.io\/service-account-token/ { print &amp;quot;kubectl patch secret -n &amp;quot; $1 &amp;quot; &amp;quot; $2 &amp;quot; -p {\\\&amp;quot;data\\\&amp;quot;:{\\\&amp;quot;token\\\&amp;quot;:null}}&amp;quot;}' | sh -x
&lt;/code>&lt;/pre>
&lt;p>删除之后，kube-controller-manager 会自动生成用新密钥签名的新令牌。不过需要注意的是并非所有的微服务都能即时更新 tokens，因此很可能需要手动重新启动使用 tokens 的容器。&lt;/p>
&lt;pre>&lt;code>kubectl get pod --field-selector 'spec.serviceAccountName!=default' --no-headers --all-namespaces | awk '{print &amp;quot;kubectl delete pod -n &amp;quot; $1 &amp;quot; &amp;quot; $2 &amp;quot; --wait=false --grace-period=0&amp;quot;}'
&lt;/code>&lt;/pre>
&lt;p>例如，这个命令会生成一个命令列表，会将所有使用非默认的 serviceAccount 的 Pod 删除，我建议从 kube-system 命名空间执行，因为 kube-proxy 和 CNI 插件都安装在这个命名空间中，它们对于处理你的微服务之间的通信至关重要。&lt;/p>
&lt;p>到这里我们的集群就恢复完成了。&lt;/p>
&lt;blockquote>
&lt;p>参考链接：&lt;a href="https://itnext.io/breaking-down-and-fixing-kubernetes-4df2f22f87c3">https://itnext.io/breaking-down-and-fixing-kubernetes-4df2f22f87c3&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: 📹11.图形处理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86/11.%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86/11.%E5%9B%BE%E5%BD%A2%E5%A4%84%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.jiqizhixin.com/articles/2019-03-22-10">https://www.jiqizhixin.com/articles/2019-03-22-10&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="什么是计算机视觉">什么是计算机视觉&lt;/h2>
&lt;p>为了简化这个问题的答案， 让我们来试想一个场景。
假设你和你的朋友去度假，然后你上传了很多照片到 Facebook 上。但是现在在每张照片中找到你朋友的脸并标记它们要花费很多时间。实际上，Facebook 已经足够智能，它可以帮你标记人物。
那么，你认为自动的特征标记是如何工作的呢？ 简单来说，它通过计算机视觉来实现。
计算机视觉是一个跨学科领域，它解决如何使计算机从数字图像或视频中获得高层次的理解的问题。
这里的想法是将人类视觉系统可以完成的任务自动化。因此，计算机应该能够识别诸如人脸或者灯柱甚至雕像之类的物体。&lt;/p>
&lt;h3 id="计算机如何读取图像">计算机如何读取图像？&lt;/h3>
&lt;p>思考以下图片：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/fhwfe4/1658568420855-b25fc9bb-0d76-4e6f-a1b7-f77be1aff1da.png" alt="image.png">
我们可以认出它是纽约天际线的图片。 但是计算机可以自己发现这一切吗？答案是不！
计算机将任何图片都读取为一组 0 到 255 之间的值。
对于任何一张彩色图片，有三个主通道——红色(R)，绿色(G)和蓝色(B)。它的工作原理非常简单。
对每个原色创建一个矩阵，然后，组合这些矩阵以提供 R, G 和 B 各个颜色的像素值。
每一个矩阵的元素提供与像素的亮度强度有关的数据。
思考下图：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/fhwfe4/1658568420764-dd8cadc1-402a-49a6-bdff-ccb3e61ba2eb.png" alt="image.png">
如图所示，图像的大小被计算为 B x A x 3。
注意：对于黑白图片，只有一个单一通道。&lt;/p>
&lt;h1 id="gpu">GPU&lt;/h1>
&lt;h1 id="opencv">OpenCV&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/opencv/opencv">GitHub 项目，opencv/opencv&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://opencv.org/">官网&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.opencv.org/">官方文档&lt;/a>，从左侧 Nightly 中选择想要查看的版本&lt;/li>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/115321759">https://zhuanlan.zhihu.com/p/115321759&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Open Source Computer Vision Library(开源计算机视觉库，简称 OpenCV)&lt;/strong> 是一个包含数百种计算机视觉算法的开源库。
官方提供了 Python 语言的 OpenCV 接口~~~在官方这没找到其他语言的&lt;/p>
&lt;h1 id="halcon">Halcon&lt;/h1>
&lt;h2 id="heading">&lt;/h2></description></item><item><title>Docs: 1.1.Keepalived 介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/1.1.keepalived-%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/1.1.keepalived-%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官网：&lt;a href="http://www.keepalived.org/">http://www.keepalived.org/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ol>
&lt;li>Active/passive 模式&lt;/li>
&lt;li>Keepalived 是 vrrp 协议在 Linux 主机上以守护进程方式的实现&lt;/li>
&lt;li>能够根据配置文件自动生成 ipvs 规则&lt;/li>
&lt;li>对各 RS 做健康状态检测&lt;/li>
&lt;/ol>
&lt;h1 id="万字长文带你从-0-学习-keepalived">万字长文带你从 0 学习 Keepalived&lt;/h1>
&lt;p>负载均衡器（Load Balancer, LB ）是一组能够将 IP 数据流以负载均衡形式转发到多台物理服务器的集成软件。有硬件负载均衡器和软件负载均衡器之分，硬件负载均衡器主要是在访问网络和服务器之间配置物理负载均衡设备，客户端对物理服务器的访问请求首先会抵达负载均衡设备，然后再由负载均衡设备根据一定的负载算法转发到后端服务器。相比而言，软件负载均衡器不需要特定的物理设备，只需在相应的操作系统上部署具有负载均衡功能的软件即可。&lt;/p>
&lt;p>在 Openstack 高可用集群部署中，服务的负载均衡和高可用主要有两种主流的实现方案，即 HAProxy+ Keepalived 和 Pacemaker+HAProxy 方案。由于 OpenStack 服务组件多样，不同服务均需要进行特定的高可用设计，并且从集群资源统一调度和集群稳定性的角度考虑，后一种方案是多数 OpenStack 厂商的高可用部署方案首选，但是选用后一方案并不意味着 Keepalived 在 OpenStack 高可用集群部署中不被使用。由于 Keepalived 的主要作用之一是进行虚拟路由的故障切换，其在 Neutron 的 L3 高可用设计与实现中起着举足轻重的作用。&lt;/p>
&lt;h1 id="11-keepalived-及-lvs-概述">1.1 keepalived 及 LVS 概述&lt;/h1>
&lt;p>Keepalived 的项目实现的主要目标是简化 LVS 项目的配置并增强其稳定性，即 Keepalived 是对 LVS 项目的扩展增强。&lt;/p>
&lt;p>Keepalived 为 Linux 系统和基于 Linux 的架构提供了负载均衡和高可用能力，其负载均衡功能主要源自集成在 Linux 内核中的 LVS 项目模块 IPVS( IP Virtual Server ），基于 IPVS 提供的 4 层 TCP/IP 协议负载均衡， Keepalived 也具备负载均衡的功能，此外， Keepalived 还实现了基于多层 TCP/IP 协议（ 3 层、4 层、5/7 层）的健康检查机制，因此， Keepalived 在 LVS 负载均衡功能的基础上，还提供了 LVS 集群物理服务器池健康检查和故障节点隔离的功能。&lt;/p>
&lt;p>除了扩展 LVS 的负载均衡服务器健康检查能力， Keepalived 还基于虚拟路由冗余协议（ Virtual Route Redundancy Protocol, VRRP )实现了 LVS 负载均衡服务器的故障切换转移，即 Keepalived 还实现了 LVS 负载均衡器的高可用性。Keepalived 就是为 LVS 集群节点提供健康检查和为 LVS 负载均衡服务器提供故障切换的用户空间进程。&lt;/p>
&lt;p>图为 Keepalived 的原理架构图，从图中可以看到， Keepalived 的多数核心功能模块均位于用户空间，而仅有 IPVS 和 NETLINK 模块位于内核空间，但是这两个内核模块正是 Keepalived 实现负载均衡和路由高可用的核心模块，其中的 NETLINK 主要用于提供高级路由及其相关的网络功能。Keepalived 的大部分功能模块位于用户空间，其中几个核心功能模块的介绍如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/watgs2/1616132514380-d0730061-c06c-433e-8b33-8868d6966373.jpeg" alt="">&lt;/p>
&lt;ul>
&lt;li>WatchDog ：其主要负责监控 Checkers 和 VRRP 子进程的运行状况。&lt;/li>
&lt;li>Checkers ：此功能模块主要负责真实服务器的健康检查（ HealthChecking )，是 Keepalived 最主要的功能之一，因为 HealthChecking 是负载均衡功能稳定运行的基础， LVS 集群节点的故障隔离和重新加入均依赖于 HealthChecking 的结果。&lt;/li>
&lt;li>VRRPStack ：此功能模块主要负责负载均衡器之间的故障切换，如果集群架构中仅使用一个 LVS 负载均衡器，由于本身不具备故障切换的条件，则 VRRPStack 不是必须的。&lt;/li>
&lt;li>IPVS Wrapper ：此模块主要用来发送设定的规则到内核 IPVS 代码。Keepalived 的设计目标是构建高可用的 LVS 负载均衡群集， Keepalived 在运行中将会通过 IPVSWrapper 模块调用 IPVSAdmin 工具来创建虚拟服务器，检查和管理 LVS 集群物理服务器池。&lt;/li>
&lt;li>Netlink Reflector ：此功能模块主要用来设定 VRRP 的 VIP 地址并提供相关的网络功能，该模块通过与内核中的 NETLINK 模块交互，从而为 Keepalived 提供路由高可用功能。&lt;/li>
&lt;/ul>
&lt;p>从 Keepalived 的实现原理和功能来看， Keepalived 是开源负载均衡项目 LVS 的增强和虚拟路由协议 VRRP 实现的集合，即 Keepalived 通过整合和增强 LVS 与 VRRP 来提供高可用的负载均衡系统架构。&lt;/p>
&lt;h1 id="12-keepalived-工作原理">1.2 KeepAlived 工作原理&lt;/h1>
&lt;p>Keepalived 本质上是提供数据流转发与服务器健康检查并具备故障切换的高可用路由，而数据转发与健康检查是对 LVS 功能的扩展和增强，因此也可以认为 Keepalived 是运行在用户空间的 LVS 路由（LVS Router) 进程。在实际应用中， Keepalived 通常部署在两台主备或一主多备的服务器上，即 Keepalived 进程既运行在 Active/Master 状态的 LVS Router 中，也运行在 Passive/Slave 状态的 LVS Router 中，而所有运行 Keepalived 进程的 LVS Router 都遵循虚拟路由冗余协议 VRRP。在 VRRP 的协议框架下，作为 Master 的 Router 将会处理两个主要任务，即转发客户端访问请求到后端物理服务器以进行负载均衡和周期性的发送 VRRP 协议报文，而作为 Slave 的 Routers 则负责接收 VRRP 报文，如果某一时刻作为 Slave 的 Routers 接收 VRRP 报文失败，则认为 Master Router 故障， 并从 Slave Routers 中重新选举产生一个新的 Master Router 。&lt;/p>
&lt;p>Keepalived 是一个与 LVS Router 相关的控制进程，在 RHEL7 /Centos7 系统中，Keepalived 由 Systemctl 命令通过读取/etc/keepalived/keepalived.conf 配置文件来启动。在遵循 VRRP 协议的 Master Router 中， Keepalived 进程会启动内核中的 LVS 服务以创建虚拟服务器，并根据配置拓扑对服务运行状况进行监控。此外，Master Router 还会向 Slave Routers 发送周期性的 VRRP 广播报文，而 Master Router 运行状态的正常与否是由 Slave Routers 上的 VRRP 实例决定的。如果在用户预置的时间段内 Slave Router 不能接收到 VRRP 报文，则 Keepalived 认为 Master Router 故障，同时触发 LVS Router 的 Failover 操作。&lt;/p>
&lt;p>在 Failover 的过程中， Keepalived 创建的虚拟服务器会被清除，新的 Master Router 将接管 VIP 发送 ARP 信息、设置 IPVS Table 记录条目（Virtual Server）以及物理服务器的健康检查和发送 VRRP 广播报文。Keepalived 的 Failover 操作针对的是四层 TCP/ IP 协议，即传输层，因为 TCP 在传输层上进行的是基于链路连接的数据传输。所以，当服务器在响应 TCP 请求时，如果出现设置时间段的 Timeout，则 Keepalived 的健康检查机制将会监测到该情况并认为该服务器故障，然后将其从服务器池中移除（故障服务器隔离） 。图 3-4 是基于 Keepalived 设计的具有二层拓扑的负载均衡架构，该架构分为两个层次。第一层为负载均衡层，由一个 Active 和多个 Backup 的 LVS Routers 组成，其中，每个 LVS Router 都配置有两个网络接口，一个接入 Internet 网络，另一个接入内部私有网络， Active 的 LVS Router 在这两个网络接口间进行数据转发。在图 3-4 的负载均衡架构中，位于第一层的 LVS Routers 和第二层的物理服务器通过私网接口接人相同的局域网中， Active 的 LVSRouter 通过 NAT 技术将 Internet 数据流转发到私网物理服务器上，而这些位于第二层的物理服务器运行着最终响应请求的服务。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/watgs2/1616132514345-97aedaa2-8dd5-40fa-a7bb-79f9c9fa0ac6.jpeg" alt="">&lt;/p>
&lt;p>位于二层私网中的服务器在与 Internet 交互时必须经过主 LVS Router 的 NAT 转发， 并且对于外部网络中的客户端而言，访问二层私网中的物理服务器就如访问同处 Internet 网络中的服务，因为从客户端的角度来看，访问请求的目的地址正是位于主 LVS Router 上的 VIP 地址，而该 VIP 与客户端地址处于相同网络中， VIP 还可以是管理员指定的互联网域名，如www.example.com 。VIP 在 Keepalived 的配置中通常被指定到一个或者多个虚拟服务器上，而虚拟服务器的主要任务便是监昕 VIP 及相应端口上的请求，当主 LVS Router 进行 Failover 操作的时候， VIP 会从一个 LVS Router 转移到另一个 LVS（因此 VIP 也称为浮动 IP)。&lt;/p>
&lt;p>在 Keepalived 负载均衡架构的 VIP 配置中，每个将 LVS Router 连接到 Internet 的物理网卡接口均可配置多个 VIP ，并且每个 VIP 对应着不同的 Virtual Server ，即多个 VirtualServers 可以同时监听相同物理网卡上的不同 VIP ，其中每个 VIP 都对应着不同的服务。例如， Linux 系统中的接口 eth0 将 LVS Router 连接到 Internet 中，则可以在 eth0 上配置一个地址为 192.168.115.100 的 VIP 以用于响应 HTTP 服务请求，同时还可以在 eth0 上配置另一个地址为 192.168.115.200 的 VIP 以用于响应 FTP 服务请求。在这里， HTTP 服务和 FTP 服务均对应着监听不同 VIP 的 Virtual Server 。在由一个 Active Router 和一个 Backup Router 组成的 Keepalived 负载均衡架构中， Active Router 的主要任务就是将 VIP 上的请求转发到选中的某个后端服务器上，具体服务器的选举机制则由 Keepalived 所支持的负载均衡算法来决定。&lt;/p>
&lt;p>此外， Active Router 还负责动态监控后端服务器上特定服务的健康状况，监控方式主要是 Keepalived 自带的三种健康检测机制，即简单 TCP 连接、HTTP 和 HTTPS。就简单 TCP 连接检测方式， Active Router 会周期性地对服务器上某个特定端口进行 TCP 连接，如果 TCP 连接超时或者中断则认为服务不可用，而对于 HTTP 和 HTTPS 检测方式， ActiveRouter 通过周期性地抓取（ Fetch ）请求 URL 并验证其内容来判断服务的可用性。与此同时， Backup Router 一直处于 Standby 状态， LVS router 的 Failover 由 VRRP 来处理。&lt;/p>
&lt;p>在 Keepalived 进程启动的时候，所有 LVS Routers 会加人一个用来接收和发送 VRRP 广播的多播组， 由于 VRRP 是一种基于优先级的协议，因此在启动之初优先级高的 LVS Router 会被选举为 Master Router ，而 Master Router 将会周期性地向多播组中的成员发送 VRRP 广播。如果多播组中的 Backup Routers 在一定时间内接收 VRRP 广播失败，则重新选举新的 Master Router ，新的 Master Router 将会接管 VIP 并广播地址解析协议（ Address ResolutionProtocol, ARP ）信息。而当故障 Router 重新恢复后，根据该 Router 的优先级情况，其可能恢复到 Master 状态也可能保持为 Backup 状态。&lt;/p>
&lt;p>图中的两层负载均衡架构是最常见的部署环境，主要用于很多数据源变化不是很频繁的数据请求服务中，如静态 Web 页面站点，因为后端独立服务器（Real Severs ）之间不会自动进行数据同步。图 3-5 为基于 Keepalived 的三层负载均衡架构，在三层负载均衡架构中，前端的 LVS Router 负责将访问请求转发到物理服务器（ Real Servers ）中，然后 Real Server 再通过网络形式访问可共享的数据源。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/watgs2/1616132514381-c261d4f3-8895-48fb-b4c8-ea8a73ab2882.jpeg" alt="">&lt;/p>
&lt;p>对于数据请求比较繁忙的 FTP 站点，三层架构是最为理想的负载均衡架构，在这种架构下，可供访问的数据源集中存储在高可用的集群服务器上， Real Servers 通过 NFS 共享目录或者 Samba 文件共享等网络文件系统形式来访问数据。此外，类似的三层负载均衡架构在需要提供中心化及数据库事务处理高可用的 Web 站点中也被普遍使用，如果将 Keepalived 负载均衡器配置为 Active/Active 双活模式，则还可以将三层负载均衡架构同时用于提供 FTP 和 Web 数据库服务。&lt;/p>
&lt;h1 id="13-keepalived-的负载均衡算法">1.3 KeepAlived 的负载均衡算法&lt;/h1>
&lt;p>Keepalived 所使用的负载均调度机制由集成到内核中的 IPVS 模块提供， IPVS 是 LVS 项目的核心功能模块，其设计的主要目的之一就是解决单 IP 多服务器的工作环境，IPVS 模块使得基于 TCP/IP 传输层（ 第 4 层）的数据交换成为可能。在实际使用中， IPVS 会在内核中创建一个名为 IPVS Table 的表，该表记录了后端服务器的地址及服务运行状态，通过 IPVS Table, Keepalived 便可跟踪并将请求路由到后端物理服务器中， 即 LVS Router 利用此表将来自 Keepalived 虚拟服务器地址的请求转发到后端服务器池中，同时将后端服务器的处理结果转发给客户端。此外， IPVS table 的表结构主要取决于管理员对指定的虚拟服务器所设置的负载均衡算法， Keepalived 支持以下几种负载均衡算法。&lt;/p>
&lt;p>( 1 ) Round-Robin&lt;/p>
&lt;p>即所谓的轮询负载均衡，在这种算法中，服务请求会被依次转发到服务器池中的每一个服务器上，而不去评估服务器的当前负载或者处理能力，服务器池中的每一个服务器都被平等对待。如果使用 Round-Robin 负载均衡算法，每台后端服务器会轮询依次处理服务请求。&lt;/p>
&lt;p>( 2 ) Weighted Round-Robin&lt;/p>
&lt;p>即加权 Round-Robin 算法，是对 Round-Robin 算法的一种扩展。在这种算法中，请求被依次转发到每一台服务器上，但是当前负载较轻或者计算能力较大的服务器会被转发更多的请求，服务器的处理能力通过用户指定的权重因子来决定，权重因子可以根据负载信息动态上调或者下调。如果服务器的配置差别较大，导致不同服务器的处理能力相差较大，则加权的 Round-Robin 算法会是不错的选择，但是如果请求负载频繁变动，则权重较大的服务器可能会超负荷工作。&lt;/p>
&lt;p>( 3 ) Least-Connection&lt;/p>
&lt;p>即最少连接算法，在这种算法中，请求被转发到活动连接较少的服务器上。在 Keepalived 的实际使用中， LVS Router 一直在利用内核中的 IPVS Table 来记录后端服务器的活动连接，从而动态跟踪每个服务器的活动连接数。最少连接数算法是一种动态决策算法，它比较适合服务器池中每个成员的处理能力都大致相当，同时负载请求又频繁变化的场景， 如果不同服务器有不同的处理能力，则下面的加权最少连接数算法较为合适。&lt;/p>
&lt;p>( 4 ) Weighted Least-Connections&lt;/p>
&lt;p>即加权最少连接数算法，在这种算法中，路由会根据服务器的权重，转发更多的请求到连接数较少的服务器上。服务器的处理能力通过用户指定的权重因子来决定，权重因子可以根据负载信息动态上调或者下调。一般来说，服务器加权算法主要用于集群存在不同类型服务器，而服务器配置和处理能力相差较大的场景中。&lt;/p>
&lt;p>( 5) Destination Hash ScheduIing&lt;/p>
&lt;p>即目标地址哈希算法，通过在静态 Hash 表中查询目的 IP 地址来确定请求要转发的服务器，这类算法主要用于缓存代理服务器集群中。&lt;/p>
&lt;p>( 6 ) Source Hash Scheduling&lt;/p>
&lt;p>即源地址哈希算法，通过在静态 Hash 表中查询源 IP 地址来确定请求要转发的服务器，这类算法主要应用于存在多防火墙的 LVS Router 中。&lt;/p>
&lt;p>( 7 ) Shortest Expected Delay&lt;/p>
&lt;p>即最小延时算法，在这种算法中，请求被转发到具有最小连接响应延时的服务器上。&lt;/p>
&lt;h1 id="14-keepalived-路由方式">1.4 Keepalived 路由方式&lt;/h1>
&lt;p>（1） NAT&lt;/p>
&lt;p>图 3-6 为基于 NAT 路由实现的 Keepalived 负载均衡器，在 NAT 机制下，每个 LVS Router 需要两个网络接口。假设 eth0 为接人 Internet 的网络接口，则 eth0 上配置有一个真实的 IP 地址，同时还配置了一个浮动 IP 地址（Floating IP ）假设 eth1 为接入后端私有网络的接口， 则 eth1 上也配置有一个真实 IP 地址和一个浮动 IP 地址。在出现故障切换 Failover 的时候， 接人 Internet 的虚拟接口和接入私有网络的虚拟接口会同时切换到 Backup 的 LVSRouter 上，而为了不影响对 Internet 客户端的请求响应，位于私有网络中的后端服务器均使用 NAT 路由的浮动 IP 作为与主 LVS Router 通信的默认路由。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/watgs2/1616132514427-959ff4e7-b87e-4b5f-a914-d6b110653af9.jpeg" alt="">&lt;/p>
&lt;p>对外提供服务的公有 VIP(Public Virtual IP Address ）和私有 NAT VIP(NAT Virtual IP Address）均被配置在物理网卡上而最佳的配置方式是将两个 VIP 各自配置到不同的物理网卡上，即在这种配置下，每个 LVS Router 节点最多只需两个物理网卡。在 NAT 路由转发中，主 LVS Router 负责接收请求，并将请求的目的地址替换成 LVS Router 的 NAT Virtual IP 地址，再将其转发到选中的后端服务器上，同时服务器处理后的应答数据也通过 LVS Router 将其地址替换成 LVS Router 的 Public Virtual IP 地址，然后再转发给 Internet 客户端，这个过程也称为 IP 伪装，因为对客户端而言，服务器的真实 IP 地址已被隐藏。&lt;/p>
&lt;p>在 NAT 路由实现的负载均衡中，后端服务器上可以运行各种操作系统，即后端服务器上的操作系统类型并不影响 LVS Router 的 NAT 路由功能，但是，使用 NAT 路由方式存在的一个缺点是， LVS Router 在大规模集群部署中可能会是一个瓶颈，因为 LVS Router 要同时负责进出双向数据流的 IP 地址替换。&lt;/p>
&lt;p>（2） DR&lt;/p>
&lt;p>相对于其他的负载均衡网络拓扑， DR(Direct Routing）路由方式为基于 Keepalived 的负载均衡系统提供了更高的网络性能， DR 路由方式允许后端服务器直接将处理后的应答数据返回给客户端，而无需经过 LVS Router 的处理操作，DR 路由方案极大降低了 LVS Router 造成网络瓶颈的可能性。如图 3-7 所示。在基于 Keepalived 的负载均衡架构中， Keepalived 的最佳路由方式是 DR 路由，即在配置 Keepalived 的路由方式时，优先将其设置为 DR 。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/watgs2/1616132514433-6d061a2c-ec2a-43ea-a749-4acef265787f.jpeg" alt="">&lt;/p></description></item><item><title>Docs: 1.1.Rancher 介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.1.rancher-%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.1.rancher-%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;h1 id="rancher-介绍">Rancher 介绍&lt;/h1>
&lt;p>官方文档：&lt;a href="https://rancher.com/">https://rancher.com/&lt;/a>&lt;/p>
&lt;p>Rancher 是为使用容器的公司打造的容器管理平台。Rancher 简化了使用 Kubernetes 的流程，开发者可以随处运行 Kubernetes（Run Kubernetes Everywhere），满足 IT 需求规范，赋能 DevOps 团队。&lt;/p>
&lt;p>Rancher 在现阶段可以看作是一个解决方案，是一套产品的统称，这套产品包括如下几个：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>K3S # 用于运行高可用 Rancher 的底层平台。是一个轻量的 kubernetes，一个 k3s 二进制文件即可包含所有 kubernetes 的主要组件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Rancher Server # Rancher 管理程序，常部署于 k3s 之上，用来管理其下游 k8s 集群。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RKE # Rancher 创建的 kubernetes 集群。是一个可以通过名为 rke 的二进制文件以及一个 yaml 文件，即可启动 kubernetes 集群的引擎。RKE 与 kubernetes 的关系，类似于 docker 与 containerd 的关系。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="rancher-server-介绍">Rancher Server 介绍&lt;/h2>
&lt;p>Rancher Server 由认证代理(Authentication Proxy)、Rancher API Server、集群控制器(Cluster Controller)、数据存储(比如 etcd、mysql 等)和集群代理(Cluster Agent) 组成。除了 Cluster Agent 以外，其他组件都部署在 Rancher Server 中。(这些组件都集中在一起，一般可以通过 docker 直接启动一个 Rancher Server。)&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kxmsmg/1616114814016-9de5267d-0813-4790-826c-7c4448e40861.png" alt="">&lt;/p>
&lt;p>Rancher Server 可以管理多种 k8s 集群&lt;/p>
&lt;ol>
&lt;li>
&lt;p>通过 Rancher Server 来创建一个 RKE 集群&lt;/p>
&lt;/li>
&lt;li>
&lt;p>托管的 kubernetes 集群。e.g.Amazon EKS、Azure AKS、Google GKE 等等&lt;/p>
&lt;/li>
&lt;li>
&lt;p>导入已有的 kubernetes 集群。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="rancher-与下游集群交互的方式">Rancher 与下游集群交互的方式&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kxmsmg/1616114813966-db373999-6c8f-4541-a09f-5f20eaa656ce.png" alt="">&lt;/p>
&lt;p>通过 Rancher 管理的 kubernetes 集群(不管是导入的还是通过 Rancher 创建的)，都会在集群中部署两种 agent，来与 Rancher 进行交互。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cattle-cluster-agent # 上图中的 Cluster Agent。用于本集群与 Rancher Server 的 Cluster Controller(集群控制器)的通信&lt;/p>
&lt;/li>
&lt;li>
&lt;p>连接 Rancher 与本集群的 API Server&lt;/p>
&lt;/li>
&lt;li>
&lt;p>管理集群内的工作负载，比如 Rancher Server 下发一个部署 pod 的任务，集群代理就会与本集群 API 交互来处理任务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据每个集群的设置，配置 Role 和 RoleBindings&lt;/p>
&lt;/li>
&lt;li>
&lt;p>实现集群和 Rancher Server 之间的消息传输，包括事件，指标，健康状况和节点信息等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cattle-node-agent # 上图中的 Node Agent。用于处理本节点的任务，比如升级 kubernetes 版本以及创建或者还原 etcd 快照等等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Note：如果 Cluster Agent 不可用，下游集群中的其中一个 Node Agent 会创建一个通信管道，由节点 Agent 连接到集群控制器，实现下游集群和 Rancher 之间的通信。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一般使用 DaemonSet 的方式部署到集群中，以保证每个节点都有一个代理可以执行 Rancher Server 下发的任务。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="rancher-配置">Rancher 配置&lt;/h1>
&lt;p>Rancher 套件中的各组件配置详见各自组件配置详解&lt;/p>
&lt;h2 id="k3s-配置">K3S 配置&lt;/h2>
&lt;h2 id="rancher-server-配置">Rancher Server 配置&lt;/h2>
&lt;h2 id="rancher-创建的集群配置">Rancher 创建的集群配置&lt;/h2>
&lt;p>Rancher 创建的集群是为 RKE 集群，配置详见：RKE 配置详解&lt;/p></description></item><item><title>Docs: 1.1.Redis 高可用概述</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/1.1.redis-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/1.1.redis-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%A6%82%E8%BF%B0/</guid><description>
&lt;p>参考：&lt;a href="http://blog.jboost.cn/redis-cluster.html">原文&lt;/a>、&lt;a href="https://zhuanlan.zhihu.com/p/129640817">知乎&lt;/a>、&lt;a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247492218&amp;amp;idx=1&amp;amp;sn=8e233cf3c3abd0e6821262d1c78b03d1&amp;amp;chksm=eac6c353ddb14a4535ae92f911ae846592066bf9250b9aca4d91287015de8b23347cb56838ce&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=1123S90JeR1nwKagOdJBlQTz&amp;amp;sharer_sharetime=1606090661231&amp;amp;sharer_shareid=09464b4a0389b967659ba78076a1ef58&amp;amp;key=48765414c6ece973eab40bd813ac84f36f02f86a2b622e5efcf4fb98864fdb95a49286689ce7513bf3642cc2a2b6d4bd4235c0de418caee7320bf928f76e57b65a7c762b34b8e80ed88a847524e64e1d6e08271ee31db62eb4ff0bce50cdc03f2b4161ba26993d58b1f19972154a004ce4cadfca3a79f41936616473cacfeff7&amp;amp;ascene=1&amp;amp;uin=MTI5NTMzMzA0MQ%3D%3D&amp;amp;devicetype=Windows+10+x64&amp;amp;version=6300002f&amp;amp;lang=zh_CN&amp;amp;exportkey=AanBDsftQoVwZuEgK1A5lo8%3D&amp;amp;pass_ticket=S1%2BMQ7vYsbZSNuJMJOqmLPRlt4Y3dQwyWaD%2FAmp3sq1Yd7omWT6hEhxkL9s%2BaMxR&amp;amp;wx_header=0">微信公众号&lt;/a>&lt;/p>
&lt;p>&lt;strong>Redis 支持三种高可用方案&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/33187771">Replication(复制) 模式&lt;/a>。
&lt;ul>
&lt;li>实际上，该模式并不是绝对的高可用，仅仅保证了数据的不丢失&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.yuque.com/go/doc/33187731">Sentinel(哨兵) 模式&lt;/a>&lt;/li>
&lt;li>Cluster(集群) 模式&lt;/li>
&lt;/ul>
&lt;h1 id="cluster集群">Cluster(集群)&lt;/h1>
&lt;p>&lt;a href="http://www.redis.cn/topics/cluster-tutorial.html">http://www.redis.cn/topics/cluster-tutorial.html&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://redis.io/topics/cluster-tutorial">https://redis.io/topics/cluster-tutorial&lt;/a>&lt;/p>
&lt;h2 id="客户端操作原理请求路由原理">客户端操作原理(请求路由原理)&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gnlint/1616134822977-6bf24d85-a95a-4729-8807-b5c63d74e9a7.jpeg" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gnlint/1616134822986-56d8d02c-0433-4df5-a829-8e7fb0435fb0.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>请求重定向&lt;/li>
&lt;/ol>
&lt;p>在集群模式下，Redis 接收任何键相关命令时首先计算键对应的槽，再根据槽找出所对应的节点，如果节点是自身，则处理键命令；否则回复 MOVED 重定向错误，通知客户端请求正确的节点。这个过程称为 MOVED 重定向。&lt;/p>
&lt;pre>&lt;code># 如果key经过计算后，其分配的slot就在当前节点，那么可以请求成功，否则，回复重定向消息
[root@node01 redis]# redis-cli -h 10.0.0.100 -p 6379
10.0.0.100:6379&amp;gt; set name tom
OK
10.0.0.100:6379&amp;gt; set age 20
(error) MOVED 741 10.0.0.101:6379
&lt;/code>&lt;/pre>
&lt;p>重定向信息包含了键所对应的槽以及负责该槽的节点地址，根据这些信息客户端就可以向正确的节点发起请求。在 10.0.0.101:6379 节点上成功执行之前的命令：&lt;/p>
&lt;pre>&lt;code>[root@node02 redis]# redis-cli -h 10.0.0.101 -p 6379
10.0.0.101:6379&amp;gt; set age 20
OK
&lt;/code>&lt;/pre>
&lt;p>使用 redis-cli 命令时，可以加入-c 参数支持自动重定向，简化手动发起重定向的操作：&lt;/p>
&lt;pre>&lt;code>[root@node01 redis]# redis-cli -c -h 10.0.0.100 -p 6379
10.0.0.100:6379&amp;gt; set age 30
-&amp;gt; Redirected to slot [741] located at 10.0.0.101:6379
OK
&lt;/code>&lt;/pre>
&lt;p>redis-cli 自动帮我们连接到正确的节点执行命令，这个过程是在 redis-cli 内部维护，实质上是 client 端接到 MOVED 信息指定的节点之后再次发起请求，并不是在当前 Redis 节点中完成请求转发，节点对于不属于它的键命令只回复重定向响应，并不负责转发。&lt;/p>
&lt;p>键命令执行步骤主要分两步：&lt;/p>
&lt;ol>
&lt;li>计算槽&lt;/li>
&lt;/ol>
&lt;p>Redis 首先需要计算键所对应的槽，根据键的有效部分使用 CRC16 函数计算出散列值，再取对 16383 的余数，得到槽的编号，这样每个键都可以映射到 0~16383 槽范围内&lt;/p>
&lt;pre>&lt;code>10.0.0.101:6379&amp;gt; cluster keyslot age
(integer) 741
&lt;/code>&lt;/pre>
&lt;p>Redis 集群相对单机在功能上存在一些限制，限制如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>key 批量操作支持有限，如 mset、mget，目前只支持具有相同 slot 值的 key 执行批量操作。对于映射为不同 slot 值的 key 由于执行 mget、mget 等操作可能存在于多个节点上因此不被支持&lt;/p>
&lt;/li>
&lt;li>
&lt;p>key 事务操作支持有限，同理只支持多 key 在同一节点上的事务操作，当多个 key 分布在不同的节点上时无法使用事务功能&lt;/p>
&lt;/li>
&lt;li>
&lt;p>key 作为数据分区的最小粒度，因此不能将一个大的键值对象如 hash、list 等映射到不同的节点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不支持多数据库空间，单机下的 Redis 可以支持 16 个数据库，集群模式下只能使用一个数据库空间，即 db0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>10.0.0.102:6379&amp;gt; mget name age
(error) CROSSSLOT Keys in request don't hash to the same slot
&lt;/code>&lt;/pre>
&lt;p>但通常会有这样的需求，例如把一个用户的信息存入到一个 slot 中，这是可以这样设置：&lt;/p>
&lt;pre>&lt;code>10.0.0.102:6379&amp;gt; set user:{user1}:name tony
-&amp;gt; Redirected to slot [8106] located at 10.0.0.100:6379
OK
10.0.0.100:6379&amp;gt; set user:{user1}:age 20
OK
10.0.0.100:6379&amp;gt; cluster keyslot user:{user1}:name
(integer) 8106
10.0.0.100:6379&amp;gt; cluster keyslot user:{user1}:age
(integer) 8106
10.0.0.100:6379&amp;gt; mget user:{user1}:name user:{user1}:age
1) &amp;quot;tony&amp;quot;
2) &amp;quot;20&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>这样，这两个 key 在计算 hash 值的时候，不会根据整个 key 来计算，而是只是拿{}中的内容的来计算，这样它们的 hash 值一定是相同的，就可以分配到同一个 slot 中，{}中的内容称为 hash_tag&lt;/p>
&lt;ol>
&lt;li>查找槽所对应的节点&lt;/li>
&lt;/ol>
&lt;p>Redis 计算得到键对应的槽后，需要查找槽所对应的节点。集群内通过消息交换每个节点都会知道所有节点的槽信息。&lt;/p>
&lt;p>根据 MOVED 重定向机制，客户端可以随机连接集群内任一 Redis 获取键所在节点，这种客户端又叫 Dummy(傀 儡)客户端，它优点是代码实现简单，对客户端协议影响较小，只需要根据重定向信息再次发送请求即可。但是它的弊端很明显，每次执行键命令前都要到 Redis 上进行重定向才能找到要执行命令的节点，额外增加了 IO 开销，这不是 Redis 集群高效的使用方式。正因为如此通常集群客户端都采用另一种实现：Smart 客户端&lt;/p>
&lt;h2 id="cluster-模式的优缺点">Cluster 模式的优缺点&lt;/h2>
&lt;p>优点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>无中心架构，数据按照 slot 分布在多个节点。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>集群中的每个节点都是平等的关系，每个节点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>可线性扩展到 1000 多个节点，节点可动态添加或删除&lt;/p>
&lt;/li>
&lt;li>
&lt;p>能够实现自动故障转移，节点之间通过 gossip 协议交换状态信息，用投票机制完成 slave 到 master 的角色转换&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>缺点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>客户端实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时更新，提高了开发难度。目前仅 JedisCluster 相对成熟，异常处理还不完善，比如常见的“max redirect exception”&lt;/p>
&lt;/li>
&lt;li>
&lt;p>节点会因为某些原因发生阻塞（阻塞时间大于 cluster-node-timeout）被判断下线，这种 failover 是没有必要的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据通过异步复制，不保证数据的强一致性&lt;/p>
&lt;/li>
&lt;li>
&lt;p>slave 充当“冷备”，不能缓解读压力&lt;/p>
&lt;/li>
&lt;li>
&lt;p>批量操作限制，目前只支持具有相同 slot 值的 key 执行批量操作，对 mset、mget、sunion 等操作支持不友好&lt;/p>
&lt;/li>
&lt;li>
&lt;p>key 事务操作支持有线，只支持多 key 在同一节点的事务操作，多 key 分布不同节点时无法使用事务功能&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不支持多数据库空间，单机 redis 可以支持 16 个 db，集群模式下只能使用一个，即 db 0&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Redis Cluster 模式不建议使用 pipeline 和 multi-keys 操作，减少 max redirect 产生的场景。&lt;/p>
&lt;h1 id="sentinel-与-cluster-的区别">Sentinel 与 Cluster 的区别&lt;/h1>
&lt;p>……我感觉楼主自己对 redis 的理解是有一定问题的，但提的问题其实是个好问题，而回帖的大部分人没有回应准确楼主的疑问，少部分评论我看一眼就明白，但楼主可能是想不到的。&lt;/p>
&lt;p>那咱们把集群和哨兵能解决的问题列出来，就比较清楚了。&lt;/p>
&lt;p>哨兵： 哨兵仅仅提供故障切换能力，在这之上，对使用方来说，和单机的 redis 是完全一样的。&lt;/p>
&lt;p>集群： 集群最主要的，解决的是一个“数据分片”的问题，它能把 redis 的数据分散到不同的 slot 里，而不是都集中在一台机器的内存里。这样也就给单进程单线程、纯内存的 redis 提供了水平扩容的能力。&lt;/p>
&lt;p>但是这是有代价的， 一部分命令无法跨节点执行，比如 zunionstore 等一些命令，它涉及多个 key，因此在集群状态下，需要自行保证这些 key 都在一个 slot 上；&lt;/p>
&lt;p>再比如 watch exec， 在单节点或哨兵场景下可以用，但集群模式下是不能使用的。&lt;/p>
&lt;p>还有一些命令，在集群状态下虽能执行或有替代方案，但会丧失原子性。 比如 mget 等。&lt;/p>
&lt;p>所以楼主的疑问是为什么集群模式没有取代哨兵模式，是因为哨兵模式作为单节点+高可用的方案而言，确实有集群模式实现不了的功能。&lt;/p>
&lt;p>……想换行不小心发出去了。&lt;/p>
&lt;p>除了功能上的区别以外，集群模式显然比哨兵模式更重、需要更多的资源去运行；再就是部署运维复杂度也是更高的。&lt;/p>
&lt;p>而哨兵和单节点，一般来说除了配置稍有区别以外，绝大部分业务代码是可以相容的，无需特地修改。&lt;/p>
&lt;p>而现有的代码如果使用了集群模式不支持的那些命令，那么集群模式下是无法正常工作的。&lt;/p>
&lt;p>所以目前哨兵模式仍然被广泛使用，没有被集群模式彻底替代。&lt;/p>
&lt;p>我们公司就是用哨兵了。为什么不用 Cluster 。因为费钱。集群需要机器太多了。本身数据量就不大。分片功能不需要。 就只是想要一个高可用的 redis 。 用哨兵符合需求了。 只需要三台机器。而且三台机器还部署了 3 个 zookeeper 和 kafka 。都是数据量不大。 节约机器钱&lt;/p></description></item><item><title>Docs: 1.1.虚拟化</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_getting_started_guide/index">RedHat 7 虚拟化入门指南&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/virtualization-in-rhel-8-an-overview_configuring-and-managing-virtualization#what-is-virtualization-in-rhel-8-virt-overview">Redhat 8 官方对虚拟化的定义&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_getting_started_guide/index">RedHat 7 对“虚拟化性能不行”这个误区的辟谣&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ubuntu.com/server/docs/virtualization-introduction">Ubuntu 官方文档，虚拟化-介绍&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Virtualization(虚拟化)&lt;/strong> 是用于运行软件的广义的计算机术语。通常情况下，&lt;strong>Virtualization(虚拟化)&lt;/strong> 体现在让单个可以运行多个操作系统，这些操作系统同时运行，而又是互相独立的。&lt;/p>
&lt;p>虚拟化是云计算的基础。简单的说，虚拟化使得在一台物理的服务器上可以跑多台虚拟机，虚拟机共享物理机的 CPU、内存、IO 硬件资源，但逻辑上虚拟机之间是相互隔离的。物理机我们一般称为 &lt;strong>Host(宿主机)&lt;/strong>，宿主机上面的虚拟机称为 &lt;strong>Guest(客户机)&lt;/strong>。那么 Host 是如何将自己的硬件资源虚拟化，并提供给 Guest 使用的呢？这个主要是通过一个叫做 Hypervisor 的程序实现的。&lt;/p>
&lt;h2 id="hypervisor">Hypervisor&lt;/h2>
&lt;p>参考：&lt;a href="https://www.redhat.com/zh/topics/virtualization/what-is-a-hypervisor">https://www.redhat.com/zh/topics/virtualization/what-is-a-hypervisor&lt;/a>&lt;/p>
&lt;p>Hypervisor 是用来创建与运行虚拟机的软件、固件或硬件。被 Hypervisor 用来运行一个或多个虚拟机的设备称为 Host Machine(宿主机)，这些虚拟机则称为 Guest Machine(客户机)。&lt;strong>Hypervisor 有时也被称为 Virtual Machine Monitor (虚拟机监视器，简称 VMM)&lt;/strong>&lt;/p>
&lt;h1 id="虚拟化技术的分类">虚拟化技术的分类&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ihdpea/1616124416735-5e89f29f-21cd-4fed-af5e-194227de3048.png" alt="">
根据 Hypervisor 的实现方式和所处的位置，虚拟化又分为两种：1 型虚拟化和 2 型虚拟化&lt;/p>
&lt;ol>
&lt;li>半虚拟化（para-virtualization）：TYPE1，也叫裸金属虚拟化比如 Vmware ESXi、Xen 等是一款类似于操作系统的 Hypervisor，直接运行在硬件之上，需要修改 Guest OS 的内核，让 VM 知道自己是虚拟机&lt;/li>
&lt;li>完全虚拟化（full-virtualization）：TYPE2，物理机上首先安装常规的操作系统，比如 Redhat、Ubuntu 和 Windows。Hypervisor 作为 OS 上的一个程序模块运行，并对管理虚拟机进行管理。比如 Vmware Workstation、KVM 等是一款类似于软件的 Hypervisor，运行于操作系统之上，VM 不知道自己是虚拟机
&lt;ol>
&lt;li>BT：软件，二进制翻译。性能很差&lt;/li>
&lt;li>HVM：硬件，硬件辅助的虚拟化。性能很好。现阶段 KVM 主要基于硬件辅助进行虚拟化
&lt;ol>
&lt;li>&lt;strong>硬件辅助全虚拟化主要使用了支持虚拟化功能的 CPU 进行支撑，CPU 可以明确的分辨出来自 GuestOS 的特权指令，并针对 GuestOS 进行特权操作，而不会影响到 HostOS。&lt;/strong>&lt;/li>
&lt;li>从更深入的层次来说，虚拟化 CPU 形成了新的 CPU 执行状态 —— _ Non-Root Mode&amp;amp; Root Mode_ 。从上图中可以看见，GuestOS 运行在 Non-Root Mode 的 Ring 0 核心态中，这表明 GuestOS 能够直接执行特却指令而不再需要 &lt;em>特权解除&lt;/em> 和 &lt;em>陷入模拟&lt;/em> 机制。并且在硬件层上面紧接的就是虚拟化层的 VMM，而不需要 HostOS。这是因为在硬件辅助全虚拟化的 VMM 会以一种更具协作性的方式来实现虚拟化 —— &lt;em>将虚拟化模块加载到 HostOS 的内核中&lt;/em>，例如：KVM，KVM 通过在 HostOS 内核中加载&lt;strong>KVM Kernel Module&lt;/strong>来将 HostOS 转换成为一个 VMM。所以此时 VMM 可以看作是 HostOS，反之亦然。这种虚拟化方式创建的 GuestOS 知道自己是正在虚拟化模式中运行的 GuestOS，KVM 就是这样的一种虚拟化实现解决方案。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>OS 级别虚拟化：容器级虚拟化，准确来说不能叫虚拟化了，只能叫容器技术无 Hypervisor，将用户空间分隔为多个，彼此互相隔离，每个 VM 中没有独立内核，OpenVZ、LXC(Linux container)、libcontainer 等，比如 Docker，Docker 的基础是 LXC。&lt;/li>
&lt;li>模拟(Emulation)：比如 QEMU，PearPC，Bochs&lt;/li>
&lt;li>库虚拟化：WINE&lt;/li>
&lt;li>应用程序虚拟化：JVM&lt;/li>
&lt;li>理论上 Type1 和 Typ2 之间的区别
&lt;ol>
&lt;li>1 型虚拟化一般对硬件虚拟化功能进行了特别优化，性能上比 2 型要高；&lt;/li>
&lt;li>2 型虚拟化因为基于普通的操作系统，会比较灵活，比如支持虚拟机嵌套。嵌套意味着可以在 KVM 虚拟机中再运行 KVM。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h1 id="虚拟化总结云计算基础实现云功能的灵活调度">虚拟化总结(云计算基础，实现云功能的灵活调度)&lt;/h1>
&lt;p>所谓的云计算：当一台虚拟机需要跨越多个物理机进行数据交互，比如拿来运行 VM 的物理主机不止一台，在每台物理机上按需启动既定数量的 VM，每个 VM 有多少 CPU 和 MEM，每个 VM 启动在哪个物理机上，启动 VM 需要的存储设备在什么地方，存储设备中的系统是临时安装，还是通过一个已经装好的系统模板直接使用，还有多个 VM 跨物理主机进行网络通信等等一系列工作，可以使用一个虚拟化管理工具(VM Manager)来实现，这个管理器的功能即可称为云计算。在没有这个管理器的时候，人们只能人为手工从把 VM 从一台物理机移动到另一台物理机，非常不灵活。&lt;/p>
&lt;p>计算机五大部件：运算器(cpu)，控制器(cpu)，存储器(memory)，输入与输出设备(磁盘 I/O，网络 I/O)。&lt;/p>
&lt;p>一般情况，VM 的 CPU 与 Memory 无法跨主机使用；但是磁盘 I/O 与网络 I/O 则可以跨主机使用。云计算的灵活性（即 VM 或者单个云计算节点挂了但是不影响数据，可以重新启动在任一一个节点等类似的功能）&lt;/p>
&lt;p>磁盘 I/O 的灵活调度&lt;/p>
&lt;p>所以，在启动一个 VM 的时候，分为这么几个启动步骤，模拟 CPU 和内存，模拟存储，模拟网络。当在多个 node 的虚拟化集群中创建完一个 VM 并想启动的时候，又分为两种情况：&lt;/p>
&lt;ol>
&lt;li>当该 VM 的虚拟存储放在某个节点上的时候，则该 VM 只能启动在该节点上，因为没有存储就没法加载系统镜像，何谈启动呢&lt;/li>
&lt;li>当该 VM 的虚拟存储放在虚拟化集群的后端存储服务器或者共享存储空间的时候，则该 VM 可以根据调度策略在任一节点启动,然后把该 VM 对应的虚拟存储挂载或下载到需要启动的节点上即可（这个所谓的虚拟存储，可以称为模板，每次 VM 启动的时候，都可以通过这个模板直接启动而不用重新安装系统了）&lt;/li>
&lt;/ol>
&lt;p>这种可以灵活调度 VM，而不让 VM 固定启动在一个虚拟机上的机制，这就是云功能的基础，用户不用关心具体运行在哪个节点上，都是由系统自动调度的。&lt;/p>
&lt;p>网络 I/O 的灵活调度&lt;/p>
&lt;p>同样的，在一个 VM 从 node1 移动到 node2 的时候，除了存储需要跟随移动外，还需要网络也跟随移动，移动的前提是所有 node 的网络配置是一样的，不管是隔离模型，还是路由模型，还是 nat 模型，还是桥接模型，都需要给每个 node 进行配置，但是，会有这么几个情况，&lt;/p>
&lt;ol>
&lt;li>一个公司，有 2 个部门，有两台物理 server，node1 最多有 4 个 VM，node2 最多有 4 个 VM，其中一个部门需要 5 台 VM，另一个部门需要 3 台 VM，而两个部门又要完全隔离，这时候可以通过对 vSwitch 进行 vlan 划分来进行隔离，。这时候就一个开源的软件应运而生，就是 Open vSwtich，简称为 OVS。&lt;/li>
&lt;li>普通 VLAN 只有 4096 个，对于公有云来说，该 vlan 数量远远不够，这时候，vxlan 技术应运而生&lt;/li>
&lt;li>每个公司有多个部门,每个部门有的需要连接公网，有的不需要连接公网,如果想隔离开两个公司，仅仅依靠虚拟交换机从二层隔离，无法隔离全面，这时候 vRouter 虚拟路由器技术应运而生，通过路由来隔离，并通过路由来访问，而这台 vRouter 就是由 linux 的 net namespace 功能来创建的&lt;/li>
&lt;/ol>
&lt;p>Openstack 中创建的每个 network 就相当于一个 vSwitch，创建的每个 route 就相当于一个 vRoute 即 net namespace，然后把 network 绑定到 route 上，就相当于把 vSwitch 连接到了 vRoute，所以，在绑定完成之后，会在 route 列表中看到个端口的 IP，这个 IP 就是 vSwitch 子网(在创建 vSwitch 的时候会设置一个可用的网段)中的一个 IP，就相当于交换机连到路由器后，路由器上这个端口的 IP&lt;/p>
&lt;h1 id="实际上一个虚拟机就是宿主机上的一个文件虚拟化程序可以通过这个文件来运行虚拟机">实际上，一个虚拟机就是宿主机上的一个文件，虚拟化程序可以通过这个文件来运行虚拟机&lt;/h1></description></item><item><title>Docs: 1.2.Keepalived 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/1.2.keepalived-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/1.2.keepalived-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="keepalived-使用">Keepalived 使用&lt;/h1>
&lt;p>keepalived 启动流程：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>启动三个进程（主进程、healthcheck 进程、vrrp 进程）之后，先进入 backup 状态，运行一次 vrrp_script 成功后发现没有主，这时候会进入 master 状态，拉起 VIP，完成启动。 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>切换的流程：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>原 keepalived master 节点，运行检查脚本异常，则 keepalived 进入 FAULT 状态，释放 vip，原 backup 的 keepalived 会接管 VIP。 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>注意事项：VIP 必须在 master 上。为了这个要求，需要在主库上先启动 keepalived。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="keepalived-配置">Keepalived 配置&lt;/h1>
&lt;p>/etc/sysconfig/keepalived #keepalived 运行时参数配置&lt;/p>
&lt;p>/etc/keepalived/keepalived.conf #keepalived 基本配置文件&lt;/p>
&lt;ol>
&lt;li>Note：keepalived 配置文件的运行时加载功能，可以通过命令 kill -HUP $(cat /var/run/keepalived.pid) 实现。该功能需要在 1.2.20 及以上版本才能实现&lt;/li>
&lt;/ol>
&lt;p>下面是一个基本的 keepalived.conf 文件的配置示例&lt;/p>
&lt;pre>&lt;code>! Configuration File for keepalived
global_defs { #全局配置段
notification_email {
admin@example. com
}
notification_email_from noreply@example.com
smtp_server 127.0.0.1
smtp_connect_timeout 60
router_id LVS_DEVEL
script_user root
}
vrrp_script chk_haproxy {
script &amp;quot;killall -0 haproxy&amp;quot;
interval 1
weight 2
}
vrrp_sync_group VG1 { #VRRP组配置段
group {
VI_1
VI_2
}
}
vrrp_instance VI_1 { #VRRP 实例VI_1配置段
state MASTER
interface eth0
virtual_router_id 50
priority 100
advert_int 1
authentication {
auth_type PASS
auth_pass password123
}
virtual_ipaddress {
10.0.0.1
}
track_script {
chk_haproxy
}
}
vrrp_instance VI_2 { #VRRP 实例VI_2配置段
state MASTER
interface eth1
virtual_router_id 2
priority 100
advert_int 1
authentication{
auth_type PASS
auth_pass password123
}
virtual_ipaddress {
192.168.1.1
}
notify_master &amp;quot;/etc/keepalived/notify.sh master&amp;quot; #定义该节点变为master后执行的脚本
notify_backup &amp;quot;/etc/keepalived/notify.sh backup&amp;quot; #定义该节点变为backup后执行的脚本
notify_fault &amp;quot;/etc/keepalived/notify.sh fault&amp;quot; #定义该节点变为fault后执行的脚本
}
virtual_server 10.0.0.1 80 { #虚拟服务器LVS 配置段
delay_loop 6
lvs_sched rr
lvs_method DR
protocol TCP
sorry_server 127.0.0.1 80
real_server 192.168.1.20 80 { #定义后端服务器1
XXX_CHECK {
connect timeout 10
}
}
real_server 192.168.1.21 80 { #定义后端服务器2
XXX_CHECK {
connect timeout 10
}
}
}
&lt;/code>&lt;/pre>
&lt;p>从 Keepalived 配置文件/etc/keepalived/keepalived.conf 中的内容可以看到， Keepalived 的配置主要分为三个模块， 即全局配置段、VRRP 定义段、虚拟服务器 LVS 配置段。&lt;/p>
&lt;p>配置文件 keywords(关键字) 详解&lt;/p>
&lt;p>注意：各个大版本之间的 keyword 有很大区别，名称以及所在位置都有区别，千万注意！！！！&lt;/p>
&lt;p>1.全局配置段&lt;/p>
&lt;p>全局配置段（ global_defs ）的主要作用之一就是 Keepalived 出现故障时的邮件通知管理员，让管理员以邮件形式知道 Keepalived 的运行情况。通常情况下，邮件通知不是必须的，用户可以选择其他监控方式来对 Keepalived 进行监控，如 Nagios。需要说明的是，全局配置段对 Keepalived 来说是可选的，其内容并不是 Keepalived 配置所必须的。全局配置段的几个主要配置参数说明如下：&lt;/p>
&lt;p>&lt;strong>global_defs&lt;/strong> #&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Notification_email # 用于配置接收邮件的负载均衡器的管理员群组邮箱。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Notification_email_from # 自定义发出邮件的邮箱地址，即管理员邮件显示的发件人。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>SMTP # 指定简单邮件参数协议服务器地址，一般为本机。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LVS_ID # LVS 负载均衡器标志，同一网络中其值唯一。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>script_user &lt;!-- raw HTML omitted -->&lt;/strong> # 指定 vrrp_script 定义的脚本&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>enable_script_security&lt;/strong> # 开启脚本安全&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>vrrp_script ScriptName { &amp;hellip; }&lt;/strong> # 定义检查脚本以便后面的 track_script 关键字来引用，若检查状态码为非 0 失败，则引用该脚本的 VRRP 变成 Fault 状态，若定义了 weight 字段且优先级比其余节点都低，则变为 Backup 状态&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>script &amp;ldquo;/PATH/FILE&amp;rdquo;&lt;/strong> #定义需要执行的脚本或者需要执行的脚本的路径&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>interval&lt;/strong> #脚本调用间隔的秒数，默认 1 秒&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>timeout&lt;/strong> #定义调用失败多少秒后，确认该脚本失败&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>weight&lt;/strong> #脚本执行失败后，调整调用了该脚本的 vrrp 的优先级(priority)。i.e.降低或者提高优先级的数值，INTEGER 值为-254 到 254 之间&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>rise&lt;/strong> # 判断服务正常的检查次数，正常多少次，会进行状态转变&lt;/p>
&lt;/li>
&lt;li>
&lt;p>**fall **# 判断服务异常的检查次数，异常多少次，会进行状态转转变&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>user USERNAME [GROUPNAME]&lt;/strong> # 运行该脚本的用户&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>init_fail&lt;/strong> # 定义该脚本默认为失败状态&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>2. VRRP 配置段&lt;/p>
&lt;p>VRRP 配置段主要用于定义 VRRP 组，在 Keepalived 发生任何状态变化时，被定义在 VRRP 组中的 VRRP 实例作为逻辑整体一致行动，如在发生 LVS Router 故障切换 Failover 的过程中， VRRP 组中的实例会作为一致整体同时切换。在本节的演示配置中，同一个 VRRP 组内配置了两个 VRRP 实例，分别是针对外部网络的 VRRP_EXT 实例和针对内部私有网络的 VRRP_INT 实例。VRRP 配置段中的关键参数说明如下。&lt;/p>
&lt;p>&lt;strong>vrrp_sync_group {&amp;hellip;}&lt;/strong> # VRRP 实例一致组，用于定义 VRRP 一致组中的成员，组内的 VRRP 实例行为是一致的，如在 Failover 的时候， 一致组内的 VRRP 实例将同时迁移。在本机示例中，当 LBl 出现故障时， VRRP INT 和 VRRP EXT 实例将同时切换到 LB2 上。如果 不定义组，那么如果一台设备上有俩网卡的时候，只有一块网卡坏了的话，定义在另一块网卡上的 VRRP 则还在原来的设备上运行，无法自动切换到备用设备上。&lt;/p>
&lt;p>&lt;strong>vrrp_instance {&amp;hellip;}&lt;/strong> # VRRP 实例，用于配置一个 VRRP 服务进程实例，其中的 state 设定了当前节点 VRRP 实例的主备状态，在主 LVS Router 中，该值应该为 MASTER,在备 LVS Router 中，其值为 BACKUP 。正常情况下只有 Master 的 LVS Router 在工作， Backup 的 LVS Router 处于 Standby 状态。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>state&lt;/strong> # 当前节点的初始状态&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>interface&lt;/strong> # 对外提供服务的网络接口，如 eth0 和 eth1，选择服务接口时，一定要核实清楚，LV Router 的 VIP 将会配置到这个物理接口上。也可以配置多个实例在同一个网卡上，然后每个实例配置不同优先级，HOST1 上的实例 1 是主实例 2 是备，HOST2 上的实例 1 是备实例 2 是主，这样可以实现两台 HOST 双主模式负载均衡流量&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>virtual_Router_id&lt;/strong> # VRID，虚拟路由标志，同一个 VRRP 实例使用唯一的标识。即同一个 VRRP 实例中，MASTER 和 BACKUP 状态的 VRRP 实例中，VRID 值是相同的，同时在全部 VRRP 组内是唯一的。&lt;/p>
&lt;ul>
&lt;li>Note：如果在同网段有相同的 vrid 号，则 keepalived 会无限输出报错日志。使用 tcpdump -nn -i any net 224.0.0.0/8 |grep vrid 命令可以查到该网段都有哪些 vrid 号正在使用&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>priority&lt;/strong> # 此参数指明了该 VRRP 实例的优先级，数字越大说明优先级越高，取值范围为 0-255 ，在同一个 VRRP 实例里， MASTER 的优先级高于 BACKUP。若 MASTER 的 Priority 值为 100 ，那 BACKUP 的 Priority 只能是 99 或更小的数值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>nopreempt&lt;/strong> # 开启非抢占模式。允许低优先级的节点保持 MASTER 角色，即使高优先级的节点从故障中恢复也是如此。i.e.不会触发选举过程。只有当前 BACKUP 节点 认为 MASTER 不存在时，才会重新选举。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Note：该模式会引发这个问题 Keepalived 非抢占模式 VIP 不漂移问题&lt;/p>
&lt;ul>
&lt;li>如果想要使用非抢占模式，主备的 keepalived 的 state 都不能是 MASTER。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>advert_int&lt;/strong> # Master 路由发送 VRRP 广播的时间间隔，单位为秒。默认为 1 秒&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>authentication {&amp;hellip;}&lt;/strong> # 包含验证类型和验证密码，类型主要有 PASS 和 AH 两种，通常使用的类型为 PASS 验证密码为明文，同一 VRRP 实例 MASTER 与 BACKUP 使用相同的密码才能正常通信。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>auth_type PASS|AH&lt;/strong> #认证类型。Note：只能是 PASS 或 AH 选项，不能写别的，否则报错：unknown authentication type &amp;rsquo;lvs'&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>auth_pass PASSWORD&lt;/strong> #认证的密码&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>virtual_ipaddress {IP}&lt;/strong> # 虚拟 IP 地址，即 VIP，可以有多个虚拟 IP 、地址，每个地址占一行，不需要指定子网掩码。作为 Standby 的负载均衡器，LB2 的 keepalived.conf 配置文件与 LB1 类似，其不同之处在于 VRRP 实例配置段中的的 VRRP 实例 State 和 Priority 参数的设置，如 LB1 中的 State 为 Master, LB2 中的 State 为 BACKUP ，并且 LB2 中 VRRP 实例的 Priority 必须小于 LB1 中的优先级。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>track_script {ScriptName}&lt;/strong> # 引用全局配置段中 vrrp_script 关键字的名为 ScriptName 的脚本&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>notify_master &amp;ldquo;/PATH/ScriptName ARGS&amp;rdquo;&lt;/strong> # 定义该节点变为 master 后执行的脚本&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>notify_backup &amp;ldquo;/PATH/ScriptName ARGS&amp;rdquo;&lt;/strong> # 定义该节点变为 backup 后执行的脚本&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>notify_fault &amp;ldquo;/PATH/ScriptName ARGS&amp;rdquo;&lt;/strong> # 定义该节点变为 fault 后执行的脚本&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>3. 虚拟服务器 LVS 配置段&lt;/p>
&lt;p>虚拟服务器（ Virtual Server ）配置段主要定义 LVS 的监昕虚拟 IP 地址和对应的后端服务器及其健康检测机制，虚拟服务器的定义段是 Keepalived 框架最重要的部分。此部分的定义主要分为一个 Virtual Server 的定义和多个 Real Servers 的定义， Virtual Server 由 VRRP 中定义的 VIP 加上端口号构成，而 Real Server 由后端服务器节点 IP 和端口号构成，相关的配置参数说明如下。&lt;/p>
&lt;p>&lt;strong>virtual_server {&amp;hellip;}&lt;/strong> # lvs 中调度器的配置&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>delay_Loop NUM&lt;/strong> # 健康检查的时间间隔，单位为秒。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>lvs_sched&lt;/strong> # 指定负载均衡算法，示例中的 rr 表示 Round-Robin 轮询算法。(老版本的 keyword 为 lb_algo)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>lvs_method&lt;/strong> # 采用的路由方法，示例中采用的是 DR 路由，还可以采用 NAT 和 TUN 路由。(老版本的 keyword 为 lb_kind)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>persistence_timeout&lt;/strong> # 指定连接持久的超时时间。默认 6 分钟。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>**protocol **# 转发协议，一般有 TCP 和 UDP 两种。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>connect_timeout #连接超时时间。默认 5 秒。当 RS 检查失败 5 秒后，即判断该 RS 无响应，从 ipvs 组中踢出&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>retry&lt;/strong> # 重试次数。默认 1 次。当 RS 检查失败后，再次检查的次数。(老版本的 keyword 为 nb_get_retry)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>delay_before_retry #失败后，让 RS 重新加回 ipvs 组重试的次数。默认 1 次。当 rs 检查 1 次成功后，就将该 RS 重新加入 ipvs 组&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sorry_server :用于定义当后端所有 real server 挂掉后，使用哪台设备进行回应&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>real_server IP PORT {&amp;hellip;}&lt;/strong> # 后端服务器配置，i.e.lvs 中 RS 的配置&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>CHECK {&amp;hellip;}&lt;/strong> # 指定健康检查的方式。TCP 就是测试目标端口通不通。HTTP 则是测试指定资源的响应码&lt;/p>
&lt;ul>
&lt;li>
&lt;p>可用的 CHECK 有如下几个，常用的标黄&lt;/p>
&lt;/li>
&lt;li>
&lt;p>HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|DNS_CHECK|MISC_CHECK|BFD_CHECK|UDP_CHECK|PING_CHECK|FILE_CHECK&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="4-其他配置">4. 其他配置&lt;/h2>
&lt;p>&lt;strong>include &lt;!-- raw HTML omitted -->&lt;/strong> # 指定要包含的其他配置文件。FILE 可以用绝对路径，也可以使用通配符。指定的 FILE 中的内容将作为 keepalived 配置内容附加到主配置后面&lt;/p>
&lt;h2 id="检查脚本配置示例">检查脚本配置示例&lt;/h2>
&lt;ol>
&lt;li>定义脚本用于把 MASTER 节点的优先值降低 20，以实现主备切换，在指定目录中创建一个 down 文件，也可以使用别的判断方式&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>vrrp_script chk_mantaince_down &lt;span style="color:#f92672">{&lt;/span> &lt;span style="color:#75715e">#配置一个名为chk_mantaince_down的脚本&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 当某个文件存在时，权重减20&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> script &lt;span style="color:#e6db74">&amp;#34;[[ -f /etc/keepalived/down ]] &amp;amp;&amp;amp; exit 1 || exit 0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> interval &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> weight -20
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>判断 nginx 进程是否存在，如果不存在则权重-2 使之变为 BACKUP&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>vrrp_script chk_haproxy {
# 判断haproxy进程是否存在，如果不存在则权重-2使之变为BACKUP
script &amp;quot;killall -0 haproxy&amp;quot; # 可以使用这个语句判断nginx，script &amp;quot;killall -0 nginx &amp;amp;&amp;gt; /dev/null&amp;quot;
interval 1 # 运行脚本的时间间隔
weight -2
}
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>引用脚本&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>vrrp_instance VI_1 {
...
track_script { # 在VRRP实例中引用哪些keepalived.conf中定义的脚本
chk_mantaince_down
}
notify_master &amp;quot;/etc/keepalived/notify.sh master&amp;quot; # 定义该节点变为master后执行的脚本
notify_backup &amp;quot;/etc/keepalived/notify.sh backup&amp;quot; # 定义该节点变为backup后执行的脚本
notify_fault &amp;quot;/etc/keepalived/notify.sh fault&amp;quot; # 定义该节点变为fault后执行的脚本
}
&lt;/code>&lt;/pre></description></item><item><title>Docs: 1.2.Rancher 部署与清理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.2.rancher-%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%B8%85%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.2.rancher-%E9%83%A8%E7%BD%B2%E4%B8%8E%E6%B8%85%E7%90%86/</guid><description>
&lt;h1 id="rancher-部署">Rancher 部署&lt;/h1>
&lt;p>常见问题：&lt;a href="https://www.bookstack.cn/read/rancher-2.4.4-zh/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.md">https://www.bookstack.cn/read/rancher-2.4.4-zh/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.md&lt;/a>&lt;/p>
&lt;h2 id="快速部署体验">快速部署体验&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker run -d --name&lt;span style="color:#f92672">=&lt;/span>rancher-server --restart&lt;span style="color:#f92672">=&lt;/span>unless-stopped &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 60080:80 -p 60443:443 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /opt/rancher:/var/lib/rancher &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --privileged &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> rancher/rancher:v2.5.3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果想让 rancher 可以验证外部 https 的自建 CA 证书，需要在启动前将证书导入 rancher-server 中，效果如下：&lt;/p>
&lt;p>参考链接： &lt;a href="https://rancher.com/docs/rancher/v2.x/en/installation/resources/chart-options/#additional-trusted-cas">https://rancher.com/docs/rancher/v2.x/en/installation/resources/chart-options/#additional-trusted-cas&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://rancher.com/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/#custom-ca-certificate">https://rancher.com/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/#custom-ca-certificate&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>docker run -d --name&lt;span style="color:#f92672">=&lt;/span>rancher-server --restart&lt;span style="color:#f92672">=&lt;/span>unless-stopped &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 60080:80 -p 60443:443 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /opt/rancher:/var/lib/rancher &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /host/certs:/container/certs &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e SSL_CERT_DIR&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;/container/certs&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --privileged &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> rancher/rancher:latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在宿主机的 /host/certs 目录中存放要导入的证书，比如可以把 harbor 的证书与私钥放入该目录中，这样 rancher 就可以添加 https 的 harbor 仓库了&lt;/p>
&lt;p>还可以传递 CATTLE_SYSTEM_DEFAULT_REGISTRY 环境变量，让 rancher 内部使用私有镜像地址。比如&lt;/p>
&lt;pre>&lt;code>docker run -d --name=rancher-server --restart=unless-stopped \
-p 60080:80 -p 60443:443 \
-v /opt/rancher:/var/lib/rancher \
--privileged \
-e CATTLE_SYSTEM_DEFAULT_REGISTRY=&amp;quot;registry.wx-net.ehualu.com&amp;quot; \
rancher/rancher:latest
&lt;/code>&lt;/pre>
&lt;h2 id="高可用部署">高可用部署&lt;/h2>
&lt;p>Rancher 的高可用本质上就是将 Rancher 作为 kubernetes 上的一个服务对外提供(这个 k8s 集群通常只用来运行 Rancher)。Rancher 的数据储存在 k8s 集群的后端存储中(i.e.ETCD)。由于原生 k8s 部署负责，资源需求大，不易维护等问题，Rancher 官方推出了一个简化版的 k8s，即 &lt;a href="https://github.com/rancher/k3s">k3s&lt;/a>。k3s 是一个简化版的 k8s，可以实现基本的 k8s 功能，但是部署更简单，资源需求更少，更易维护。&lt;a href="https://github.com/rancher/k3s">k3s&lt;/a> 介绍详见 k3s 介绍&lt;/p>
&lt;h3 id="可选启动-k3s-集群">(可选)启动 k3s 集群&lt;/h3>
&lt;p>启动 mysql 用于 k3s 存储数据&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d --name k3s-mysql --restart&lt;span style="color:#f92672">=&lt;/span>always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-v /opt/k3s-cluster/mysql/conf:/etc/mysql/conf.d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-v /opt/k3s-cluster/mysql/data:/var/lib/mysql &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-e MYSQL_ROOT_PASSWORD&lt;span style="color:#f92672">=&lt;/span>root &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>-p 3306:3306 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>mysql:5.7.29 --default-time-zone&lt;span style="color:#f92672">=&lt;/span>+8:00 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--character-set-server&lt;span style="color:#f92672">=&lt;/span>utf8mb4 --collation-server&lt;span style="color:#f92672">=&lt;/span>utf8mb4_general_ci &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--explicit_defaults_for_timestamp&lt;span style="color:#f92672">=&lt;/span>true --lower_case_table_names&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span> --max_allowed_packet&lt;span style="color:#f92672">=&lt;/span>128M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>启动 k3s&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR&lt;span style="color:#f92672">=&lt;/span>cn sh -s - server &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span>--docker --datastore-endpoint&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;mysql://root:root@tcp(172.38.40.212:3306)/k3s&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 配置 kubectl 的 kubeconfig 文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp /etc/rancher/k3s/k3s.yaml /root/.kube/config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;source &amp;lt;(kubectl completion bash)&amp;#34;&lt;/span> &amp;gt;&amp;gt; ~/.bashrc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="部署-rancher">部署 Rancher&lt;/h3>
&lt;p>创建证书参考：&lt;a href="https://thoughts.teambition.com/workspaces/5f90e312c800160016ea22fb/docs/5fa4f848eaa1190001257bba">&lt;strong>自建 CA 脚本&lt;/strong>&lt;/a>，该脚本参考：&lt;a href="https://docs.rancher.cn/docs/rancher2/installation/options/self-signed-ssl/_index">https://docs.rancher.cn/docs/rancher2/installation/options/self-signed-ssl/_index&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建用于运行 Rancher 的名称空间&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create namespace cattle-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建CA证书&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out ca.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -x509 -new -nodes -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=k3s-rancher.desistdaydream.ltd&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key ca.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out k3s-rancher.desistdaydream.ltd.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -sha512 -new &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=k3s-rancher.desistdaydream.ltd&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key k3s-rancher.desistdaydream.ltdn.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out k3s-rancher.desistdaydream.ltd.csr
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; v3.ext &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">authorityKeyIdentifier=keyid,issuer
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">basicConstraints=CA:FALSE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">extendedKeyUsage = serverAuth
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">subjectAltName = @alt_names
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[alt_names]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">DNS.1=k3s-rancher.desistdaydream.ltd
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl x509 -req -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -extfile v3.ext &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -CA ca.crt -CAkey ca.key -CAcreateserial &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -in k3s-rancher.desistdaydream.ltd.csr &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out k3s-rancher.desistdaydream.ltd.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将证书添加到 secret 资源，以便让 Rancher 读取&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp k3s-rancher.desistdaydream.ltd.crt tls.crt &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp k3s-rancher.desistdaydream.ltd.key tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret tls tls-rancher-ingress &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert&lt;span style="color:#f92672">=&lt;/span>tls.crt &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key&lt;span style="color:#f92672">=&lt;/span>tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ca.crt cacerts.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret generic tls-ca &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-file&lt;span style="color:#f92672">=&lt;/span>cacerts.pem&lt;span style="color:#f92672">=&lt;/span>./cacerts.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 添加 rancher 的 helm 仓库&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo add rancher-stable http://rancher-mirror.oss-cn-beijing.aliyuncs.com/server-charts/stable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 部署指定版本的 Rancher&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm install rancher rancher-stable/rancher &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --namespace cattle-system &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set hostname&lt;span style="color:#f92672">=&lt;/span>k3s-rancher.desistdaydream.ltd &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set ingress.tls.source&lt;span style="color:#f92672">=&lt;/span>secret &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set privateCA&lt;span style="color:#f92672">=&lt;/span>true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="高可用离线部署">高可用离线部署&lt;/h2>
&lt;p>与在线部署类似，但是需要一个私有镜像仓库，所需相关的部署镜像需要先推送到私有镜像仓库中。&lt;/p>
&lt;p>需要提前准备的文件列表：&lt;/p>
&lt;p>在 &lt;a href="https://github.com/rancher/rancher/releases">https://github.com/rancher/rancher/releases&lt;/a> 页面，下载推送镜像所需的文件，这里以 2.4.5 为例。一共需要三个文件。&lt;/p>
&lt;p>rancher-images.txt rancher 镜像列表。&lt;/p>
&lt;p>rancher-save-images.sh 根据镜像列表文件打包所有镜像。&lt;/p>
&lt;p>rancher-load-images.sh 将打包好的镜像推送到私有仓库。&lt;/p>
&lt;ol>
&lt;li>rancher-images.tar.gz # rancher 的镜像
&lt;ol>
&lt;li>curl -LO &lt;a href="https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-images.txt">https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-images.txt&lt;/a>&lt;/li>
&lt;li>curl -LO &lt;a href="https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-save-images.sh">https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-save-images.sh&lt;/a>&lt;/li>
&lt;li>sort -u rancher-images.txt -o rancher-images.txt&lt;/li>
&lt;li>./rancher-save-images.sh &amp;ndash;image-list ./rancher-images.txt&lt;/li>
&lt;li>保存完成后，会生成名为 rancher-images.tar.gz 的镜像打包文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>rancher-load-images.sh # 推送 rancher 镜像的脚本
&lt;ol>
&lt;li>curl -LO &lt;a href="https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-load-images.sh">https://github.com/rancher/rancher/releases/download/v2.4.5/rancher-load-images.sh&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>mysql.tar # mysql 镜像。&lt;/li>
&lt;li>k3s # k3s 二进制文件&lt;/li>
&lt;li>k3s-airgap-images-amd64.tar # k3s 运行所需的镜像
&lt;ol>
&lt;li>从 &lt;a href="https://github.com/rancher/k3s/releases">此处&lt;/a> 下载 k3s 二进制文件以及镜像的压缩包。二进制文件名称为 &lt;a href="https://github.com/rancher/k3s/releases/download/v1.18.6%2Bk3s1/k3s">k3s&lt;/a>。镜像压缩包的文件名为 &lt;a href="https://github.com/rancher/k3s/releases/download/v1.18.6%2Bk3s1/k3s-airgap-images-amd64.tar">k3s-airgap-images-amd64.tar&lt;/a>。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>install.sh # 部署 k3s 的脚本
&lt;ol>
&lt;li>从 &lt;a href="https://get.k3s.io">此处&lt;/a> 获取离线安装所需的脚本。国内用户从 &lt;a href="http://mirror.cnrancher.com/">这个页面&lt;/a>的 k3s 目录下获取脚本，脚本名为 k3s-install.sh。&lt;/li>
&lt;li>curl -LO &lt;a href="https://raw.githubusercontent.com/rancher/k3s/master/install.sh">https://raw.githubusercontent.com/rancher/k3s/master/install.sh&lt;/a>&lt;/li>
&lt;li>curl -LO &lt;a href="https://docs.rancher.cn/k3s/k3s-install.sh">https://docs.rancher.cn/k3s/k3s-install.sh&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>rancher-2.4.5.tgz # 用于部署 rancher 的 helm chart。
&lt;ol>
&lt;li>helm repo add rancher-stable &lt;a href="https://releases.rancher.com/server-charts/stable">https://releases.rancher.com/server-charts/stable&lt;/a>&lt;/li>
&lt;li>helm pull rancher-stable/rancher&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>helm # helm 二进制文件
&lt;ol>
&lt;li>从 git 上下载 tar 包，解压获取二进制文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>kubectl #kubectl 二进制文件，用于在 Rancher 创建的集群节点上操作集群。&lt;/li>
&lt;/ol>
&lt;h3 id="部署私有镜像仓库">部署私有镜像仓库&lt;/h3>
&lt;p>略。&lt;/p>
&lt;p>推送用于部署 Rancher 所需的镜像，到私有镜像仓库。&lt;/p>
&lt;pre>&lt;code># 拷贝 rancher-images.tar.gz 文件到当前目录
# 假如私有镜像仓库的访问路径为 http://172.38.40.180
./rancher-load-images.sh --image-list ./rancher-images.txt --registry 172.38.40.180
&lt;/code>&lt;/pre>
&lt;h3 id="启动-mysql">启动 mysql&lt;/h3>
&lt;p>注意修改 ${CustomRegistry} 变量为指定的私有镜像仓库的仓库名&lt;/p>
&lt;pre>&lt;code>docker run -d --name k3s-mysql --restart=always \
-v /opt/k3s-cluster/mysql/conf:/etc/mysql/conf.d \
-v /opt/k3s-cluster/mysql/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=root \
-p 3306:3306 \
${CustomRegistry}mysql:5.7.29 --default-time-zone=+8:00 \
--character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci \
--explicit_defaults_for_timestamp=true --lower_case_table_names=1 --max_allowed_packet=128M
docker run -d --name k3s-mysql --restart=always \
-v /opt/k3s-cluster/mysql/conf:/etc/mysql/conf.d \
-v /opt/k3s-cluster/mysql/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=root \
-p 3306:3306 ${CustomRegistry}/mysql:5.7.29 --default-time-zone=+8:00
&lt;/code>&lt;/pre>
&lt;h3 id="部署-k3s">部署 k3s&lt;/h3>
&lt;p>准备部署环境
需要 k3s、k3s-airgap-images-amd64.tar、install.sh 文件，拷贝到同一个目录中。并在需要部署 k3s 节点的设备上执行如下命令。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将下载好的k3s镜像包放到指定目录中，k3s 启动时直接使用该目录的镜像压缩包，加载并启动容器。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p /var/lib/rancher/k3s/agent/images/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ./k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将二进制文件放在每个节点的 /usr/local/bin 目录中。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod &lt;span style="color:#ae81ff">755&lt;/span> ./k3s &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp ./k3s /usr/local/bin/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 拷贝 helm 二进制文件到 /usr/local/bin 目录下&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod &lt;span style="color:#ae81ff">755&lt;/span> ./helm &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp ./helm /usr/local/bin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 离线安装脚本放在任意路径下。&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv ./k3s-install.sh ./install.sh &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> chmod &lt;span style="color:#ae81ff">755&lt;/span> install.sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 准备 k3s containerd 操作私有镜像仓库的配置文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir -p /etc/rancher/k3s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/rancher/k3s/registries.yaml &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">mirrors:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> registry-test.ehualu.com:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> endpoint:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - &amp;#34;http://172.38.40.180&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">configs:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;registry-test.ehualu.com&amp;#34;:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> auth:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> username: admin
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> password: Harbor12345
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 配置解析以访问私有仓库&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt;&amp;gt; /etc/hosts &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">172.38.40.180 registry-test.ehualu.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>开始部署 k3s&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 在 install.sh 所在目录执行部署命令&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>INSTALL_K3S_SKIP_DOWNLOAD&lt;span style="color:#f92672">=&lt;/span>true INSTALL_K3S_EXEC&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;server --docker --datastore-endpoint=mysql://root:root@tcp(172.38.40.212:3306)/k3s&amp;#39;&lt;/span> ./install.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note：&lt;/p>
&lt;ol>
&lt;li>若 k3s.service 无法启动，报错 msg=&amp;ldquo;starting kubernetes: preparing server: creating storage endpoint: building kine: dial tcp: unknown network tcp&amp;rdquo;，则修改 /etc/systemd/system/k3s.service 文件。
&lt;ol>
&lt;li>将其中 &amp;lsquo;&amp;ndash;datastore-endpoint=mysql://root:root@tcp(172.38.40.214:3306)/k3s&amp;rsquo; \ 这行改为 &amp;lsquo;&amp;ndash;datastore-endpoint=mysql://root:root@tcp(172.38.40.214:3306)/k3s&amp;rsquo;  \&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>配置 kubectl config 文件&lt;/p>
&lt;p>虽然高版本 k3s 在 /usr/local/bin/ 目录下生成了 kubectl 的软连接，但是 kubeconfig 文件依然需要拷贝到 .kube 目录中，因为 helm 也会使用。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 获取 kubectl 二进制文件并放入 /usr/local/bin/ 目录中&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 从别的机器 copy 一个对应 k3s 版本的 kubeclt 二进制文件&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 拷贝 kubeconfig 文件到 kubectl 配置目录，并配置命令补全功能&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mkdir ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp /etc/rancher/k3s/k3s.yaml /root/.kube/config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#e6db74">&amp;#34;source &amp;lt;(kubectl completion bash)&amp;#34;&lt;/span> &amp;gt;&amp;gt; ~/.bashrc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="部署-rancher-1">部署 Rancher&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建所需 namespaces&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create namespace cattle-system
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>创建 Rancher 所需证书&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建证书&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out ca.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -x509 -new -nodes -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=rancher.ehualu.com&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key ca.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl genrsa -out ehualu.com.key &lt;span style="color:#ae81ff">4096&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -sha512 -new &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -subj &lt;span style="color:#e6db74">&amp;#34;/C=CN/ST=Tianjin/L=Tianjin/O=eHualu/OU=Operations/CN=rancher.ehualu.com&amp;#34;&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -key ehualu.com.key &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ehualu.com.csr
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; v3.ext &lt;span style="color:#e6db74">&amp;lt;&amp;lt;-EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">authorityKeyIdentifier=keyid,issuer
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">basicConstraints=CA:FALSE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">extendedKeyUsage = serverAuth
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">subjectAltName = @alt_names
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[alt_names]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">DNS.1=rancher.ehualu.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl x509 -req -sha512 -days &lt;span style="color:#ae81ff">36500&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -extfile v3.ext &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -CA ca.crt -CAkey ca.key -CAcreateserial &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -in ehualu.com.csr &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -out ehualu.com.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 将证书添加到 secret 资源，以便让 Rancher 读取&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ehualu.com.crt tls.crt &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> cp ehualu.com.key tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret tls tls-rancher-ingress &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cert&lt;span style="color:#f92672">=&lt;/span>tls.crt &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --key&lt;span style="color:#f92672">=&lt;/span>tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp ca.crt cacerts.pem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl -n cattle-system create secret generic tls-ca &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-file&lt;span style="color:#f92672">=&lt;/span>cacerts.pem&lt;span style="color:#f92672">=&lt;/span>./cacerts.pem
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>部署&lt;/p>
&lt;pre>&lt;code># 通过 helm 生成部署 rancher 的 yaml。
helm template rancher ./rancher-2.4.5.tgz --output-dir . \
--namespace cattle-system \
--set hostname=rancher.ehualu.com \
--set rancherImage=registry.ehualu.com/rancher/rancher \
--set ingress.tls.source=secret \
--set privateCA=true \
--set systemDefaultRegistry=registry.ehualu.com \
--set useBundledSystemChart=true \
--set rancherImageTag=v2.4.5
# 部署 Rancher
kubectl -n cattle-system apply -R -f ./rancher
&lt;/code>&lt;/pre>
&lt;h1 id="rancher-升级">Rancher 升级&lt;/h1>
&lt;p>Rancher 本身的升级，就是 k8s 集群中服务的升级，使用 helm 更新即可，新版 pod 创建后销毁旧版 pod。&lt;/p>
&lt;p>Rancher 管理的 k8s 集群升级参考 &lt;a href="https://docs.rancher.cn/docs/rancher2/cluster-admin/upgrading-kubernetes/_index/">官方文档&lt;/a>，在 web 页面点两下就好很简单 。&lt;/p>
&lt;h1 id="rancher-清理">Rancher 清理&lt;/h1>
&lt;h2 id="清理-rancher">清理 Rancher&lt;/h2>
&lt;p>官方文档：&lt;a href="https://rancher.com/docs/rancher/v2.x/en/faq/removing-rancher/#what-if-i-don-t-want-rancher-anymore">https://rancher.com/docs/rancher/v2.x/en/faq/removing-rancher/#what-if-i-don-t-want-rancher-anymore&lt;/a>&lt;/p>
&lt;p>通过 &lt;a href="https://rancher.com/docs/rancher/v2.x/en/system-tools/#remove">rancher 系统工具的 remove 子命令&lt;/a>来清理 k8s 集群上 rancher&lt;/p>
&lt;h2 id="清理通过-rancher-创建的-k8s-集群">清理通过 Rancher 创建的 k8s 集群&lt;/h2>
&lt;p>官方文档：&lt;a href="https://docs.rancher.cn/docs/rancher2/cluster-admin/cleaning-cluster-nodes/_index/">https://docs.rancher.cn/docs/rancher2/cluster-admin/cleaning-cluster-nodes/_index/&lt;/a>&lt;/p>
&lt;p>在 Rancher web UI 上删除集群后，手动执行一些 &lt;a href="https://rancher2.docs.rancher.cn/docs/cluster-admin/cleaning-cluster-nodes/_index#%E6%89%8B%E5%8A%A8%E4%BB%8E%E9%9B%86%E7%BE%A4%E4%B8%AD%E5%88%A0%E9%99%A4-rancher-%E7%BB%84%E4%BB%B6">命令&lt;/a> 以删除在节点上生成的数据，并重启相关节点。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#！/bin/bash&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理所有 Docker 容器、镜像和卷：&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker rm -f &lt;span style="color:#66d9ef">$(&lt;/span>docker ps -qa&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker rmi -f &lt;span style="color:#66d9ef">$(&lt;/span>docker images -q&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker volume rm &lt;span style="color:#66d9ef">$(&lt;/span>docker volume ls -q&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 卸载挂载&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> mount in &lt;span style="color:#66d9ef">$(&lt;/span>mount | grep tmpfs | grep &lt;span style="color:#e6db74">&amp;#39;/var/lib/kubelet&amp;#39;&lt;/span> | awk &lt;span style="color:#e6db74">&amp;#39;{ print $3 }&amp;#39;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span> /var/lib/kubelet /var/lib/rancher; &lt;span style="color:#66d9ef">do&lt;/span> umount $mount; &lt;span style="color:#66d9ef">done&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理目录及数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rm -rf /etc/ceph &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /etc/cni &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /etc/kubernetes &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /opt/cni &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /opt/rke &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /run/secrets/kubernetes.io &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /run/calico &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /run/flannel &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/calico &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/etcd &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/cni &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/kubelet &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/lib/rancher/rke/log &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/log/containers &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/log/kube-audit &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/log/pods &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /var/run/calico
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 清理 iptables 与 网络设备，需要重启设备，也可以自己手动清理&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># reboot&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 1.2.实现虚拟化的工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/1.2.%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%B7%A5%E5%85%B7/</guid><description/></item><item><title>Docs: 1.3.OpenStack 虚拟机编排系统</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</guid><description/></item><item><title>Docs: 1.3.Rancher 配置</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.3.rancher-%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/1.3.rancher-%E9%85%8D%E7%BD%AE/</guid><description>
&lt;p>Rancher Server URL 的修改&lt;/p>
&lt;p>当 Rancher Server URL 变更后(比如从 40443 变到 60443)，则还需要连带修改以下几部分&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Rancher Web 页面最上面的标签，进入&lt;code>系统设置&lt;/code>，修改&lt;code>server-url&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>k8s 集群中，修改 cattle-system 名称空间中，名为&lt;code>cattle-credentials-XXX&lt;/code>的 secret 资源中的 .data.url 字段的值，这个值是用 base64 编码的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>echo -n &amp;ldquo;https://X.X.X.X:60443&amp;rdquo; | bas64 ，通过该命令获取编码后的 url，然后填入 .data.url 字段中&lt;/p>
&lt;/li>
&lt;li>
&lt;p>k8s 集群中，修改 cattle-cluster-agent-XX 和 cattle-node-agent-XX 这些 pod 的 env 参数，将其中的 CATTLE_SERVER 的值改为想要的 URL。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cattle-node-agent 在 2.5.0 版本之后没有了，就不用改了。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>导入集群的 yaml 文件位置&lt;/p>
&lt;p>打开 &lt;code>https://RancherIP/v3/cluster/集群ID/clusterregistrationtokens&lt;/code> 页面&lt;/p>
&lt;p>在 data 字段下，可以看到获取 yaml 文件的 URL，可能会有多组，一般以时间最新的那组为准。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggn0dn/1616114779749-bd6fd7cc-32cb-41b8-9122-2047f125c4a7.png" alt="">&lt;/p></description></item><item><title>Docs: 1.API、Resource(资源)、Object(对象)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/1.apiresource%E8%B5%84%E6%BA%90object%E5%AF%B9%E8%B1%A1/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">官方文档，概念-概述-Kubernetes API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/">官方文档，参考-API 概述&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubernetes API 使我们可以查询和操纵 Kubernetes API 中资源的状态。Kubernetes API 符合 &lt;a href="https://www.yuque.com/go/doc/33220383">RESTful 规范&lt;/a>。&lt;/p>
&lt;p>Kubernetes 把自身一切抽象理解为 &lt;strong>Resource(资源)&lt;/strong>，也叫 &lt;strong>API Resource&lt;/strong>(有地方也叫 Group Resource)。对集群的所有操作都是通过对 Kubernetes API 的 HTTP(s) 请求来实现的。可以使用命令 kubectl api-resources 命令查看所有支持的资源。&lt;/p>
&lt;p>kubernetes 控制平面的核心是 &lt;strong>API Server&lt;/strong>。API Server 是实现了 Kubernets API 的应用程序，并为 Kubernetes 公开了一个 HTTP(s) 的 API，以供用户、集群中的不同部分和集群外部组件相互通信。&lt;/p>
&lt;p>Kubernetes 中各种资源(对象)的数据都通过 API 接口被提交到后端的持久化存储（etcd）中，Kubernetes 集群中的各部件之间通过该 API 接口实现解耦合，同时 Kubernetes 集群中一个重要且便捷的管理工具 kubectl 也是通过访问该 API 接口实现其强大的管理功能的。&lt;/p>
&lt;blockquote>
&lt;p>Note：kubectl 就是代替用户执行各种 http 请求的工具&lt;/p>
&lt;/blockquote>
&lt;p>在 Kubernetes 系统中，在大多数情况下，API 定义和实现都符合标准的 HTTP REST 格式，比如通过标准的 HTTP 动词（POST、PUT、GET、DELETE）来完成对相关资源对象的查询、创建、修改、删除等操作。但同时，Kubernetes 也为某些非标准的 REST 行为实现了附加的 API 接口，例如 Watch 某个资源的变化、进入容器执行某个操作等。另外，某些 API 接口可能违背严格的 REST 模式，因为接口返回的不是单一的 JSON 对象，而是其他类型的数据，比如 JSON 对象流或非结构化的文本日志数据等。&lt;/p>
&lt;p>另外，从另一个角度看，其实 kubernetes 就是提供了一个 web 服务，只是这个 web 服务不像传统的 B/S 架构那样，可以通过浏览器直接操作~kubernetes API 就是这个 web 服务的入口。&lt;/p>
&lt;blockquote>
&lt;p>注意：Kubernetes 的 API 与传统意义上的 API 不太一样。传统 API，一个 API 就是一个功能；而 Kubernetes API 中，一个 API 实际上又可以当作功能，也可以当作一个资源。对 API 的操作，就是对 Kubernets 资源进行操作&lt;/p>
&lt;/blockquote>
&lt;h2 id="api-resource资源-分类">API Resource(资源) 分类&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/">API 参考&lt;/a>、&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/">1.19 版本 API 参考(一页模式)&lt;/a>(要查看其他版本，修改 URL 最后的版本号即可)。&lt;/p>
&lt;/blockquote>
&lt;p>资源大体可以分为下面几类：&lt;/p>
&lt;ol>
&lt;li>**workload(工作负载) **# 用于在集群上管理和运行容器
&lt;ol>
&lt;li>Pod，Deployment，StatefuSet，DaemonSet，Job 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Discovery &amp;amp; LB(服务发现及均衡)&lt;/strong> # 可以使用这些资源类型的对象将工作负载“缝合”到一个外部可访问的、负载均衡的服务中。
&lt;ol>
&lt;li>Service，Ingress 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Config &amp;amp; Storage(配置与存储)&lt;/strong> # 这种类型的资源是用于将初始化数据注入到应用程序中并保留容器外部数据的对象。
&lt;ol>
&lt;li>Volume，ConifgMap，secret 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Cluster(集群级资源)&lt;/strong> # 这种类型的资源对象定义了群集本身的配置方式。这些通常仅由集群运营商使用。
&lt;ol>
&lt;li>Namesapces,Node,Role,ClusterRole,RoleBinding,ClusterRoleBinding 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Metadata(元数据型资源)&lt;/strong> # 这种类型的资源是用于配置集群中其他资源行为的对象。
&lt;ol>
&lt;li>HPA，PodTemplate，LimitRange 等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>各种资源所用的 manifest 文件中的各个字段的含义就可以参考该页面找到详解。&lt;/p>
&lt;h2 id="api-resource资源-的-url-结构">API Resource(资源) 的 URL 结构&lt;/h2>
&lt;p>在 Kubernetes 中，资源的 URL 结构是由：Group（组）、Version（版本）和 Resource（资源种类）三个部分组成的。(还有一种 /metrics，/healthz 之类的结构，这里面的资源是系统自带的，不在任何组里)&lt;/p>
&lt;p>通过这样的结构，整个 Kubernetes 里的所有资源，实际上就可以用如下图的树形结构表示出来：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/sz9hgm/1616120310758-dc53a2df-2a39-45e9-92e3-9beb5d9101f0.png" alt="">&lt;/p>
&lt;p>比如，如果要创建一个 CronJob 资源&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">batch/v2alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">CronJob&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在这个 YAML 文件中，“CronJob”就是资源的种类(Resource)，“batch”就是它的组(Group)，v2alpha1 就是它的版本(Version)。&lt;/p>
&lt;p>现阶段，有两个 API Groups 正在使用&lt;/p>
&lt;ol>
&lt;li>&lt;strong>core group(核心组)&lt;/strong> # 在/api/v1 路径下(由于某些历史原因而并没有在/apis/core/v1 路径下)。核心组是不需要 Group 的（即：它们 Group 是 &lt;code>&amp;quot;&amp;quot;&lt;/code>）。URI 路径为/api/v1，并且在定义资源的 manifest 文件中 apiVersion 字段的值不用包含组名，直接使用 v1 即可&lt;/li>
&lt;li>&lt;strong>named groups(已命名组)&lt;/strong> # URI 路径为/apis/$GROUP_NAME/$VERSION，在定义资源的 manifest 文件中 apiVersion 中省略 apis，使用 GroupName/Version&lt;/li>
&lt;/ol>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>有的资源是 cluster 级别的(比如 node)，有的资源是 namespace 级别的(比如 pod)，对于 namespace 级别的资源，可以在 Version 和 Resource 中间添加 namespace 字段以获取指定 namespace 下的资源。i.e./api/v1/namespaces/$NAMESPACE/pods/ ($NAMESPACE 就是具体的 namesapce 的名称)。&lt;/li>
&lt;li>所以 namesapce 级别资源的对象的 URI 应该像这样：/api/v1/namespaces/kube-system/pods/coredns-5644d7b6d9-tw4rh&lt;/li>
&lt;li>而 cluster 级别资源的对象的 URI 则是：/api/v1/nodes/master1&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>所有资源类型要么受集群范围限制（/apis/GROUP/VERSION/&lt;em>），要么受命名空间限制（/apis/GROUP/VERSION/namespaces/NAMESPACE/&lt;/em>）&lt;/strong>&lt;/p>
&lt;p>集群范围的资源：&lt;/p>
&lt;ol>
&lt;li>GET /apis/GROUP/VERSION/RESOURCETYPE #返回指定资源类型的资源集合(返回的是一个 list 列表，比如 NodeList 等)&lt;/li>
&lt;li>GET /apis/GROUP/VERSION/RESOURCETYPE/NAME #返回指定资源类型下名为 NAME 的的资源&lt;/li>
&lt;/ol>
&lt;p>名称空间范围的资源：&lt;/p>
&lt;ol>
&lt;li>GET /apis/GROUP/VERSION/RESOURCETYPE #返回所有名称空间指定资源类型的实例集合(返回的是一个 list 列表，比如 podList、serviceList 等)&lt;/li>
&lt;li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE #返回 NAMESPACE 下指定 ResourceType 的所有实例集合(返回的是一个 list 列表，比如 podList、serviceList 等)&lt;/li>
&lt;li>GET /apis/GROUP/VERSION/namespaces/NAMESPACE/RESOURCETYPE/NAME #返回 NAMESPACE 下指定 ResourceType，名为 NAME 的实例&lt;/li>
&lt;/ol>
&lt;h1 id="declarative-api声明式-api-的特点">Declarative API(声明式 API) 的特点：&lt;/h1>
&lt;ol>
&lt;li>首先，所谓 &lt;strong>Declarative(声明式)&lt;/strong>，指的就是我只需要提交一个定义好的 API 对象来 **Declarative(声明) **我所期望的状态是什么样子。&lt;/li>
&lt;li>其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。&lt;/li>
&lt;li>最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。&lt;/li>
&lt;/ol>
&lt;p>所以说，声明式 API，才是 Kubernetes 项目编排能力“赖以生存”的核心所在。而想要实现 声明式 API，离不开 Controller 控制器，K8S 的大脑 的工作。&lt;/p>
&lt;h1 id="api-url-使用示例">API URL 使用示例&lt;/h1>
&lt;p>下面是在 1.18.8 版本下获取到的 api 路径结构&lt;/p>
&lt;p>根路径将列出所有可用路径&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/ -s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;paths&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/api&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/api/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/admissionregistration.k8s.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/admissionregistration.k8s.io/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/admissionregistration.k8s.io/v1beta1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiextensions.k8s.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiextensions.k8s.io/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiextensions.k8s.io/v1beta1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;/apis/apiregistration.k8s.io&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果访问到错误的资源，还会返回 404 的响应码&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-s&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/api/v1/service
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Status&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Failure&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;the server could not find the requested resource&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NotFound&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;details&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">404&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在“组/版本”下面可以看到该“组/版本”下所包含的 API 资源列表&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-s&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/api/v1/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;APIResourceList&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;groupVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resources&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">.......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;configmaps&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;singularName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespaced&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ConfigMap&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;verbs&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;create&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;delete&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;deletecollection&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;get&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;list&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;patch&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;watch&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;shortNames&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;cm&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;storageVersionHash&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;qFsyl6wFWjQ=&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;endpoints&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;singularName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespaced&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Endpoints&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;verbs&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;create&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;delete&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;deletecollection&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;get&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;list&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;patch&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;watch&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;shortNames&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;ep&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;storageVersionHash&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;fWeeMqaN/OA=&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在“资源”下可以看到该“资源”下所包含的所有对象，下图是 pod 资源的列表，包含所有 pod 对象及其信息&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#960050;background-color:#1e0010">root@master&lt;/span>&lt;span style="color:#ae81ff">-1&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">~&lt;/span>]&lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">curl&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-s&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">--cacert&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">/etc/kubernetes/pki/ca.crt&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">-H&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Authorization: Bearer ${TOKEN}&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">https:&lt;/span>&lt;span style="color:#75715e">//172.38.40.215:6443/api/v1/pods | more
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;PodList&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;selfLink&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/api/v1/pods&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resourceVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;618871&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;items&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent-cc6ddc6dc-7f89l&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;generateName&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent-cc6ddc6dc-&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;selfLink&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/api/v1/namespaces/cattle-system/pods/cattle-cluster-agent-cc6ddc6dc-7f89l&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;72f4a825-feb2-416a-900d-d8401acc9a18&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resourceVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;452264&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;creationTimestamp&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2020-09-13T09:59:49Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;app&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;pod-template-hash&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cc6ddc6dc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;ownerReferences&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;apps/v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ReplicaSet&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;cattle-cluster-agent-cc6ddc6dc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;7d4b6cbe-d6d1-46e3-99e5-8410095880c7&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;controller&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;blockOwnerDeletion&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;managedFields&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 1.Authenticating(认证)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/1.authenticating%E8%AE%A4%E8%AF%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/1.authenticating%E8%AE%A4%E8%AF%81/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">官方文档,参考-API 访问控制-认证&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Authenticating(动词) 也称为 Authentication(名词) 身份验证。指明客户端是否有权限访问 API Server。&lt;/p>
&lt;p>就好比我们在登录一个网站时，需要输入账户和密码的概念类似。在使用 API Server 时，也是通过类似的方式，使用账户来登录 API server(虽然不是真的登录)。&lt;/p>
&lt;h2 id="accounts--kubernetes-集群中的账号">Accounts # Kubernetes 集群中的账号&lt;/h2>
&lt;p>Accounts 是一个在认证授权系统里的逻辑概念。Accounts 需要通过认证概念中的东西(比如证书、token、或者用户名和密码等)来建立。类似于登陆系统的账户。而在 Kubernetes 中，Accounts 分为如下两类&lt;/p>
&lt;ol>
&lt;li>UserAccoun(用户账户，简称 User)&lt;/li>
&lt;li>ServiceAccount(服务账户，简称 SA)&lt;/li>
&lt;/ol>
&lt;h3 id="user-account-用户账号">User Account 用户账号&lt;/h3>
&lt;p>详见：[User Account 详解](✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/7.API%20 访问控制/1.Authenticating(认证)/User%20Account%20 详解.md Account 详解.md)
User 不属于 K8S 中的一个资源。这类 Account 适用于：客户端访问集群时使用(比如使用 kubectl、scheduler 等访问 api)&lt;/p>
&lt;p>一个 User 可以管理多个 k8s 集群、也可以多个 User 管理一个集群，权限不同。User 只有在 KubeConfig 文件中才具有实际意义。&lt;/p>
&lt;p>由于 User 不属于 K8S 资源，那么则无法通过 API 调用来添加 User Account。但是任何提供了由群集的证书颁发机构(CA)签名的有效证书的用户都将被视为已认证。基于这种情况，Kubernetes 使用证书中的 subject 字段中的 Common Name(通用名称,即 CN)的值，作为用户名。接下来，基于授权概念中的 RBAC 子系统会确定用户是否有权针对某资源执行特定的操作。&lt;/p>
&lt;p>如果想创建一个 User，则可以通过证书的方式来创建。比如像下面这样， 这就创建了一个名为 lch 的 User Account。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>openssl genrsa -out lch.key &lt;span style="color:#ae81ff">2048&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -new -key lch.key -out lch.csr -subj &lt;span style="color:#e6db74">&amp;#34;/CN=lch&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果想使用 lch 这个 UA，则需要使用 kubectl config set-credentials 命令指定 lch 所需的相关凭证即可。还需要为 lch 绑定[授权概念](&amp;lt;✏IT 学习笔记/☁️10.云原生/2.3.Kubernetes%20 容器编排系统/7.API%20 访问控制/2.Authorization(授权).md&amp;raquo;)中的 Role 以便让该用户具有某些操作权限，然后 lch 这个 UA 即可对所绑定的集群有 Role 中所指定的操作权限。其中为 -subj 选项中 CN 的值就是 User 的名称。这个值也是在后面为 User 赋予 RBAC 权限的 rolebinding 时所使用的 &lt;code>subjects.name&lt;/code> 字段的值。&lt;/p>
&lt;p>进一步的细节可参阅 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">证书请求&lt;/a> 下普通用户主题。&lt;/p>
&lt;h3 id="service-account-服务账号">Service Account 服务账号&lt;/h3>
&lt;blockquote>
&lt;p>详见：&lt;a href="https://www.yuque.com/go/doc/33165738">Service Account 详解&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>SA 属于 K8S 中的一个资源。这类 Account 适用于：Pod 访问集群时使用。&lt;/p>
&lt;p>为什么需要 Service Account 呢？&lt;/p>
&lt;p>Service Account(服务账户) 概念的引入是基于这样的使用场景：运行在 pod 里的进程需要调用 Kubernetes API 以及非 Kubernetes API 的其它服务。Service Account 是给 pod 里面 Container 中的进程使用的，它为 pod 提供必要的身份认证。(与用户控制 kubectl 去调用 API 一样，这里相当于 Pod 中 Container 在调用 API 的时候需要的认证)&lt;/p>
&lt;h3 id="useraccount-与-serviceaccount-的区别httpskubernetesiodocsreferenceaccess-authn-authzservice-accounts-adminuser-accounts-versus-service-accounts">&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#user-accounts-versus-service-accounts">UserAccount 与 ServiceAccount 的区别&lt;/a>&lt;/h3>
&lt;ol>
&lt;li>UA 用来给人。SA 用来给运行在 pod 中的进程&lt;/li>
&lt;li>UA 作用于全局，UA 的名字在集群的所有 namespace 中必须是唯一的。SA 作用于 namespace&lt;/li>
&lt;li>UA 于 SA 的账户审核注意事项是不同的，UA 的凭证信息需要在使用 kubectl config 命令时候的手动指定；SA 的凭证信息在创建 SA 后会自动生成对应的 secret 并把凭证信息保存其中。&lt;/li>
&lt;/ol>
&lt;h2 id="accounts-group--账户组useraccount-与-serviceaccount-都有-group">Accounts Group # 账户组，UserAccount 与 ServiceAccount 都有 Group&lt;/h2>
&lt;p>UA 与 SA 都可以属于一个或多个 Group&lt;/p>
&lt;p>Group 是 Account 的集合，本身并没有操作权限，但附加于 Group 上的权限可由其内部的所有用户继承，以实现高效的授权管理机制。Kubernetes 有几个内建的用于特殊目的的 Group：&lt;/p>
&lt;ol>
&lt;li>system:unauthenticated&lt;/li>
&lt;li>system:authenticated&lt;/li>
&lt;li>system:serviceaccounts&lt;/li>
&lt;li>system:serviceaccounts:&lt;!-- raw HTML omitted -->&lt;/li>
&lt;/ol>
&lt;p>kubeconfig 会给 UserAccount 提供与 APIServer 交互时所用的证书&lt;/p>
&lt;p>Secret 会给 ServiceAccount 提供与 APIServer 交互时所用的证书&lt;/p>
&lt;h1 id="authentication-strategies-认证策略httpskubernetesiozhdocsreferenceaccess-authn-authzauthenticationauthentication-strategiesieaccount-可用的认证方式">&lt;a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#authentication-strategies">Authentication Strategies 认证策略&lt;/a>(i.e.Account 可用的认证方式)&lt;/h1>
&lt;p>Kubernetes 接受的认证方式有如下几种：&lt;/p>
&lt;ol>
&lt;li>client certificates&lt;/li>
&lt;li>bearer tokens&lt;/li>
&lt;li>an authenticating proxy&lt;/li>
&lt;li>HTTP basic auth&lt;/li>
&lt;/ol>
&lt;p>向 API Server 发起 HTTPS 请求时，kubernetes 通过身份验证插件对请求进行身份验证。&lt;/p>
&lt;h2 id="x509-client-certshttpskubernetesiodocsreferenceaccess-authn-authzauthenticationx509-client-certsx509-客户端证书">&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs">X509 Client Certs&lt;/a>(X509 客户端证书)&lt;/h2>
&lt;p>通过给 API 服务器传递 &amp;ndash;client-ca-file=SOMEFILE 选项，就可以启动客户端证书身份认证。 所引用的文件必须包含一个或者多个证书机构，用来验证向 API 服务器提供的客户端证书。 如果提供了客户端证书并且证书被验证通过，则 subject 中的公共名称（Common Name）就被 作为请求的用户名。 自 Kubernetes 1.4 开始，客户端证书还可以通过证书的 organization 字段标明用户的组成员信息。 要包含用户的多个组成员信息，可以在证书种包含多个 organization 字段。&lt;/p>
&lt;p>例如，使用 openssl 命令行工具生成一个证书签名请求：&lt;/p>
&lt;pre>&lt;code>openssl req -new -key jbeda.pem -out jbeda-csr.pem -subj &amp;quot;/CN=jbeda/O=app1/O=app2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>此命令将使用用户名 jbeda 生成一个证书签名请求（CSR），且该用户属于 &amp;ldquo;app&amp;rdquo; 和 &amp;ldquo;app2&amp;rdquo; 两个用户组。&lt;/p>
&lt;p>参阅&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/">管理证书&lt;/a>了解如何生成客户端证书&lt;/p>
&lt;h2 id="static-token-filehttpskubernetesiodocsreferenceaccess-authn-authzauthenticationstatic-token-file静态令牌文件">&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file">Static Token File&lt;/a>(静态令牌文件)&lt;/h2>
&lt;p>当 API 服务器的命令行设置了 &amp;ndash;token-auth-file=SOMEFILE 选项时，会从文件中 读取持有者令牌。目前，令牌会长期有效，并且在不重启 API 服务器的情况下 无法更改令牌列表。&lt;/p>
&lt;p>令牌文件是一个 CSV 文件，包含至少 3 个列：令牌、用户名和用户的 UID。 其余列被视为可选的组名。&lt;/p>
&lt;p>说明：&lt;/p>
&lt;p>如果要设置的组名不止一个，则对应的列必须用双引号括起来，例如&lt;/p>
&lt;pre>&lt;code>token,user,uid,&amp;quot;group1,group2,group3&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>在请求中放入持有者令牌&lt;/p>
&lt;p>当使用持有者令牌来对某 HTTP 客户端执行身份认证时，API 服务器希望看到 一个名为 Authorization 的 HTTP 头，其值格式为 Bearer THETOKEN。 持有者令牌必须是一个可以放入 HTTP 头部值字段的字符序列，至多可使用 HTTP 的编码和引用机制。 例如：如果持有者令牌为 31ada4fd-adec-460c-809a-9e56ceb75269，则其 出现在 HTTP 头部时如下所示：&lt;/p>
&lt;pre>&lt;code>Authorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269
# 比如一个 curl 请求中，可以通过 -H 参数加入请求头
curl --cacert ${CAPATH} -H &amp;quot;Authorization: Bearer ${TOKEN}&amp;quot; https://${IP}:6443/
&lt;/code>&lt;/pre></description></item><item><title>Docs: 1.Namespaces</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/1.namespaces/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/1.namespaces/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Linux_namespaces">Wiki,Linux_namespaces&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://segmentfault.com/a/1190000009732550">思否，Linux Namespace 和 Cgroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.junmajinlong.com/virtual/namespace">骏马金龙博客，Linux namespace&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/rhO5GUuWycRiFxdYaV-yiQ">公众号，YP 小站-Namespace 机制详解&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/lscMpc5BWAEzjgYw6H0wBw">公众号，开发内功修炼-Linux 网络名称空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/jJ9peydbNSd6Fv5bsJR3yA">公众号，MoeLove-彻底搞懂容器技术的基石：namespace&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/o5nZZzOTNXOFjv2aaIZ6OA">https://mp.weixin.qq.com/s/o5nZZzOTNXOFjv2aaIZ6OA&lt;/a>(下)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Linux Namespaces(Linux 名称空间) 是 Linux 内核的一个特性，Namespaces 可以对内核资源进行划分，使得一组进程看到一组资源，而另一组进程看到一组不同的资源。&lt;/p>
&lt;blockquote>
&lt;p>这里的资源包括 进程 ID、主机名、用户 ID、网络 等等。&lt;/p>
&lt;/blockquote>
&lt;p>如果把 Linux 操作系统比作一个大房子，那名称空间指的就是这个房子中的一个个房间，住在每个房间里的人都自以为独享了整个房子的资源，但其实大家仅仅只是在共享的基础之上互相隔离，共享指的是共享全局的资源，而隔离指的是局部上彼此保持隔离，因而名称空间的本质就是指：一种在空间上隔离的概念，当下盛行的许多容器虚拟化技术（典型代表如 LXC、Docker）就是基于 Linux 名称空间的概念而来的。&lt;/p>
&lt;p>很早以前的 Unix 有一个叫 chroot 的系统调用(通过修改根目录把用户 &lt;strong>jail(监狱)&lt;/strong> 到一个特定目录下)，chroot 提供了一种简单的隔离模式(隔离目录)：chroot 内部的文件系统无法访问外部的内容，详见 ftp 实现工具，chroot 说明。Linux Namespace 就是基于 chroot 的概念扩展而来，提供了对系统下更多资源的隔离机制。&lt;/p>
&lt;p>操作系统通过虚拟内存技术，使得每个用户进程都认为自己拥有所有的物理内存，这是操作系统对内存的虚拟化。操作系统通过分时调度系统，每个进程都能被【公平地】调度执行，即每个进程都能获取到 CPU，使得每个进程都认为自己在进程活动期间拥有所有的 CPU 时间，这是操作系统对 CPU 的虚拟化。&lt;/p>
&lt;p>从这两种虚拟化方式可推知，当使用某种虚拟化技术去管理进程时，进程会认为自己拥有某种物理资源的全部。&lt;/p>
&lt;p>虚拟内存和分时系统均是对物理资源进行虚拟化，其实操作系统中还有很多非物理资源，比如用户权限系统资源、网络协议栈资源、文件系统挂载路径资源等。通过 Linux 的 namespace 功能，可以对这些非物理全局资源进行虚拟化。&lt;/p>
&lt;p>Linux namespace 是在当前运行的系统环境中创建(隔离)另一个进程的运行环境出来，并在此运行环境中将一些必要的系统全局资源进行【虚拟化】。进程可以运行在指定的 namespace 中，因此，namespace 中的每个进程都认为自己拥有所有这些虚拟化的全局资源。&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gp34mf/1640133594098-db8cd29e-8628-4117-a5ec-ea14de312485.webp" alt="">
Linux Namespaces 的灵感来自 &lt;a href="https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs">Plan 9 from Bell Labs&lt;/a> 中大量使用的名称空间功能。Plan 9 from Bell Labs 是贝尔实验室弄出来的分布式操作系统。&lt;/p>
&lt;h2 id="linux-namespace-类型">Linux Namespace 类型&lt;/h2>
&lt;p>Note：随着技术的发展，Linux 内核支持的的 Namespace 类型在逐步增加&lt;/p>
&lt;p>目前，Linux 已经支持 8 种全局资源的虚拟化(每种资源都是随着 Linux 内核版本的迭代而逐渐加入的，因此有些内核版本可能不具备某种 namespace)：&lt;/p>
&lt;ul>
&lt;li>cgroup namespace：该 namespace 可单独管理自己的 cgroup&lt;/li>
&lt;li>ipc namespace：该 namespace 有自己的 IPC，比如共享内存、信号量等&lt;/li>
&lt;li>network namespace：该 namespace 有自己的网络资源，包括网络协议栈、网络设备、路由表、防火墙、端口等&lt;/li>
&lt;li>mount namespace：该 namespace 有自己的挂载信息，即拥有独立的目录层次&lt;/li>
&lt;li>pid namespace：该 namespace 有自己的进程号，使得 namespace 中的进程 PID 单独编号，比如可以 PID=1&lt;/li>
&lt;li>time namespace：该 namespace 有自己的启动时间点信息和单调时间，比如可设置某个 namespace 的开机时间点为 1 年前启动，再比如不同的 namespace 创建后可能流逝的时间不一样&lt;/li>
&lt;li>user namespace：该 namespace 有自己的用户权限管理机制(比如独立的 UID/GID)，使得 namespace 更安全&lt;/li>
&lt;li>uts namespace：该 namepsace 有自己的主机信息，包括主机名(hostname)、NIS domain name&lt;/li>
&lt;/ul>
&lt;p>用户可以同时创建具有多种资源类型的 namespace，比如创建一个同时具有 uts、pid 和 user 的 namespace。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>类型&lt;/th>
&lt;th>功能说明&lt;/th>
&lt;th>系统调用参数&lt;/th>
&lt;th>内核版本&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MNT Namespace&lt;/td>
&lt;td>提供磁盘挂载点和文件系统的隔离能力&lt;/td>
&lt;td>CLONE_NEWNS&lt;/td>
&lt;td>2.4.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>IPC Namespace&lt;/td>
&lt;td>提供进程间通信的隔离能力&lt;/td>
&lt;td>CLONE_NEWIPC&lt;/td>
&lt;td>2.6.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Net Namespace&lt;/td>
&lt;td>提供网络隔离能力&lt;/td>
&lt;td>CLONE_NEWNET&lt;/td>
&lt;td>2.6.29&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UTS Namespace&lt;/td>
&lt;td>提供主机名隔离能力&lt;/td>
&lt;td>CLONE_NEWUTS&lt;/td>
&lt;td>2.6.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PID Namespace&lt;/td>
&lt;td>提供进程隔离能力&lt;/td>
&lt;td>CLONE_NEWPID&lt;/td>
&lt;td>2.6.24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>User Namespace&lt;/td>
&lt;td>提供用户隔离能力&lt;/td>
&lt;td>CLONE_NEWUSER&lt;/td>
&lt;td>3.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CGroup Namespace&lt;/td>
&lt;td>Cgroup root directory&lt;/td>
&lt;td>&lt;/td>
&lt;td>4.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="理解-linux-namespace">理解 Linux namespace&lt;/h2>
&lt;p>用户可以创建指定类型的 namespace 并将进程放入该 namespace 中运行，这表示从当前的系统运行环境中隔离一个进程的运行环境，在此 namespace 中运行的进程将认为自己享有该 namespace 中的独立资源。&lt;/p>
&lt;p>实际上，即使用户没有手动创建 Linux namespace，Linux 系统开机后也会创建一个默认的 namespace，称为 root namespace，所有进程默认都运行在 root namespace 中，每个进程都认为自己拥有该 namespace 中的所有系统全局资源。&lt;/p>
&lt;p>回顾一下 Linux 的开机启动流程，内核加载成功后将初始化系统运行环境，这个运行环境就是 root namespace 环境，系统运行环境初始化完成后，便可以认为操作系统已经开始工作了。&lt;/p>
&lt;p>每一个 namespace 都基于当前内核，无论是默认的 root namespace 还是用户创建的每一个 namespace，都基于当前内核工作。所以可以认为 namespace 是内核加载后启动的一个特殊系统环境，用户进程可以在此环境中独立享用资源。更严格地说，root namespace 直接基于内核，而用户创建的 namespace 运行环境基于当前所在的 namespace。之所以用户创建的 namespace 不直接基于内核环境，是因为每一个 namespace 可能都会修改某些运行时内核参数。&lt;/p>
&lt;p>比如，用户创建的 uts namespace1 中修改了主机名为 ns1，然后在 namespace1 中创建 uts namespace2 时，namespace2 默认将共享 namespace1 的其他资源并拷贝 namespace1 的主机名资源，因此 namespace2 的主机名初始时也是 ns1。当然，namespace2 是隔离的，可以修改其主机名为 ns2，这不会影响其他 namespace，修改后，将只有 namespace2 中的进程能看到其主机名为 ns2。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gp34mf/1616122902978-3836fe05-d484-4fba-8626-939d6795c4d2.png" alt="">&lt;/p>
&lt;p>可以通过如下方式查看某个进程运行在哪一个 namespace 中，即该进程享有的独立资源来自于哪一个 namespace。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ls -l /proc/&amp;lt;PID&amp;gt;/ns&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ls -l /proc/$$/ns | awk &lt;span style="color:#e6db74">&amp;#39;{print $1,$(NF-2),$(NF-1),$NF}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx cgroup -&amp;gt; cgroup:&lt;span style="color:#f92672">[&lt;/span>4026531835&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx ipc -&amp;gt; ipc:&lt;span style="color:#f92672">[&lt;/span>4026531839&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx mnt -&amp;gt; mnt:&lt;span style="color:#f92672">[&lt;/span>4026531840&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx net -&amp;gt; net:&lt;span style="color:#f92672">[&lt;/span>4026531992&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid_for_children -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx user -&amp;gt; user:&lt;span style="color:#f92672">[&lt;/span>4026531837&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx uts -&amp;gt; uts:&lt;span style="color:#f92672">[&lt;/span>4026531838&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ls -l /proc/1/ns | awk &lt;span style="color:#e6db74">&amp;#39;{print $1,$(NF-2),$(NF-1),$NF}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx cgroup -&amp;gt; cgroup:&lt;span style="color:#f92672">[&lt;/span>4026531835&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx ipc -&amp;gt; ipc:&lt;span style="color:#f92672">[&lt;/span>4026531839&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx mnt -&amp;gt; mnt:&lt;span style="color:#f92672">[&lt;/span>4026531840&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx net -&amp;gt; net:&lt;span style="color:#f92672">[&lt;/span>4026531992&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx pid_for_children -&amp;gt; pid:&lt;span style="color:#f92672">[&lt;/span>4026531836&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx user -&amp;gt; user:&lt;span style="color:#f92672">[&lt;/span>4026531837&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lrwxrwxrwx uts -&amp;gt; uts:&lt;span style="color:#f92672">[&lt;/span>4026531838&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这些文件表示当前进程打开的 namespace 资源，每一个文件都是一个软链接，所指向的文件是一串格式特殊的名称。冒号后面中括号内的数值表示该 namespace 的 inode。如果不同进程的 namespace inode 相同，说明这些进程属于同一个 namespace。&lt;/p>
&lt;p>从结果上来看，每个进程都运行在多个 namespace 中，且 pid=1 和 pid=$$(当前 Shell 进程)两个进程的 namespace 完全一样，说明它们运行在相同的环境下(root namespace)。&lt;/p>
&lt;pre>&lt;code># namespace概念和细节相关man文档。这些 man 手册在 3.10 内核及之前版本是没有的
man namespaces
man uts_namespaces
man network_namespaces
man ipc_namespaces
man pid_namespaces
man mount_namespaces
man user_namespaces
man time_namespaces
man cgroup_namespaces
# namespace管理工具
man unshare # 创建namespace
man nscreate # 创建namespace，老版本的内核没有该工具
man nsenter # 切换namespace
man lsns # 查看当前已创建的namespace
&lt;/code>&lt;/pre>
&lt;h1 id="namespace-的具体实现">Namespace 的具体实现&lt;/h1>
&lt;p>对于 Linux 系统来说，自己本身就是一个 Namespace。系统启动的第一个进程 systemd 自己就有对应的 6 个名称空间，可以通过 lsns 命令看到 pid 为 1 的进程所使用的 Namespace，我们平时操作的地方就是 systemd 所在的 jail，所以能看到的 &lt;code>/&lt;/code> 就是 systemd 所在 jail 规定出来的 &lt;code>/&lt;/code>&lt;/p>
&lt;p>Linux Namespace 主要使用三个系统调用来实现&lt;/p>
&lt;ul>
&lt;li>&lt;strong>clone()&lt;/strong> # 实现线程的系统调用，用来创建一个新的进程 。&lt;/li>
&lt;li>&lt;strong>unshare()&lt;/strong> # 使某进程脱离某个 Namespace&lt;/li>
&lt;li>&lt;strong>setns()&lt;/strong> # 把某进程加入到某个 Namespace&lt;/li>
&lt;/ul>
&lt;p>每个 NameSpace 的说明：&lt;/p>
&lt;ol>
&lt;li>当调用 clone 时，设定了 CLONE_NEWPID，就会创建一个新的 PID Namespace，clone 出来的新进程将成为 Namespace 里的第一个进程。一个 PID Namespace 为进程提供了一个独立的 PID 环境，PID Namespace 内的 PID 将从 1 开始，在 Namespace 内调用 fork，vfork 或 clone 都将产生一个在该 Namespace 内独立的 PID。新创建的 Namespace 里的第一个进程在该 Namespace 内的 PID 将为 1，就像一个独立的系统里的 init 进程一样。该 Namespace 内的孤儿进程都将以该进程为父进程，当该进程被结束时，该 Namespace 内所有的进程都会被结束。PID Namespace 是层次性，新创建的 Namespace 将会是创建该 Namespace 的进程属于的 Namespace 的子 Namespace。子 Namespace 中的进程对于父 Namespace 是可见的，一个进程将拥有不止一个 PID，而是在所在的 Namespace 以及所有直系祖先 Namespace 中都将有一个 PID。系统启动时，内核将创建一个默认的 PID Namespace，该 Namespace 是所有以后创建的 Namespace 的祖先，因此系统所有的进程在该 Namespace 都是可见的。&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWIPC，就会创建一个新的 IPC Namespace，clone 出来的进程将成为 Namespace 里的第一个进程。一个 IPC Namespace 有一组 System V IPC objects 标识符构成，这标识符有 IPC 相关的系统调用创建。在一个 IPC Namespace 里面创建的 IPC object 对该 Namespace 内的所有进程可见，但是对其他 Namespace 不可见，这样就使得不同 Namespace 之间的进程不能直接通信，就像是在不同的系统里一样。当一个 IPC Namespace 被销毁，该 Namespace 内的所有 IPC object 会被内核自动销毁。
&lt;ol>
&lt;li>PID Namespace 和 IPC Namespace 可以组合起来一起使用，只需在调用 clone 时，同时指定 CLONE_NEWPID 和 CLONE_NEWIPC，这样新创建的 Namespace 既是一个独立的 PID 空间又是一个独立的 IPC 空间。不同 Namespace 的进程彼此不可见，也不能互相通信，这样就实现了进程间的隔离&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWNS，就会创建一个新的 mount Namespace。每个进程都存在于一个 mount Namespace 里面，mount Namespace 为进程提供了一个文件层次视图。如果不设定这个 flag，子进程和父进程将共享一个 mount Namespace，其后子进程调用 mount 或 umount 将会影响到所有该 Namespace 内的进程。如果子进程在一个独立的 mount Namespace 里面，就可以调用 mount 或 umount 建立一份新的文件层次视图。该 flag 配合 pivot_root 系统调用，可以为进程创建一个独立的目录空间。&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWNET，就会创建一个新的 Network Namespace。一个 Network Namespace 为进程提供了一个完全独立的网络协议栈的视图。包括网络设备接口，IPv4 和 IPv6 协议栈，IP 路由表，防火墙规则，sockets 等等。一个 Network Namespace 提供了一份独立的网络环境，就跟一个独立的系统一样。一个物理设备只能存在于一个 Network Namespace 中，可以从一个 Namespace 移动另一个 Namespace 中。虚拟网络设备(virtual network device)提供了一种类似管道的抽象，可以在不同的 Namespace 之间建立隧道。利用虚拟化网络设备，可以建立到其他 Namespace 中的物理设备的桥接。当一个 Network Namespace 被销毁时，物理设备会被自动移回 init Network Namespace，即系统最开始的 Namespace&lt;/li>
&lt;li>当调用 clone 时，设定了 CLONE_NEWUTS，就会创建一个新的 UTS Namespace。一个 UTS Namespace 就是一组被 uname 返回的标识符。新的 UTS Namespace 中的标识符通过复制调用进程所属的 Namespace 的标识符来初始化。Clone 出来的进程可以通过相关系统调用改变这些标识符，比如调用 sethostname 来改变该 Namespace 的 hostname。这一改变对该 Namespace 内的所有进程可见。CLONE_NEWUTS 和 CLONE_NEWNET 一起使用，可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样。&lt;/li>
&lt;/ol>
&lt;p>以上所有 clone flag 都可以一起使用，为进程提供了一个独立的运行环境。LXC 正是通过在 clone 时设定这些 flag，为进程创建一个有独立 PID，IPC，FS，Network，UTS 空间的 container。一个 container 就是一个虚拟的运行环境，对 container 里的进程是透明的，它会以为自己是直接在一个系统上运行的。&lt;/p>
&lt;h1 id="namespace-关联文件">Namespace 关联文件&lt;/h1>
&lt;p>主信息：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>/proc/PID/ns/*&lt;/strong> # 由于 namespace 都是与进程相关联，那么可以通过从每个进程的 ns 目录查看相关进程的 namespace 使用情况&lt;/li>
&lt;/ul>
&lt;p>Network Namespace：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>/var/run/netns/NAME&lt;/strong> # 该目录为 &lt;code>ip netns&lt;/code> 命令所能调取查看的目录
&lt;ol>
&lt;li>如果想让 &lt;code>ip netns&lt;/code> 命令查看到网络名称空间的信息，则需要把 /proc/PID/ns/net 文件链接到该目录即可&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 1.nova</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.nova/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.nova/</guid><description>
&lt;p>语法格式：nova [OPTIONS] [SubCommand [OPTIONS]]&lt;/p>
&lt;p>nova list [OPTIONS] #列出SERVER相关信息&lt;/p>
&lt;p>nova show #显示指定SERVER的详细信息，非常详细&lt;/p>
&lt;p>nova instacne-action-list #列出指定SERVER的操作，创建、启动、停止、删除时间等，&lt;/p>
&lt;p>语法格式：nova [OPTIONS] [SubCommand [OPTIONS]]&lt;/p>
&lt;p>注意：语法中的SERVER指的都是已经创建的虚拟服务器，SERVER可以用实例的NAME(实例名)或者UUID(实例的ID)来表示，SERVER的ID和NAME可以用过nova list命令查到&lt;/p>
&lt;p>UUID:Universally Unique Identifier,即通用唯一识别码&lt;/p>
&lt;p>可以使用nova help SubCommand命令查看相关子命令的使用方法&lt;/p>
&lt;p>nova list [OPTIONS] #列出SERVER相关信息&lt;/p>
&lt;ol>
&lt;li>
&lt;p>OPTIONS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--all-tenants #显示所有租户的SERVER信息，可简写为&amp;ndash;all-t&lt;/p>
&lt;/li>
&lt;li>
&lt;p>--tenant [] #显示指定租户的SERVER信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova list &amp;ndash;all-t #显示所有正在运行的实例，可以查看实例以及ID和主机名&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova list &amp;ndash;all-t &amp;ndash;host &lt;code>cat /etc/uuid&lt;/code> #显示&lt;code>cat /etc/uuid&lt;/code>命令输出的主机名的节点运行的实例信息&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nova show #显示指定SERVER的详细信息，非常详细&lt;/p>
&lt;ol>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova show ID #以实例ID展示该实例的详细信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova show ID | grep host #以实例ID查看所在节点的主机名&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nova instacne-action-list #列出指定SERVER的操作，创建、启动、停止、删除时间等，&lt;/p>
&lt;ol>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nova instance-action-list ID|NAME #以实例ID显示该实例的活动信息，包括启动、停止、创建时间等&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>nova&lt;/p>
&lt;p>nova [&amp;ndash;version] [&amp;ndash;debug] [&amp;ndash;os-cache] [&amp;ndash;timings]&lt;/p>
&lt;pre>&lt;code> [--os-region-name ] [--service-type ]
[--service-name ]
[--os-endpoint-type ]
[--os-compute-api-version ]
[--endpoint-override ] [--profile HMAC_KEY]
[--insecure] [--os-cacert ]
[--os-cert ] [--os-key ] [--timeout ]
[--os-auth-type ] [--os-auth-url OS_AUTH_URL]
[--os-system-scope OS_SYSTEM_SCOPE] [--os-domain-id OS_DOMAIN_ID]
[--os-domain-name OS_DOMAIN_NAME] [--os-project-id OS_PROJECT_ID]
[--os-project-name OS_PROJECT_NAME]
[--os-project-domain-id OS_PROJECT_DOMAIN_ID]
[--os-project-domain-name OS_PROJECT_DOMAIN_NAME]
[--os-trust-id OS_TRUST_ID]
[--os-default-domain-id OS_DEFAULT_DOMAIN_ID]
[--os-default-domain-name OS_DEFAULT_DOMAIN_NAME]
[--os-user-id OS_USER_ID] [--os-username OS_USERNAME]
[--os-user-domain-id OS_USER_DOMAIN_ID]
[--os-user-domain-name OS_USER_DOMAIN_NAME]
[--os-password OS_PASSWORD]
...
&lt;/code>&lt;/pre>
&lt;p>Command-line interface to the OpenStack Nova API.&lt;/p>
&lt;p>Positional arguments:位置参数&lt;/p>
&lt;p>子命令&lt;/p>
&lt;pre>&lt;code>add-secgroup Add a Security Group to a server.
agent-create Create new agent build.
agent-delete Delete existing agent build.
agent-list List all builds.
agent-modify Modify existing agent build.
aggregate-add-host Add the host to the specified aggregate.
aggregate-create Create a new aggregate with the specified
details.
aggregate-delete Delete the aggregate.
aggregate-list Print a list of all aggregates.
aggregate-remove-host Remove the specified host from the specified
aggregate.
aggregate-set-metadata Update the metadata associated with the
aggregate.
aggregate-show Show details of the specified aggregate.
aggregate-update Update the aggregate's name and optionally
availability zone.
availability-zone-list List all the availability zones.
backup Backup a server by creating a 'backup' type
snapshot.
boot Boot a new server.
cell-capacities Get cell capacities for all cells or a given
cell.
cell-show Show details of a given cell.
clear-password Clear the admin password for a server from the
metadata server. This action does not actually
change the instance server password.
console-log Get console log output of a server.
delete Immediately shut down and delete specified
server(s).
diagnostics Retrieve server diagnostics.
evacuate Evacuate server from failed host.
flavor-access-add Add flavor access for the given tenant.
flavor-access-list Print access information about the given
flavor.
flavor-access-remove Remove flavor access for the given tenant.
flavor-create Create a new flavor.
flavor-delete Delete a specific flavor
flavor-key Set or unset extra_spec for a flavor.
flavor-list Print a list of available 'flavors' (sizes of
servers).
flavor-show Show details about the given flavor.
flavor-update Update the description of an existing flavor.
(Supported by API versions '2.55' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
force-delete Force delete a server.
get-mks-console Get an MKS console to a server. (Supported by
API versions '2.8' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
get-password Get the admin password for a server. This
operation calls the metadata service to query
metadata information and does not read
password information from the server itself.
get-rdp-console Get a rdp console to a server.
get-serial-console Get a serial console to a server.
get-spice-console Get a spice console to a server.
get-vnc-console Get a vnc console to a server.
host-evacuate Evacuate all instances from failed host.
host-evacuate-live Live migrate all instances of the specified
host to other available hosts.
host-meta Set or Delete metadata on all instances of a
host.
host-servers-migrate Cold migrate all instances off the specified
host to other available hosts.
hypervisor-list List hypervisors. (Supported by API versions
'2.0' - '2.latest') [hint: use '--os-compute-
api-version' flag to show help message for
proper version]
hypervisor-servers List servers belonging to specific
hypervisors.
hypervisor-show Display the details of the specified
hypervisor.
hypervisor-stats Get hypervisor statistics over all compute
nodes.
hypervisor-uptime Display the uptime of the specified
hypervisor.
image-create Create a new image by taking a snapshot of a
running server.
instance-action Show an action.
instance-action-list List actions on a server. (Supported by API
versions '2.0' - '2.latest') [hint: use '--os-
compute-api-version' flag to show help message
for proper version]
interface-attach Attach a network interface to a server.
interface-detach Detach a network interface from a server.
interface-list List interfaces attached to a server.
keypair-add Create a new key pair for use with servers.
keypair-delete Delete keypair given by its name. (Supported
by API versions '2.0' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
keypair-list Print a list of keypairs for a user (Supported
by API versions '2.0' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
keypair-show Show details about the given keypair.
(Supported by API versions '2.0' - '2.latest')
[hint: use '--os-compute-api-version' flag to
show help message for proper version]
limits Print rate and absolute limits.
list List servers.
list-extensions List all the os-api extensions that are
available.
list-secgroup List Security Group(s) of a server.
live-migration Migrate running server to a new machine.
live-migration-abort Abort an on-going live migration. (Supported
by API versions '2.24' - '2.latest') [hint:
use '--os-compute-api-version' flag to show
help message for proper version]
live-migration-force-complete
Force on-going live migration to complete.
(Supported by API versions '2.22' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
lock Lock a server. A normal (non-admin) user will
not be able to execute actions on a locked
server.
meta Set or delete metadata on a server.
migrate Migrate a server.
migration-list Print a list of migrations. (Supported by API
versions '2.0' - '2.latest') [hint: use '--os-
compute-api-version' flag to show help message
for proper version]
pause Pause a server.
quota-class-show List the quotas for a quota class.
quota-class-update Update the quotas for a quota class.
(Supported by API versions '2.0' - '2.latest')
[hint: use '--os-compute-api-version' flag to
show help message for proper version]
quota-defaults List the default quotas for a tenant.
quota-delete Delete quota for a tenant/user so their quota
will Revert back to default.
quota-show List the quotas for a tenant/user.
quota-update Update the quotas for a tenant/user.
(Supported by API versions '2.0' - '2.latest')
[hint: use '--os-compute-api-version' flag to
show help message for proper version]
reboot Reboot a server.
rebuild Shutdown, re-image, and re-boot a server.
refresh-network Refresh server network information.
remove-secgroup Remove a Security Group from a server.
rescue Reboots a server into rescue mode, which
starts the machine from either the initial
image or a specified image, attaching the
current boot disk as secondary.
reset-network Reset network of a server.
reset-state Reset the state of a server.
resize Resize a server.
resize-confirm Confirm a previous resize.
resize-revert Revert a previous resize (and return to the
previous VM).
restore Restore a soft-deleted server.
resume Resume a server.
server-group-create Create a new server group with the specified
details.
server-group-delete Delete specific server group(s).
server-group-get Get a specific server group.
server-group-list Print a list of all server groups.
server-migration-list Get the migrations list of specified server.
(Supported by API versions '2.23' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
server-migration-show Get the migration of specified server.
(Supported by API versions '2.23' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
server-tag-add Add one or more tags to a server. (Supported
by API versions '2.26' - '2.latest') [hint:
use '--os-compute-api-version' flag to show
help message for proper version]
server-tag-delete Delete one or more tags from a server.
(Supported by API versions '2.26' -
'2.latest') [hint: use '--os-compute-api-
version' flag to show help message for proper
version]
server-tag-delete-all Delete all tags from a server. (Supported by
API versions '2.26' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
server-tag-list Get list of tags from a server. (Supported by
API versions '2.26' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
server-tag-set Set list of tags to a server. (Supported by
API versions '2.26' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
service-delete Delete the service by UUID ID. (Supported by
API versions '2.0' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
service-disable Disable the service. (Supported by API
versions '2.0' - '2.latest') [hint: use '--os-
compute-api-version' flag to show help message
for proper version]
service-enable Enable the service. (Supported by API versions
'2.0' - '2.latest') [hint: use '--os-compute-
api-version' flag to show help message for
proper version]
service-force-down Force service to down. (Supported by API
versions '2.11' - '2.latest') [hint: use
'--os-compute-api-version' flag to show help
message for proper version]
service-list Show a list of all running services. Filter by
host &amp;amp; binary.
set-password Change the admin password for a server.
shelve Shelve a server.
shelve-offload Remove a shelved server from the compute node.
show Show details about the given server.
ssh SSH into a server.
start Start the server(s).
stop Stop the server(s).
suspend Suspend a server.
trigger-crash-dump Trigger crash dump in an instance. (Supported by API versions '2.17' - '2.latest') [hint:
use '--os-compute-api-version' flag to show
help message for proper version]
unlock Unlock a server.
unpause Unpause a server.
unrescue Restart the server from normal boot disk
again.
unshelve Unshelve a server.
update Update the name or the description for a
server.
usage Show usage data for a single tenant.
usage-list List usage data for all tenants.
version-list List all API versions.
volume-attach Attach a volume to a server.
volume-attachments List all the volumes attached to a server.
volume-detach Detach a volume from a server.
volume-update Update the attachment on the server. Migrates
the data from an attached volume to the
specified available volume and swaps out the
active attachment to the new volume.
bash-completion Prints all of the commands and options to
stdout so that the nova.bash_completion script
doesn't have to hard code them.
help Display help about this program or one of its
subcommands.
&lt;/code>&lt;/pre>
&lt;p>Optional arguments:可选参数&lt;/p>
&lt;p>--version show program&amp;rsquo;s version number and exit&lt;/p>
&lt;p>--debug Print debugging output.&lt;/p>
&lt;p>--os-cache Use the auth token cache. Defaults to False if&lt;/p>
&lt;pre>&lt;code> env[OS_CACHE] is not set.
&lt;/code>&lt;/pre>
&lt;p>--timings Print call timing info.&lt;/p>
&lt;p>--os-region-name&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_REGION_NAME].
&lt;/code>&lt;/pre>
&lt;p>--service-type&lt;/p>
&lt;pre>&lt;code> Defaults to compute for most actions.
&lt;/code>&lt;/pre>
&lt;p>--service-name&lt;/p>
&lt;pre>&lt;code> Defaults to env[NOVA_SERVICE_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-endpoint-type&lt;/p>
&lt;pre>&lt;code> Defaults to env[NOVA_ENDPOINT_TYPE],
env[OS_ENDPOINT_TYPE] or publicURL.
&lt;/code>&lt;/pre>
&lt;p>--os-compute-api-version&lt;/p>
&lt;pre>&lt;code> Accepts X, X.Y (where X is major and Y is
minor part) or &amp;quot;X.latest&amp;quot;, defaults to
env[OS_COMPUTE_API_VERSION].
&lt;/code>&lt;/pre>
&lt;p>--endpoint-override&lt;/p>
&lt;pre>&lt;code> Use this API endpoint instead of the Service
Catalog. Defaults to
env[NOVACLIENT_ENDPOINT_OVERRIDE].
&lt;/code>&lt;/pre>
&lt;p>--profile HMAC_KEY HMAC key to use for encrypting context data&lt;/p>
&lt;pre>&lt;code> for performance profiling of operation. This
key should be the value of the HMAC key
configured for the OSprofiler middleware in
nova; it is specified in the Nova
configuration file at &amp;quot;/etc/nova/nova.conf&amp;quot;.
Without the key, profiling will not be
triggered even if OSprofiler is enabled on the
server side.
&lt;/code>&lt;/pre>
&lt;p>--os-auth-type , &amp;ndash;os-auth-plugin&lt;/p>
&lt;pre>&lt;code> Authentication type to use
&lt;/code>&lt;/pre>
&lt;p>API Connection Options:API连接选项&lt;/p>
&lt;p>Options controlling the HTTP API Connections&lt;/p>
&lt;p>--insecure Explicitly allow client to perform &amp;ldquo;insecure&amp;rdquo;&lt;/p>
&lt;pre>&lt;code> TLS (https) requests. The server's certificate
will not be verified against any certificate
authorities. This option should be used with
caution.
&lt;/code>&lt;/pre>
&lt;p>--os-cacert Specify a CA bundle file to use in verifying a&lt;/p>
&lt;pre>&lt;code> TLS (https) server certificate. Defaults to
env[OS_CACERT].
&lt;/code>&lt;/pre>
&lt;p>--os-cert Defaults to env[OS_CERT].&lt;/p>
&lt;p>--os-key Defaults to env[OS_KEY].&lt;/p>
&lt;p>--timeout Set request timeout (in seconds).&lt;/p>
&lt;p>Authentication Options:认证选项&lt;/p>
&lt;p>Options specific to the password plugin.&lt;/p>
&lt;p>--os-auth-url OS_AUTH_URL Authentication URL&lt;/p>
&lt;p>--os-system-scope OS_SYSTEM_SCOPE&lt;/p>
&lt;pre>&lt;code> Scope for system operations
&lt;/code>&lt;/pre>
&lt;p>--os-domain-id OS_DOMAIN_ID Domain ID to scope to&lt;/p>
&lt;p>--os-domain-name OS_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> Domain name to scope to
&lt;/code>&lt;/pre>
&lt;p>--os-project-id OS_PROJECT_ID, &amp;ndash;os-tenant-id OS_PROJECT_ID&lt;/p>
&lt;pre>&lt;code> Project ID to scope to
&lt;/code>&lt;/pre>
&lt;p>--os-project-name OS_PROJECT_NAME, &amp;ndash;os-tenant-name OS_PROJECT_NAME&lt;/p>
&lt;pre>&lt;code> Project name to scope to
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-id OS_PROJECT_DOMAIN_ID&lt;/p>
&lt;pre>&lt;code> Domain ID containing project
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-name OS_PROJECT_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> Domain name containing project
&lt;/code>&lt;/pre>
&lt;p>--os-trust-id OS_TRUST_ID Trust ID&lt;/p>
&lt;p>--os-default-domain-id OS_DEFAULT_DOMAIN_ID&lt;/p>
&lt;pre>&lt;code> Optional domain ID to use with v3 and v2
parameters. It will be used for both the user
and project domain in v3 and ignored in v2
authentication.
&lt;/code>&lt;/pre>
&lt;p>--os-default-domain-name OS_DEFAULT_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> Optional domain name to use with v3 API and v2
parameters. It will be used for both the user
and project domain in v3 and ignored in v2
authentication.
&lt;/code>&lt;/pre>
&lt;p>--os-user-id OS_USER_ID User id&lt;/p>
&lt;p>--os-username OS_USERNAME, &amp;ndash;os-user-name OS_USERNAME&lt;/p>
&lt;pre>&lt;code> Username
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-id OS_USER_DOMAIN_ID&lt;/p>
&lt;pre>&lt;code> User's domain id
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-name OS_USER_DOMAIN_NAME&lt;/p>
&lt;pre>&lt;code> User's domain name
&lt;/code>&lt;/pre>
&lt;p>--os-password OS_PASSWORD User&amp;rsquo;s password&lt;/p>
&lt;p>See &amp;ldquo;nova help COMMAND&amp;rdquo; for help on a specific command.&lt;/p></description></item><item><title>Docs: 1.openstack Command Line基础</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.openstack-command-line%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/1.openstack-command-line%E5%9F%BA%E7%A1%80/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>常用命令&lt;/p>
&lt;p>openstack 命令行控制简介&lt;/p>
&lt;p>openstack CLI 的认证方式：&lt;/p>
&lt;p>语法格式：openstack [OPTIONS] Command [CommandArguments]&lt;/p>
&lt;p>Command&lt;/p>
&lt;p>openstack.cli #CLI 命令行控制命令组&lt;/p>
&lt;p>openstack.common # common 通用命令组&lt;/p>
&lt;p>openstack.compute.v2 #compute 计算服务命令组&lt;/p>
&lt;p>openstack.identity.v3 #identity 身份服务命令组&lt;/p>
&lt;p>openstack.image.v2 #image 镜像服务命令组&lt;/p>
&lt;p>openstack.network.v2 #network 网络服务命令组&lt;/p>
&lt;p>openstack.neutronclient.v2 #neutron 客户端命令组&lt;/p>
&lt;p>openstack.object_store.v1 #objectStore 对象存储服务命令组&lt;/p>
&lt;p>openstack.volume.v2 #volume 卷服务命令组&lt;/p>
&lt;p>常用命令&lt;/p>
&lt;p>虚拟机的：server、console&lt;/p>
&lt;p>网络的：network、subnet、port、router&lt;/p>
&lt;p>镜像的：image&lt;/p>
&lt;p>存储的：&lt;/p>
&lt;p>实例类型：flavor&lt;/p>
&lt;p>openstack 命令行控制简介&lt;/p>
&lt;p>Openstack Command Line Client 官方介绍：&lt;a href="https://docs.openstack.org/python-openstackclient/rocky/">https://docs.openstack.org/python-openstackclient/rocky/&lt;/a>&lt;/p>
&lt;p>OpenStackClient(又名 OSC)是&lt;strong>openstack 的命令行客户端&lt;/strong>，这个客户端将 compute、identity、image、object、storage 和 blockStorage 这些的 APIs 一起放在一个具有统一命令结构的 shell 中。e.g.nova、neutron、glance 等命令，都会集中在 openstack 的子命令中。&lt;/p>
&lt;p>openstack CLI 的认证：&lt;/p>
&lt;p>使用 openstack CLI 需要进行认证，才能通过该客户端与 openstack 各个组件的 API 进行交互，否则，是没有权限对 openstack 集群进行任何控制的。&lt;/p>
&lt;p>认证方式一般是通过环境变量来进行的。不同的安装方式，认证方式不同&lt;/p>
&lt;ol>
&lt;li>
&lt;p>kolla-ansible 部署的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>直接使用 kolla 提供的配置文件，加载文件中的环境变量即可。i.e.source /etc/kolla/admin-openrc.sh&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>如果不进行认证，一般会出现以下几种报错&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ERROR (CommandError): You must provide a user name/id (via &amp;ndash;os-username, &amp;ndash;os-user-id, env[OS_USERNAME] or env[OS_USER_ID]) or an auth token (via &amp;ndash;os-token).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Missing value auth-url required for auth plugin password&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>语法格式：openstack [OPTIONS] Command [CommandArguments]&lt;/p>
&lt;p>启动一个 shell 来执行 openstack 客户端中的 Command，或者直接使用 openstack+Command 来对 openstack 进行管理与控制&lt;/p>
&lt;p>OPTIONS #global options 全局选项，用来控制 openstack 程序，大部分都是关于认证的选项&lt;/p>
&lt;p>Command&lt;/p>
&lt;p>所有 openstack 的可用的 Command 可以通过&lt;code>openstack command list&lt;/code> 命令所列出的列表来查看。这些命令通过组来划分，每个命令组代表对一类服务的控制命令&lt;/p>
&lt;p>openstack.cli #CLI 命令行控制命令组&lt;/p>
&lt;p>command list [&amp;ndash;group ] #按组列出 openstack 可以支持的所有 Command，可以在选项中指定要查看的具体组名，只查看该组的命令。GroupKeyword 可以使组名中的关键字，不用使用完整的组名&lt;/p>
&lt;p>module list [&amp;ndash;all] #显示 OSC 程序已经安装的 python 模块&lt;/p>
&lt;p>openstack.common # common 通用命令组&lt;/p>
&lt;p>availability&lt;/p>
&lt;p>configuration&lt;/p>
&lt;p>extension&lt;/p>
&lt;p>limits&lt;/p>
&lt;p>project&lt;/p>
&lt;p>quota&lt;/p>
&lt;p>versions&lt;/p>
&lt;p>openstack.compute.v2 #compute 计算服务命令组&lt;/p>
&lt;p>aggregate&lt;/p>
&lt;p>compute&lt;/p>
&lt;p>console #实例控制台相关控制命令&lt;/p>
&lt;p>openstack console log show [&amp;ndash;lines ] SERVER&lt;/p>
&lt;p>openstack console url show [&amp;ndash;novnc | &amp;ndash;xvpvnc | &amp;ndash;spice][&amp;ndash;rdp | &amp;ndash;serial | &amp;ndash;mks] SERVER&lt;/p>
&lt;p>flavor #实例类型相关控制命令&lt;/p>
&lt;p>openstack flavor SubCommand [OPTIONS] [ARGS]&lt;/p>
&lt;p>SubCommand 包括：&lt;/p>
&lt;ol>
&lt;li>list&lt;/li>
&lt;/ol>
&lt;p>host&lt;/p>
&lt;p>hypervisor&lt;/p>
&lt;p>ip&lt;/p>
&lt;p>keypair&lt;/p>
&lt;p>server #控制 openstack 上所开虚拟机的命令&lt;/p>
&lt;p>openstack server SubCommand [OPTIONS] [ARGS]&lt;/p>
&lt;p>SubCommand 包括：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>list #列出 openstack 上所开的虚拟机 &lt;/p>
&lt;/li>
&lt;li>
&lt;p>OPTIONS &lt;/p>
&lt;/li>
&lt;li>
&lt;p>--long #列出更多的关于虚拟机的信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>openstack server list #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>set #设置服务器属性&lt;/p>
&lt;/li>
&lt;li>
&lt;p>OPTIONS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>openstack server set &amp;ndash;root-password centos7 #为名为 centos7 的实例修改 root 密码&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>usage&lt;/p>
&lt;p>openstack.identity.v3 #identity 身份服务命令组&lt;/p>
&lt;p>access&lt;/p>
&lt;p>application&lt;/p>
&lt;p>catalog&lt;/p>
&lt;p>consumer&lt;/p>
&lt;p>credential&lt;/p>
&lt;p>domain #domain 域，是用户、组、项目的集合，每个组合项目仅有一个域&lt;/p>
&lt;p>ec2&lt;/p>
&lt;p>endpoint&lt;/p>
&lt;p>federation&lt;/p>
&lt;p>group&lt;/p>
&lt;p>identity&lt;/p>
&lt;p>implied&lt;/p>
&lt;p>limit&lt;/p>
&lt;p>mapping&lt;/p>
&lt;p>policy&lt;/p>
&lt;p>project&lt;/p>
&lt;p>region&lt;/p>
&lt;p>registered&lt;/p>
&lt;p>request&lt;/p>
&lt;p>role&lt;/p>
&lt;p>service&lt;/p>
&lt;p>token&lt;/p>
&lt;p>trust&lt;/p>
&lt;p>user&lt;/p>
&lt;p>openstack.image.v2 #image 镜像服务命令组&lt;/p>
&lt;p>image&lt;/p>
&lt;p>openstack.network.v2 #network 网络服务命令组&lt;/p>
&lt;p>address&lt;/p>
&lt;p>floating&lt;/p>
&lt;p>ip&lt;/p>
&lt;p>network&lt;/p>
&lt;p>port&lt;/p>
&lt;p>router&lt;/p>
&lt;p>security&lt;/p>
&lt;p>subnet&lt;/p>
&lt;p>openstack.neutronclient.v2 #neutron 客户端命令组&lt;/p>
&lt;p>bgp&lt;/p>
&lt;p>bgpvpn&lt;/p>
&lt;p>firewall&lt;/p>
&lt;p>network&lt;/p>
&lt;p>sfc&lt;/p>
&lt;p>vpn&lt;/p>
&lt;p>openstack.object_store.v1 #objectStore 对象存储服务命令组&lt;/p>
&lt;p>container&lt;/p>
&lt;p>object&lt;/p>
&lt;p>openstack.volume.v2 #volume 卷服务命令组&lt;/p>
&lt;p>backup&lt;/p>
&lt;p>consistency&lt;/p>
&lt;p>snapshot&lt;/p>
&lt;p>volume&lt;/p></description></item><item><title>Docs: 1.Redis</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/1.redis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/1.redis/</guid><description>
&lt;h1 id="redis-介绍">Redis 介绍&lt;/h1>
&lt;p>参考：&lt;a href="https://redis.io/">官网&lt;/a>&lt;/p>
&lt;p>Redis 是一个开源的、网络化的、内存中的、具有持久化的键值数据存储。(是否持久化根据配置决定)&lt;/p>
&lt;p>Redis 是一个内存数据库, 所有数据默认都存在于内存当中,可以配置“定时以追加或者快照”的方式储存到硬盘中. 由于 redis 是一个内存数据库, 所以读取写入的速度是非常快的, 所以经常被用来做数据, 页面等的缓存。&lt;/p>
&lt;p>Redis 的组件&lt;/p>
&lt;ol>
&lt;li>
&lt;p>redis-server # 服务端&lt;/p>
&lt;/li>
&lt;li>
&lt;p>redis-cli # 命令行客户端&lt;/p>
&lt;/li>
&lt;li>
&lt;p>redis-benchmark # 压测工具&lt;/p>
&lt;/li>
&lt;li>
&lt;p>redis-check-dump &amp;amp;&amp;amp; redis-check-aof # 检测工具&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Redis 的数据类型&lt;/p>
&lt;p>string(字符串)，hash(哈希)，list(列表)，set(集合) 及 zset(sorted set：有序集合)。&lt;/p>
&lt;p>后面增加了：&lt;/p>
&lt;p>Bit arrays (或者说 simply bitmaps)&lt;/p>
&lt;p>在 2.8.9 版本添加了 HyperLogLog 结构&lt;/p>
&lt;p>Redis 部署&lt;/p>
&lt;p>Docker 启动 Redis&lt;/p>
&lt;pre>&lt;code>docker run -d --name redis \
--network=host \
redis:5.0.10-alpine
&lt;/code>&lt;/pre>
&lt;p>Redis 配置&lt;/p>
&lt;p>**/etc/redis.conf **# Redis 主程序的配置文件&lt;/p>
&lt;p>&lt;strong>/var/lib/redis/*&lt;/strong> # 默认的数据存储目录&lt;/p>
&lt;p>Redis 数据持久化的方式&lt;/p>
&lt;p>Redis 的数据是保存在内存中的，如果设备宕机，则数据丢失，所以 Redis 提供两种可以将数据从内存中写入硬盘的方式，默认为 RDB，AOF 默认不开启&lt;/p>
&lt;p>RDB Redis Data Base&lt;/p>
&lt;p>相关配置在配置文件的 SNAPSHOT 配置环境中。&lt;/p>
&lt;p>在默认情况下， Redis 将数据库快照保存在名字为 dump.rdb 的二进制文件中。可以对 Redis 进行设置， 让 Redis 在“ N 秒内数据集至少有 M 个 key 改动”这一条件被满足时， 自动保存一次数据集。也可以通过调用 SAVE 或者 BGSAVE ， 手动让 Redis 进行数据集保存操作。&lt;/p>
&lt;p>这种持久化的方式被称为快照（snapshotting），会将数据保存在一个指定的文件中。&lt;/p>
&lt;p>RDB 工作原理：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>触发 RDB 后，redis 会调用 fork(),产生一个与主程序同名的子进程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>该子进程会将现有内存中的数据写入到一个临时的 RDB 文件中。(文件名一般为：temp-XXX.rdb)&lt;/p>
&lt;ol>
&lt;li>Redis 默认会使用 LZF 算法对数据进行压缩。该算法会消耗大量 CPU，可以在配置中关闭压缩功能，但是数据量写入到磁盘后，会占用大量磁盘空间。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>当子进程完成对临时 RDB 文件的写入时，Redis 用这个临时的 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。&lt;/p>
&lt;p>RDB 优点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>RDB 文件是一个很简洁的单文件，它保存了某个时间点的 Redis 数据，很适合用于做备份。你可以设定一个时间点对 RDB 文件进行归档，这样就能在需要的时候很轻易的把数据恢复到不同的版本。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>基于上面所描述的特性，RDB 很适合用于灾备。单文件很方便就能传输到远程的服务器上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RDB 的性能很好，需要进行持久化时，主进程会 fork 一个子进程出来，然后把持久化的工作交给子进程，自己不会有相关的 I/O 操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>比起 AOF，在数据量比较大的情况下，RDB 的启动速度更快。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>RDB 缺点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>RDB 容易造成数据的丢失。假设每 5 分钟保存一次快照，如果 Redis 因为某些原因不能正常工作，那么从上次产生快照到 Redis 出现问题这段时间的数据就会丢失了。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RDB 使用 fork()产生子进程进行数据的持久化，如果数据比较大的话可能就会花费点时间，造成 Redis 停止服务几毫秒。如果数据量很大且 CPU 性能不是很好的时候，停止服务的时间甚至会到 1 秒。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="aof-append-only-file">AOF Append Only File&lt;/h2>
&lt;p>配置文件中配置环境 APPEND ONLY MOD E 可以进项相关配置。&lt;/p>
&lt;p>默认 redis 使用的是 rdb 方式持久化，这种方式在许多应用中已经足够用了。但是 redis 如果中途宕机，会导致可能有几分钟的数据丢失。&lt;/p>
&lt;p>Append Only File 是另一种持久化方式，可以提供更好的持久化特性。Redis 会把每次写入的数据在接收后都写入 appendonly.aof 文件，每次启动时 Redis 都会先把这个文件的数据读入内存里，先忽略 RDB 文件。&lt;/p>
&lt;p>AOF 工作方式：&lt;/p>
&lt;p>每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Redis 执行 fork() ，现在同时拥有父进程和子进程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>子进程开始将新 AOF 文件的内容写入到临时文件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾,这样样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>AOF 优点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>使用 AOF 会让你的 Redis 更加耐久: 你可以使用不同的 fsync 策略：无 fsync,每秒 fsync,每次写的时候 fsync.使用默认的每秒 fsync 策略,Redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求),一旦出现故障，你最多丢失 1 秒的数据.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AOF 文件是一个只进行追加的日志文件,所以不需要写入 seek,即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令,你也也可使用 redis-check-aof 工具修复这些问题.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>AOF 缺点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>下图是 redis 启动后读取本地存储文件的过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gghosd/1616134974086-4020ec57-f508-4a12-b30e-aba72d1730e4.jpeg" alt="">&lt;/p></description></item><item><title>Docs: 1.Server Message Block</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/network-attached-storage%E7%BD%91%E7%BB%9C%E9%99%84%E5%8A%A0%E5%AD%98%E5%82%A8/1.server-message-block/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/network-attached-storage%E7%BD%91%E7%BB%9C%E9%99%84%E5%8A%A0%E5%AD%98%E5%82%A8/1.server-message-block/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block">Wiki,SMB&lt;/a>、&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block">Wiki,Samba&lt;/a>、&lt;a href="https://searchstorage.techtarget.com/definition/Common-Internet-File-System-CIFS">Techtarget&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Server Message Block(服务器消息块，简称 SMB)&lt;/strong> 是一个通信协议，该协议可以让网络中的各个节点可以共享访问 文件、打印机、串行端口，还可以提供经过身份验证的 &lt;a href="https://www.yuque.com/go/doc/33222674">IPC&lt;/a> 机制。&lt;/p>
&lt;p>现在，SMB 协议主要用来让 Windows 和 Linux 之间可以互相传输文件。&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;h3 id="smb--cifs--smb1edithttpsenwikipediaorgwindexphptitleserver_message_blockactioneditsection3smb--cifs--smb1-编辑">SMB / CIFS / SMB1[&lt;a href="https://en.wikipedia.org/w/index.php?title=Server_Message_Block&amp;amp;action=edit&amp;amp;section=3">edit&lt;/a>]SMB / CIFS / SMB1 [编辑]&lt;/h3>
&lt;p>&lt;a href="https://en.wikipedia.org/w/index.php?title=Barry_Feigenbaum&amp;amp;action=edit&amp;amp;redlink=1">Barry Feigenbaum&lt;/a> originally designed SMB at &lt;a href="https://en.wikipedia.org/wiki/IBM">IBM&lt;/a> in early 1983 with the aim of turning &lt;a href="https://en.wikipedia.org/wiki/DOS">DOS&lt;/a> &lt;a href="https://en.wikipedia.org/wiki/INT_21h">INT 21h&lt;/a> local file access into a networked file system.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-tridgemyths-9">[9]&lt;/a> &lt;a href="https://en.wikipedia.org/wiki/Microsoft">Microsoft&lt;/a> has made considerable modifications to the most commonly used version. Microsoft merged the SMB protocol with the &lt;a href="https://en.wikipedia.org/wiki/LAN_Manager">LAN Manager&lt;/a> product which it had started developing for &lt;a href="https://en.wikipedia.org/wiki/OS/2">OS/2&lt;/a> with &lt;a href="https://en.wikipedia.org/wiki/3Com">3Com&lt;/a> around 1990, and continued to add features to the protocol in &lt;a href="https://en.wikipedia.org/wiki/Windows_for_Workgroups">Windows for Workgroups&lt;/a> (c. 1992) and in later versions of Windows.Barry Feigenbaum 在 1983 年初在 IBM 设计了 SMB，目的是将 DOS INT 21H 本地文件访问转换为网络文件系统。[9] Microsoft 对最常用的版本进行了相当大的修改。 Microsoft 将 SMB 协议与 LAN Manager 产品合并，它已开始为 1990 年左右开始为 OS / 2 开发的 OS / 2，并继续向工作组（C.192）和更高版本的 Windows 中的 Windows 中的协议添加功能。
SMB was originally designed to run on top of the &lt;a href="https://en.wikipedia.org/wiki/NetBIOS">NetBIOS&lt;/a>/NetBEUI &lt;a href="https://en.wikipedia.org/wiki/Application_programming_interface">API&lt;/a> (typically implemented with &lt;a href="https://en.wikipedia.org/wiki/NetBIOS_Frames">NBF&lt;/a>, NetBIOS over &lt;a href="https://en.wikipedia.org/wiki/IPX/SPX">IPX/SPX&lt;/a>, or &lt;a href="https://en.wikipedia.org/wiki/NetBIOS_over_TCP/IP">NBT&lt;/a>). Since &lt;a href="https://en.wikipedia.org/wiki/Windows_2000">Windows 2000&lt;/a>, SMB runs, by default, with a thin layer, similar to the Session Message packet of NBT&amp;rsquo;s Session Service, on top of &lt;a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP&lt;/a>, using TCP port 445 rather than TCP port 139—a feature known as &amp;ldquo;direct host SMB&amp;rdquo;.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-direct-10">[10]&lt;/a>SMB 最初设计用于在 NetBIOS / NetBEUI API 之上（通常使用 NBF，NETBIOS 通过 IPX / SPX 或 NBT 实现）。由于 Windows 2000，SMB 默认情况下，使用 TCP 端口 445 而不是 TCP 端口 139-A 称为“Direct Host SMB”的特征。[10]
Windows Server 2003, and older &lt;a href="https://en.wikipedia.org/wiki/Network-attached_storage">NAS&lt;/a> devices use SMB1/CIFS natively. SMB1/CIFS is an extremely chatty protocol which is not such an issue on a local area network with low latency but becomes very slow on wide area networks as the back and forth handshake of the protocol magnifies the inherent high latency of such network. Later versions of the protocol reduced the high number of handshake exchanges. While Microsoft estimates that SMB1/CIFS comprises less than 10% of network traffic in the average Enterprise network, that is still a significant amount of traffic. One approach to mitigating the inefficiencies in the protocol is to use WAN Acceleration products such as those provided by Riverbed, Silver Peak, or Cisco Systems. A better approach is simply to eliminate SMB1/CIFS by upgrading the server infrastructure that uses it. This includes both NAS devices as well as Windows Server 2003. The most effective method in use currently to identify SMB1/CIFS traffic is to use a network analyzer tool such as Wireshark, etc., to identify SMB1/CIFS &amp;ldquo;talkers&amp;rdquo; and then decommission or upgrade them over time. Microsoft also provides an auditing tool in Windows Server 2016, which can be used to track down SMB1/CIFS talkers.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-messageanalyzer-11">[11]&lt;/a>Windows Server 2003 和旧的 NAS 设备本地使用 SMB1 / CIFS。 SMB1 / CIFS 是一种极其聊天协议，在局域网上并不具有低延迟的问题，但在广域网上变得非常慢，因为协议的后退握手放大了这种网络的固有高延迟。后来的协议版本减少了大量握手交换。虽然 Microsoft 估计 SMB1 / CIFS 在普通企业网络中不到 10％的网络流量，但仍然是大量流量。一种减轻协议低效率的一种方法是使用 WAN 加速产品，例如河床，银峰或思科系统提供的产品。更好的方法只是通过升级使用它的服务器基础架构来消除 SMB1 / CIFS。这包括 NAS 设备以及 Windows Server 2003.目前用于识别 SMB1 / CIFS 流量的最有效的方法是使用网络分析工具，如 Wireshark 等，以识别 SMB1 / CIFS“Talkers”，然后再次退役或随着时间的推移升级它们。 Microsoft 还在 Windows Server 2016 中提供了一个审计工具，可用于跟踪 SMB1 / CIFS Talkers。[11]
In 1996, when Sun Microsystems announced &lt;a href="https://en.wikipedia.org/wiki/WebNFS">WebNFS&lt;/a>,&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-WebNFS-12">[12]&lt;/a> Microsoft launched an initiative to rename SMB to Common Internet File System (CIFS)&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-tridgemyths-9">[9]&lt;/a> and added more features, including support for &lt;a href="https://en.wikipedia.org/wiki/Symbolic_link">symbolic links&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Hard_link">hard links&lt;/a>, larger file sizes, and an initial attempt at supporting direct connections over TCP port 445 without requiring &lt;a href="https://en.wikipedia.org/wiki/NetBIOS">NetBIOS&lt;/a> as a transport (a largely experimental effort that required further refinement). Microsoft submitted some partial specifications as &lt;a href="https://en.wikipedia.org/wiki/Internet_Draft">Internet-Drafts&lt;/a> to the &lt;a href="https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force">IETF&lt;/a>,&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-IETF-13">[13]&lt;/a> though these submissions have expired.1996 年，当 Sun Microsystems 宣布 WebNFS，[12] Microsoft 推出了一项计划将 SMB 重命名为常见的 Internet 文件系统（CIFS）[9]并增加了更多功能，包括支持符号链接，硬链接，更大的文件大小和一个初步尝试在不需要 NetBIOS 作为运输的情况下支持直接连接的初步尝试（需要进一步改进的主要实验努力）。 Microsoft 将某些部分规格作为 IETF 提交了一些部分规范，[13]虽然这些提交已过期。
Microsoft &amp;ldquo;added SMB1 to the Windows Server 2012 R2 &lt;a href="https://en.wikipedia.org/wiki/Deprecation">deprecation&lt;/a> list in June 2013.&amp;rdquo;&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-14">[14]&lt;/a> Windows Server 2016 and some versions of Windows 10 Fall Creators Update do not have SMB1 installed by default.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-15">[15]&lt;/a>Microsoft 将于 2013 年 6 月添加到 Windows Server 2012 R2 弃用列表中的 SMB1。“[14] Windows Server 2016 和一些版本的 Windows 10 秋季创建者更新没有 SMB1 默认安装。[15]&lt;/p>
&lt;h3 id="smb-20edithttpsenwikipediaorgwindexphptitleserver_message_blockactioneditsection4smb-20-编辑">SMB 2.0[&lt;a href="https://en.wikipedia.org/w/index.php?title=Server_Message_Block&amp;amp;action=edit&amp;amp;section=4">edit&lt;/a>]SMB 2.0 [编辑]&lt;/h3>
&lt;p>Microsoft introduced a new version of the protocol (SMB 2.0 or SMB2) with &lt;a href="https://en.wikipedia.org/wiki/Windows_Vista">Windows Vista&lt;/a> in 2006&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-smb2-16">[16]&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/Windows_Server_2008">Server 2008&lt;/a>. Although the protocol is proprietary, its specification has been published to allow other systems to interoperate with Microsoft operating systems that use the new protocol.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-17">[17]&lt;/a>Microsoft 在 2006 年的 Windows Vista 引入了一个新版本的协议（SMB 2.0 或 SMB2）[16]和 Server 2008.虽然该协议是专有的，但它已发布其规范，以允许其他系统与使用使用的 Microsoft 操作系统互操作新协议。[17]
SMB2 reduces the &amp;lsquo;chattiness&amp;rsquo; of the SMB 1.0 protocol by reducing the number of commands and subcommands from over a hundred to just nineteen.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-barreto-18">[18]&lt;/a> It has mechanisms for &lt;a href="https://en.wikipedia.org/wiki/Pipelining">pipelining&lt;/a>, that is, sending additional requests before the response to a previous request arrives, thereby improving performance over high-&lt;a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency&lt;/a> links. It adds the ability to compound multiple actions into a single request, which significantly reduces the number of &lt;a href="https://en.wikipedia.org/wiki/Round-trip_delay_time">round-trips&lt;/a> the client needs to make to the server, improving performance as a result.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-barreto-18">[18]&lt;/a> SMB1 also has a compounding mechanism—known as AndX—to compound multiple actions, but Microsoft clients rarely use AndX.&lt;em>&lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed">citation needed&lt;/a>&lt;/em> It also introduces the notion of &amp;ldquo;durable file handles&amp;rdquo;: these allow a connection to an SMB server to survive brief network outages, as are typical in a wireless network, without having to incur the overhead of re-negotiating a new session.SMB2 通过减少超过一百到十九岁的命令和子命令的数量来减少 SMB 1.0 协议的“干脆”。[18]它具有流水线的机制，即，在对先前请求到达之前发送附加请求，从而提高了高延迟链路的性能。它增加了将多个操作复制到单个请求中的能力，这显着减少了客户端需要对服务器所需的往返的数量，从而提高性能。[18] SMB1 还具有复合机制，称为 Andx-to Compase 多种动作，但 Microsoft 客户端很少使用 Andx。[所需的引用]它还介绍了“持久文件处理函数”的概念：这些概述允许与 SMB 服务器的连接来生存简介网络中断，正如无线网络中的典型版本，而不必触发重新协商新会话的开销。
SMB2 includes support for &lt;a href="https://en.wikipedia.org/wiki/Symbolic_link">symbolic links&lt;/a>. Other improvements include caching of file properties, improved message signing with &lt;a href="https://en.wikipedia.org/wiki/HMAC">HMAC&lt;/a> &lt;a href="https://en.wikipedia.org/wiki/SHA-256">SHA-256&lt;/a> hashing algorithm and better scalability by increasing the number of users, shares and open files per server among others.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-barreto-18">[18]&lt;/a> The SMB1 protocol uses 16-bit data sizes, which amongst other things, limits the maximum block size to 64K. SMB2 uses 32- or 64-bit wide storage fields, and 128 bits in the case of &lt;a href="https://en.wikipedia.org/wiki/File_handle">file-handles&lt;/a>, thereby removing previous constraints on block sizes, which improves performance with large file transfers over fast networks.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-barreto-18">[18]&lt;/a>SMB2 包括对符号链接的支持。其他改进包括缓存文件属性，通过增加每个服务器等用户的用户，共享和打开文件的数量，改进了 HMAC SHA-256 散列算法和更好的可扩展性。[18] SMB1 协议使用 16 位数据大小，其中包括其他内容，将最大块大小限制为 64K。 SMB2 使用 32 个或 64 位宽的存储字段，以及在文件处理的情况下使用 128 位，从而在块大小上移除先前的约束，从而提高了大文件传输的性能。[18]
Windows Vista/&lt;a href="https://en.wikipedia.org/wiki/Server_2008">Server 2008&lt;/a> and later operating systems use SMB2 when communicating with other machines also capable of using SMB2. SMB1 continues in use for connections with older versions of Windows, as well various vendors&amp;rsquo; &lt;a href="https://en.wikipedia.org/wiki/Network-attached_storage">NAS&lt;/a> solutions. Samba 3.5 also includes experimental support for SMB2.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-19">[19]&lt;/a> Samba 3.6 fully supports SMB2, except the modification of user quotas using the Windows quota management tools.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-20">[20]&lt;/a>Windows Vista / Server 2008 及更高版本的操作系统在与其他机器通信时使用 SMB2，该机器也能够使用 SMB2。 SMB1 继续用于与旧版本的 Windows 连接，以及各种供应商的 NAS 解决方案。 Samba 3.5 还包括对 SMB2 的实验支持。[19] Samba 3.6 完全支持 SMB2，除了使用 Windows 配额管理工具修改用户配额。[20]
When SMB2 was introduced it brought a number of benefits over SMB1 for third party implementers of SMB protocols. SMB1, originally designed by &lt;a href="https://en.wikipedia.org/wiki/IBM">IBM&lt;/a>, was &lt;a href="https://en.wikipedia.org/wiki/Reverse_engineering">reverse engineered&lt;/a>, and later became part of a wide variety of non-Windows operating systems such as &lt;a href="https://en.wikipedia.org/wiki/Xenix">Xenix&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/OS/2">OS/2&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/OpenVMS">VMS&lt;/a> (&lt;a href="https://en.wikipedia.org/wiki/Pathworks">Pathworks&lt;/a>). &lt;a href="https://en.wikipedia.org/wiki/X/Open">X/Open&lt;/a> standardized it partially; it also had draft &lt;a href="https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force">IETF&lt;/a> standards which lapsed. (See &lt;a href="http://ubiqx.org/cifs/Intro.html">http://ubiqx.org/cifs/Intro.html&lt;/a> for historical detail.) SMB2 is also a relatively clean break with the past. Microsoft&amp;rsquo;s SMB1 code has to work with a large variety of SMB clients and servers. SMB1 features many versions of information for commands (selecting what structure to return for a particular request) because features such as &lt;a href="https://en.wikipedia.org/wiki/Unicode">Unicode&lt;/a> support were retro-fitted at a later date. SMB2 involves significantly reduced compatibility-testing for implementers of the protocol. SMB2 code has considerably less complexity since far less variability exists (for example, non-Unicode code paths become redundant as SMB2 requires Unicode support).当 SMB2 引入时，它为 SMB 协议的第三方实施者带来了许多福利。最初由 IBM 设计的 SMB1 是反向设计的，后来成为 Xenix，OS / 2 和 VM（Pathworks）等各种非 Windows 操作系统的一部分。 X /开放部分地标准化;它还有 IETF 标准草案，退却。 （有关历史细节，请参阅http://ubiqx.org/cifs/intro.html。）SMB2也与过去相对干净。微软的SMB1代码必须使用各种SMB客户端和服务器。 SMB1 具有命令的许多信息版本（选择要返回的特定请求的结构），因为在以后的日期中重新装配诸如 Unicode 支持的功能。 SMB2 涉及对协议实施者的兼容性测试显着降低。 SMB2 代码具有很大的复杂性，因为存在较小的可变性（例如，当 SMB2 需要 Unicode 支持时，非 Unicode 代码路径变得冗余）。
Apple is also migrating to SMB2 (from their own &lt;a href="https://en.wikipedia.org/wiki/Apple_Filing_Protocol">Apple Filing Protocol&lt;/a>, now legacy) with OS X 10.9.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-ai2013-21">[21]&lt;/a> This transition was fraught with compatibility problems though.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-22">[22]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-23">[23]&lt;/a> Non-default support for SMB2 appeared in fact in OS X 10.7, when Apple abandoned Samba in favor of its own SMB implementation called SMBX.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-ai2013-21">[21]&lt;/a> Apple switched to its own SMBX implementation after Samba adopted &lt;a href="https://en.wikipedia.org/wiki/GPLv3">GPLv3&lt;/a>.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-24">[24]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-25">[25]&lt;/a>Apple 还与 OS X 10.9 迁移到 SMB2（从他们自己的 Apple 申请协议，现在遗留）。[21]这种转变充满了兼容性问题。[22] [23] SMB2 的非默认支持实际上出现在 OS X 10.7 中，当 Apple 废弃 Samba 有利于它自己的 SMB 实现，称为 SMBx。[21] Apple 在 Samba 采用 GPLv3 后切换到自己的 SMBX 实现。[24] [25]
The &lt;a href="https://en.wikipedia.org/wiki/Linux_kernel">Linux kernel&lt;/a>&amp;rsquo;s CIFS client file system has SMB2 support since version 3.7.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-26">[26]&lt;/a>自版本 3.7 以来，Linux 内核的 CIFS 客户端文件系统具有 SMB2 支持。[26]&lt;/p>
&lt;h3 id="smb-21edithttpsenwikipediaorgwindexphptitleserver_message_blockactioneditsection5smb-21-编辑">SMB 2.1[&lt;a href="https://en.wikipedia.org/w/index.php?title=Server_Message_Block&amp;amp;action=edit&amp;amp;section=5">edit&lt;/a>]SMB 2.1 [编辑]&lt;/h3>
&lt;p>SMB 2.1, introduced with Windows 7 and Server 2008 R2, introduced minor performance enhancements with a new opportunistic locking mechanism.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-27">[27]&lt;/a>使用 Windows 7 和 Server 2008 R2 引入的 SMB 2.1，带来了新的机会锁定机制的次要性能增强。[27]&lt;/p>
&lt;h3 id="smb-30edithttpsenwikipediaorgwindexphptitleserver_message_blockactioneditsection6smb-30-编辑">SMB 3.0[&lt;a href="https://en.wikipedia.org/w/index.php?title=Server_Message_Block&amp;amp;action=edit&amp;amp;section=6">edit&lt;/a>]SMB 3.0 [编辑]&lt;/h3>
&lt;p>SMB 3.0 (previously named SMB 2.2)&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-smb3-28">[28]&lt;/a> was introduced with &lt;a href="https://en.wikipedia.org/wiki/Windows_8">Windows 8&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-smb3-28">[28]&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/Windows_Server_2012">Windows Server 2012&lt;/a>.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-smb3-28">[28]&lt;/a> It brought several significant changes that are intended to add functionality and improve SMB2 performance,&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-29">[29]&lt;/a> notably in virtualized &lt;a href="https://en.wikipedia.org/wiki/Data_center">data centers&lt;/a>:使用 Windows 8 [28]和 Windows Server 2012 引入了 SMB 3.0（以前命名的 SMB 2.2）[28]。[28]它带来了几种重大变化，旨在添加功能并提高 SMB2 性能，[29]显着涉及虚拟化数据中心：&lt;/p>
&lt;ul>
&lt;li>the SMB Direct Protocol (SMB over &lt;a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">remote direct memory access&lt;/a> [RDMA])SMB 直接协议（SMB OVER 远程直接内存访问[RDMA]）&lt;/li>
&lt;li>SMB Multichannel (multiple connections per SMB session),&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-30">[30]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-31">[31]&lt;/a>SMB MultiShannel（每个 SMB 会话的多个连接），[30] [31]&lt;/li>
&lt;li>SMB Transparent Failover&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-32">[32]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-33">[33]&lt;/a>SMB 透明故障转移[32] [33]&lt;/li>
&lt;/ul>
&lt;p>It also introduces several security enhancements, such as &lt;a href="https://en.wikipedia.org/wiki/End-to-end_encryption">end-to-end encryption&lt;/a> and a new &lt;a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES&lt;/a> based signing algorithm.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-34">[34]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-35">[35]&lt;/a>它还引入了几种安全增强功能，例如端到端加密和基于新的 AES 签名算法。[34] [35]&lt;/p>
&lt;h3 id="smb-302edithttpsenwikipediaorgwindexphptitleserver_message_blockactioneditsection7smb-302-编辑">SMB 3.0.2[&lt;a href="https://en.wikipedia.org/w/index.php?title=Server_Message_Block&amp;amp;action=edit&amp;amp;section=7">edit&lt;/a>]SMB 3.0.2 [编辑]&lt;/h3>
&lt;p>SMB 3.0.2 (known as 3.02 at the time) was introduced with Windows 8.1 and Windows Server 2012 R2;&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-36">[36]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-37">[37]&lt;/a> in those and later releases, the earlier SMB version 1 can be optionally disabled to increase security.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-38">[38]&lt;/a>&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-barreto-smb3-links-ws2012r2-39">[39]&lt;/a>Windows 8.1 和 Windows Server 2012 R2 引入了 SMB 3.0.2（当时的 3.02）; [36] [37]在那些和稍后的版本中，早期的 SMB 版本 1 可以选择禁用以增加安全性。[38 ] [39]&lt;/p>
&lt;h3 id="smb-311edithttpsenwikipediaorgwindexphptitleserver_message_blockactioneditsection8smb-311-编辑">SMB 3.1.1[&lt;a href="https://en.wikipedia.org/w/index.php?title=Server_Message_Block&amp;amp;action=edit&amp;amp;section=8">edit&lt;/a>]SMB 3.1.1 [编辑]&lt;/h3>
&lt;p>SMB 3.1.1 was introduced with &lt;a href="https://en.wikipedia.org/wiki/Windows_10">Windows 10&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/Windows_Server_2016">Windows Server 2016&lt;/a>.&lt;a href="https://en.wikipedia.org/wiki/Server_Message_Block#cite_note-40">[40]&lt;/a> This version supports &lt;a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES-128&lt;/a> &lt;a href="https://en.wikipedia.org/wiki/Galois/Counter_Mode">GCM&lt;/a> encryption in addition to AES-128 &lt;a href="https://en.wikipedia.org/wiki/CCM_mode">CCM&lt;/a> encryption added in SMB3, and implements pre-authentication integrity check using &lt;a href="https://en.wikipedia.org/wiki/SHA-512">SHA-512&lt;/a> hash. SMB 3.1.1 also makes secure negotiation mandatory when connecting to clients using SMB 2.x and higher.使用 Windows 10 和 Windows Server 2016 引入了 SMB 3.1.1. [40]此版本还支持 AES-128 GCM 加密，除了在 SMB3 中添加的 AES-128 CCM 加密，使用 SHA-512 哈希实现预认证完整性检查。 SMB 3.1.1 还使用 SMB 2.x 和更高的客户端连接到客户端时强制使用安全协商。&lt;/p>
&lt;h2 id="cifs">CIFS&lt;/h2>
&lt;p>CIFS (Common Internet File System) is a protocol that gained popularity around the year 2000, as vendors worked to establish an Internet Protocol-based &lt;a href="https://searchmobilecomputing.techtarget.com/definition/file-sharing">file-sharing&lt;/a> protocol.CIFS（常见的 Internet 文件系统）是一项协议，它在 2000 年左右获得了普及的协议，因为供应商建立基于 Internet 协议的文件共享协议。
At its peak, CIFS was supported by operating systems (OSes) such as Windows, Linux and Unix. CIFS used the &lt;a href="https://searchnetworking.techtarget.com/definition/client-server">client-server&lt;/a> programming model in which a client program makes a request of a server program &amp;ndash; usually in another computer &amp;ndash; to access a file or pass a message to a program that runs in the server computer. The server takes the requested action and returns a response.在其峰值处，CIFS 由操作系统（OS）支持，例如 Windows，Linux 和 Unix。 CIFS 使用客户端 - 服务器编程模型，其中客户端程序使服务器程序的请求 - 通常在另一台计算机中 - 访问文件或将消息传递给在服务器计算机中运行的程序。服务器获取请求的操作并返回响应。
CIFS is now considered obsolete, because most modern data storage systems use the more robust Server Message Block (&lt;a href="https://searchnetworking.techtarget.com/definition/Server-Message-Block-Protocol">SMB&lt;/a>) 2.0 and 3.0 file-sharing protocols, which were major upgrades to CIFS.CIFS 现在被认为已过时，因为大多数现代数据存储系统使用更强大的服务器消息块（SMB）2.0 和 3.0 文件共享协议，这是 CIFS 的重大升级。
CIFS/SMB and the Network File System (&lt;a href="https://searchenterprisedesktop.techtarget.com/definition/Network-File-System">NFS&lt;/a>) are the two major protocols used in network-attached storage (&lt;a href="https://searchstorage.techtarget.com/definition/network-attached-storage">NAS&lt;/a>) systems.CIFS / SMB 和网络文件系统（NFS）是网络附加存储（NAS）系统中使用的两个主要协议。&lt;/p>
&lt;h3 id="cifs-vs-nfscifs-与-nfs">CIFS vs. NFSCIFS 与 NFS.&lt;/h3>
&lt;p>Developed by Sun Microsystems in the 1980s, NFS is now managed by the Internet Engineering Task Force (&lt;a href="https://whatis.techtarget.com/definition/IETF-Internet-Engineering-Task-Force">IETF&lt;/a>). NFS was originally used more in Unix and Linux OSes, while CIFS and SMB were used for Windows, but most major NAS vendors now support both protocols.由 Sun Microsystems 在 20 世纪 80 年代开发，NFS 现在由互联网工程任务组（IETF）管理。 NFS 最初在 UNIX 和 Linux OS 中使用更多，而 CIFS 和 SMB 用于 Windows，但大多数主要的 NAS 供应商现在支持这两个协议。
NFS is a client-server application that permits transparent file sharing between servers, desktops, laptops and other devices. Using NFS, users can store, view and update files remotely as though they were on their own computer. With CIFS/SMB, a client program requests a file from a server program located on another computer, and the server responds. This makes CIFS a &lt;a href="https://searchnetworking.techtarget.com/definition/chatty-protocol">chattier protocol&lt;/a> than NFS.NFS 是一个客户端 - 服务器应用程序，可允许服务器，桌面，笔记本电脑和其他设备之间的透明文件共享。使用 NFS，用户可以远程存储，查看和更新文件，好像它们在自己的计算机上。使用 CIFS / SMB，客户端程序请求来自位于另一台计算机上的服务器程序的文件，服务器响应。这使 CIFS 成为比 NFS 的淘汰的协议。&lt;/p>
&lt;h3 id="cifs-vs-smb-20-30cifs-与-smb-2030">CIFS vs. SMB 2.0, 3.0CIFS 与 SMB 2.0,3.0&lt;/h3>
&lt;p>The SMB &lt;a href="https://searchnetworking.techtarget.com/definition/Application-layer">application-layer&lt;/a> network protocol has been around since the 1980s. Developed at IBM, SMB allowed computers to read and write files over a local area network. Although CIFS and SMB are often used interchangeably, the CIFS protocol was introduced by Microsoft in early Windows OSes as an updated version of SMB.自 20 世纪 80 年代以来，SMB 应用层网络协议已经存在。在 IBM 开发，SMB 允许计算机通过局域网读取和写入文件。虽然 CIFS 和 SMB 通常互换使用，但 CIFS 协议由 Microsoft 在早期 Windows OS 中作为 SMB 的更新版本引入。
CIFS used the internet&amp;rsquo;s &lt;a href="https://searchnetworking.techtarget.com/definition/TCP-IP">TCP/IP&lt;/a> protocol and was viewed as a complement to existing internet application protocols, such as the File Transfer Protocol (&lt;a href="https://searchnetworking.techtarget.com/definition/File-Transfer-Protocol-FTP">FTP&lt;/a>) and the Hypertext Transfer Protocol (&lt;a href="https://whatis.techtarget.com/definition/HTTP-Hypertext-Transfer-Protocol">HTTP&lt;/a>). However, CIFS was considered a chatty protocol that was buggy and had issues with network latency. The protocol was also hard to maintain and not very secure because of the large number of commands and subcommands it processed. It was replaced when Microsoft introduced SMB in Windows 2000, Windows XP, Windows Server 2003 and Windows Server 2003 R2. Updated versions of the protocol were subsequently used in Windows Vista, Windows Server 2008, Windows 7 and Windows Server 2008.CIFS 使用 Internet 的 TCP / IP 协议，并被视为对现有的 Internet 应用程序协议的补充，例如文件传输协议（FTP）和超文本传输协议（HTTP）。但是，CIFS 被认为是一个笨拙的协议，它是错误的，并且具有网络延迟的问题。由于它处理了大量的命令和子命令，该协议也很难维护，而不是非常安全。当 Microsoft 在 Windows 2000，Windows XP，Windows Server 2003 和 Windows Server 2003 R2 中引入 SMB 时，它被替换。随后在 Windows Vista，Windows Server 2008，Windows 7 和 Windows Server 2008 中使用更新的协议版本。
SMB 2.0, introduced in the Windows OS in 2006, provided performance improvements over SMB 1.0 by reducing the number of commands and subcommands from more than 100 to 19. The 2.0 specification packs multiple actions into a single request &amp;ndash; to reduce the number of round-trip requests made between the client and server &amp;ndash; since the client may now &lt;a href="https://whatis.techtarget.com/definition/caching">cache&lt;/a> all changes to the file before committing it to the server.在 2006 年的 Windows OS 中引入的 SMB 2.0，通过减少超过 100 到 19 的命令和子命令的数量为 SMB 1.0 提供了性能改进。2.0 规范将多个操作包装为单个请求 - 以减少轮数 - 客户端和服务器之间的条件 - 由于客户端现在可以在向服务器提交到服务器之前缓存到文件的所有更改。
&lt;a href="https://searchwindowsserver.techtarget.com/definition/SMB-30-Server-Message-Block-30">SMB 3.0&lt;/a> was introduced in Windows 8 and Windows Server 2012, and launched SMB Direct, SMB Multichannel and SMB Transport Failover. It also introduced better security mechanisms, such as end-to-end encryption and the Advanced Encryption Standard (&lt;a href="https://searchsecurity.techtarget.com/definition/Advanced-Encryption-Standard">AES&lt;/a>) algorithm.SMB 3.0 是在 Windows 8 和 Windows Server 2012 中引入的，并推出了 SMB Direct，SMB MultiShannel 和 SMB 传输故障转移。它还引入了更好的安全机制，例如端到端加密和高级加密标准（AES）算法。
SMB 3.1.1, which became available in Windows 10 and Windows Server 2016, supports military-grade AES 128 GCM and AES 128 CCM encryption. It also uses the SHA-512 &lt;a href="https://searchsqlserver.techtarget.com/definition/hashing">hash&lt;/a> for preauthentication integrity checks.SMB 3.1.1 在 Windows 10 和 Windows Server 2016 中获得，支持军用 AEES 128 GCM 和 AES 128 CCM 加密。它还使用 SHA-512 散列进行预先认真完整性检查。
The &lt;a href="https://whatis.techtarget.com/definition/Samba">Samba&lt;/a> project played a major role in making SMB compatible with Unix. Samba is a free software implementation of the CIFS/SMB networking protocols that supports Microsoft Windows Server Domain, Active Directory and Microsoft Windows NT domains. With Samba, Unix-like OSes can interoperate with Windows and provided file and print services to Windows clients.Samba 项目在使 SMB 与 UNIX 兼容时发挥了重要作用。 Samba 是一个免费的软件实现，用于支持 Microsoft Windows Server 域，Active Directory 和 Microsoft Windows NT 域的 CIFS / SMB 网络协议。使用 Samba，Unix 类似的 oS 可以与 Windows 互操作，并为 Windows 客户端提供文件和打印服务。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/eysgyr/1618755274203-ef35cba6-f8d2-4371-8080-a9a8db101597.png" alt="">A comparison of CIFS, NFS and SMB file-sharing protocols.&lt;/p>
&lt;h3 id="cifs-protocol-featurescifs-协议功能">CIFS Protocol FeaturesCIFS 协议功能&lt;/h3>
&lt;p>The CIFS protocol includes a number of features as documented by Microsoft. These features include:CIFS 协议包括 Microsoft 记录的许多功能。这些功能包括：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Transport Intelligence&lt;/strong>: Although the CIFS protocol is normally used over top of a connection-oriented protocol, it can also make use of a connectionless protocol.传输智能：虽然 CIFS 协议通常在面向连接的协议之上使用，但它也可以使用无连接协议。&lt;/li>
&lt;li>&lt;strong>Flexible Connectivity&lt;/strong>: The CIFS protocol is extremely flexible with regard to client/server connectivity. A single client can connect to multiple servers, and can even make multiple connections to a single server if necessary.灵活的连接：CIFS 协议在客户端/服务器连接方面非常灵活。单个客户端可以连接到多个服务器，如果需要，甚至可以与单个服务器进行多次连接。&lt;/li>
&lt;li>&lt;strong>Feature Negotiation&lt;/strong>: The protocol&amp;rsquo;s dialect and supported features are negotiated on a connection-by-connection basis.功能协商：协议的方言和支持的功能在连接基础上协商。&lt;/li>
&lt;li>&lt;strong>Resource Access&lt;/strong>: The CIFS protocol does not limit the types of resources that clients are able to connect to. CIFS clients are able to concurrently connect to shared files, &lt;a href="https://whatis.techtarget.com/definition/named-pipe">named pipes&lt;/a>, print queues and other resources.资源访问：CIFS 协议不会限制客户端能够连接的资源类型。 CIFS 客户端能够同时连接到共享文件，命名为管道，打印队列和其他资源。&lt;/li>
&lt;li>&lt;strong>Security Context&lt;/strong>: The CIFS protocol does not limit the client to the use of a single security context. Multiple security contexts can be used over a connection if necessary.安全性上下文：CIFS 协议不会将客户端限制为使用单个安全上下文。如果需要，可以在连接上使用多个安全上下文。&lt;/li>
&lt;li>&lt;strong>File Access&lt;/strong>: A CIFS client is able to interact with multiple files simultaneously. Additionally, file sharing is a function of the server operating system and CIFS does not impose &lt;a href="https://whatis.techtarget.com/definition/lock">file locks&lt;/a>. This means that multiple clients can access a file simultaneously.文件访问：CIFS 客户端能够同时与多个文件进行交互。此外，文件共享是服务器操作系统的函数，并且 CIFS 不会强加文件锁定。这意味着多个客户端可以同时访问文件。&lt;/li>
&lt;li>&lt;strong>Extended Sub Protocols&lt;/strong>: The CIFS protocol allows the use of sub protocols, which can be used to extend functionality.扩展子协议：CIFS 协议允许使用子协议，该协议可用于扩展功能。&lt;/li>
&lt;li>&lt;strong>Named Pipe Interprocess Communication&lt;/strong>: CIFS allows named pipes to be used as a communication path between the client and the server.命名管道进程通信：CIFS 允许命名管道用作客户端和服务器之间的通信路径。&lt;/li>
&lt;li>&lt;strong>File and Record Locking and Safe Caching&lt;/strong>: Although the CIFS protocol allows multiple clients to simultaneously access a file, the protocol does support file and record locking, as well as file caching.文件和记录锁定和安全缓存：虽然 CIFS 协议允许多个客户端同时访问文件，但协议确实支持文件并记录锁定，以及文件缓存。&lt;/li>
&lt;li>&lt;strong>File, Directory and Volume Attributes&lt;/strong>: The CIFS protocol is designed to recognize and respect attributes that have been assigned at the file, folder and volume levels. The protocol is also compatible with Windows Access Control Lists (&lt;a href="https://searchsoftwarequality.techtarget.com/definition/access-control-list">ACL&lt;/a>).文件，目录和卷属性：CIFS 协议旨在识别并尊重已分配在文件，文件夹和卷级别的属性。该协议也与 Windows 访问控制列表（ACL）兼容。&lt;/li>
&lt;li>&lt;strong>File and Directory Change Notifications&lt;/strong>: The CIFS protocol includes a mechanism that allows clients to be notified when a change has been made to a shared resource. A Windows client that is accessing a shared folder through &lt;a href="https://searchenterprisedesktop.techtarget.com/definition/Microsoft-Windows-Explorer">File Explorer&lt;/a> for example, will generally display a current view of the shared folder&amp;rsquo;s contents because of this feature.文件和目录更改通知：CIFS 协议包括一种机制，该机制允许在对共享资源进行更改时通知客户端。例如，通过文件资源管理器访问共享文件夹的 Windows 客户端通常会因为此功能而显示共享文件夹内容的当前视图。&lt;/li>
&lt;li>&lt;strong>Batched Commands&lt;/strong>: The CIFS protocol allows messages to be linked together and processed in sequence.批量命令：CIFS 协议允许以序列链接在一起并处理消息。&lt;/li>
&lt;li>&lt;strong>Support for the Distributed File System&lt;/strong>: The Windows Server operating system supports the use of a Distributed File System (&lt;a href="https://searchwindowsserver.techtarget.com/definition/distributed-file-system-DFS">DFS&lt;/a>) which creates a global namespace that can include resources on multiple servers. The CIFS protocol fully supports the DFS feature.支持分布式文件系统：Windows Server 操作系统支持使用分布式文件系统（DFS），该文件系统（DFS）创建一个全局命名空间，可以在多个服务器上包含资源。 CIFS 协议完全支持 DFS 功能。&lt;/li>
&lt;li>&lt;strong>Remote Procedure Call Transport&lt;/strong>: The CIFS protocol supports the use of &lt;a href="https://searchapparchitecture.techtarget.com/definition/Remote-Procedure-Call-RPC">RPC&lt;/a> protocols such as MS-RPCE and MS-RAP.远程过程呼叫传输：CIFS 协议支持使用 RPC 协议，例如 MS-RPCE 和 MS-RAP。&lt;/li>
&lt;li>&lt;strong>Message Verification&lt;/strong>: Message signing can be used in conjunction with the CIFS protocol as a way of guaranteeing that messages have not been modified in transit.消息验证：消息签名可以与 CIFS 协议结合使用，作为保证在运输过程中未修改消息的方式。&lt;/li>
&lt;li>&lt;strong>Unicode File Name Support&lt;/strong>: The CIFS protocol supports the &lt;a href="https://whatis.techtarget.com/definition/ASCII-American-Standard-Code-for-Information-Interchange">ASCII&lt;/a> character set and &lt;a href="https://whatis.techtarget.com/definition/Unicode">Unicode&lt;/a> Legacy 8.3 filenames are supported, as are long file names.Unicode 文件名支持：CIFS 协议支持 ASCII 字符集，支持 Unicode 传统 8.3 文件名，也可以是长文件名。&lt;/li>
&lt;/ul>
&lt;h3 id="uses-of-cifs-用-cifs-的用途">Uses of CIFS 用 CIFS 的用途&lt;/h3>
&lt;p>The CIFS protocol was the genesis for the current generation SMB protocol that is used for file sharing in Windows systems. SMB is widely used for accessing files and folders on Windows networks.CIFS 协议是当前生成 SMB 协议的 Genesis，用于在 Windows 系统中共享文件共享。 SMB 广泛用于访问 Windows 网络上的文件和文件夹。
Although the CIFS protocol is probably most often associated with Microsoft, there are &lt;a href="https://whatis.techtarget.com/definition/open-source">open source&lt;/a> versions of the protocol available. CIFSD for example, is an open source CIFS/SMB protocol for Linux. Similarly, Samba &amp;ndash; the Windows interoperability suite for Linux and Unix &amp;ndash; includes an SMB/CIFS client. The CIFS/SMB protocol is also sometimes used to provide containers with connectivity to shared resources.虽然 CIFS 协议可能最常与 Microsoft 相关，但是有可用协议的开源版本。例如，CIFSD 是用于 Linux 的开源 CIFS / SMB 协议。同样，Samba - Linux 和 UNIX 的 Windows 互操作套件 - 包括 SMB / CIFS 客户端。 CIFS / SMB 协议有时也用于为包含与共享资源的连接提供的容器。&lt;/p>
&lt;h3 id="how-does-cifs-workcifs-如何工作">How Does CIFS Work?CIFS 如何工作？&lt;/h3>
&lt;p>When a CIFS client needs to communicate with a CIFS server, the action is almost always initiated at the application level. A user might, for example, open Windows File Explorer and attempt to access a shared folder.当 CIFS 客户端需要与 CIFS 服务器通信时，操作几乎始终在应用程序级别启动。例如，用户可能会打开 Windows 文件资源管理器并尝试访问共享文件夹。
The first step in accessing a shared resource is that the client establishes a &lt;a href="https://searchnetworking.techtarget.com/definition/NetBIOS">NetBIOS&lt;/a> session with the server (using a full duplex TCP session over port 139). CIFS messages can then be transmitted across this session.访问共享资源的第一步是客户端与服务器建立 NetBIOS 会话（使用端口 139 上的全双工 TCP 会话）。然后可以在本次会话中传输 CIFS 消息。
With the NetBIOS session in place, the client and server perform a negotiation process in which they determine which dialect will be used. This negotiation process is initiated by the client by way of the SMB_COM_Negotiate command. This command effectively transmits to the server a list of the dialects that the client understands. The server responds with the dialect that it will be using. This dialect must be one that is supported by both the client and the server.使用 NetBIOS 会话到位，客户端和服务器执行协商过程，其中它们确定将使用哪个方言。此协商过程由客户端发起 SMB_COM_NEGOTIET 命令。该命令有效地向服务器发送客户端理解的方言列表。服务器响应它将使用的方言。此方言必须是客户端和服务器支持的方言。
Once the client and server agree on a dialect, the client transmits authentication credentials (usually a username and password) to the server, and is furnished with a Unique Identifier (&lt;a href="https://internetofthingsagenda.techtarget.com/definition/unique-identifier-UID">UID&lt;/a>). When the client transmits its authentication credentials, it also sends a list of its capabilities. As such, this step is necessary even if the server does not require authentication.一旦客户端和服务器对方言达成一致，客户端将身份验证凭据（通常是用户名和密码）传输到服务器，并且具有唯一标识符（UID）。当客户端发送其认证凭据时，它还发送其功能列表。因此，即使服务器不需要认证，也必须执行此步骤。
On modern systems, the authentication process is generally handled by Active Directory. However, authentication is not a direct function of the CIFS protocol. As such, other authentication mechanisms, such as &lt;a href="https://searchsecurity.techtarget.com/definition/RADIUS">RADIUS&lt;/a>, could also be used. It is worth noting that CIFS can also be used for file sharing in a workgroup environment. Since a workgroup lacks a centralized authentication mechanism, the authentication process would utilize local user accounts on the workgroup computers.在现代系统上，身份验证过程通常由 Active Directory 处理。但是，身份验证不是 CIFS 协议的直接功能。因此，还可以使用其他认证机制，例如半径。值得注意的是，CIFS 也可以用于工作组环境中的文件共享。由于工作组缺乏集中式认证机制，因此认证过程将在工作组计算机上使用本地用户帐户。
If the authentication is successful, the server returns the assigned UID to the client. Now, the client transmits the Universal Naming Convention (&lt;a href="https://whatis.techtarget.com/definition/Universal-Naming-Convention-UNC">UNC&lt;/a>) name of the share that it wishes to attach to. The server checks to make sure that the share name is valid and that the client has the required permissions. If these checks are successful, the client is granted access to the share, and can then begin requesting access to resources within the share, such as files and folders.如果身份验证成功，则服务器将分配的 UID 返回给客户端。现在，客户端发送通用命名约定（UNC）所希望附加到的共享的名称。服务器检查以确保共享名称有效，并且客户端具有所需的权限。如果这些检查成功，则客户端被授予对共享的访问权限，然后可以开始请求访问共享中的资源，例如文件和文件夹。&lt;/p>
&lt;h3 id="downfalls-of-cifscifs-的垮台">Downfalls of CIFSCIFS 的垮台&lt;/h3>
&lt;p>Early on, Microsoft&amp;rsquo;s CIFS proposal held great potential. With CIFS, Microsoft sought to create a standard version of SMB. Among its benefits was support for direct communications over &lt;a href="https://searchnetworking.techtarget.com/definition/TCP">TCP&lt;/a> port 445, completely bypassing NetBIOS. In spite of this capability however, most CIFS clients and servers continued to be based on NetBIOS and LAN Manager (LanMan) authentication.早期，微软的 CIFS 提案持有巨大的潜力。使用 CIFS，Microsoft 试图创建一个标准版本的 SMB。它的好处是支持通过 TCP 端口 445 的直接通信，完全绕过 NetBIOS。然而，尽管这种能力，但大多数 CIFS 客户端和服务器都继续基于 NetBIOS 和 LAN Manager（Lanman）身份验证。
LAN Manager 1.0 was originally created to support various file system features and operating system functions within IBM&amp;rsquo;s OS/2. Subsequent versions of LAN Manager supported &lt;a href="https://searchsecurity.techtarget.com/definition/DOS">DOS&lt;/a> and Windows. The CIFS specification was based around the use of NT LAN Manager &amp;ndash; otherwise known as NTLM or NT LanMan 0.12.LAN Manager 1.0 最初创建，以支持 IBM 的 OS / 2 中的各种文件系统功能和操作系统功能。后续版本的 LAN Manager 支持 DOS 和 Windows。 CIFS 规范基于 NT LAN 管理器的使用 - 否则称为 NTLM 或 NT Lanman 0.12。
Over time, CIFS became obsolete as Microsoft released newer versions of its SMB protocol, with SMB 3.0 being the current version.随着时间的推移，CIFS 将随着 Microsoft 发布的新版本的 SMB 协议而变得过时，SMB 3.0 是当前版本。&lt;/p>
&lt;h3 id="how-to-configure-cifs-for-windows-如何为-windows-配置-cifs">How to Configure CIFS for Windows 如何为 Windows 配置 CIFS&lt;/h3>
&lt;p>The CIFS protocol is fully supported by Windows 10, but CIFS sharing is disabled by default (although Windows 10 is configured by default to act as a CIFS client). SMB is Microsoft&amp;rsquo;s preferred file sharing protocol, and supersedes CIFS, so it is unlikely that most admins will need to fully enable the CIFS protocol. Even so, the protocol is available for use if needed. Here is how the configuration process works.CIFS 协议由 Windows 10 完全支持，但默认情况下禁用 CIFS 共享（尽管默认情况下，Windows 10 配置为 CIFS 客户端）。 SMB 是 Microsoft 的首选文件共享协议，并取代了 CIFS，因此大多数管理员不太可能需要完全启用 CIFS 协议。即便如此，如果需要，协议可供使用。以下是配置过程的工作原理。
To enable the CIFS protocol, enter the Control command at the Windows Run prompt. This will cause Windows to open the &lt;a href="https://searchwindowsserver.techtarget.com/definition/Microsoft-Windows-Control-Panel">Control Panel&lt;/a>. Next, click on &lt;strong>Programs&lt;/strong>, and then click on the &lt;strong>Turn Windows Features On or Off&lt;/strong> link. This will cause Windows to display a dialog box within which features can be enabled or disabled by selecting the corresponding checkbox.要启用 CIFS 协议，请在 Windows 运行提示符下输入控制命令。这将导致 Windows 打开控制面板。接下来，单击“程序”，然后单击“或关闭”或“关闭”链接的“窗口”功能。这将导致窗口显示一个对话框，通过选择相应的复选框可以启用或禁用功能。
Scroll through the list of features until you locate SMB 1.0/CIFS File Sharing Support. By default, Windows 10 is configured to act as a CIFS client, but not as a CIFS server. Hence, some CIFS components are enabled by default. To fully enable CIFS, expand the &lt;strong>SMB 1.0/CIFS File Sharing Support&lt;/strong> container and then select the &lt;strong>SMB 1.0/CIFS Server&lt;/strong> checkbox. The other SMB 1.0/CIFS checkboxes (Automatic Removal and Client) should be selected by default. Click &lt;strong>OK&lt;/strong> to install the feature.滚动功能列表，直到找到 SMB 1.0 / CIFS 文件共享支持。默认情况下，Windows 10 被配置为充当 CIFS 客户端，但不是 CIFS 服务器。因此，默认情况下，某些 CIFS 组件将启用。要完全启用 CIFS，请展开 SMB 1.0 / CIFS 文件共享支持容器，然后选择 SMB 1.0 / CIFS 服务器复选框。默认情况下，应选择其他 SMB 1.0 / CIFS 复选框（自动删除和客户端）。单击“确定”以安装该功能。
In Windows 10, no additional CIFS configuration is necessary. Sharing a folder across a network works the same way, regardless if SMB or CIFS is being used. Right click on the folder that is to be shared and select the &lt;strong>Properties&lt;/strong> command from the shortcut menu. This will cause Windows to display the folder&amp;rsquo;s properties dialog box. Select the dialog box&amp;rsquo;s &lt;strong>Sharing&lt;/strong> tab and click the &lt;strong>Share&lt;/strong> button. Select the users with whom you wish to share the folder and adjust their permissions as necessary. If the folder needs to be shared among additional users, click the &lt;strong>Add&lt;/strong> button and then enter the name of the user or group with whom the folder should be shared. When you are done configuring the folder&amp;rsquo;s users and permissions, click the &lt;strong>Share&lt;/strong> button, followed by the &lt;strong>Close&lt;/strong> button.在 Windows 10 中，不需要额外的 CIFS 配置。在网络上共享文件夹以相同的方式运行，无论是否正在使用 SMB 或 CIFS。右键单击要共享的文件夹，然后从快捷菜单中选择“属性”命令。这将导致 Windows 显示文件夹的属性对话框。选择对话框的共享选项卡，然后单击“共享”按钮。选择与您希望共享该文件夹并根据需要调整其权限的用户。如果需要在其他用户之间共享文件夹，请单击“添加”按钮，然后输入应共享文件夹的用户或组的名称。完成文件夹的用户和权限时，单击“共享”按钮，然后单击“关闭”按钮。&lt;/p></description></item><item><title>Docs: 1.存储</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/</guid><description/></item><item><title>Docs: 10.1.bootstrap 认证配置步骤介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/kubelet-%E7%89%B9%E6%80%A7/10.1.bootstrap-%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/kubelet-%E7%89%B9%E6%80%A7/10.1.bootstrap-%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;p>kubelet 授权 kube-apiserver 的一些操作 exec run logs 等&lt;/p>
&lt;p>&lt;strong>RBAC 只需创建一次就可以&lt;/strong>&lt;/p>
&lt;pre>&lt;code>kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>创建 bootstrap kubeconfig 文件&lt;/strong>&lt;/p>
&lt;p>注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token&lt;/p>
&lt;pre>&lt;code>kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-clientgroup --kubeconfig ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>查看生成的 token&lt;/p>
&lt;pre>&lt;code>kubeadm token list --kubeconfig ~/.kube/config
&lt;/code>&lt;/pre>
&lt;p>TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS&lt;/p>
&lt;p>** 2kcmsb.hyl5s4g0l1mkff9z** &lt;strong>23h&lt;/strong> 2018-11-16T11:08:00+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-clientgroup&lt;/p>
&lt;p>配置集群参数，生成 kubernetes-clientgroup-bootstrap.kubeconfig&lt;/p>
&lt;pre>&lt;code>kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=https://192.168.1.7:6443 \ #master节点ip
--kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>配置客户端认证&lt;/p>
&lt;pre>&lt;code>kubectl config set-credentials kubelet-bootstrap \
--token= 2kcmsb.hyl5s4g0l1mkff9z \ #上面生成的token
--kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>配置关联&lt;/p>
&lt;pre>&lt;code>kubectl config set-context default \
--cluster=kubernetes \
--user=kubelet-bootstrap \
--kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>配置默认关联&lt;/p>
&lt;pre>&lt;code>kubectl config use-context default --kubeconfig=kubernetes-clientgroup-bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>拷贝生成的 kubernetes-clientgroup-bootstrap.kubeconfig 文件到其它所有的 node 节点，并重命名&lt;/p>
&lt;pre>&lt;code>scp kubernetes-clientgroup-bootstrap.kubeconfig 192.168.1.8:/etc/kubernetes/bootstrap.kubeconfig
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>配置 bootstrap RBAC 权限&lt;/strong>&lt;/p>
&lt;pre>&lt;code>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
&lt;/code>&lt;/pre>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>否则报如下错误
failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &amp;quot;system:bootstrap:1jezb7&amp;quot; cannot create
certificatesigningrequests.certificates.k8s.io at the cluster scope
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>创建自动批准相关 CSR 请求的 ClusterRole&lt;/strong>&lt;/p>
&lt;pre>&lt;code>vi /etc/kubernetes/tls-instructs-csr.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver
rules:
- apiGroups: [&amp;quot;certificates.k8s.io&amp;quot;]
resources: [&amp;quot;certificatesigningrequests/selfnodeserver&amp;quot;]
verbs: [&amp;quot;create&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>导入 yaml 文件&lt;/p>
&lt;pre>&lt;code>kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml
&lt;/code>&lt;/pre>
&lt;p>clusterrole.rbac.authorization.k8s.io &amp;ldquo;system:certificates.k8s.io:certificatesigningrequests:selfnodeserver&amp;rdquo; created&lt;/p>
&lt;p>查看创建的 ClusterRole&lt;/p>
&lt;pre>&lt;code>kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>将 ClusterRole 绑定到适当的用户组&lt;/strong>&lt;/p>
&lt;pre>&lt;code># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求
kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers
# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求
kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes
# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求
kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes
查看已有绑定 kubectl get clusterrolebindings
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>动态 kubelet 配置&lt;/strong>&lt;/p>
&lt;p>创建 kubelet 服务文件&lt;/p>
&lt;pre>&lt;code>mkdir -p /var/lib/kubelet
vim /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
--hostname-override=k8s-wjoyxt \ #本地node节点的hostname
--pod-infra-container-image=jicki/pause-amd64:3.1 \ #pod的基础镜像，即gcr的gcr.io/google_containers/pause-amd64:3.1镜像
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
--config=/etc/kubernetes/kubelet.config.json \
--cert-dir=/etc/kubernetes/ssl \
--logtostderr=true \
--v=2
[Install]
WantedBy=multi-user.target创建 kubelet config 配置文件
&lt;/code>&lt;/pre>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>vim /etc/kubernetes/kubelet.config.json
{
&amp;quot;kind&amp;quot;: &amp;quot;KubeletConfiguration&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;kubelet.config.k8s.io/v1beta1&amp;quot;,
&amp;quot;authentication&amp;quot;: {
&amp;quot;x509&amp;quot;: {
&amp;quot;clientCAFile&amp;quot;: &amp;quot;/etc/kubernetes/ssl/ca.pem&amp;quot;
},
&amp;quot;webhook&amp;quot;: {
&amp;quot;enabled&amp;quot;: true,
&amp;quot;cacheTTL&amp;quot;: &amp;quot;2m0s&amp;quot;
},
&amp;quot;anonymous&amp;quot;: {
&amp;quot;enabled&amp;quot;: false
}
},
&amp;quot;authorization&amp;quot;: {
&amp;quot;mode&amp;quot;: &amp;quot;Webhook&amp;quot;,
&amp;quot;webhook&amp;quot;: {
&amp;quot;cacheAuthorizedTTL&amp;quot;: &amp;quot;5m0s&amp;quot;,
&amp;quot;cacheUnauthorizedTTL&amp;quot;: &amp;quot;30s&amp;quot;
}
},
&amp;quot;address&amp;quot;: &amp;quot;172.16.6.66&amp;quot;, #本地node节点的IP
&amp;quot;port&amp;quot;: 10250,
&amp;quot;readOnlyPort&amp;quot;: 0,
&amp;quot;cgroupDriver&amp;quot;: &amp;quot;cgroupfs&amp;quot;,
&amp;quot;hairpinMode&amp;quot;: &amp;quot;promiscuous-bridge&amp;quot;,
&amp;quot;serializeImagePulls&amp;quot;: false,
&amp;quot;RotateCertificates&amp;quot;: true,
&amp;quot;featureGates&amp;quot;: {
&amp;quot;RotateKubeletClientCertificate&amp;quot;: true,
&amp;quot;RotateKubeletServerCertificate&amp;quot;: true
},
&amp;quot;MaxPods&amp;quot;: &amp;quot;512&amp;quot;,
&amp;quot;failSwapOn&amp;quot;: false,
&amp;quot;containerLogMaxSize&amp;quot;: &amp;quot;10Mi&amp;quot;,
&amp;quot;containerLogMaxFiles&amp;quot;: 5,
&amp;quot;clusterDomain&amp;quot;: &amp;quot;cluster.local.&amp;quot;,
&amp;quot;clusterDNS&amp;quot;: [&amp;quot;10.254.0.2&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>以上配置中:&lt;/p>
&lt;p>cluster.local. 为 kubernetes 集群的 domain&lt;/p>
&lt;p>10.254.0.2 预分配的 dns 地址&lt;/p>
&lt;p>&amp;ldquo;clusterDNS&amp;rdquo;: [&amp;ldquo;10.254.0.2&amp;rdquo;] 可配置多个 dns 地址，逗号可开, 可配置宿主机 dns&lt;/p>
&lt;p>&lt;strong>启动 Kubelet 服务&lt;/strong>&lt;/p>
&lt;pre>&lt;code>systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
&lt;/code>&lt;/pre>
&lt;p>验证 nodes&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/crt3fo/1616120007572-6678d863-7c6f-4000-8c7f-6e7c17ff42ca.png" alt="">&lt;/p>
&lt;p>注意:这里的 ROLES 是节点标签&lt;/p>
&lt;p>关于 kubectl get node 中的 ROLES 的标签&lt;/p>
&lt;p>单 Master 打标签 kubectl label node es-60 node-role.kubernetes.io/master=&amp;quot;&amp;quot;，当标签为 NoSchedule，表示不进行资源调度&lt;/p>
&lt;p>更新标签命令为 kubectl label nodes es-60 node-role.kubernetes.io/master=:NoSchedule &amp;ndash;overwrite&lt;/p>
&lt;p>单 Node 打标签 kubectl label node es-61 node-role.kubernetes.io/node=&amp;quot;&amp;quot;&lt;/p>
&lt;p>关于删除 label 可使用 - 号相连 如: kubectl label nodes es-61 node-role.kubernetes.io/node-&lt;/p>
&lt;p>查看自动生成的证书配置文件&lt;/p>
&lt;pre>&lt;code>ls -lt /etc/kubernetes/ssl/kubelet-*
&lt;/code>&lt;/pre></description></item><item><title>Docs: 10.1.CPU 执行程序的秘密，藏在了这 15 张图里</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.1.cpu-%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%A7%98%E5%AF%86%E8%97%8F%E5%9C%A8%E4%BA%86%E8%BF%99-15-%E5%BC%A0%E5%9B%BE%E9%87%8C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.1.cpu-%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%A7%98%E5%AF%86%E8%97%8F%E5%9C%A8%E4%BA%86%E8%BF%99-15-%E5%BC%A0%E5%9B%BE%E9%87%8C/</guid><description>
&lt;p>&lt;strong>CPU 执行程序的秘密，藏在了这 15 张图里&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986609-04faec22-1781-4936-b503-011b61590870.png" alt="">&lt;/p>
&lt;p>&lt;strong>前言&lt;/strong>&lt;/p>
&lt;p>代码写了那么多，你知道 a = 1 + 2 这条代码是怎么被 CPU 执行的吗？&lt;/p>
&lt;p>软件用了那么多，你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运行在 64 位的电脑上吗？64 位的操作系统可以运行在 32 位的电脑上吗？如果不行，原因是什么？&lt;/p>
&lt;p>CPU 看了那么多，我们都知道 CPU 通常分为 32 位和 64 位，你知道 64 位相比 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能一定比 32 位 CPU 高很多吗？&lt;/p>
&lt;p>不知道也不用慌张，接下来就循序渐进的、一层一层的攻破这些问题。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986635-4f0dd824-1f30-45ba-b584-15ac570aca3a.png" alt="">&lt;/p>
&lt;p>&lt;strong>正文&lt;/strong>&lt;/p>
&lt;h1 id="图灵机的工作方式">图灵机的工作方式&lt;/h1>
&lt;p>要想知道程序执行的原理，我们可以先从「图灵机」说起，图灵的基本思想是用机器来模拟人们用纸笔进行数学运算的过程，而且还定义了计算机由哪些部分组成，程序又是如何执行的。&lt;/p>
&lt;p>图灵机长什么样子呢？你从下图可以看到图灵机的实际样子：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986635-96dcf906-5b92-45b8-97a5-3cbdf0bba696.jpeg" alt="">&lt;/p>
&lt;p>图来源自：&lt;a href="http://www.kristergustafsson.me/turing-machine/">http://www.kristergustafsson.me/turing-machine/&lt;/a>&lt;/p>
&lt;p>图灵机的基本组成如下：&lt;/p>
&lt;p>有一条「纸带」，纸带由一个个连续的格子组成，每个格子可以写入字符，纸带就好比内存，而纸带上的格子的字符就好比内存中的数据或程序；&lt;/p>
&lt;p>有一个「读写头」，读写头可以读取纸带上任意格子的字符，也可以把字符写入到纸带的格子；&lt;/p>
&lt;p>读写头上有一些部件，比如存储单元、控制单元以及运算单元：&lt;/p>
&lt;p>1、存储单元用于存放数据；&lt;/p>
&lt;p>2、控制单元用于识别字符是数据还是指令，以及控制程序的流程等；&lt;/p>
&lt;p>3、运算单元用于执行运算指令；&lt;/p>
&lt;p>知道了图灵机的组成后，我们以简单数学运算的 1 + 2 作为例子，来看看它是怎么执行这行代码的。&lt;/p>
&lt;p>首先，用读写头把 「1、2、+」这 3 个字符分别写入到纸带上的 3 个格子，然后读写头先停在 1 字符对应的格子上；&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986659-6cca3462-fc81-4efe-a577-6b95d8ca1413.png" alt="">&lt;/p>
&lt;p>接着，读写头读入 1 到存储设备中，这个存储设备称为图灵机的状态；&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986644-ca6ea397-384a-4dfb-b215-02c9b09127cc.png" alt="">&lt;/p>
&lt;p>然后读写头向右移动一个格，用同样的方式把 2 读入到图灵机的状态，于是现在图灵机的状态中存储着两个连续的数字， 1 和 2；&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986643-d338c88c-73dc-4141-8351-044432fa93b5.png" alt="">&lt;/p>
&lt;p>读写头再往右移动一个格，就会碰到 + 号，读写头读到 + 号后，将 + 号传输给「控制单元」，控制单元发现是一个 + 号而不是数字，所以没有存入到状态中，因为 + 号是运算符指令，作用是加和目前的状态，于是通知「运算单元」工作。运算单元收到要加和状态中的值的通知后，就会把状态中的 1 和 2 读入并计算，再将计算的结果 3 存放到状态中；&lt;/p>
&lt;p>最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头向右移动，把结果 3 写入到纸带的格子中；&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986660-0f18003b-5037-45db-8f8f-7be09e53c639.png" alt="">&lt;/p>
&lt;p>通过上面的图灵机计算 1 + 2 的过程，可以发现图灵机主要功能就是读取纸带格子中的内容，然后交给控制单元识别字符是数字还是运算符指令，如果是数字则存入到图灵机状态中，如果是运算符，则通知运算符单元读取状态中的数值进行计算，计算结果最终返回给读写头，读写头把结果写入到纸带的格子中。&lt;/p>
&lt;p>事实上，图灵机这个看起来很简单的工作方式，和我们今天的计算机是基本一样的。接下来，我们一同再看看当今计算机的组成以及工作方式。&lt;/p>
&lt;h1 id="冯诺依曼模型">冯诺依曼模型&lt;/h1>
&lt;p>在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，而且还提出用电子元件构造计算机，并约定了用二进制进行计算和存储，还定义计算机基本结构为 5 个部分，分别是&lt;strong>中央处理器（CPU）、内存、输入设备、输出设备、总线&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986644-4f2bef64-9ee8-4943-8a81-e6707ab11814.png" alt="">&lt;/p>
&lt;p>这 5 个部分也被称为冯诺依曼模型，接下来看看这 5 个部分的具体作用。&lt;/p>
&lt;p>&lt;strong>内存&lt;/strong>&lt;/p>
&lt;p>我们的程序和数据都是存储在内存，存储的区域是线性的。&lt;/p>
&lt;p>数据存储的单位是一个&lt;strong>二进制位（***&lt;strong>bit&lt;/strong>*&lt;/strong>）&lt;strong>，即 0 或 1。最小的存储单位是&lt;/strong>字节（&lt;strong>*&lt;strong>byte&lt;/strong>*&lt;/strong>）**，1 字节等于 8 位。&lt;/p>
&lt;p>内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。&lt;/p>
&lt;p>&lt;strong>中央处理器&lt;/strong>&lt;/p>
&lt;p>中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于一次能计算多少字节数据：&lt;/p>
&lt;p>32 位 CPU 一次可以计算 4 个字节；&lt;/p>
&lt;p>64 位 CPU 一次可以计算 8 个字节；&lt;/p>
&lt;p>这里的 32 位和 64 位，通常称为 CPU 的位宽。&lt;/p>
&lt;p>之所以 CPU 要这样设计，是为了能计算更大的数值，如果是 8 位的 CPU，那么一次只能计算 1 个字节 0~255 范围内的数值，这样就无法一次完成计算 10000 * 500 ，于是为了能一次计算大数的运算，CPU 需要支持多个 byte 一起计算，所以 CPU 位宽越大，可以计算的数值就越大，比如说 32 位 CPU 能计算的最大整数是 4294967295。&lt;/p>
&lt;p>CPU 内部还有一些组件，常见的有寄存器、控制单元和逻辑运算单元等。其中，控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。&lt;/p>
&lt;p>CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。&lt;/p>
&lt;p>常见的寄存器种类：&lt;/p>
&lt;p>&lt;em>通用寄存器&lt;/em>，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。&lt;/p>
&lt;p>&lt;em>程序计数器&lt;/em>，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令的地址。&lt;/p>
&lt;p>&lt;em>指令寄存器&lt;/em>，用来存放程序计数器指向的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。&lt;/p>
&lt;p>&lt;strong>总线&lt;/strong>&lt;/p>
&lt;p>总线是用于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种：&lt;/p>
&lt;p>&lt;em>地址总线&lt;/em>，用于指定 CPU 将要操作的内存地址；&lt;/p>
&lt;p>&lt;em>数据总线&lt;/em>，用于读写内存的数据；&lt;/p>
&lt;p>&lt;em>控制总线&lt;/em>，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；&lt;/p>
&lt;p>当 CPU 要读写内存数据的时候，一般需要通过两个总线：&lt;/p>
&lt;p>首先要通过「地址总线」来指定内存的地址；&lt;/p>
&lt;p>再通过「数据总线」来传输数据；&lt;/p>
&lt;p>&lt;strong>输入、输出设备&lt;/strong>&lt;/p>
&lt;p>输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键时是需要和 CPU 进行交互的，这时就需要用到控制总线了。&lt;/p>
&lt;h1 id="线路位宽与-cpu-位宽">线路位宽与 CPU 位宽&lt;/h1>
&lt;p>数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示 0，高压电压则表示 1。&lt;/p>
&lt;p>如果构造了高低高这样的信号，其实就是 101 二进制数据，十进制则表示 5，如果只有一条线路，就意味着每次只能传递 1 bit 的数据，即 0 或 1，那么传输 101 这个数据，就需要 3 次才能传输完成，这样的效率非常低。&lt;/p>
&lt;p>这样一位一位传输的方式，称为串行，下一个 bit 必须等待上一个 bit 传输完成才能进行传输。当然，想一次多传一些数据，增加线路即可，这时数据就可以并行传输。&lt;/p>
&lt;p>为了避免低效率的串行传输的方式，线路的位宽最好一次就能访问到所有的内存地址。CPU 要想操作的内存地址就需要地址总线，如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种情况，所以 CPU 一次只能操作 2 个内存地址，如果想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为 2 ^ 32 = 4G。&lt;/p>
&lt;p>知道了线路位宽的意义后，我们再来看看 CPU 位宽。&lt;/p>
&lt;p>CPU 的位宽最好不要小于线路位宽，比如 32 位 CPU 控制 40 位宽的地址总线和数据总线的话，工作起来就会非常复杂且麻烦，所以 32 位的 CPU 最好和 32 位宽的线路搭配，因为 32 位 CPU 一次最多只能操作 32 位宽的地址总线和数据总线。&lt;/p>
&lt;p>如果用 32 位 CPU 去加和两个 64 位大小的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个高位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个高位的 32 位数字，最后再加上进位，就能算出结果了，可以发现 32 位 CPU 并不能一次性计算出加和两个 64 位数字的结果。&lt;/p>
&lt;p>对于 64 位 CPU 就可以一次性算出加和两个 64 位数字的结果，因为 64 位 CPU 可以一次读入 64 位的数字，并且 64 位 CPU 内部的逻辑运算单元也支持 64 位数字的计算。&lt;/p>
&lt;p>但是并不代表 64 位 CPU 性能比 32 位 CPU 高很多，很少应用需要算超过 32 位的数字，所以&lt;strong>如果计算的数额不超过 32 位数字的情况下，32 位和 64 位 CPU 之间没什么区别的，只有当计算超过 32 位数字的情况下，64 位的优势才能体现出来&lt;/strong>。&lt;/p>
&lt;p>另外，32 位 CPU 最大只能操作 4GB 内存，就算你装了 8 GB 内存条，也没用。而 64 位 CPU 寻址范围则很大，理论最大的寻址空间为 2^64。&lt;/p>
&lt;h1 id="程序执行的基本过程">程序执行的基本过程&lt;/h1>
&lt;p>在前面，我们知道了程序在图灵机的执行过程，接下来我们来看看程序在冯诺依曼模型上是怎么执行的。&lt;/p>
&lt;p>程序实际上是一条一条指令，所以程序的运行过程就是把每一条指令一步一步的执行起来，负责执行指令的就是 CPU 了。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986660-52f55e75-28af-466c-8a6b-7f736a5c9fee.png" alt="">&lt;/p>
&lt;p>那 CPU 执行程序的过程如下：&lt;/p>
&lt;p>第一步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存入到「指令寄存器」。&lt;/p>
&lt;p>第二步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；&lt;/p>
&lt;p>第三步，CPU 执行完指令后，「程序计数器」的值自增，表示指向下一条指令。这个自增的大小，由 CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4；&lt;/p>
&lt;p>简单总结一下就是，一个程序执行的时候，CPU 会根据程序计数器里的内存地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。&lt;/p>
&lt;p>CPU 从程序计数器读取指令、到执行、再到下一条指令，这个过程会不断循环，直到程序执行结束，这个不断循环的过程被称为 &lt;strong>CPU 的指令周期&lt;/strong>。&lt;/p>
&lt;h1 id="a--1--2-执行具体过程">a = 1 + 2 执行具体过程&lt;/h1>
&lt;p>知道了基本的程序执行过程后，接下来用 a = 1 + 2 的作为例子，进一步分析该程序在冯诺伊曼模型的执行过程。&lt;/p>
&lt;p>CPU 是不认识 a = 1 + 2 这个字符串，这些字符串只是方便我们程序员认识，要想这段程序能跑起来，还需要把整个程序翻译成&lt;strong>汇编语言&lt;/strong>的程序，这个过程称为编译成汇编代码。&lt;/p>
&lt;p>针对汇编代码，我们还需要用汇编器翻译成机器码，这些机器码由 0 和 1 组成的机器语言，这一条条机器码，就是一条条的&lt;strong>计算机指令&lt;/strong>，这个才是 CPU 能够真正认识的东西。&lt;/p>
&lt;p>下面来看看 a = 1 + 2 在 32 位 CPU 的执行过程。&lt;/p>
&lt;p>程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：&lt;/p>
&lt;p>数据 1 被存放到 0x100 位置；&lt;/p>
&lt;p>数据 2 被存放到 0x104 位置；&lt;/p>
&lt;p>注意，数据和指令是分开区域存放的，存放指令区域的地方称为「正文段」。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986667-0dc00e68-0216-4422-b789-cee6595d926a.png" alt="">&lt;/p>
&lt;p>编译器会把 a = 1 + 2 翻译成 4 条指令，存放到正文段中。如图，这 4 条指令被存放到了 0x200 ~ 0x20c 的区域中：&lt;/p>
&lt;p>0x200 的内容是 load 指令将 0x100 地址中的数据 1 装入到寄存器 R0；&lt;/p>
&lt;p>0x204 的内容是 load 指令将 0x104 地址中的数据 2 装入到寄存器 R1；&lt;/p>
&lt;p>0x208 的内容是 add 指令将寄存器 R0 和 R1 的数据相加，并把结果存放到寄存器 R2；&lt;/p>
&lt;p>0x20c 的内容是 store 指令将寄存器 R2 中的数据存回数据段中的 0x108 地址中，这个地址也就是变量 a 内存中的地址；&lt;/p>
&lt;p>编译完成后，具体执行程序的时候，程序计数器会被设置为 0x200 地址，然后依次执行这 4 条指令。&lt;/p>
&lt;p>上面的例子中，由于是在 32 位 CPU 执行的，因此一条指令是占 32 位大小，所以你会发现每条指令间隔 4 个字节。&lt;/p>
&lt;p>而数据的大小是根据你在程序中指定的变量类型，比如 int 类型的数据则占 4 个字节，char 类型的数据则占 1 个字节。&lt;/p>
&lt;p>&lt;strong>指令&lt;/strong>&lt;/p>
&lt;p>上面的例子中，图中指令的内容我写的是简易的汇编代码，目的是为了方便理解指令的具体内容，事实上指令的内容是一串二进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。&lt;/p>
&lt;p>不同的 CPU 有不同的指令集，也就是对应着不同的汇编语言和不同的机器码，接下来选用最简单的 MIPS 指集，来看看机器码是如何生成的，这样也能明白二进制的机器码的具体含义。&lt;/p>
&lt;p>MIPS 的指令是一个 32 位的整数，高 6 位代表着操作码，表示这条指令是一条什么样的指令，剩下的 26 位不同指令类型所表示的内容也就不相同，主要有三种类型 R、I 和 J。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986662-fadca5ca-0ef4-4fee-8a44-7caa05f36c1c.png" alt="">&lt;/p>
&lt;p>一起具体看看这三种类型的含义：&lt;/p>
&lt;p>&lt;em>R 指令&lt;/em>，用在算术和逻辑操作，里面由读取和写入数据的寄存器地址。如果是逻辑位移操作，后面还有位移操作的「位移量」，而最后的「功能码」则是再前面的操作码不够的时候，扩展操作码来表示对应的具体指令的；&lt;/p>
&lt;p>&lt;em>I 指令&lt;/em>，用在数据传输、条件分支等。这个类型的指令，就没有了位移量和操作码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或一个常数；&lt;/p>
&lt;p>&lt;em>J 指令&lt;/em>，用在跳转，高 6 位之外的 26 位都是一个跳转后的地址；&lt;/p>
&lt;p>接下来，我们把前面例子的这条指令：「add 指令将寄存器 R0 和 R1 的数据相加，并把结果放入到 R3」，翻译成机器码。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986665-5620a3ba-3048-4722-8bd3-139e0f4b37f0.png" alt="">&lt;/p>
&lt;p>加和运算 add 指令是属于 R 指令类型：&lt;/p>
&lt;p>add 对应的 MIPS 指令里操作码是 000000，以及最末尾的功能码是 100000，这些数值都是固定的，查一下 MIPS 指令集的手册就能知道的；&lt;/p>
&lt;p>rs 代表第一个寄存器 R0 的编号，即 00000；&lt;/p>
&lt;p>rt 代表第二个寄存器 R1 的编号，即 00001；&lt;/p>
&lt;p>rd 代表目标的临时寄存器 R2 的编号，即 00010；&lt;/p>
&lt;p>因为不是位移操作，所以位移量是 00000&lt;/p>
&lt;p>把上面这些数字拼在一起就是一条 32 位的 MIPS 加法指令了，那么用 16 进制表示的机器码则是 0x00011020。&lt;/p>
&lt;p>编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执行程序的时候，就会解析指令，这个过程叫作指令的解码。&lt;/p>
&lt;p>现代大多数 CPU 都使用来流水线的方式来执行指令，所谓的流水线就是把一个任务拆分成多个小任务，于是一条指令通常分为 4 个阶段，称为 4 级流水线，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986662-4a7d3fce-516a-485c-8123-60ae96bb4281.png" alt="">&lt;/p>
&lt;p>四个阶段的具体含义：&lt;/p>
&lt;p>CPU 通过程序计数器读取对应内存地址的指令，这个部分称为 &lt;strong>Fetch（取得指令）&lt;/strong>；&lt;/p>
&lt;p>CPU 对指令进行解码，这个部分称为 &lt;strong>Decode（指令译码）&lt;/strong>；&lt;/p>
&lt;p>CPU 执行指令，这个部分称为 &lt;strong>Execution（执行指令）&lt;/strong>；&lt;/p>
&lt;p>CPU 将计算结果存回寄存器或者将寄存器的值存入内存，这个部分称为 &lt;strong>Store（数据回写）&lt;/strong>；&lt;/p>
&lt;p>上面这 4 个阶段，我们称为&lt;strong>指令周期（***&lt;strong>Instrution Cycle&lt;/strong>*&lt;/strong>）**，CPU 的工作就是一个周期接着一个周期，周而复始。&lt;/p>
&lt;p>事实上，不同的阶段其实是由计算机中的不同组件完成的：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986677-ccdce1c2-c357-4c5a-8413-00c2e4501486.png" alt="">&lt;/p>
&lt;p>取指令的阶段，我们的指令是存放在&lt;strong>存储器&lt;/strong>里的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由&lt;strong>控制器&lt;/strong>操作的；&lt;/p>
&lt;p>指令的译码过程，也是由&lt;strong>控制器&lt;/strong>进行的；&lt;/p>
&lt;p>指令执行的过程，无论是进行算术操作、逻辑操作，还是进行数据传输、条件分支操作，都是由&lt;strong>算术逻辑单元&lt;/strong>操作的，也就是由&lt;strong>运算器&lt;/strong>处理的。但是如果是一个简单的无条件地址跳转，则是直接在&lt;strong>控制器&lt;/strong>里面完成的，不需要用到运算器。&lt;/p>
&lt;p>&lt;strong>指令的类型&lt;/strong>&lt;/p>
&lt;p>指令从功能角度划分，可以分为 5 大类：&lt;/p>
&lt;p>&lt;em>数据传输类型的指令&lt;/em>，比如 store/load 是寄存器与内存间数据传输的指令，mov 是将一个内存地址的数据移动到另一个内存地址的指令；&lt;/p>
&lt;p>&lt;em>运算类型的指令&lt;/em>，比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；&lt;/p>
&lt;p>&lt;em>跳转类型的指令&lt;/em>，通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的 if-else、swtich-case、函数调用等。&lt;/p>
&lt;p>&lt;em>信号类型的指令&lt;/em>，比如发生中断的指令 trap；&lt;/p>
&lt;p>&lt;em>闲置类型的指令&lt;/em>，比如指令 nop，执行后 CPU 会空转一个周期；&lt;/p>
&lt;p>&lt;strong>指令的执行速度&lt;/strong>&lt;/p>
&lt;p>CPU 的硬件参数都会有 GHz 这个参数，比如一个 1 GHz 的 CPU，指的是时钟频率是 1 G，代表着 1 秒会产生 1G 次数的脉冲信号，每一次脉冲信号高低电平的转换就是一个周期，称为时钟周期。&lt;/p>
&lt;p>对于 CPU 来说，在一个时钟周期内，CPU 仅能完成一个最基本的动作，时钟频率越高，时钟周期就越短，工作速度也就越快。&lt;/p>
&lt;p>一个时钟周期一定能执行完一条指令吗？答案是不一定的，大多数指令不能在一个时钟周期完成，通常需要若干个时钟周期。不同的指令需要的时钟周期是不同的，加法和乘法都对应着一条 CPU 指令，但是乘法需要的时钟周期就要比加法多。&lt;/p>
&lt;blockquote>
&lt;p>如何让程序跑的更快？&lt;/p>
&lt;/blockquote>
&lt;p>程序执行的时候，耗费的 CPU 时间少就说明程序是快的，对于程序的 CPU 执行时间，我们可以拆解成 &lt;strong>CPU 时钟周期数（***&lt;strong>CPU Cycles&lt;/strong>*&lt;/strong>）和时钟周期时间（&lt;strong>*&lt;strong>Clock Cycle Time&lt;/strong>*&lt;/strong>）的乘积**。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986686-313c2570-8f61-4e7a-998b-9a4d61aa59cf.png" alt="">&lt;/p>
&lt;p>时钟周期时间就是我们前面提及的 CPU 主频，主频越高说明 CPU 的工作速度就越快，比如我手头上的电脑的 CPU 是 2.4 GHz 四核 Intel Core i5，这里的 2.4 GHz 就是电脑的主频，时钟周期时间就是 1/2.4G。&lt;/p>
&lt;p>要想 CPU 跑的更快，自然缩短时钟周期时间，也就是提升 CPU 主频，但是今非彼日，摩尔定律早已失效，当今的 CPU 主频已经很难再做到翻倍的效果了。&lt;/p>
&lt;p>另外，换一个更好的 CPU，这个也是我们软件工程师控制不了的事情，我们应该把目光放到另外一个乘法因子 —— CPU 时钟周期数，如果能减少程序所需的 CPU 时钟周期数量，一样也是能提升程序的性能的。&lt;/p>
&lt;p>对于 CPU 时钟周期数我们可以进一步拆解成：「&lt;strong>指令数 x 每条指令的平均时钟周期数（***&lt;strong>Cycles Per Instruction&lt;/strong>*&lt;/strong>，简称 CPI）**」，于是程序的 CPU 执行时间的公式可变成如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lucr7r/1616167986659-a71dea63-d1c9-489c-843a-e0ad3ba0382e.png" alt="">&lt;/p>
&lt;p>因此，要想程序跑的更快，优化这三者即可：&lt;/p>
&lt;p>&lt;em>指令数&lt;/em>，表示执行程序所需要多少条指令，以及哪些指令。这个层面是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示方式。&lt;/p>
&lt;p>&lt;em>每条指令的平均时钟周期数 CPI&lt;/em>，表示一条指令需要多少个时钟周期数，现代大多数 CPU 通过流水线技术（Pipline），让一条指令需要的 CPU 时钟周期数尽可能的少；&lt;/p>
&lt;p>&lt;em>时钟周期时间&lt;/em>，表示计算机主频，取决于计算机硬件。有的 CPU 支持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU 工作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压力就会越大，CPU 会很容易奔溃。&lt;/p>
&lt;p>很多厂商为了跑分而跑分，基本都是在这三个方面入手的哦，特别是超频这一块。&lt;/p>
&lt;p>总结&lt;/p>
&lt;p>最后我们再来回答开头的问题。&lt;/p>
&lt;blockquote>
&lt;p>64 位相比 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能一定比 32 位 CPU 高很多吗？&lt;/p>
&lt;/blockquote>
&lt;p>64 位相比 32 位 CPU 的优势主要体现在两个方面：&lt;/p>
&lt;p>64 位 CPU 可以一次计算超过 32 位的数字，而 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进行计算，效率就没那么高，但是大部分应用程序很少会计算那么大的数字，所以&lt;strong>只有运算大数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不大&lt;/strong>。&lt;/p>
&lt;p>64 位 CPU 可以&lt;strong>寻址更大的内存空间&lt;/strong>，32 位 CPU 最大的寻址地址是 4G，即使你加了 8G 大小的内存，也还是只能寻址到 4G，而 64 位 CPU 最大寻址地址是 2^64，远超于 32 位 CPU 最大寻址地址的 2^32。&lt;/p>
&lt;blockquote>
&lt;p>你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运行在 64 位的电脑上吗？64 位的操作系统可以运行在 32 位的电脑上吗？如果不行，原因是什么？&lt;/p>
&lt;/blockquote>
&lt;p>64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的：&lt;/p>
&lt;p>如果 32 位指令在 64 位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是&lt;strong>如果 64 位指令在 32 位机器上执行，就比较困难了，因为 32 位的寄存器存不下 64 位的指令&lt;/strong>；&lt;/p>
&lt;p>操作系统其实也是一种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，比如 64 位操作系统，指令也就是 64 位，因此不能装在 32 位机器上。&lt;/p>
&lt;p>总之，硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽。&lt;/p></description></item><item><title>Docs: 10.1.Keepalived 配置示例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/10.1.keepalived-%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/10.1.keepalived-%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/</guid><description>
&lt;h1 id="满足基本-ha-功能的配置">满足基本 HA 功能的配置&lt;/h1>
&lt;pre>&lt;code>global_defs {
notification_email {
root@localhost
}
notification_email_from Alexandre.Cassen@firewall.loc
smtp_server 127.0.0.1
smtp_connect_timeout 30
router_id LVS_DEVEL
script_user root
}
vrrp_script chk_haproxy {
script “killall -0 haproxy”
interval 1
weight -2
}
vrrp_instance VI_1 {
state MASTER #备节点改成BACKUP
interface ens33
virtual_router_id 51
priority 101 #倍节点改成100
advert_int 1
authentication {
auth_type PASS
auth_pass 1111
}
virtual_ipaddress {
192.168.0.75
}
track_script {
chk_haproxy
}
notify_master &amp;quot;/etc/keepalived/notify master&amp;quot; root
notify_backup &amp;quot;/etc/keepalived/notify backup&amp;quot; root
notify_fault &amp;quot;/etc/keepalived/notify fault&amp;quot; root
}
include /etc/keepalived/include/*
&lt;/code>&lt;/pre>
&lt;p>基本 LVS 的配置&lt;/p>
&lt;pre>&lt;code>virtual_server 192.168.0.63 80 {
delay_loop 6
lvs_sched rr
lvs_method DR
protocol TCP
real_server 192.168.1.71 80 {
TCP_CHECK {
connect_timeout 10
}
}
real_server 192.168.0.72 80 {
TCP_CHECK {
connect_timout 10
}
}
}
&lt;/code>&lt;/pre>
&lt;p>keepalive 双主模型&lt;/p>
&lt;pre>&lt;code>global_defs {
notification_email {
linuxedu@foxmail.com
}
notification_email_from kanotify@magedu.com
smtp_connect_timeout 3
smtp_server 127.0.0.1
router_id LVS_DEVEL
}
vrrp_script chk_haproxy {
script &amp;quot;killall -0 haproxy&amp;quot;
interval 1
weight -2
}
vrrp_instance VI_1 {
interface eth0
state MASTER # BACKUP for slave routers
priority 101 # 100 for BACKUP
virtual_router_id 51
garp_master_delay 1
authentication {
auth_type PASS
auth_pass password
}
track_interface {
eth0
}
virtual_ipaddress {
192.168.0.75
}
track_script {
chk_haproxy
}
notify_master &amp;quot;/etc/keepalived/notify.sh master&amp;quot;
notify_backup &amp;quot;/etc/keepalived/notify.sh backup&amp;quot;
notify_fault &amp;quot;/etc/keepalived/notify.sh fault&amp;quot;
}
vrrp_instance VI_2 {
interface eth0
state BACKUP # BACKUP for slave routers
priority 100 # 100 for BACKUP
virtual_router_id 52
garp_master_delay 1
authentication {
auth_type PASS
auth_pass password
}
track_interface {
eth0
}
virtual_ipaddress {
192.168.0.75
}
track_script {
chk_haproxy
}
notify_master &amp;quot;/etc/keepalived/notify.sh master&amp;quot;
notify_backup &amp;quot;/etc/keepalived/notify.sh backup&amp;quot;
notify_fault &amp;quot;/etc/keepalived/notify.sh fault&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;h1 id="keepalived-所用到的脚本示例">keepalived 所用到的脚本示例&lt;/h1>
&lt;p>下面是一个 notify.sh 脚本的简单示例：&lt;/p>
&lt;pre>&lt;code>#!/bin/bash
# Author: MageEdu &amp;lt;linuxedu@foxmail.com&amp;gt;
# description: An example of notify script
vip=192.168.0.75
contact='root@localhost'
notify() {
mailsubject=&amp;quot;`hostname` to be $1: $vip floating&amp;quot;
mailbody=&amp;quot;`date '+%F %H:%M:%S'`: vrrp transition, `hostname` changed to be $1&amp;quot;
echo $mailbody | mail -s &amp;quot;$mailsubject&amp;quot; $contact
}
case &amp;quot;$1&amp;quot; in
master)
notify master
systemctl start haproxy.service
exit 0
;;
backup)
notify backup
systemctl stop haproxy.service
exit 0
;;
fault)
notify fault
/etc/rc.d/init.d/haproxy stop
exit 0
;;
*)
echo 'Usage: `basename $0` {master|backup|fault}'
exit 1
;;
esac
&lt;/code>&lt;/pre>
&lt;p>keepalived 通知脚本进阶示例：&lt;/p>
&lt;p>下面的脚本可以接受选项，其中：&lt;/p>
&lt;p>-s, &amp;ndash;service SERVICE,&amp;hellip;：指定服务脚本名称，当状态切换时可自动启动、重启或关闭此服务；&lt;/p>
&lt;p>-a, &amp;ndash;address VIP: 指定相关虚拟路由器的 VIP 地址；&lt;/p>
&lt;p>-m, &amp;ndash;mode {mm|mb}：指定虚拟路由的模型，mm 表示主主，mb 表示主备；它们表示相对于同一种服务而方，其 VIP 的工作类型；&lt;/p>
&lt;p>-n, &amp;ndash;notify {master|backup|fault}：指定通知的类型，即 vrrp 角色切换的目标角色；&lt;/p>
&lt;p>-h, &amp;ndash;help：获取脚本的使用帮助；&lt;/p>
&lt;pre>&lt;code>#!/bin/bash
# Author: MageEdu &amp;lt;linuxedu@foxmail.com&amp;gt;
# description: An example of notify script
# Usage: notify.sh -m|--mode {mm|mb} -s|--service SERVICE1,... -a|--address VIP -n|--notify {master|backup|falut} -h|--help
#contact='linuxedu@foxmail.com'
helpflag=0
serviceflag=0
modeflag=0
addressflag=0
notifyflag=0
contact='root@localhost'
Usage() {
echo &amp;quot;Usage: notify.sh [-m|--mode {mm|mb}] [-s|--service SERVICE1,...] &amp;lt;-a|--address VIP&amp;gt; &amp;lt;-n|--notify {master|backup|falut}&amp;gt;&amp;quot;
echo &amp;quot;Usage: notify.sh -h|--help&amp;quot;
}
ParseOptions() {
local I=1;
if [ $# -gt 0 ]; then
while [ $I -le $# ]; do
case $1 in
-s|--service)
[ $# -lt 2 ] &amp;amp;&amp;amp; return 3
serviceflag=1
services=(`echo $2|awk -F&amp;quot;,&amp;quot; '{for(i=1;i&amp;lt;=NF;i++) print $i}'`)
shift 2 ;;
-h|--help)
helpflag=1
return 0
shift
;;
-a|--address)
[ $# -lt 2 ] &amp;amp;&amp;amp; return 3
addressflag=1
vip=$2
shift 2
;;
-m|--mode)
[ $# -lt 2 ] &amp;amp;&amp;amp; return 3
mode=$2
shift 2
;;
-n|--notify)
[ $# -lt 2 ] &amp;amp;&amp;amp; return 3
notifyflag=1
notify=$2
shift 2
;;
*)
echo &amp;quot;Wrong options...&amp;quot;
Usage
return 7
;;
esac
done
return 0
fi
}
#workspace=$(dirname $0)
RestartService() {
if [ ${#@} -gt 0 ]; then
for I in $@; do
if [ -x /etc/rc.d/init.d/$I ]; then
/etc/rc.d/init.d/$I restart
else
echo &amp;quot;$I is not a valid service...&amp;quot;
fi
done
fi
}
StopService() {
if [ ${#@} -gt 0 ]; then
for I in $@; do
if [ -x /etc/rc.d/init.d/$I ]; then
/etc/rc.d/init.d/$I stop
else
echo &amp;quot;$I is not a valid service...&amp;quot;
fi
done
fi
}
Notify() {
mailsubject=&amp;quot;`hostname` to be $1: $vip floating&amp;quot;
mailbody=&amp;quot;`date '+%F %H:%M:%S'`, vrrp transition, `hostname` changed to be $1.&amp;quot;
echo $mailbody | mail -s &amp;quot;$mailsubject&amp;quot; $contact
}
# Main Function
ParseOptions $@
[ $? -ne 0 ] &amp;amp;&amp;amp; Usage &amp;amp;&amp;amp; exit 5
[ $helpflag -eq 1 ] &amp;amp;&amp;amp; Usage &amp;amp;&amp;amp; exit 0
if [ $addressflag -ne 1 -o $notifyflag -ne 1 ]; then
Usage
exit 2
fi
mode=${mode:-mb}
case $notify in
'master')
if [ $serviceflag -eq 1 ]; then
RestartService ${services[*]}
fi
Notify master
;;
'backup')
if [ $serviceflag -eq 1 ]; then
if [ &amp;quot;$mode&amp;quot; == 'mb' ]; then
StopService ${services[*]}
else
RestartService ${services[*]}
fi
fi
Notify backup
;;
'fault')
Notify fault
;;
*)
Usage
exit 4
;;
esac
&lt;/code>&lt;/pre>
&lt;p>在 keepalived.conf 配置文件中，其调用方法如下所示：&lt;/p>
&lt;pre>&lt;code>notify_master &amp;quot;/etc/keepalived/notify.sh -n master -a 172.16.100.1&amp;quot;
notify_backup &amp;quot;/etc/keepalived/notify.sh -n backup -a 172.16.100.1&amp;quot;
notify_fault &amp;quot;/etc/keepalived/notify.sh -n fault -a 172.16.100.1&amp;quot;
&lt;/code>&lt;/pre>
&lt;h1 id="keepalived-日志配置">keepalived 日志配置&lt;/h1>
&lt;p>建议使用 local2 级别日志，因为 keepalived_healthcheckers 默认为 local2 级别，新版 keepalived 已不用改该配置&lt;/p>
&lt;p>修改启动参数,将 -D 改为 -D -d -S 0&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sed -i &lt;span style="color:#e6db74">&amp;#39;s/\(KEEPALIVED_OPTIONS=\)&amp;#34;-D&amp;#34;/\1&amp;#34;-D -d -S 0&amp;#34;/&amp;#39;&lt;/span> /etc/sysconfig/keepalived
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>修改 rsyslog 配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/rsyslog.d/keepalived-log.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">local0.* /var/log/keepalived/keepalived.log
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;amp; stop
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>配置日志轮替&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/logrotate.d/keepalived &lt;span style="color:#e6db74">&amp;lt;&amp;lt; \EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">/var/log/keepalived/keepalived.log {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> daily
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> copytruncate
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> rotate 10
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> missingok
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> dateext
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> notifempty
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> compress
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> sharedscripts
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> postrotate
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> /bin/kill -HUP `cat /var/run/syslogd.pid 2&amp;gt; /dev/null` 2&amp;gt; /dev/null || true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> /bin/kill -HUP `cat /var/run/rsyslogd.pid 2&amp;gt; /dev/null` 2&amp;gt; /dev/null || true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> endscript
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>systemctl restart rsyslog
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 10.1.零拷贝</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/6.file-system-%E7%AE%A1%E7%90%86/10.1.%E9%9B%B6%E6%8B%B7%E8%B4%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/6.file-system-%E7%AE%A1%E7%90%86/10.1.%E9%9B%B6%E6%8B%B7%E8%B4%9D/</guid><description>
&lt;hr>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。&lt;/p>
&lt;p>这次，我们就以「文件传输」作为切入点，来分析 I/O 工作方式，以及如何优化传输文件的性能。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581743-fe1685b9-8478-478c-bec2-c7d550e1f54a.png" alt="">&lt;/p>
&lt;hr>
&lt;h2 id="正文">正文&lt;/h2>
&lt;h3 id="为什么要有-dma-技术">为什么要有 DMA 技术?&lt;/h3>
&lt;p>在没有 DMA 技术前，I/O 的过程是这样的：&lt;/p>
&lt;ul>
&lt;li>CPU 发出对应的指令给磁盘控制器，然后返回；&lt;/li>
&lt;li>磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个&lt;strong>中断&lt;/strong>；&lt;/li>
&lt;li>CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。&lt;/li>
&lt;/ul>
&lt;p>为了方便你理解，我画了一副图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581747-833fd3d8-b587-44e5-a4d7-2118e13b1b44.png" alt="">&lt;/p>
&lt;p>可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。&lt;/p>
&lt;p>简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。&lt;/p>
&lt;p>计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是&lt;strong>直接内存访问（***&lt;strong>Direct Memory Access&lt;/strong>*&lt;/strong>）** 技术。&lt;/p>
&lt;p>什么是 DMA 技术？简单理解就是，&lt;strong>在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务&lt;/strong>。&lt;/p>
&lt;p>那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581757-ef21d29d-7f2b-4cd1-a747-98b242f710cf.png" alt="">&lt;/p>
&lt;p>具体过程：&lt;/p>
&lt;ul>
&lt;li>用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；&lt;/li>
&lt;li>操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；&lt;/li>
&lt;li>DMA 进一步将 I/O 请求发送给磁盘；&lt;/li>
&lt;li>磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；&lt;/li>
&lt;li>&lt;strong>DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务&lt;/strong>；&lt;/li>
&lt;li>当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；&lt;/li>
&lt;li>CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；&lt;/li>
&lt;/ul>
&lt;p>可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。&lt;/p>
&lt;p>早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。&lt;/p>
&lt;hr>
&lt;h3 id="传统的文件传输有多糟糕">传统的文件传输有多糟糕？&lt;/h3>
&lt;p>如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。&lt;/p>
&lt;p>传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。&lt;/p>
&lt;p>代码通常如下，一般会需要两个系统调用：&lt;/p>
&lt;pre>&lt;code>read(file, tmp_buf, len);write(socket, tmp_buf, len);
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>代码很简单，虽然就两行代码，但是这里面发生了不少的事情。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581754-e4951f6d-41a5-4564-a890-901bf62db701.png" alt="">&lt;/p>
&lt;p>首先，期间共&lt;strong>发生了 4 次用户态与内核态的上下文切换&lt;/strong>，因为发生了两次系统调用，一次是 &lt;code>read()&lt;/code> ，一次是 &lt;code>write()&lt;/code>，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。&lt;/p>
&lt;p>上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。&lt;/p>
&lt;p>其次，还&lt;strong>发生了 4 次数据拷贝&lt;/strong>，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：&lt;/p>
&lt;ul>
&lt;li>&lt;em>第一次拷贝&lt;/em>，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。&lt;/li>
&lt;li>&lt;em>第二次拷贝&lt;/em>，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。&lt;/li>
&lt;li>&lt;em>第三次拷贝&lt;/em>，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。&lt;/li>
&lt;li>&lt;em>第四次拷贝&lt;/em>，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。&lt;/li>
&lt;/ul>
&lt;p>我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。&lt;/p>
&lt;p>这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。&lt;/p>
&lt;p>所以，&lt;strong>要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数&lt;/strong>。&lt;/p>
&lt;hr>
&lt;h3 id="如何优化文件传输的性能">如何优化文件传输的性能？&lt;/h3>
&lt;blockquote>
&lt;p>先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？&lt;/p>
&lt;/blockquote>
&lt;p>读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。&lt;/p>
&lt;p>而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。&lt;/p>
&lt;p>所以，&lt;strong>要想减少上下文切换到次数，就要减少系统调用的次数&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>再来看看，如何减少「数据拷贝」的次数？&lt;/p>
&lt;/blockquote>
&lt;p>在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。&lt;/p>
&lt;p>因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此&lt;strong>用户的缓冲区是没有必要存在的&lt;/strong>。&lt;/p>
&lt;hr>
&lt;h3 id="如何实现零拷贝">如何实现零拷贝？&lt;/h3>
&lt;p>零拷贝技术实现的方式通常有 2 种：&lt;/p>
&lt;ul>
&lt;li>mmap + write&lt;/li>
&lt;li>sendfile&lt;/li>
&lt;/ul>
&lt;p>下面就谈一谈，它们是如何减少「上下文切换」和「数据拷贝」的次数。&lt;/p>
&lt;p>mmap + write&lt;/p>
&lt;p>在前面我们知道，&lt;code>read()&lt;/code> 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 &lt;code>mmap()&lt;/code> 替换 &lt;code>read()&lt;/code> 系统调用函数。&lt;/p>
&lt;pre>&lt;code>buf = mmap(file, len);write(sockfd, buf, len);
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>&lt;code>mmap()&lt;/code> 系统调用函数会直接把内核缓冲区里的数据「&lt;strong>映射&lt;/strong>」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581773-7d9df0ea-5e23-48c7-9e90-5575bf286553.png" alt="">&lt;/p>
&lt;p>具体过程如下：&lt;/p>
&lt;ul>
&lt;li>应用进程调用了 &lt;code>mmap()&lt;/code> 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；&lt;/li>
&lt;li>应用进程再调用 &lt;code>write()&lt;/code>，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；&lt;/li>
&lt;li>最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。&lt;/li>
&lt;/ul>
&lt;p>我们可以得知，通过使用 &lt;code>mmap()&lt;/code> 来代替 &lt;code>read()&lt;/code>， 可以减少一次数据拷贝的过程。&lt;/p>
&lt;p>但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。&lt;/p>
&lt;p>sendfile&lt;/p>
&lt;p>在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 &lt;code>sendfile()&lt;/code>，函数形式如下：&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;sys/socket.h&amp;gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。&lt;/p>
&lt;p>首先，它可以替代前面的 &lt;code>read()&lt;/code> 和 &lt;code>write()&lt;/code> 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。&lt;/p>
&lt;p>其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581761-10aa5962-487a-444d-9c3b-878f1a5f0d4d.png" alt="">&lt;/p>
&lt;p>但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（&lt;em>The Scatter-Gather Direct Memory Access&lt;/em>）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。&lt;/p>
&lt;p>你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：&lt;/p>
&lt;pre>&lt;code>$ ethtool -k eth0 | grep scatter-gatherscatter-gather: on
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>于是，从 Linux 内核 &lt;code>2.4&lt;/code> 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， &lt;code>sendfile()&lt;/code> 系统调用的过程发生了点变化，具体过程如下：&lt;/p>
&lt;ul>
&lt;li>第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；&lt;/li>
&lt;li>第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；&lt;/li>
&lt;/ul>
&lt;p>所以，这个过程之中，只进行了 2 次数据拷贝，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581761-cd2da78d-0ce1-4048-ae1c-ec5d19830fc6.png" alt="">&lt;/p>
&lt;p>这就是所谓的&lt;strong>零拷贝（***&lt;strong>Zero-copy&lt;/strong>*&lt;/strong>）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。**。&lt;/p>
&lt;p>零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，&lt;strong>只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。&lt;/strong>&lt;/p>
&lt;p>所以，总体来看，&lt;strong>零拷贝技术可以把文件传输的性能提高至少一倍以上&lt;/strong>。&lt;/p>
&lt;p>使用零拷贝技术的项目&lt;/p>
&lt;p>事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。&lt;/p>
&lt;p>如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 &lt;code>transferTo&lt;/code> 方法：&lt;/p>
&lt;pre>&lt;code>@Overridepublic long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { return fileChannel.transferTo(position, count, socketChannel);}
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>如果 Linux 系统支持 &lt;code>sendfile()&lt;/code> 系统调用，那么 &lt;code>transferTo()&lt;/code> 实际上最后就会使用到 &lt;code>sendfile()&lt;/code> 系统调用函数。曾经有大佬专门写过程序测试过，在同样的硬件条件下，传统文件传输和零拷拷贝文件传输的性能差异，你可以看到下面这张测试数据图，使用了零拷贝能够缩短 &lt;code>65%&lt;/code> 的时间，大幅度提升了机器传输数据的吞吐量。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581773-3ca39dfc-4c62-4827-a5f8-1ccd96ce7a88.png" alt="">&lt;/p>
&lt;p>数据来源于：&lt;a href="https://developer.ibm.com/articles/j-zerocopy/">https://developer.ibm.com/articles/j-zerocopy/&lt;/a>&lt;/p>
&lt;p>另外，Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：&lt;/p>
&lt;pre>&lt;code>http {... sendfile on...}
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>sendfile 配置的具体意思:&lt;/p>
&lt;ul>
&lt;li>设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。&lt;/li>
&lt;li>设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。&lt;/li>
&lt;/ul>
&lt;p>当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。&lt;/p>
&lt;hr>
&lt;h3 id="pagecache-有什么作用">PageCache 有什么作用？&lt;/h3>
&lt;p>回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是&lt;strong>磁盘高速缓存（***&lt;strong>PageCache&lt;/strong>*&lt;/strong>）**。&lt;/p>
&lt;p>由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的。&lt;/p>
&lt;p>读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。&lt;/p>
&lt;p>但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。&lt;/p>
&lt;p>那问题来了，选择哪些磁盘数据拷贝到内存呢？&lt;/p>
&lt;p>我们都知道程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 &lt;strong>PageCache 来缓存最近被访问的数据&lt;/strong>，当空间不足时淘汰最久未被访问的缓存。&lt;/p>
&lt;p>所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。&lt;/p>
&lt;p>还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，&lt;strong>PageCache 使用了「预读功能」&lt;/strong>。&lt;/p>
&lt;p>比如，假设 read 方法每次只会读 &lt;code>32 KB&lt;/code> 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32 ～ 64 KB 也读取到 PageCache，这样后面读取 32 ～ 64 KB 的成本就很低，如果在 32 ～ 64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。&lt;/p>
&lt;p>所以，PageCache 的优点主要是两个：&lt;/p>
&lt;ul>
&lt;li>缓存最近被访问的数据；&lt;/li>
&lt;li>预读功能；&lt;/li>
&lt;/ul>
&lt;p>这两个做法，将大大提高读写磁盘的性能。&lt;/p>
&lt;p>&lt;strong>但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能&lt;/strong>&lt;/p>
&lt;p>这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。&lt;/p>
&lt;p>另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题：&lt;/p>
&lt;ul>
&lt;li>PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了；&lt;/li>
&lt;li>PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次；&lt;/li>
&lt;/ul>
&lt;p>所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。&lt;/p>
&lt;hr>
&lt;h3 id="大文件传输用什么方式实现">大文件传输用什么方式实现？&lt;/h3>
&lt;p>那针对大文件的传输，我们应该使用什么方式呢？&lt;/p>
&lt;p>我们先来看看最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581758-1e058506-1c0b-4bad-ae68-9846cd0f3016.png" alt="">&lt;/p>
&lt;p>具体过程：&lt;/p>
&lt;ul>
&lt;li>当调用 read 方法时，会阻塞着，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好；&lt;/li>
&lt;li>内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里；&lt;/li>
&lt;li>最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。&lt;/li>
&lt;/ul>
&lt;p>对于阻塞的问题，可以用异步 I/O 来解决，它工作方式如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581774-bf45125c-8de0-493f-892e-1b57b4b150b9.png" alt="">&lt;/p>
&lt;p>它把读操作分为两部分：&lt;/p>
&lt;ul>
&lt;li>前半部分，内核向磁盘发起读请求，但是可以&lt;strong>不等待数据就位就可以返回&lt;/strong>，于是进程此时可以处理其他任务；&lt;/li>
&lt;li>后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的&lt;strong>通知&lt;/strong>，再去处理数据；&lt;/li>
&lt;/ul>
&lt;p>而且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。&lt;/p>
&lt;p>绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。&lt;/p>
&lt;p>前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。&lt;/p>
&lt;p>于是，&lt;strong>在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术&lt;/strong>。&lt;/p>
&lt;p>直接 I/O 应用场景常见的两种：&lt;/p>
&lt;ul>
&lt;li>应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；&lt;/li>
&lt;li>传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。&lt;/li>
&lt;/ul>
&lt;p>另外，由于直接 I/O 绕过了 PageCache，就无法享受内核的这两点的优化：&lt;/p>
&lt;ul>
&lt;li>内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「&lt;strong>合并&lt;/strong>」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；&lt;/li>
&lt;li>内核也会「&lt;strong>预读&lt;/strong>」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作；&lt;/li>
&lt;/ul>
&lt;p>于是，传输大文件的时候，使用「异步 I/O + 直接 I/O」了，就可以无阻塞地读取文件了。&lt;/p>
&lt;p>所以，传输文件的时候，我们要根据文件的大小来使用不同的方式：&lt;/p>
&lt;ul>
&lt;li>传输大文件的时候，使用「异步 I/O + 直接 I/O」；&lt;/li>
&lt;li>传输小文件的时候，则使用「零拷贝技术」；&lt;/li>
&lt;/ul>
&lt;p>在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式：&lt;/p>
&lt;pre>&lt;code>location /video/ { sendfile on; aio on; directio 1024m; }
&lt;/code>&lt;/pre>
&lt;p>1
Plain Text&lt;/p>
&lt;p>当文件大小大于 &lt;code>directio&lt;/code> 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。&lt;/p>
&lt;hr>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。&lt;/p>
&lt;p>于是，为了解决这一问题，DMA 技术就出现了，每个 I/O 设备都有自己的 DMA 控制器，通过这个 DMA 控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的工作。&lt;/p>
&lt;p>传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。&lt;/p>
&lt;p>为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次系统调用（&lt;code>sendfile&lt;/code> 方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。&lt;/p>
&lt;p>Kafka 和 Nginx 都有实现零拷贝技术，这将大大提高文件传输的性能。&lt;/p>
&lt;p>零拷贝技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读比随机读性能好的原因。这些优势，进一步提升了零拷贝的性能。&lt;/p>
&lt;p>需要注意的是，零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送。&lt;/p>
&lt;p>另外，当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。&lt;/p>
&lt;p>在 Nginx 里，可以通过配置，设定一个文件大小阈值，针对大文件使用异步 IO 和直接 IO，而对小文件使用零拷贝。&lt;/p>
&lt;hr>
&lt;h2 id="絮叨">絮叨&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/zl0fv1/1616167581788-f2a0299b-a76a-4d58-b228-f6d81888f5fb.png" alt="">&lt;/p>
&lt;p>&lt;strong>大家好，我是小林，一个专为大家图解的工具人，如果觉得文章对你有帮助，欢迎分享给你的朋友，这对小林非常重要，谢谢你们，我们下次见！&lt;/strong>&lt;/p></description></item><item><title>Docs: 10.2.keepalived+nginx 配置示例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/10.2.keepalived+nginx-%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/keepalived/10.2.keepalived+nginx-%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/</guid><description>
&lt;h1 id="适用于-keepalived-的-node-节点的前端负载均衡的配置">适用于 keepalived 的 node 节点的前端负载均衡的配置&lt;/h1>
&lt;h3 id="keepalivedconf-主节点配置内容xa">keepalived.conf 主节点配置内容
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/keepalived/keepalived.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">global_defs {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> router_id k8s-master-dr
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> script_user root
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> enable_script_security
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">vrrp_script check_nginx {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> script &amp;#34;/etc/keepalived/check_nginx.sh&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> interval 3
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> weight -2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> fall 2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> rise 2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">vrrp_instance VI_K8S {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> state BACKUP
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> interface eth0
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> virtual_router_id 60
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> priority 101
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> nopreempt
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> authentication {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> auth_type PASS
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> auth_pass 4be37dc3b4c90194d1600c483e10ad1d
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> virtual_ipaddress {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> 172.40.0.60
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> track_script {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> check_nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="keepalivedconf-备节点配置内容">keepalived.conf 备节点配置内容&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/keepalived/keepalived.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">global_defs {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> router_id k8s-master-dr
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> script_user root
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> enable_script_security
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">vrrp_script check_nginx {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> script &amp;#34;/etc/keepalived/check_nginx.sh&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> interval 3
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> weight -2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> fall 2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> rise 2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">vrrp_instance VI_K8S {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> state BACKUP
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> interface eth0
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> virtual_router_id 60
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> priority 100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> nopreempt
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> authentication {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> auth_type PASS
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> auth_pass 4be37dc3b4c90194d1600c483e10ad1d
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> virtual_ipaddress {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> 172.40.0.60
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> track_script {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> check_nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="check_nginxsh-配置内容">check_nginx.sh 配置内容&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/keepalived/check_nginx.sh &lt;span style="color:#e6db74">&amp;lt;&amp;lt; \EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">pidof nginx #检查memcached服务
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">if [[ $? == 0 ]];then #检查成功
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> /sbin/iptables -S | grep vrrp
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> if [[ $? == 0 ]]; then #如果iptable中有vrrp的配置，删除它
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> /sbin/iptables -D OUTPUT -p vrrp -j DROP
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> fi
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> exit 0
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">else #检查失败
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> /sbin/iptables -S | grep vrrp
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> if [[ $? != 0 ]]; then
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> /sbin/iptables -A OUTPUT -p vrrp -j DROP #如果iptable中没有vrrp的条目，禁止vrrp发出
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> fi
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> exit 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">fi
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod &lt;span style="color:#ae81ff">755&lt;/span> /etc/keepalived/check_nginx.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 10.2.深入理解 Cilium 的 eBPF 收发包路径</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/bpf/bpf-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/10.2.%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-cilium-%E7%9A%84-ebpf-%E6%94%B6%E5%8F%91%E5%8C%85%E8%B7%AF%E5%BE%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/bpf/bpf-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6/10.2.%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-cilium-%E7%9A%84-ebpf-%E6%94%B6%E5%8F%91%E5%8C%85%E8%B7%AF%E5%BE%84/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;h1 id="深入理解-cilium-的-ebpf-收发包路径">深入理解 Cilium 的 eBPF 收发包路径&lt;/h1>
&lt;p>本文翻译自 2019 年 DigitalOcean 的工程师 Nate Sweet 在 KubeCon 的一篇分享: Understanding (and Troubleshooting) the eBPF Datapath in Cilium[1] 。&lt;/p>
&lt;p>由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。&lt;/p>
&lt;p>以下是译文。&lt;/p>
&lt;ol>
&lt;li>为什么要关注 eBPF？&lt;/li>
&lt;/ol>
&lt;p>网络成为瓶颈&lt;/p>
&lt;p>大家已经知道网络成为瓶颈，但我是从下面这个角度考虑的：近些年业界使用网络的方式 ，使其成为瓶颈（it is the bottleneck in a way that is actually pretty recent） 。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>网络一直都是 I/O 密集型的，但直到最近，这件事情才变得尤其重要。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>分布式任务（workloads）业界一直都在用，但直到近些年，这种模型才成为主流。虽然何时成为主流众说纷纭，但我认为最早不会早于 90 年代晚期。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>公有云的崛起，我认为可能是网络成为瓶颈的最主要原因。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这种情况下，用于管理依赖和解决瓶颈的工具都已经过时了。&lt;/p>
&lt;p>但像 eBPF 这样的技术使得网络调优和整流（tune and shape this traffic）变得简单很多。eBPF 提供的许多能力是其他工具无法提供的，或者即使提供了，其代价也要比 eBPF 大 的多。&lt;/p>
&lt;p>eBPF 无处不在&lt;/p>
&lt;p>eBPF 正在变得无处不在，我们可能会争论这到底是一件好事还是坏事（eBPF 也确实带了一 些安全问题），但当前无法忽视的事实是：Linux 内核的网络开发者们正在将 eBPF 应用 于各种地方（putting it everywhere）。其结果是，eBPF 与内核的默认收发包路径（ datapath）耦合得越来越紧（more and more tightly coupled with the default datapath）。&lt;/p>
&lt;p>性能就是金钱&lt;/p>
&lt;p>“Metrics are money”[2]， 这是今年 Paris Kernel Recipes 峰会上，来自 Synthesio 的 Aurelian Rougemont 的 精彩分享。&lt;/p>
&lt;p>他展示了一些史诗级的调试（debugging）案例，感兴趣的可以去看看；但更重要的是，他 从更高层次提出了这样一个观点：理解这些东西是如何工作的，最终会产生资本收益（ understanding how this stuff works translates to money）。为客户节省金钱，为 自己带来收入。&lt;/p>
&lt;p>如果你能从更少的资源中榨取出更高的性能，使软件运行更快，那 显然你对公司的贡献就更大。Cilium 就是这样一个能让你带来更大价值的工具。&lt;/p>
&lt;p>在进一步讨论之前，我先简要介绍一下 eBPF 是什么，以及为什么它如此强大。&lt;/p>
&lt;ol start="2">
&lt;li>eBPF 是什么？&lt;/li>
&lt;/ol>
&lt;p>BPF 程序有多种类型，图 2.1 是其中一种，称为 XDP BPF 程序。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>XDP 是 eXpress DataPath（特快数据路径）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>XDP 程序可以直接加载到网络设备上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>XDP 程序在数据包收发路径上很前面的位置就开始执行，下面会看到例子。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>BPF 程序开发方式：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>编写一段 BPF 程序&lt;/p>
&lt;/li>
&lt;li>
&lt;p>编译这段 BPF 程序&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用一个特殊的系统调用将编译后的代码加载到内核&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这实际上就是编写了一段内核代码，并动态插入到了内核（written kernel code and dynamically inserted it into the kernel）。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781351-8a096334-1fbf-483b-96ee-8759c09df60d.jpeg" alt="">&lt;/p>
&lt;p>图 2.1. eBPF 代码示例：丢弃源 IP 命中黑名单的 ARP 包&lt;/p>
&lt;p>图 2.1 中的程序使用了一种称为 map 的东西，这是一种特殊的数据结构，可用于 在内核和用户态之间传递数据，例如通过一个特殊的系统从用户态向 map 里插入数据。&lt;/p>
&lt;p>这段程序的功能：丢弃所有源 IP 命中黑名单的 ARP 包。右侧四个框内的代码功能：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>初始化以太帧结构体（ethernet packet）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果不是 ARP 包，直接退出，将包交给内核继续处理。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>至此已确定是 ARP，因此初始化一个 ARP 数据结构，对包进行下一步处理。例 如，提取出 ARP 中的源 IP，去之前创建好的黑名单中查询该 IP 是否存在。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果存在，返回丢弃判决（XDP_DROP）；否则，返回允许通行判决（ XDP_PASS），内核会进行后续处理。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>你可能不会相信，就这样一段简单的程序，会让服务器性能产生质的飞跃，因为它此时已 经拥有了一条极为高效的网络路径（an extremely efficient network path）。&lt;/p>
&lt;ol start="3">
&lt;li>为什么 eBPF 如此强大？&lt;/li>
&lt;/ol>
&lt;p>三方面原因：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>快速（fast）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>灵活（flexible）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据与功能分离（separates data from functionality）&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>快速&lt;/p>
&lt;p>eBPF 几乎总是比 iptables 快，这是有技术原因的。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>eBPF 程序本身并不比 iptables 快，但 eBPF 程序更短。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>iptables 基于一个非常庞大的内核框架（Netfilter），这个框架出现在内核 datapath 的多个地方，有很大冗余。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>因此，同样是实现 ARP drop 这样的功能，基于 iptables 做冗余就会非常大，导致性能很低。&lt;/p>
&lt;p>灵活&lt;/p>
&lt;p>这可能是最主要的原因。你可以用 eBPF 做几乎任何事情。&lt;/p>
&lt;p>eBPF 基于内核提供的一组接口，运行 JIT 编译的字节码，并将计算结果返回给内核。例如 内核只关心 XDP 程序的返回是 PASS, DROP 还是 REDIRECT。至于在 XDP 程序里做什么， 完全看你自己。&lt;/p>
&lt;p>数据与功能分离&lt;/p>
&lt;p>eBPF separates data from functionality.&lt;/p>
&lt;p>nftables 和 iptables 也能干这个事情，但功能没有 eBPF 强大。例如，eBPF 可以使 用 per-cpu 的数据结构，因此能取得更极致的性能。&lt;/p>
&lt;p>eBPF 真正的优势是将 “数据与功能分离” 这件事情做地非常干净（clean separation）：可以在 eBPF 程序不中断的情况下修改它的运行方式。具体方式是修改它访 问的配置数据或应用数据，例如黑名单里规定的 IP 列表和域名。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781318-616b3cfa-ef39-48c2-a3bc-b237c8f14596.jpeg" alt="">&lt;/p>
&lt;ol start="4">
&lt;li>eBPF 简史&lt;/li>
&lt;/ol>
&lt;p>这里是简单介绍几句，后面 datapath 才是重点。&lt;/p>
&lt;p>两篇论文，可读性还是比较好的，感兴趣的自行阅读：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Steven McCanne, et al, in 1993 - The BSD Packet Filter&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Jeffrey C. Mogul, et al, in 1987 - first open source implementation of a packet filter.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;ol start="5">
&lt;li>Cilium 是什么，为什么要关注它？&lt;/li>
&lt;/ol>
&lt;p>我认为理解 eBPF 代码还比较简单，多看看内核代码就行了，但配置和编写 eBPF 就要难多了。&lt;/p>
&lt;p>Cilium 是一个很好的 eBPF 之上的通用抽象，覆盖了分布式系统的绝大多数场景。Cilium 封装了 eBPF，提供一个更上层的 API。如果你使用的是 Kubernetes，那你至少应该听说过 Cilium。&lt;/p>
&lt;p>Cilium 提供了 CNI 和 kube-proxy replacement 功能，相比 iptables 性能要好很多。&lt;/p>
&lt;p>接下来开始进入本文重点。&lt;/p>
&lt;ol start="6">
&lt;li>内核默认 datapath&lt;/li>
&lt;/ol>
&lt;p>本节将介绍数据包是如何穿过 network datapath（网络数据路径）的：包括从硬件到 内核，再到用户空间。&lt;/p>
&lt;p>这里将只介绍 Cilium 所使用的 eBPF 程序，其中有 Cilium logo 的地方，都是 datapath 上 Cilium 重度使用 BPF 程序的地方。&lt;/p>
&lt;p>本文不会过多介绍硬件相关内容，因为理解 eBPF 基本不需要硬件知识，但显然理解了硬件 原理也并无坏处。另外，由于时间限制，我将只讨论接收部分。&lt;/p>
&lt;p>L1 -&amp;gt; L2（物理层 -&amp;gt; 数据链路层）&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781303-b3b39b25-bc79-4cc3-aac9-6aa9b8474ffd.jpeg" alt="">&lt;/p>
&lt;p>网卡收包简要流程：&lt;/p>
&lt;ol>
&lt;li>网卡驱动初始化。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>网卡获得一块物理内存，作用收发包的缓冲区（ring-buffer）。这种方式成为 DMA（直接内存访问）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>驱动向内核 NAPI（New API）注册一个轮询（poll ）方法。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;p>网卡从云上收到一个包，将包放到 ring-buffer。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果此时 NAPI 没有在执行，网卡就会触发一个硬件中断（HW IRQ），告诉处理器 DMA 区域中有包等待处理。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>收到硬中断信号后，处理器开始执行 NAPI。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>NAPI 执行网卡注册的 poll 方法开始收包。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>关于 NAPI poll 机制：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>这是 Linux 内核中的一种通用抽象，任何等待不可抢占状态发生（wait for a preemptible state to occur）的模块，都可以使用这种注册回调函数的方式。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>驱动注册的这个 poll 是一个主动式 poll（active poll），一旦执行就会持续处理 ，直到没有数据可供处理，然后进入 idle 状态。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在这里，执行 poll 方法的是运行在某个或者所有 CPU 上的内核线程（kernel thread）。虽然这个线程没有数据可处理时会进入 idle 状态，但如前面讨论的，在当前大部分分布 式系统中，这个线程大部分时间内都是在运行的，不断从驱动的 DMA 区域内接收数据包。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>poll 会告诉网卡不要再触发硬件中断，使用软件中断（softirq）就行了。此后这些 内核线程会轮询网卡的 DMA 区域来收包。之所以会有这种机制，是因为硬件中断代价太 高了，因为它们比系统上几乎所有东西的优先级都要高。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>我们接下来还将多次看到这个广义的 NAPI 抽象，因为它不仅仅处理驱动，还能处理许多 其他场景。内核用 NAPI 抽象来做驱动读取（driver reads）、epoll 等等。&lt;/p>
&lt;p>NAPI 驱动的 poll 机制将数据从 DMA 区域读取出来，对数据做一些准备工作，然后交给比 它更上一层的内核协议栈。&lt;/p>
&lt;p>L2 续（数据链路层 - 续）&lt;/p>
&lt;p>同样，这里不会深入展开驱动层做的事情，而主要关注内核所做的一些更上层的事情，例如&lt;/p>
&lt;ul>
&lt;li>
&lt;p>分配 socket buffers（skb）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BPF&lt;/p>
&lt;/li>
&lt;li>
&lt;p>iptables&lt;/p>
&lt;/li>
&lt;li>
&lt;p>将包送到网络栈（network stack）和用户空间&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Step 1：NAPI poll&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781309-125b38ab-2e65-40ef-aa15-62f61eb13ceb.jpeg" alt="">&lt;/p>
&lt;p>首先，NAPI poll 机制不断调用驱动实现的 poll 方法，后者处理 RX 队列内的包，并最终 将包送到正确的程序。这就到了我们前面的 XDP 类型程序。&lt;/p>
&lt;p>Step 2：XDP 程序处理&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781326-521284ff-d334-4b96-aedb-b33d0b2de4bc.jpeg" alt="">&lt;/p>
&lt;p>如果驱动支持 XDP，那 XDP 程序将在 poll 机制内执行。如果不支持，那 XDP 程序将只能在更后面执行（run significantly upstack，见 Step 6），性能会变差， 因此确定你使用的网卡是否支持 XDP 非常重要。&lt;/p>
&lt;p>XDP 程序返回一个判决结果给驱动，可以是 PASS, TRANSMIT, 或 DROP。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Transmit 非常有用，有了这个功能，就可以用 XDP 实现一个 TCP/IP 负载均衡器。XDP 只适合对包进行较小修改，如果是大动作修改，那这样的 XDP 程序的性能 可能并不会很高，因为这些操作会降低 poll 函数处理 DMA ring-buffer 的能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更有趣的是 DROP 方法，因为一旦判决为 DROP，这个包就可以直接原地丢弃了，而 无需再穿越后面复杂的协议栈然后再在某个地方被丢弃，从而节省了大量资源。如果本次 分享我只能给大家一个建议，那这个建议就是：在 datapath 越前面做 tuning 和 dropping 越好，这会显着增加系统的网络吞吐。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果返回是 PASS，内核会继续沿着默认路径处理包，到达 clean_rx() 方法。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Step 3：clean_rx()：创建 skb&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781362-6a9e47a1-27b7-4c04-9561-20c8e31f36ac.jpeg" alt="">&lt;/p>
&lt;p>如果返回是 PASS，内核会继续沿着默认路径处理包，到达 clean_rx() 方法。&lt;/p>
&lt;p>这个方法创建一个 socket buffer（skb）对象，可能还会更新一些统计信息，对 skb 进行硬件校验和检查，然后将其交给 gro_receive() 方法。&lt;/p>
&lt;p>Step 4：gro_receive()&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781355-174a04b7-6899-4edb-b03e-f367bb5964f8.jpeg" alt="">&lt;/p>
&lt;p>GRO 是一种较老的硬件特性（LRO）的软件实现，功能是对分片的包进行重组然后交给更 上层，以提高吞吐。&lt;/p>
&lt;p>GRO 给协议栈提供了一次将包交给网络协议栈之前，对其检查校验和 、修改协议头和发送应答包（ACK packets）的机会。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>如果 GRO 的 buffer 相比于包太小了，它可能会选择什么都不做。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果当前包属于某个更大包的一个分片，调用 enqueue_backlog 将这个分片放到某个 CPU 的包队列。当包重组完成后，会交给 receive_skb() 方法处理。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果当前包不是分片包，直接调用 receive_skb()，进行一些网络栈最底层的处理。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Step 5：receive_skb()&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781356-3ece92df-59d7-4c45-a9e1-d2c93c7ed8c8.jpeg" alt="">&lt;/p>
&lt;p>receive_skb() 之后会再次进入 XDP 程序点。&lt;/p>
&lt;p>L2 -&amp;gt; L3（数据链路层 -&amp;gt; 网络层）&lt;/p>
&lt;p>Step 6：通用 XDP 处理（gXDP）&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781340-de37c24f-f1a6-4301-ba6d-9714dcbd4441.jpeg" alt="">&lt;/p>
&lt;p>receive_skb() 之后，我们又来到了另一个 XDP 程序执行点。这里可以通过 receive_xdp() 做一些通用（generic）的事情，因此我在图中将其标注为 (g)XDP&lt;/p>
&lt;p>Step 2 中提到，如果网卡驱动不支持 XDP，那 XDP 程序将延迟到更后面执行，这个 “更后面” 的位置指的就是这里的 (g)XDP。&lt;/p>
&lt;p>Step 7：Tap 设备处理&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781402-cd7bbf78-f5ef-4707-9fc9-f07c344a9cd5.jpeg" alt="">&lt;/p>
&lt;p>图中有个 *check_taps 框，但其实并没有这个方法：receive_skb() 会轮询所有的 socket tap，将包放到正确的 tap 设备的缓冲区。&lt;/p>
&lt;p>tap 设备监听的是三层协议（L3 protocols），例如 IPv4、ARP、IPv6 等等。如果 tap 设 备存在，它就可以操作这个 skb 了。&lt;/p>
&lt;p>Step 8：tc（traffic classifier）处理&lt;/p>
&lt;p>接下来我们遇到了第二种 eBPF 程序：tc eBPF。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781385-cf6fb476-67e1-460a-a67b-ca69a3c7f29b.jpeg" alt="">&lt;/p>
&lt;p>tc（traffic classifier，流量分类器）是 Cilium 依赖的最基础的东西，它提供了多种功 能，例如修改包（mangle，给 skb 打标记）、重路由（reroute）、丢弃包（drop），这 些操作都会影响到内核的流量统计，因此也影响着包的排队规则（queueing discipline ）。&lt;/p>
&lt;p>Cilium 控制的网络设备，至少被加载了一个 tc eBPF 程序。&lt;/p>
&lt;p>译者注：如何查看已加载的 eBPF 程序，可参考 Cilium Network Topology and Traffic Path on AWS[3]。&lt;/p>
&lt;p>Step 9：Netfilter 处理&lt;/p>
&lt;p>如果 tc BPF 返回 OK，包会再次进入 Netfilter。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781357-7af070a5-c730-43b3-a4d5-a9dc8c2a9f6b.jpeg" alt="">&lt;/p>
&lt;p>Netfilter 也会对入向的包进行处理，这里包括 nftables 和 iptables 模块。&lt;/p>
&lt;p>有一点需要记住的是：Netfilter 是网络栈的下半部分（the “bottom half” of the network stack），因此 iptables 规则越多，给网络栈下半部分造成的瓶颈就越大。&lt;/p>
&lt;p>*def_dev_protocol 框是二层过滤器（L2 net filter），由于 Cilium 没有用到任何 L2 filter，因此这里我就不展开了。&lt;/p>
&lt;p>Step 10：L3 协议层处理：ip_rcv()&lt;/p>
&lt;p>最后，如果包没有被前面丢弃，就会通过网络设备的 ip_rcv() 方法进入协议栈的三层（ L3）—— 即 IP 层 —— 进行处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781390-e8226b64-65b0-480a-8269-62782e6d97b0.jpeg" alt="">&lt;/p>
&lt;p>接下来我们将主要关注这个函数，但这里需要提醒大家的是，Linux 内核也支持除了 IP 之 外的其他三层协议，它们的 datapath 会与此有些不同。&lt;/p>
&lt;p>L3 -&amp;gt; L4（网络层 -&amp;gt; 传输层）&lt;/p>
&lt;p>Step 11：Netfilter L4 处理&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781353-857fa1b3-fcd0-4052-9b4c-a7539b7a2907.jpeg" alt="">&lt;/p>
&lt;p>ip_rcv() 做的第一件事情是再次执行 Netfilter 过滤，因为我们现在是从四层（L4）的 视角来处理 socker buffer。因此，这里会执行 Netfilter 中的任何四层规则（L4 rules ）。&lt;/p>
&lt;p>Step 12：ip_rcv_finish() 处理&lt;/p>
&lt;p>Netfilter 执行完成后，调用回调函数 ip_rcv_finish()。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781360-4b6933f0-7d4c-4d95-bc21-fe4217382495.jpeg" alt="">&lt;/p>
&lt;p>ip_rcv_finish() 立即调用 ip_routing() 对包进行路由判断。&lt;/p>
&lt;p>Step 13：ip_routing() 处理&lt;/p>
&lt;p>ip_routing() 对包进行路由判断，例如看它是否是在 lookback 设备上，是否能 路由出去（could egress），或者能否被路由，能否被 unmangle 到其他设备等等。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781356-e2454884-0357-4e17-8154-094f13cf535f.jpeg" alt="">&lt;/p>
&lt;p>在 Cilium 中，如果没有使用隧道模式（tunneling），那就会用到这里的路由功能。相比 隧道模式，路由模式会的 datapath 路径更短，因此性能更高。&lt;/p>
&lt;p>Step 14：目的是本机：ip_local_deliver() 处理&lt;/p>
&lt;p>根据路由判断的结果，如果包的目的端是本机，会调用 ip_local_deliver() 方法。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781519-8415d07c-c86d-47c4-aa3a-ce840006eab8.jpeg" alt="">&lt;/p>
&lt;p>ip_local_deliver() 会调用 xfrm4_policy()。&lt;/p>
&lt;p>Step 15：xfrm4_policy() 处理&lt;/p>
&lt;p>xfrm4_policy() 完成对包的封装、解封装、加解密等工作。例如，IPSec 就是在这里完成的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781375-3453d8c9-b62b-4ba8-9621-ffb3496bf618.jpeg" alt="">&lt;/p>
&lt;p>最后，根据四层协议的不同，ip_local_deliver() 会将最终的包送到 TCP 或 UDP 协议 栈。这里必须是这两种协议之一，否则设备会给源 IP 地址回一个 ICMP destination unreachable 消息。&lt;/p>
&lt;p>接下来我将拿 UDP 协议作为例子，因为 TCP 状态机太复杂了，不适合这里用于理解 datapath 和数据流。但不是说 TCP 不重要，Linux TCP 状态机还是非常值得好好学习的。&lt;/p>
&lt;p>L4（传输层，以 UDP 为例）&lt;/p>
&lt;p>Step 16：udp_rcv() 处理&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781393-aae801ea-ac79-4dad-bae2-ddbbd8f0ac19.jpeg" alt="">&lt;/p>
&lt;p>udp_rcv() 对包的合法性进行验证，检查 UDP 校验和。然后，再次将包送到 xfrm4_policy() 进行处理。&lt;/p>
&lt;p>Step 17：xfrm4_policy() 再次处理&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781393-80815a9b-7c61-4644-911e-bff8f08b31e3.jpeg" alt="">&lt;/p>
&lt;p>这里再次对包执行 transform policies 是因为，某些规则能指定具体的四层协议，所以只 有到了协议层之后才能执行这些策略。&lt;/p>
&lt;p>Step 18：将包放入 socket_receive_queue&lt;/p>
&lt;p>这一步会拿端口（port）查找相应的 socket，然后将 skb 放到一个名为 socket_receive_queue 的链表。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781401-cafb7408-4dc8-4338-9a75-5fef6714b75b.jpeg" alt="">&lt;/p>
&lt;p>Step 19：通知 socket 收数据：sk_data_ready()&lt;/p>
&lt;p>最后，udp_rcv() 调用 sk_data_ready() 方法，标记这个 socket 有数据待收。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781394-3ce9c54f-b38e-4c77-a8f6-7b2095b7c51d.jpeg" alt="">&lt;/p>
&lt;p>本质上，一个 socket 就是 Linux 中的一个文件描述符，这个描述符有一组相关的文件操 作抽象，例如 read、write 等等。&lt;/p>
&lt;p>网络栈下半部分小结&lt;/p>
&lt;p>以上 Step 1~19 就是 Linux 网络栈下半部分（bottom half of the network stack）的全部内容。&lt;/p>
&lt;p>接下来我们还会介绍几个内核函数，但它们都是与进程上下文相关的。&lt;/p>
&lt;p>L4 - User Space&lt;/p>
&lt;p>下图左边是一段 socket listening 程序，这里省略了错误检查，而且 epoll 本质上也 是不需要的，因为 UDP 的 recv 方法以及在帮我们 poll 了。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/mvtx1i/1616164781397-a531fb3e-8a78-4628-8e1a-5be07d06b5c6.jpeg" alt="">&lt;/p>
&lt;p>由于大家还是对 TCP 熟悉一些，因此在这里我假设这是一段 TCP 代码。事实上当我们调 用 recvmsg() 方法时，内核所做的事情就和上面这段代码差不多。对照右边的图：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>首先初始化一个 epoll 实例和一个 UDP socket，然后告诉 epoll 实例我们想 监听这个 socket 上的 receive 事件，然后等着事件到来。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当 socket buffer 收到数据时，其 wait queue 会被上一节的 sk_data_ready() 方法置位（标记）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>epoll 监听在 wait queue，因此 epoll 收到事件通知后，提取事件内容，返回给用户空间。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户空间程序调用 recv 方法，它接着调用 udp_recv_msg 方法，后者又会 调用 cgroup eBPF 程序 —— 这是本文出现的第三种 BPF 程序。Cilium 利用 cgroup eBPF 实现 socket level 负载均衡，这非常酷：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>一般的客户端负载均衡对客户端并不是透明的，即，客户端应用必须将负载均衡逻辑内置到应用里。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>有了 cgroup BPF，客户端根本感知不到负载均衡的存在。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>本文介绍的最后一种 BPF 程序是 sock_ops BPF，用于 socket level 整流（traffic shaping ），这对某些功能至关重要，例如客户端级别的限速（rate limiting）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后，我们有一个用户空间缓冲区，存放收到的数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>以上就是 Cilium 基于 eBPF 的内核收包之旅（traversing the kernel’s datapath）。太壮观了！&lt;/p>
&lt;ol start="7">
&lt;li>Kubernets、Cilium 和 Kernel：原子对象对应关系&lt;/li>
&lt;/ol>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kubernetes&lt;/td>
&lt;td>Cilium&lt;/td>
&lt;td>Kernel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Endpoint (includes Pods)&lt;/td>
&lt;td>Endpoint&lt;/td>
&lt;td>tc, cgroup socket BPF, sock_ops BPF, XDP&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Network Policy&lt;/td>
&lt;td>Cilium Network Policy&lt;/td>
&lt;td>XDP, tc, sock-ops&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Service (node ports, cluster ips, etc)&lt;/td>
&lt;td>Service&lt;/td>
&lt;td>XDP, tc&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Node&lt;/td>
&lt;td>Node&lt;/td>
&lt;td>ip-xfrm (for encryption), ip tables for initial decapsulation routing (if vxlan), veth-pair, ipvlan&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>以上就是 Kubernetes 的所有网络对象（the only artificial network objects）。什么意思？这就是 k8s CNI 所依赖的全部网络原语（network primitives）。例如，LoadBalancer 对象只是 ClusterIP 和 NodePort 的组合，而后二者都属于 Service 对象，所以他们并不 是一等对象。&lt;/p>
&lt;p>这张图非常有价值，但不幸的是，实际情况要比这里列出的更加复杂，因为 Cilium 本身的 实现是很复杂的。这有两个主要原因，我觉得值得拿出来讨论和体会：&lt;/p>
&lt;p>首先，内核 datapath 要远比我这里讲的复杂。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>前面只是非常简单地介绍了协议栈每个位置（Netfilter、iptables、eBPF、XDP）能执行的动作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这些位置提供的处理能力是不同的。例如&lt;/p>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>XDP 可能是能力最受限的，因为它只是设计用来做快速丢包（fast dropping）和 非本地重定向（non-local redirecting）；但另一方面，它又是最快的程序，因为 它在整个 datapath 的最前面，具备对整个 datapath 进行短路处理（short circuit the entire datapath）的能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>tc 和 iptables 程序能方便地 mangle 数据包，而不会对原来的转发流程产生显着影响。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>理解这些东西非常重要，因为这是 Cilium 乃至广义 datapath 里非常核心的东西。如 果遇到底层网络问题，或者需要做 Cilium/kernel 调优，那你必须要理解包的收发 / 转发 路径，有时你会发现包的某些路径非常反直觉。&lt;/p>
&lt;p>第二个原因是，eBPF 还非常新，某些最新特性只有在 5.x 内核中才有。尤其是 XDP BPF， 可能一个节点的内核版本支持，调度到另一台节点时，可能就不支持。&lt;/p>
&lt;p>略，视频见 油管[4]。&lt;/p>
&lt;p>参考资料&lt;/p>
&lt;p>[1]&lt;/p>
&lt;p>Understanding (and Troubleshooting) the eBPF Datapath in Cilium: &lt;a href="https://kccncna19.sched.com/event/Uae7/understanding-and-troubleshooting-the-ebpf-datapath-in-cilium-nathan-sweet-digitalocean">https://kccncna19.sched.com/event/Uae7/understanding-and-troubleshooting-the-ebpf-datapath-in-cilium-nathan-sweet-digitalocean&lt;/a>&lt;/p>
&lt;p>[2]&lt;/p>
&lt;p>“Metrics are money”: &lt;a href="https://kernel-recipes.org/en/2019/metrics-are-money/">https://kernel-recipes.org/en/2019/metrics-are-money/&lt;/a>&lt;/p>
&lt;p>[3]&lt;/p>
&lt;p>Cilium Network Topology and Traffic Path on AWS: &lt;a href="http://arthurchiao.art/blog/cilium-network-topology-on-aws/">http://arthurchiao.art/blog/cilium-network-topology-on-aws/&lt;/a>&lt;/p>
&lt;p>[4]&lt;/p>
&lt;p>油管: &lt;a href="https://www.youtube.com/watch?v=Kmm8Hl57WDU">https://www.youtube.com/watch?v=Kmm8Hl57WDU&lt;/a>&lt;/p></description></item><item><title>Docs: 10.2.知道硬盘很慢，但没想到比 CPU Cache 慢 10000000 倍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.2.%E7%9F%A5%E9%81%93%E7%A1%AC%E7%9B%98%E5%BE%88%E6%85%A2%E4%BD%86%E6%B2%A1%E6%83%B3%E5%88%B0%E6%AF%94-cpu-cache-%E6%85%A2-10000000-%E5%80%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.2.%E7%9F%A5%E9%81%93%E7%A1%AC%E7%9B%98%E5%BE%88%E6%85%A2%E4%BD%86%E6%B2%A1%E6%83%B3%E5%88%B0%E6%AF%94-cpu-cache-%E6%85%A2-10000000-%E5%80%8D/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>&lt;strong>天啦噜！知道硬盘很慢，但没想到比 CPU Cache 慢 10000000 倍&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976182-90f7bdd1-15f5-4e79-9992-a3d41847a5da.png" alt="">&lt;/p>
&lt;p>&lt;strong>前言&lt;/strong>&lt;/p>
&lt;p>大家如果想自己组装电脑的话，肯定需要购买一个 CPU，但是存储器方面的设备，分类比较多，那我们肯定不能只买一种存储器，比如你除了要买内存，还要买硬盘，而针对硬盘我们还可以选择是固态硬盘还是机械硬盘。&lt;/p>
&lt;p>相信大家都知道内存和硬盘都属于计算机的存储设备，断电后内存的数据是会丢失的，而硬盘则不会，因为硬盘是持久化存储设备，同时也是一个 I/O 设备。&lt;/p>
&lt;p>但其实 CPU 内部也有存储数据的组件，这个应该比较少人注意到，比如&lt;strong>寄存器、CPU L1/L2/L3 Cache&lt;/strong> 也都是属于存储设备，只不过它们能存储的数据非常小，但是它们因为靠近 CPU 核心，所以访问速度都非常快，快过硬盘好几个数量级别。&lt;/p>
&lt;p>问题来了，&lt;strong>那机械硬盘、固态硬盘、内存这三个存储器，到底和 CPU L1 Cache 相比速度差多少倍呢？&lt;/strong>&lt;/p>
&lt;p>在回答这个问题之前，我们先来看看「&lt;strong>存储器的层次结构&lt;/strong>」，好让我们对存储器设备有一个整体的认识。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976178-e4b64e37-9cd2-4db1-a4e7-5cb4fd1e498f.png" alt="">&lt;/p>
&lt;p>&lt;strong>正文&lt;/strong>&lt;/p>
&lt;p>存储器的层次结构&lt;/p>
&lt;p>我们想象中一个场景，大学期末准备考试了，你前去图书馆临时抱佛脚。那么，在看书的时候，我们的大脑会思考问题，也会记忆知识点，另外我们通常也会把常用的书放在自己的桌子上，当我们要找一本不常用的书，则会去图书馆的书架找。&lt;/p>
&lt;p>就是这么一个小小的场景，已经把计算机的存储结构基本都涵盖了。&lt;/p>
&lt;p>我们可以把 CPU 比喻成我们的大脑，大脑正在思考的东西，就好比 CPU 中的&lt;strong>寄存器&lt;/strong>，处理速度是最快的，但是能存储的数据也是最少的，毕竟我们也不能一下同时思考太多的事情，除非你练过。&lt;/p>
&lt;p>我们大脑中的记忆，就好比 &lt;strong>CPU Cache&lt;/strong>，中文称为 CPU 高速缓存，处理速度相比寄存器慢了一点，但是能存储的数据也稍微多了一些。&lt;/p>
&lt;p>CPU Cache 通常会分为 &lt;strong>L1、L2、L3 三层&lt;/strong>，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1 是距离 CPU 最近的，因此它比 L2、L3 的读写速度都快、存储空间都小。我们大脑中短期记忆，就好比 L1 Cache，而长期记忆就好比 L2/L3 Cache。&lt;/p>
&lt;p>寄存器和 CPU Cache 都是在 CPU 内部，跟 CPU 挨着很近，因此它们的读写速度都相当的快，但是能存储的数据很少，毕竟 CPU 就这么丁点大。&lt;/p>
&lt;p>知道 CPU 内部的存储器的层次分布，我们放眼看看 CPU 外部的存储器。&lt;/p>
&lt;p>当我们大脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌子上的书，就好比&lt;strong>内存&lt;/strong>，我们虽然可以一伸手就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好比&lt;strong>硬盘&lt;/strong>，能存储的数据非常大，但是读写速度相比内存差好几个数量级，更别说跟寄存器的差距了。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976201-6b166920-3f84-42b1-9e26-d7647ca04812.png" alt="">&lt;/p>
&lt;p>我们从图书馆书架取书，把书放到桌子上，再阅读书，我们大脑就会记忆知识点，然后再经过大脑思考，这一系列过程相当于，数据从硬盘加载到内存，再从内存加载到 CPU 的寄存器和 Cache 中，然后再通过 CPU 进行处理和计算。&lt;/p>
&lt;p>&lt;strong>对于存储器，它的速度越快、能耗会越高、而且材料的成本也是越贵的，以至于速度快的存储器的容量都比较小。&lt;/strong>&lt;/p>
&lt;p>CPU 里的寄存器和 Cache，是整个计算机存储器中价格最贵的，虽然存储空间很小，但是读写速度是极快的，而相对比较便宜的内存和硬盘，速度肯定比不上 CPU 内部的存储器，但是能弥补存储空间的不足。&lt;/p>
&lt;p>存储器通常可以分为这么几个级别：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976207-7bc9e5b2-f04a-4fbe-a0d8-8b6226c3f760.png" alt="">&lt;/p>
&lt;p>寄存器；&lt;/p>
&lt;p>CPU Cache；&lt;/p>
&lt;p>L1-Cache；&lt;/p>
&lt;p>L2-Cache；&lt;/p>
&lt;p>L3-Cahce；&lt;/p>
&lt;p>内存；&lt;/p>
&lt;p>SSD/HDD 硬盘&lt;/p>
&lt;p>&lt;strong>寄存器&lt;/strong>&lt;/p>
&lt;p>最靠近 CPU 的控制单元和逻辑计算单元的存储器，就是寄存器了，它使用的材料速度也是最快的，因此价格也是最贵的，那么数量不能很多。&lt;/p>
&lt;p>存储器的数量通常在几十到几百之间，每个寄存器可以用来存储一定的字节（byte）的数据。比如：&lt;/p>
&lt;p>32 位 CPU 中大多数寄存器可以存储 4 个字节；&lt;/p>
&lt;p>64 位 CPU 中大多数寄存器可以存储 8 个字节。&lt;/p>
&lt;p>寄存器的访问速度非常快，一般要求在半个 CPU 时钟周期内完成读写，CPU 时钟周期跟 CPU 主频息息相关，比如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。&lt;/p>
&lt;p>CPU 处理一条指令的时候，除了读写寄存器，还需要解码指令、控制指令执行和计算。如果寄存器的速度太慢，则会拉长指令的处理周期，从而给用户的感觉，就是电脑「很慢」。&lt;/p>
&lt;p>&lt;strong>CPU Cache&lt;/strong>&lt;/p>
&lt;p>CPU Cache 用的是一种叫 &lt;strong>SRAM（***&lt;strong>Static Random-Access&lt;/strong>*&lt;/strong> Memory，静态随机存储器）** 的芯片。&lt;/p>
&lt;p>SRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，而一旦断电，数据就会丢失了。&lt;/p>
&lt;p>在 SRAM 里面，一个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不高，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度非常快。&lt;/p>
&lt;p>CPU 的高速缓存，通常可以分为 L1、L2、L3 这样的三层高速缓存，也称为一级缓存、二次缓存、三次缓存。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976196-0ddbce05-2f7e-4a12-b171-42621ad3551a.png" alt="">&lt;/p>
&lt;p>&lt;strong>L1 高速缓存&lt;/strong>&lt;/p>
&lt;p>L1 高速缓存的访问速度几乎和寄存器一样快，通常只需要 2~4 个时钟周期，而大小在几十 KB 到几百 KB 不等。&lt;/p>
&lt;p>每个 CPU 核心都有一块属于自己的 L1 高速缓存，指令和数据在 L1 是分开存放的，所以 L1 高速缓存通常分成&lt;strong>指令缓存&lt;/strong>和&lt;strong>数据缓存&lt;/strong>。&lt;/p>
&lt;p>在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L1 Cache 「数据」缓存的容量大小：&lt;/p>
&lt;p>$ cat /sys/devices/system/cpu/cpu0/cache/index0/size&lt;/p>
&lt;p>32K&lt;/p>
&lt;p>而查看 L1 Cache 「指令」缓存的容量大小，则是：&lt;/p>
&lt;p>$ cat /sys/devices/system/cpu/cpu0/cache/index1/size&lt;/p>
&lt;p>32K&lt;/p>
&lt;p>&lt;strong>L2 高速缓存&lt;/strong>&lt;/p>
&lt;p>L2 高速缓存同样每个 CPU 核心都有，但是 L2 高速缓存位置比 L1 高速缓存距离 CPU 核心 更远，它大小比 L1 高速缓存更大，CPU 型号不同大小也就不同，通常大小在几百 KB 到几 MB 不等，访问速度则更慢，速度在 10~20 个时钟周期。&lt;/p>
&lt;p>在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L2 Cache 的容量大小：&lt;/p>
&lt;p>$ cat /sys/devices/system/cpu/cpu0/cache/index2/size&lt;/p>
&lt;p>256K&lt;/p>
&lt;p>&lt;strong>L3 高速缓存&lt;/strong>&lt;/p>
&lt;p>L3 高速缓存通常是多个 CPU 核心共用的，位置比 L2 高速缓存距离 CPU 核心 更远，大小也会更大些，通常大小在几 MB 到几十 MB 不等，具体值根据 CPU 型号而定。&lt;/p>
&lt;p>访问速度相对也比较慢一些，访问速度在 20~60 个时钟周期。&lt;/p>
&lt;p>在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L3 Cache 的容量大小：&lt;/p>
&lt;p>$ cat /sys/devices/system/cpu/cpu0/cache/index3/size&lt;/p>
&lt;p>3072K&lt;/p>
&lt;p>&lt;strong>内存&lt;/strong>&lt;/p>
&lt;p>内存用的芯片和 CPU Cache 有所不同，它使用的是一种叫作 &lt;strong>DRAM （***&lt;strong>Dynamic Random Access Memory&lt;/strong>*&lt;/strong>，动态随机存取存储器）** 的芯片。&lt;/p>
&lt;p>相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。&lt;/p>
&lt;p>DRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。&lt;/p>
&lt;p>DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问的速度会更慢，内存速度大概在 200~300 个 时钟周期之间。&lt;/p>
&lt;p>&lt;strong>SSD/HDD 硬盘&lt;/strong>&lt;/p>
&lt;p>SSD（&lt;em>Solid-state disk&lt;/em>） 就是我们常说的固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。内存的读写速度比 SSD 大概快 10~1000 倍。&lt;/p>
&lt;p>当然，还有一款传统的硬盘，也就是机械硬盘（&lt;em>Hard Disk Drive, HDD&lt;/em>），它是通过物理读写的方式来访问数据的，因此它访问速度是非常慢的，它的速度比内存慢 10W 倍左右。&lt;/p>
&lt;p>由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。&lt;/p>
&lt;p>存储器的层次关系&lt;/p>
&lt;p>现代的一台计算机，都用上了 CPU Cahce、内存、到 SSD 或 HDD 硬盘这些存储器设备了。&lt;/p>
&lt;p>其中，存储空间越大的存储器设备，其访问速度越慢，所需成本也相对越少。&lt;/p>
&lt;p>CPU 并不会直接和每一种存储器设备直接打交道，而是每一种存储器设备只和它相邻的存储器设备打交道。&lt;/p>
&lt;p>比如，CPU Cache 的数据是从内存加载过来的，写回数据的时候也只写回到内存，CPU Cache 不会直接把数据写到硬盘，也不会直接从硬盘加载数据，而是先加载到内存，再从内存加载到 CPU Cache 中。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976245-22932169-bec2-42f5-8712-f60b0d797733.png" alt="">&lt;/p>
&lt;p>所以，&lt;strong>每个存储器只和相邻的一层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更高，也正因为成本太高，所以 CPU 内部的寄存器、L1L2L3 Cache 只好用较小的容量，相反内存、硬盘则可用更大的容量，这就我们今天所说的存储器层次结构&lt;/strong>。&lt;/p>
&lt;p>另外，当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即可，如果寄存器没有这个数据，CPU 就会查询 L1 高速缓存，如果 L1 没有，则查询 L2 高速缓存，L2 还是没有的话就查询 L3 高速缓存，L3 依然没有的话，才去内存中取数据。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976227-007bd129-b81d-4e1f-906c-234362160317.png" alt="">&lt;/p>
&lt;p>所以，存储层次结构也形成了&lt;strong>缓存&lt;/strong>的体系。&lt;/p>
&lt;p>存储器之间的实际价格和性能差距&lt;/p>
&lt;p>前面我们知道了，速度越快的存储器，造价成本往往也越高，那我们就以实际的数据来看看，不同层级的存储器之间的性能和价格差异。&lt;/p>
&lt;p>下面这张表格是不同层级的存储器之间的成本对比图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ggyell/1616167976218-0076c2a7-a83f-4f00-a6ac-29d76746d7ad.png" alt="">&lt;/p>
&lt;p>你可以看到 L1 Cache 的访问延时是 1 纳秒，而内存已经是 100 纳秒了，相比 L1 Cache 速度慢了 100 倍。另外，机械硬盘的访问延时更是高达 10 毫秒，相比 L1 Cache 速度慢了 10000000 倍，差了好几个数量级别。&lt;/p>
&lt;p>在价格上，每生产 MB 大小的 L1 Cache 相比内存贵了 466 倍，相比机械硬盘那更是贵了 175000 倍。&lt;/p>
&lt;p>我在某东逛了下各个存储器设备的零售价，8G 内存 + 1T 机械硬盘 + 256G 固态硬盘的总价格，都不及一块 Intle i5-10400 的 CPU 的价格，这款 CPU 的高速缓存的总大小也就十多 MB。&lt;/p>
&lt;p>总结&lt;/p>
&lt;p>各种存储器之间的关系，可以用我们在图书馆学习这个场景来理解。&lt;/p>
&lt;p>CPU 可以比喻成我们的大脑，我们当前正在思考和处理的知识的过程，就好比 CPU 中的&lt;strong>寄存器&lt;/strong>处理数据的过程，速度极快，但是容量很小。而 CPU 中的 &lt;strong>L1-L3 Cache&lt;/strong> 好比我们大脑中的短期记忆和长期记忆，需要小小花费点时间来调取数据并处理。&lt;/p>
&lt;p>我们面前的桌子就相当于&lt;strong>内存&lt;/strong>，能放下更多的书（数据），但是找起来和看起来就要花费一些时间，相比 CPU Cache 慢不少。而图书馆的书架相当于&lt;strong>硬盘&lt;/strong>，能放下比内存更多的数据，但找起来就更费时间了，可以说是最慢的存储器设备了。&lt;/p>
&lt;p>从 寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，访问速度越来越慢，存储容量越来越大，价格也越来越便宜，而且每个存储器只和相邻的一层存储器设备打交道，于是这样就形成了存储器的层次结构。&lt;/p>
&lt;p>再来回答，开头的问题：那机械硬盘、固态硬盘、内存这三个存储器，到底和 CPU L1 Cache 相比速度差多少倍呢？&lt;/p>
&lt;p>CPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 &lt;strong>CPU L1 Cache 比内存快 100 倍左右&lt;/strong>。&lt;/p>
&lt;p>SSD 随机访问延时是 150 微妙，所以 &lt;strong>CPU L1 Cache 比 SSD 快 150000 倍左右&lt;/strong>。&lt;/p>
&lt;p>最慢的机械硬盘随机访问延时已经高达 10 毫秒，我们来看看机械硬盘到底有多「龟速」：&lt;/p>
&lt;p>&lt;strong>SSD 比机械硬盘快 70 倍左右；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>内存比机械硬盘快 100000 倍左右，即 10W 倍；&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CPU L1 Cache 比机械硬盘快 10000000 倍左右，即 1000W 倍；&lt;/strong>&lt;/p>
&lt;p>我们把上述的时间比例差异放大后，就能非常直观感受到它们的性能差异了。如果 CPU 访问 L1 Cache 的缓存时间是 1 秒，那访问内存则需要大约 2 分钟，随机访问 SSD 里的数据则需要 1.7 天，访问机械硬盘那更久，长达近 4 个月。&lt;/p>
&lt;p>可以发现，不同的存储器之间性能差距很大，构造存储器分级很有意义，分级的目的是要构造&lt;strong>缓存&lt;/strong>体系。&lt;/p>
&lt;p>&lt;strong>絮叨&lt;/strong>&lt;/p>
&lt;p>新的&lt;strong>技术交流群&lt;/strong>已经慢慢人多起来了，群里的大牛真的多，大家交流都很踊跃，也有很多热心分享和回答问题的小伙伴，是你交朋友好地方，更是你上班划水的好入口。&lt;/p>
&lt;p>准备入冬了，一起来抱团取暖吧，加群方式很简单，只需要加我的微信二维码，备注「&lt;strong>加群&lt;/strong>」即可。&lt;/p>
&lt;p>&lt;em>哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎分享给你的朋友，也给小林点个「在看」，这对小林非常重要，谢谢你们，给各位小姐姐小哥哥们抱拳了，我们下次见！&lt;/em>&lt;/p>
&lt;p>&lt;strong>推荐阅读&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CPU 执行程序的秘密，藏在了这 15 张图里&lt;/strong>&lt;/p>
&lt;p>&lt;strong>原来 8 张图，就可以搞懂「零拷贝」了&lt;/strong>&lt;/p>
&lt;p>喜欢此内容的人还喜欢&lt;/p></description></item><item><title>Docs: 10.3.如何写出让 CPU 跑得更快的代码？</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.3.%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E8%AE%A9-cpu-%E8%B7%91%E5%BE%97%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BB%A3%E7%A0%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.3.%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E8%AE%A9-cpu-%E8%B7%91%E5%BE%97%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BB%A3%E7%A0%81/</guid><description>
&lt;p>&lt;strong>面试官：如何写出让 CPU 跑得更快的代码？&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966333-fcc2cbcc-df13-4b74-904f-8fbdbc6aaa18.png" alt="">&lt;/p>
&lt;p>&lt;strong>前言&lt;/strong>&lt;/p>
&lt;p>代码都是由 CPU 跑起来的，我们代码写的好与坏就决定了 CPU 的执行效率，特别是在编写计算密集型的程序，更要注重 CPU 的执行效率，否则将会大大影响系统性能。&lt;/p>
&lt;p>CPU 内部嵌入了 CPU Cache（高速缓存），它的存储容量很小，但是离 CPU 核心很近，所以缓存的读写速度是极快的，那么如果 CPU 运算时，直接从 CPU Cache 读取数据，而不是从内存的话，运算速度就会很快。&lt;/p>
&lt;p>但是，大多数人不知道 CPU Cache 的运行机制，以至于不知道如何才能够写出能够配合 CPU Cache 工作机制的代码，一旦你掌握了它，你写代码的时候，就有新的优化思路了。&lt;/p>
&lt;p>那么，接下来我们就来看看，CPU Cache 到底是什么样的，是如何工作的呢，又该写出让 CPU 执行更快的代码呢？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966344-839ec392-0df1-430a-be2b-d275b1dedefe.png" alt="">&lt;/p>
&lt;p>&lt;strong>正文&lt;/strong>&lt;/p>
&lt;p>CPU Cache 有多快？&lt;/p>
&lt;p>你可能会好奇为什么有了内存，还需要 CPU Cache？根据摩尔定律，CPU 的访问速度每 18 个月就会翻倍，相当于每年增长 60% 左右，内存的速度当然也会不断增长，但是增长的速度远小于 CPU，平均每年只增长 7% 左右。于是，CPU 与内存的访问性能的差距不断拉大。&lt;/p>
&lt;p>到现在，一次内存访问所需时间是 200~300 多个时钟周期，这意味着 CPU 和内存的访问速度已经相差 200~300 多倍了。&lt;/p>
&lt;p>为了弥补 CPU 与内存两者之间的性能差异，就在 CPU 内部引入了 CPU Cache，也称高速缓存。&lt;/p>
&lt;p>CPU Cache 通常分为大小不等的三级缓存，分别是 &lt;strong>L1 Cache、L2 Cache 和 L3 Cache&lt;/strong>。&lt;/p>
&lt;p>由于 CPU Cache 所使用的材料是 SRAM，价格比内存使用的 DRAM 高出很多，在当今每生产 1 MB 大小的 CPU Cache 需要 7 美金的成本，而内存只需要 0.015 美金的成本，成本方面相差了 466 倍，所以 CPU Cache 不像内存那样动辄以 GB 计算，它的大小是以 KB 或 MB 来计算的。&lt;/p>
&lt;p>在 Linux 系统中，我们可以使用下图的方式来查看各级 CPU Cache 的大小，比如我这手上这台服务器，离 CPU 核心最近的 L1 Cache 是 32KB，其次是 L2 Cache 是 256KB，最大的 L3 Cache 则是 3MB。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966311-20378e45-3623-4562-808a-9f94352ee96f.png" alt="">&lt;/p>
&lt;p>其中，&lt;strong>L1 Cache 通常会分为「数据缓存」和「指令缓存」&lt;/strong>，这意味着数据和指令在 L1 Cache 这一层是分开缓存的，上图中的 index0 也就是数据缓存，而 index1 则是指令缓存，它两的大小通常是一样的。&lt;/p>
&lt;p>另外，你也会注意到，L3 Cache 比 L1 Cache 和 L2 Cache 大很多，这是因为 &lt;strong>L1 Cache 和 L2 Cache 都是每个 CPU 核心独有的，而 L3 Cache 是多个 CPU 核心共享的。&lt;/strong>&lt;/p>
&lt;p>程序执行时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核心独有的 L2 Cache，最后进入到最快的 L1 Cache，之后才会被 CPU 读取。它们之间的层级关系，如下图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966328-3691d0d5-5c9f-462c-b524-a5c041404c0f.png" alt="">&lt;/p>
&lt;p>越靠近 CPU 核心的缓存其访问速度越快，CPU 访问 L1 Cache 只需要 2~4 个时钟周期，访问 L2 Cache 大约 10~20 个时钟周期，访问 L3 Cache 大约 20~60 个时钟周期，而访问内存速度大概在 200~300 个 时钟周期之间。如下表格：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966337-070a39a4-d3c6-4d56-8f34-79fee79119d6.png" alt="">&lt;/p>
&lt;p>&lt;strong>所以，CPU 从 L1 Cache 读取数据的速度，相比从内存读取的速度，会快 100 多倍。&lt;/strong>&lt;/p>
&lt;p>CPU Cache 的数据结构和读取过程是什么样的？&lt;/p>
&lt;p>CPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 &lt;strong>Cache Line（缓存块）&lt;/strong>。&lt;/p>
&lt;p>你可以在你的 Linux 系统，用下面这种方式来查看 CPU 的 Cache Line，你可以看我服务器的 L1 Cache Line 大小是 64 字节，也就意味着 &lt;strong>L1 Cache 一次载入数据的大小是 64 字节&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966325-a8749b7a-e2e9-4dff-aa68-5886817385a0.png" alt="">&lt;/p>
&lt;p>比如，有一个 int array[100] 的数组，当载入 array[0] 时，由于这个数组元素的大小在内存只占 4 字节，不足 64 字节，CPU 就会&lt;strong>顺序加载&lt;/strong>数组元素到 array[15]，意味着 array[0]~array[15] 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，而不用再从内存中读取，大大提高了 CPU 读取数据的性能。&lt;/p>
&lt;p>事实上，CPU 读取数据的时候，无论数据是否存放到 Cache 中，CPU 都是先访问 Cache，只有当 Cache 中找不到数据时，才会去访问内存，并把内存中的数据读入到 Cache 中，CPU 再从 CPU Cache 读取数据。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966332-609f98be-defe-4e78-a338-f38e9a9df854.png" alt="">&lt;/p>
&lt;p>这样的访问机制，跟我们使用「内存作为硬盘的缓存」的逻辑是一样的，如果内存有缓存的数据，则直接返回，否则要访问龟速一般的硬盘。&lt;/p>
&lt;p>那 CPU 怎么知道要访问的内存数据，是否在 Cache 里？如果在的话，如何找到 Cache 对应的数据呢？我们从最简单、基础的&lt;strong>直接映射 Cache（***&lt;strong>Direct Mapped Cache&lt;/strong>*&lt;/strong>）** 说起，来看看整个 CPU Cache 的数据结构和访问逻辑。&lt;/p>
&lt;p>前面，我们提到 CPU 访问内存数据时，是一小块一小块数据读取的，具体这一小块数据的大小，取决于 coherency_line_size 的值，一般 64 字节。在内存中，这一块的数据我们称为&lt;strong>内存块（***&lt;strong>Bock&lt;/strong>*&lt;/strong>）**，读取的时候我们要拿到数据所在内存块的地址。&lt;/p>
&lt;p>对于直接映射 Cache 采用的策略，就是把内存块的地址始终「映射」在一个 CPU Line（缓存块） 的地址，至于映射关系实现方式，则是使用「取模运算」，取模运算的结果就是内存块地址对应的 CPU Line（缓存块） 的地址。&lt;/p>
&lt;p>举个例子，内存共被划分为 32 个内存块，CPU Cache 共有 8 个 CPU Line，假设 CPU 想要访问第 15 号内存块，如果 15 号内存块中的数据已经缓存在 CPU Line 中的话，则是一定映射在 7 号 CPU Line 中，因为 15 % 8 的值是 7。&lt;/p>
&lt;p>机智的你肯定发现了，使用取模方式映射的话，就会出现多个内存块对应同一个 CPU Line，比如上面的例子，除了 15 号内存块是映射在 7 号 CPU Line 中，还有 7 号、23 号、31 号内存块都是映射到 7 号 CPU Line 中。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966355-998adf7a-4dd3-405d-a75c-c7d501add3a8.png" alt="">&lt;/p>
&lt;p>因此，为了区别不同的内存块，在对应的 CPU Line 中我们还会存储一个&lt;strong>组标记（Tag）&lt;/strong>。这个组标记会记录当前 CPU Line 中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。&lt;/p>
&lt;p>除了组标记信息外，CPU Line 还有两个信息：&lt;/p>
&lt;p>一个是，从内存加载过来的实际存放&lt;strong>数据（***&lt;strong>Data&lt;/strong>*&lt;/strong>）**。&lt;/p>
&lt;p>另一个是，&lt;strong>有效位（***&lt;strong>Valid bit&lt;/strong>*&lt;/strong>）**，它是用来标记对应的 CPU Line 中的数据是否是有效的，如果有效位是 0，无论 CPU Line 中是否有数据，CPU 都会直接访问内存，重新加载数据。&lt;/p>
&lt;p>CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个&lt;strong>字（***&lt;strong>Word&lt;/strong>*&lt;/strong>）&lt;strong>。那怎么在对应的 CPU Line 中数据块中找到所需的字呢？答案是，需要一个&lt;/strong>偏移量（Offset）**。&lt;/p>
&lt;p>因此，一个内存的访问地址，包括&lt;strong>组标记、CPU Line 索引、偏移量&lt;/strong>这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。而对于 CPU Cache 里的数据结构，则是由&lt;strong>索引 + 有效位 + 组标记 + 数据块&lt;/strong>组成。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966336-8de340f2-1dc3-43b2-bab4-fe709883899e.png" alt="">&lt;/p>
&lt;p>如果内存中的数据已经在 CPU Cahe 中了，那 CPU 访问一个内存地址的时候，会经历这 4 个步骤：&lt;/p>
&lt;p>根据内存地址中索引信息，计算在 CPU Cahe 中的索引，也就是找出对应的 CPU Line 的地址；&lt;/p>
&lt;p>找到对应 CPU Line 后，判断 CPU Line 中的有效位，确认 CPU Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；&lt;/p>
&lt;p>对比内存地址中组标记和 CPU Line 中的组标记，确认 CPU Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；&lt;/p>
&lt;p>根据内存地址中偏移量信息，从 CPU Line 的数据块中，读取对应的字。&lt;/p>
&lt;p>到这里，相信你对直接映射 Cache 有了一定认识，但其实除了直接映射 Cache 之外，还有其他通过内存地址找到 CPU Cache 中的数据的策略，比如全相连 Cache （&lt;em>Fully Associative Cache&lt;/em>）、组相连 Cache （&lt;em>Set Associative Cache&lt;/em>）等，这几种策策略的数据结构都比较相似，我们理解流直接映射 Cache 的工作方式，其他的策略如果你有兴趣去看，相信很快就能理解的了。&lt;/p>
&lt;p>如何写出让 CPU 跑得更快的代码？&lt;/p>
&lt;p>我们知道 CPU 访问内存的速度，比访问 CPU Cache 的速度慢了 100 多倍，所以如果 CPU 所要操作的数据在 CPU Cache 中的话，这样将会带来很大的性能提升。访问的数据在 CPU Cache 中的话，意味着&lt;strong>缓存命中&lt;/strong>，缓存命中率越高的话，代码的性能就会越好，CPU 也就跑的越快。&lt;/p>
&lt;p>于是，「如何写出让 CPU 跑得更快的代码？」这个问题，可以改成「如何写出 CPU 缓存命中率高的代码？」。&lt;/p>
&lt;p>在前面我也提到， L1 Cache 通常分为「数据缓存」和「指令缓存」，这是因为 CPU 会别处理数据和指令，比如 1+1=2 这个运算，+ 就是指令，会被放在「指令缓存」中，而输入数字 1 则会被放在「数据缓存」里。&lt;/p>
&lt;p>因此，&lt;strong>我们要分开来看「数据缓存」和「指令缓存」的缓存命中率&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>如何提升数据缓存的命中率？&lt;/strong>&lt;/p>
&lt;p>假设要遍历二维数组，有以下两种形式，虽然代码执行结果是一样，但你觉得哪种形式效率最高呢？为什么高呢？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966322-4154dab1-01c4-4529-a35c-c2fc0b58d0c3.png" alt="">&lt;/p>
&lt;p>经过测试，形式一 array[i][j] 执行时间比形式二 array[j][i] 快好几倍。&lt;/p>
&lt;p>之所以有这么大的差距，是因为二维数组 array 所占用的内存是连续的，比如长度 N 的指是 2 的话，那么内存中的数组元素的布局顺序是这样的：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966317-b57a3a48-955f-40b0-a349-eaf10221bc77.png" alt="">&lt;/p>
&lt;p>形式一用 array[i][j] 访问数组元素的顺序，正是和内存中数组元素存放的顺序一致。当 CPU 访问 array[0][0] 时，由于该数据不在 Cache 中，于是会「顺序」把跟随其后的 3 个元素从内存中加载到 CPU Cache，这样当 CPU 访问后面的 3 个数组元素时，就能在 CPU Cache 中成功地找到数据，这意味着缓存命中率很高，缓存命中的数据不需要访问内存，这便大大提高了代码的性能。&lt;/p>
&lt;p>而如果用形式二的 array[j][i] 来访问，则访问的顺序就是：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966343-b1bd88fb-ea52-4dce-b579-890397fa5469.png" alt="">&lt;/p>
&lt;p>你可以看到，访问的方式跳跃式的，而不是顺序的，那么如果 N 的数值很大，那么操作 array[j][i] 时，是没办法把 array[j+1][i] 也读入到 CPU Cache 中的，既然 array[j+1][i] 没有读取到 CPU Cache，那么就需要从内存读取该数据元素了。很明显，这种不连续性、跳跃式访问数据元素的方式，可能不能充分利用到了 CPU Cache 的特性，从而代码的性能不高。&lt;/p>
&lt;p>那访问 array[0][0] 元素时，CPU 具体会一次从内存中加载多少元素到 CPU Cache 呢？这个问题，在前面我们也提到过，这跟 CPU Cache Line 有关，它表示 &lt;strong>CPU Cache 一次性能加载数据的大小&lt;/strong>，可以在 Linux 里通过 coherency_line_size 配置查看 它的大小，通常是 64 个字节。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966332-54c661d9-1e92-4842-ad7a-bbb36d017c1b.png" alt="">&lt;/p>
&lt;p>也就是说，当 CPU 访问内存数据时，如果数据不在 CPU Cache 中，则会一次性会连续加载 64 字节大小的数据到 CPU Cache，那么当访问 array[0][0] 时，由于该元素不足 64 字节，于是就会往后&lt;strong>顺序&lt;/strong>读取 array[0][0]~array[0][15] 到 CPU Cache 中。顺序访问的 array[i][j] 因为利用了这一特点，所以就会比跳跃式访问的 array[j][i] 要快。&lt;/p>
&lt;p>&lt;strong>因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升，&lt;/strong>&lt;/p>
&lt;p>&lt;strong>如何提升指令缓存的命中率？&lt;/strong>&lt;/p>
&lt;p>提升数据的缓存命中率的方式，是按照内存布局顺序访问，那针对指令的缓存该如何提升呢？&lt;/p>
&lt;p>我们以一个例子来看看，有一个元素为 0 到 100 之间随机数字组成的一维数组：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966335-6aaaed0e-683e-430b-9353-77fd5e6c8faf.png" alt="">&lt;/p>
&lt;p>接下来，对这个数组做两个操作：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966340-751d6efc-c848-46df-86e1-e510e00bd386.png" alt="">&lt;/p>
&lt;p>第一个操作，循环遍历数组，把小于 50 的数组元素置为 0；&lt;/p>
&lt;p>第二个操作，将数组排序；&lt;/p>
&lt;p>那么问题来了，你觉得先遍历再排序速度快，还是先排序再遍历速度快呢？&lt;/p>
&lt;p>在回答这个问题之前，我们先了解 CPU 的&lt;strong>分支预测器&lt;/strong>。对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，&lt;strong>如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快&lt;/strong>。&lt;/p>
&lt;p>当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。&lt;/p>
&lt;p>因此，先排序再遍历速度会更快，这是因为排序之后，数字是从小到大的，那么前几次循环命中 if &amp;lt; 50 的次数会比较多，于是分支预测就会缓存 if 里的 array[i] = 0 指令到 Cache 中，后续 CPU 执行该指令就只需要从 Cache 读取就好了。&lt;/p>
&lt;p>如果你肯定代码中的 if 中的表达式判断为 true 的概率比较高，我们可以使用显示分支预测工具，比如在 C/C++ 语言中编译器提供了 likely 和 unlikely 这两种宏，如果 if 条件为 ture 的概率大，则可以用 likely 宏把 if 里的表达式包裹起来，反之用 unlikely 宏。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966345-b3b92b9e-0575-44ac-8224-7c1e4f638267.png" alt="">&lt;/p>
&lt;p>实际上，CPU 自身的动态分支预测已经是比较准的了，所以只有当非常确信 CPU 预测的不准，且能够知道实际的概率情况时，才建议使用这两种宏。&lt;/p>
&lt;p>&lt;strong>如果提升多核 CPU 的缓存命中率？&lt;/strong>&lt;/p>
&lt;p>在单核 CPU，虽然只能执行一个进程，但是操作系统给每个进程分配了一个时间片，时间片用完了，就调度下一个进程，于是各个进程就按时间片交替地占用 CPU，从宏观上看起来各个进程同时在执行。&lt;/p>
&lt;p>而现代 CPU 都是多核心的，进程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的，&lt;strong>如果一个进程在不同核心来回切换，各个核心的缓存命中率就会受到影响&lt;/strong>，相反如果进程都在同一个核心上执行，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以减少访问 内存的频率。&lt;/p>
&lt;p>当有多个同时执行「计算密集型」的线程，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把&lt;strong>线程绑定在某一个 CPU 核心上&lt;/strong>，这样性能可以得到非常可观的提升。&lt;/p>
&lt;p>在 Linux 上提供了 sched_setaffinity 方法，来实现将线程绑定到某个 CPU 核心这一功能。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966341-95bc957c-656a-4e0b-af3f-52b13f6e6bd0.png" alt="">&lt;/p>
&lt;p>总结&lt;/p>
&lt;p>由于随着计算机技术的发展，CPU 与 内存的访问速度相差越来越多，如今差距已经高达好几百倍了，所以 CPU 内部嵌入了 CPU Cache 组件，作为内存与 CPU 之间的缓存层，CPU Cache 由于离 CPU 核心很近，所以访问速度也是非常快的，但由于所需材料成本比较高，它不像内存动辄几个 GB 大小，而是仅有几十 KB 到 MB 大小。&lt;/p>
&lt;p>当 CPU 访问数据的时候，先是访问 CPU Cache，如果缓存命中的话，则直接返回数据，就不用每次都从内存读取速度了。因此，缓存命中率越高，代码的性能越好。&lt;/p>
&lt;p>但需要注意的是，当 CPU 访问数据时，如果 CPU Cache 没有缓存该数据，则会从内存读取数据，但是并不是只读一个数据，而是一次性读取一块一块的数据存放到 CPU Cache 中，之后才会被 CPU 读取。&lt;/p>
&lt;p>内存地址映射到 CPU Cache 地址里的策略有很多种，其中比较简单是直接映射 Cache，它巧妙的把内存地址拆分成「索引 + 组标记 + 偏移量」的方式，使得我们可以将很大的内存地址，映射到很小的 CPU Cache 地址里。&lt;/p>
&lt;p>要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率高的代码，CPU L1 Cache 分为数据缓存和指令缓存，因而需要分别提高它们的缓存命中率：&lt;/p>
&lt;p>对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；&lt;/p>
&lt;p>对于指令缓存，有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率；&lt;/p>
&lt;p>另外，对于多核 CPU 系统，线程可能在不同 CPU 核心来回切换，这样各个核心的缓存命中率就会受到影响，于是要想提高进程的缓存命中率，可以考虑把线程绑定 CPU 到某一个 CPU 核心。&lt;/p>
&lt;p>&lt;strong>絮叨&lt;/strong>&lt;/p>
&lt;p>分享个喜事，小林平日里忙着输出文章，今天收到一份特别的快递，是 CSDN 寄来的奖状。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966339-251ae183-2daf-4f55-83a3-1c832a0f772b.jpeg" alt="">&lt;/p>
&lt;p>骄傲的说，你们关注的是 CSDN 首届技术原创第一名的博主，以后简历又可以吹牛逼了&lt;/p>
&lt;p>没有啦，其实主要还是&lt;strong>谢谢你们不离不弃的支持&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/yrrbwa/1616167966347-a0f25fc4-8940-466b-8305-e8d5108835b5.png" alt="">&lt;/p>
&lt;p>&lt;em>哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎分享给你的朋友，也给小林点个「在看」，这对小林非常重要，谢谢你们，给各位小姐姐小哥哥们抱拳了，我们下次见！&lt;/em>&lt;/p>
&lt;p>&lt;strong>推荐阅读&lt;/strong>&lt;/p>
&lt;p>这个星期不知不觉输出了 3 篇文章了，前面的 2 篇还没看过的同学，赶紧去看看呀！&lt;/p>
&lt;p>&lt;strong>天啦噜！知道硬盘很慢，但没想到比 CPU Cache 慢 10000000 倍&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CPU 执行程序的秘密，藏在了这 15 张图里&lt;/strong>&lt;/p>
&lt;p>喜欢此内容的人还喜欢&lt;/p></description></item><item><title>Docs: 10.4.CPU 缓存一致性</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.4.cpu-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.kernel%E5%86%85%E6%A0%B8/4.cpu-%E7%AE%A1%E7%90%86/10.4.cpu-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/PDUqwAIaUxNkbjvRfovaCg">公众号,小林 coding-10 张图打开 CPU 缓存一致性的大门&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/otJfvn1M3ObNqonrkFTiYg">公众号,小林 coding-用动图的方式，理解 CPU 缓存一致性协议！&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESIHelp.htm">在线体验 MESI 协议状态转换&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="cpu-cache-的数据写入">CPU Cache 的数据写入&lt;/h1>
&lt;p>随着时间的推移，CPU 和内存的访问性能相差越来越大，于是就在 CPU 内部嵌入了 CPU Cache（高速缓存），CPU Cache 离 CPU 核心相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存角色。&lt;/p>
&lt;p>CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核心越近，访问速度也快，但是存储容量相对就会越小。其中，在多核心的 CPU 里，每个核心都有各自的 L1/L2 Cache，而 L3 Cache 是所有核心共享使用的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951900-5b473df4-ff48-449e-a160-95bf38f43801.jpeg" alt="">&lt;/p>
&lt;p>我们先简单了解下 CPU Cache 的结构，CPU Cache 是由很多个 Cache Line 组成的，CPU Line 是 CPU 从内存读取数据的基本单位，而 CPU Line 是由各种标志（Tag）+ 数据块（Data Block）组成，你可以在下图清晰的看到：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951908-bd5942d3-a3de-45e7-8488-540091ab9e59.jpeg" alt="">&lt;/p>
&lt;p>我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，而不是每一次都要从内存中获取数据。所以，身为程序员，我们要尽可能写出缓存命中率高的代码，这样就有效提高程序的性能，具体的做法，你可以参考我上一篇文章「如何写出让 CPU 跑得更快的代码？」&lt;/p>
&lt;p>事实上，数据不光是只有读操作，还有写操作，那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。&lt;/p>
&lt;p>问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？为了应对这个问题，下面介绍两种针对写入数据的方法：&lt;/p>
&lt;ul>
&lt;li>写直达（Write Through）&lt;/li>
&lt;li>写回（Write Back）&lt;/li>
&lt;/ul>
&lt;h2 id="写直达">写直达&lt;/h2>
&lt;p>保持内存与 Cache 一致性最简单的方式是，把数据同时写入内存和 Cache 中，这种方法称为写直达（Write Through）。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951906-df475ba6-3f90-4374-99a5-ae55826374e8.jpeg" alt="">&lt;/p>
&lt;p>在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：&lt;/p>
&lt;ul>
&lt;li>如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；&lt;/li>
&lt;li>如果数据没有在 Cache 里面，就直接把数据更新到内存里面。&lt;/li>
&lt;/ul>
&lt;p>写直达法很直观，也很简单，但是问题明显，无论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费大量的时间，无疑性能会受到很大的影响。&lt;/p>
&lt;h2 id="写回">写回&lt;/h2>
&lt;p>既然写直达由于每次写操作都会把数据写回到内存，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了写回（Write Back）的方法。&lt;/p>
&lt;p>在写回机制中，当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中，减少了数据写回内存的频率，这样便可以提高系统的性能。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951903-03199e9b-3ad0-460c-a0a7-6735e031f7a9.jpeg" alt="">&lt;/p>
&lt;p>那具体如何做到的呢？下面来详细说一下：&lt;/p>
&lt;ul>
&lt;li>如果当发生写操作时，数据已经在 CPU Cache 里的话，则把数据更新到 CPU Cache 里，同时标记 CPU Cache 里的这个 Cache Block 为脏（Dirty）的，这个脏的标记代表这个时候，我们 CPU Cache 里面的这个 Cache Block 的数据和内存是不一致的，这种情况是不用把数据写到内存里的；&lt;/li>
&lt;li>如果当发生写操作时，数据所对应的 Cache Block 里存放的是「别的内存地址的数据」的话，就要检查这个 Cache Block 里的数据有没有被标记为脏的，如果是脏的话，我们就要把这个 Cache Block 里的数据写回到内存，然后再把当前要写入的数据，写入到这个 Cache Block 里，同时也把它标记为脏的；如果 Cache Block 里面的数据没有被标记为脏，则就直接将数据写入到这个 Cache Block 里，然后再把这个 Cache Block 标记为脏的就好了。&lt;/li>
&lt;/ul>
&lt;p>可以发现写回这个方法，在把数据写入到 Cache 的时候，只有在缓存不命中，同时数据对应的 Cache 中的 Cache Block 为脏标记的情况下，才会将数据写到内存中，而在缓存命中的情况下，则在写入后 Cache 后，只需把该数据对应的 Cache Block 标记为脏即可，而不用写到内存里。&lt;/p>
&lt;p>这样的好处是，如果我们大量的操作都能够命中缓存，那么大部分时间里 CPU 都不需要读写内存，自然性能相比写直达会高很多。&lt;/p>
&lt;hr>
&lt;h1 id="缓存一致性问题">缓存一致性问题&lt;/h1>
&lt;p>现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的缓存一致性（Cache Coherence） 的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。&lt;/p>
&lt;p>那缓存一致性的问题具体是怎么发生的呢？我们以一个含有两个核心的 CPU 作为例子看一看。&lt;/p>
&lt;p>假设 A 号核心和 B 号核心同时运行两个线程，都操作共同的变量 i（初始值为 0 ）。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951921-649fb781-7ec2-47eb-9a35-aabc2a1883ec.jpeg" alt="">&lt;/p>
&lt;p>这时如果 A 号核心执行了 i++ 语句的时候，为了考虑性能，使用了我们前面所说的写回策略，先把值为 1 的执行结果写入到 L1/L2 Cache 中，然后把 L1/L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核心中的这个 Cache Block 要被替换的时候，数据才会写入到内存里。&lt;/p>
&lt;p>如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951914-92906a08-0de2-4801-b39e-e2f1ac5b5bb4.jpeg" alt="">&lt;/p>
&lt;p>那么，要解决这一问题，就需要一种机制，来同步两个不同核心里面的缓存数据。要实现的这个机制的话，要保证做到下面这 2 点：&lt;/p>
&lt;ul>
&lt;li>第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为写传播（Wreite Propagation）；&lt;/li>
&lt;li>第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为事务的串形化（Transaction Serialization）。&lt;/li>
&lt;/ul>
&lt;p>第一点写传播很容易就理解，当某个核心在 Cache 更新了数据，就需要同步到其他核心的 Cache 里。&lt;/p>
&lt;p>而对于第二点事务的串形化，我们举个例子来理解它。&lt;/p>
&lt;p>假设我们有一个含有 4 个核心的 CPU，这 4 个核心都操作共同的变量 i（初始值为 0 ）。A 号核心先把 i 值变为 100，而此时同一时间，B 号核心先把 i 值变为 200，这里两个修改，都会「传播」到 C 和 D 号核心。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951934-35edc494-8326-40b1-a58f-c9b5624f0526.jpeg" alt="">&lt;/p>
&lt;p>那么问题就来了，C 号核心先收到了 A 号核心更新数据的事件，再收到 B 号核心更新数据的事件，因此 C 号核心看到的变量 i 是先变成 100，后变成 200。&lt;/p>
&lt;p>而如果 D 号核心收到的事件是反过来的，则 D 号核心看到的是变量 i 先变成 200，再变成 100，虽然是做到了写传播，但是各个 Cache 里面的数据还是不一致的。&lt;/p>
&lt;p>所以，我们要保证 C 号核心和 D 号核心都能看到相同顺序的数据变化，比如变量 i 都是先变成 100，再变成 200，这样的过程就是事务的串形化。&lt;/p>
&lt;p>要实现事务串形化，要做到 2 点：&lt;/p>
&lt;ul>
&lt;li>CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；&lt;/li>
&lt;li>要引入「锁」的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。&lt;/li>
&lt;/ul>
&lt;p>那接下来我们看看，写传播和事务串形化具体是用什么技术实现的。&lt;/p>
&lt;hr>
&lt;h1 id="总线嗅探">总线嗅探&lt;/h1>
&lt;p>写传播的原则就是当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常见实现的方式是总线嗅探（Bus Snooping）。&lt;/p>
&lt;p>我还是以前面的 i 变量例子来说明总线嗅探的工作机制，当 A 号 CPU 核心修改了 L1 Cache 中 i 变量的值，通过总线把这个事件广播通知给其他所有的核心，然后每个 CPU 核心都会监听总线上的广播事件，并检查是否有相同的数据在自己的 L1 Cache 里面，如果 B 号 CPU 核心的 L1 Cache 中有该数据，那么也需要把该数据更新到自己的 L1 Cache。&lt;/p>
&lt;p>可以发现，总线嗅探方法很简单， CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。&lt;/p>
&lt;p>另外，总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串形化。&lt;/p>
&lt;p>于是，有一个协议基于总线嗅探机制实现了事务串形化，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存一致性。&lt;/p>
&lt;hr>
&lt;h1 id="mesi-协议">MESI 协议&lt;/h1>
&lt;p>MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：&lt;/p>
&lt;ul>
&lt;li>Modified，已修改&lt;/li>
&lt;li>Exclusive，独占&lt;/li>
&lt;li>Shared，共享&lt;/li>
&lt;li>Invalidated，已失效&lt;/li>
&lt;/ul>
&lt;p>这四个状态来标记 Cache Line 四个不同的状态。&lt;/p>
&lt;p>「已修改」状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。而「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。&lt;/p>
&lt;p>「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。&lt;/p>
&lt;p>「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。&lt;/p>
&lt;p>另外，在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。&lt;/p>
&lt;p>那么，「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。&lt;/p>
&lt;p>我们举个具体的例子来看看这四个状态的转换：&lt;/p>
&lt;ol>
&lt;li>当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是一致的；&lt;/li>
&lt;li>然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是一致的；&lt;/li>
&lt;li>当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后 A 号 CPU 核心才更新 Cache 里面的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不一致了。&lt;/li>
&lt;li>如果 A 号 CPU 核心「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核心发送消息，直接更新数据即可。&lt;/li>
&lt;li>如果 A 号 CPU 核心的 Cache 里的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。&lt;/li>
&lt;/ol>
&lt;p>所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送广播给其他 CPU 核心，这在一定程度上减少了总线带宽压力。&lt;/p>
&lt;p>事实上，整个 MESI 的状态可以用一个有限状态机来表示它的状态流转。还有一点，对于不同状态触发的事件操作，可能是来自本地 CPU 核心发出的广播事件，也可以是来自其他 CPU 核心通过总线发出的广播事件。下图即是 MESI 协议的状态图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951942-efbb5e13-2833-489a-ab0c-b19f9f08cc7f.jpeg" alt="">&lt;/p>
&lt;p>MESI 协议的四种状态之间的流转过程，我汇总成了下面的表格，你可以更详细的看到每个状态转换的原因：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951966-4cb5c454-ae77-4873-ae61-6d60d2d09d1c.jpeg" alt="">&lt;/p>
&lt;hr>
&lt;h1 id="总结">总结&lt;/h1>
&lt;p>CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相比内存高出很多。对于 Cache 里没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache 里面，最后 CPU 再从 Cache 读取数据。&lt;/p>
&lt;p>而对于数据的写入，CPU 都会先写入到 Cache 里面，然后再在找个合适的时机写入到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性：&lt;/p>
&lt;ul>
&lt;li>写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；&lt;/li>
&lt;li>写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；&lt;/li>
&lt;/ul>
&lt;p>当今 CPU 都是多核的，每个核心都有各自独立的 L1/L2 Cache，只有 L3 Cache 是多个核心之间共享的。所以，我们要确保多核缓存是一致性的，否则会出现错误的结果。&lt;/p>
&lt;p>要想实现缓存一致性，关键是要满足 2 点：&lt;/p>
&lt;ul>
&lt;li>第一点是写传播，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心；&lt;/li>
&lt;li>第二点是事物的串行化，这个很重要，只有保证了这个，次啊能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；&lt;/li>
&lt;/ul>
&lt;p>基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。&lt;/p>
&lt;p>MESI 协议，是已修改、独占、共享、已实现这四个状态的英文缩写的组合。整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心。&lt;/p>
&lt;hr>
&lt;p>说几句&lt;/p>
&lt;p>前几个星期建的技术交流群，没想到很快就满 500 人了，群里的大牛真的多，大家交流都很踊跃，也有很多热心分享和回答问题的小伙伴。&lt;/p>
&lt;p>不过没关系，小林最近又新建了技术交流群，相信这里是你交朋友好地方，也是你上班划水的好入口。&lt;/p>
&lt;p>准备入冬了，一起来抱团取暖吧，群满 100、200、300、500 人，小林都会发红包的，赶快来吧，加群方式很简单，扫码下方二维码，回复「加群」。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rzzghc/1616167951941-dd55c18e-4d43-4d12-9da6-b237c07d6caf.jpeg" alt="">&lt;/p>
&lt;p>哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎分享给你的朋友，也给小林点个「在看」，这对小林非常重要，谢谢你们，给各位小姐姐小哥哥们抱拳了，我们下次见！&lt;/p>
&lt;hr>
&lt;p>推荐阅读&lt;/p>
&lt;p>如何写出让 CPU 跑得更快的代码？&lt;/p>
&lt;p>读者问：小林你的 500 张图是怎么画的？&lt;/p>
&lt;p>喜欢此内容的人还喜欢&lt;/p></description></item><item><title>Docs: 2.1.Redis 高可用部署</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/2.1.redis-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/2.1.redis-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/</guid><description>
&lt;h1 id="docker-部署-redis-高可用">Docker 部署 Redis 高可用&lt;/h1>
&lt;p>Docker 部署 Redis Sentinel 模式&lt;/p>
&lt;p>Sentinel 模式至少需要 3 个节点，所以这里假设有如下三个节点&lt;/p>
&lt;ul>
&lt;li>
&lt;p>172.19.42.231&lt;/p>
&lt;/li>
&lt;li>
&lt;p>172.19.42.232&lt;/p>
&lt;/li>
&lt;li>
&lt;p>172.19.42.233&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="创建配置文件与存储所在路径xa">创建配置文件与存储所在路径
&lt;/h3>
&lt;pre>&lt;code>mkdir -p /opt/redis/config
mkdir -p /opt/redis/data
chmod 777 /opt/redis/data
&lt;/code>&lt;/pre>
&lt;h3 id="启动-redis-xa">启动 Redis
&lt;/h3>
&lt;p>master 节点配置&lt;/p>
&lt;pre>&lt;code>cat &amp;gt; /opt/redis/config/redis.conf &amp;lt;&amp;lt;EOF
save 900 1
maxmemory 1G
EOF
chmod 666 /opt/redis/config/redis.conf
&lt;/code>&lt;/pre>
&lt;p>replica 节点配置&lt;/p>
&lt;pre>&lt;code>cat &amp;gt; /opt/redis/config/redis.conf &amp;lt;&amp;lt;EOF
save 900 1
maxmemory 1G
replicaof 172.19.42.231 6379
EOF
chmod 666 /opt/redis/config/redis.conf
&lt;/code>&lt;/pre>
&lt;p>启动 Redis&lt;/p>
&lt;pre>&lt;code>docker run -d --name redis \
--network=host \
-v /opt/redis/config:/etc/redis \
-v /opt/redis/data:/data \
redis:5.0.10-alpine \
/etc/redis/redis.conf
&lt;/code>&lt;/pre>
&lt;h3 id="启动-redis-sentinelxa">启动 Redis Sentinel
&lt;/h3>
&lt;p>所有节点配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /opt/redis/config/sentinel.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">sentinel monitor mymaster 172.19.42.231 6379 2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">sentinel down-after-milliseconds mymaster 60000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">sentinel failover-timeout mymaster 180000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">sentinel parallel-syncs mymaster 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod &lt;span style="color:#ae81ff">666&lt;/span> /opt/redis/config/sentinel.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>启动 Sentinel&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d --name redis-sentinel &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --network&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /opt/redis/config:/etc/redis &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> redis:5.0.10-alpine &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> /etc/redis/sentinel.conf &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --sentinel
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="kubernetes-中部署-redis-高可用">Kubernetes 中部署 Redis 高可用&lt;/h1>
&lt;h2 id="helm-官方维护的-redis-ha-chart">Helm 官方维护的 Redis-HA Chart&lt;/h2>
&lt;p>参考：&lt;a href="https://github.com/helm/charts/tree/master/stable/redis-ha">Helm 官方网站&lt;/a>、&lt;a href="https://github.com/DandyDeveloper/charts">GitHub&lt;/a>、&lt;a href="https://artifacthub.io/packages/helm/dandydev-charts/redis-ha">ArtifactHub&lt;/a>&lt;/p>
&lt;p>Grafana Dashboard:11835&lt;/p>
&lt;h2 id="第三方-redis-operator-部署-cluster-模式-redis">第三方 redis operator 部署 Cluster 模式 Redis&lt;/h2>
&lt;p>&lt;a href="https://github.com/ucloud/redis-cluster-operator">https://github.com/ucloud/redis-cluster-operator&lt;/a> ucloud 出品&lt;/p>
&lt;h2 id="第三方-redis-operator-部署-sentinel-模式-redis">第三方 redis operator 部署 Sentinel 模式 redis&lt;/h2>
&lt;p>&lt;a href="https://github.com/spotahome/redis-operator">https://github.com/spotahome/redis-operator&lt;/a>，通过 operator 可以简单得创建出 6 个 pod，3 个 redis 节点，3 个 sentinel 节点。&lt;/p>
&lt;p>ucloud 基于该项目推出了一个类似的：&lt;a href="https://github.com/ucloud/redis-operator">https://github.com/ucloud/redis-operator&lt;/a>&lt;/p>
&lt;p>部署所需 yaml 在 github 上&lt;/p>
&lt;p>创建 operator&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/spotahome/redis-operator/master/example/operator/all-redis-operator-resources.yaml
&lt;/code>&lt;/pre>
&lt;p>配置 redis 密码认证&lt;/p>
&lt;pre>&lt;code># “密码”修改为自己想设置的密码
echo -n &amp;quot;密码&amp;quot; &amp;gt; password
kubectl create -n redis secret generic redis-auth --from-file=password
&lt;/code>&lt;/pre>
&lt;p>部署 redis&lt;/p>
&lt;pre>&lt;code>kubectl create -f https://raw.githubusercontent.com/spotahome/redis-operator/master/example/redisfailover/basic.yaml
&lt;/code>&lt;/pre>
&lt;p>Bitnami 官方用于部署 redis 的 helm chart&lt;/p>
&lt;p>&lt;a href="https://github.com/bitnami/charts/tree/master/bitnami/redis/">https://github.com/bitnami/charts/tree/master/bitnami/redis/&lt;/a>&lt;/p>
&lt;p>获取 charts 文件&lt;/p>
&lt;ol>
&lt;li>
&lt;p>helm repo add bitnami &lt;a href="https://charts.bitnami.com/bitnami">https://charts.bitnami.com/bitnami&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>helm pull bitnami/redis&lt;/p>
&lt;/li>
&lt;li>
&lt;p>tar -zxvf redis-XX.X.X.tgz&lt;/p>
&lt;/li>
&lt;li>
&lt;p>修改值文件，参考：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>部署 redis&lt;/p>
&lt;ol>
&lt;li>helm install redis -n redis &amp;ndash;set password=oc123 .&lt;/li>
&lt;/ol>
&lt;p>Bitnami 版问题：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>无法故障恢复，删除 pod 后， master 无法切换&lt;/p>
&lt;/li>
&lt;li>
&lt;p>是有了安全环境容器，导致容器内无法读取 /proc/sys/net/core/somaxconn 参数的值&lt;/p>
&lt;/li>
&lt;li>
&lt;p>问题跟踪：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/bitnami/charts/issues/3700">https://github.com/bitnami/charts/issues/3700&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/bitnami/charts/issues/4569">https://github.com/bitnami/charts/issues/4569&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 2.1.容器</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.1.%E5%AE%B9%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.1.%E5%AE%B9%E5%99%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/OS-level_virtualization">Wiki,OS-level virtualization&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Container(容器)&lt;/strong> 是一种基础工具；泛指任何可以用于容纳其它物品的工具，可以部分或完全封闭，被用于容纳、储存、运输物品。物体可以被放置在容器中，而容器则可以保护内容物。人类使用容器的历史至少有十万年，甚至可能有数百万年的历史。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1616122918518-107de7cd-51b4-4427-8281-8e81f7c2383d.png" alt="">
自 1979 年，Unix 版本 7 引用 Chroot Jail 以及 Chroot 系统调用开始，直到 2013 年开源出的 Docker，2014 年开源出来的 Kubernetes，直到现在的云原生生态的火热。容器技术已经逐步成为主流的基础技术之一。&lt;/p>
&lt;h2 id="一什么是容器">一、什么是容器&lt;/h2>
&lt;p>IT 里的容器技术是英文单词 Linux Container 的直译。Container 这个单词有集装箱、容器的含义（主要偏集装箱意思）。不过，在中文环境下，咱们要交流要传授，如果翻译成“集装箱技术” 就有点拗口，所以结合中国人的吐字习惯和文化背景，更喜欢用容器这个词。不过，如果要形象的理解 Linux Container 技术的话，还是得念成集装箱会比较好。我们知道，海边码头里的集装箱是运载货物用的，它是一种按规格标准化的钢制箱子。集装箱的特色，在于其格式划一，并可以层层重叠，所以可以大量放置在特别设计的远洋轮船中（早期航运是没有集装箱概念的，那时候货物杂乱无章的放，很影响出货和运输效率）。有了集装箱，那么这就更加快捷方便的为生产商提供廉价的运输服务。&lt;/p>
&lt;p>因此，IT 世界里借鉴了这一理念。早期，大家都认为硬件抽象层基于 hypervisor 的虚拟化方式可以最大程度上提供虚拟化管理的灵活性。各种不同操作系统的虚拟机都能通过 hypervisor（KVM、XEN 等）来衍生、运行、销毁。然而，随着时间推移，用户发现 hypervisor 这种方式麻烦越来越多。为什么？因为对于 hypervisor 环境来说，每个虚拟机都需要运行一个完整的操作系统以及其中安装好的大量应用程序。但实际生产开发环境里，我们更关注的是自己部署的应用程序，如果每次部署发布我都得搞一个完整操作系统和附带的依赖环境，那么这让任务和性能变得很重和很低下。&lt;/p>
&lt;p>基于上述情况，人们就在想，有没有其他什么方式能让人更加的关注应用程序本身，底层多余的操作系统和环境我可以共享和复用？换句话来说，那就是我部署一个服务运行好后，我再想移植到另外一个地方，我可以不用再安装一套操作系统和依赖环境。这就像集装箱运载一样，我把货物一辆兰博基尼跑车（好比开发好的应用 APP），打包放到一容器集装箱里，它通过货轮可以轻而易举的从上海码头（CentOS7.2 环境）运送到纽约码头（Ubuntu14.04 环境）。而且运输期间，我的兰博基尼（APP）没有受到任何的损坏（文件没有丢失），在另外一个码头卸货后，依然可以完美风骚的赛跑（启动正常）。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1616122918592-560b0741-407f-4354-bc87-d0ef48160754.png" alt="">&lt;/p>
&lt;h2 id="二容器技术的实现方式lxcrunckata-等">二、容器技术的实现方式，lxc、runc、kata 等&lt;/h2>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1616122918517-21a8c653-b45e-44a9-b54e-86ca53db6fd7.png" alt="">&lt;/p>
&lt;p>Linux Container(LXC)容器技术的诞生（2008 年）就解决了 IT 世界里“集装箱运输”的问题。Linux Container（简称 LXC）它是一种 内核轻量级的操作系统层 虚拟化技术，也称为容器的运行时(runtime 运行环境)。Linux Container 主要由 Namespace 和 Cgroup 两大机制来保证实现。那么 Namespace 和 Cgroup 是什么呢？刚才我们上面提到了集装箱，集装箱的作用当然是可以对货物进行打包隔离了，不让 A 公司的货跟 B 公司的货混在一起，不然卸货就分不清楚了。那么 Namespace 也是一样的作用，做隔离。光有隔离还没用，我们还需要对货物进行资源的管理。同样的，航运码头也有这样的管理机制：货物用什么样规格大小的集装箱，货物用多少个集装箱，货物哪些优先运走，遇到极端天气怎么暂停运输服务怎么改航道等等&amp;hellip; 通用的，与此对应的 Cgroup 就负责资源管理控制作用，比如进程组使用 CPU/MEM 的限制，进程组的优先级控制，进程组的挂起和恢复等等。&lt;/p>
&lt;p>经过多年的发展，陆续推出了 runc、kata 等容器底层技术&lt;/p>
&lt;p>runc 是 lxc 的替代品，官方说明：&lt;a href="https://www.docker.com/blog/runc/">https://www.docker.com/blog/runc/&lt;/a>&lt;/p>
&lt;p>kata 是自带内核的虚拟机型的容器 runtime，官方网址：&lt;a href="https://katacontainers.io/">https://katacontainers.io/&lt;/a>&lt;/p>
&lt;h2 id="三容器技术的特点">三、容器技术的特点&lt;/h2>
&lt;p>容器的特点其实我们拿跟它跟硬件抽象层虚拟化 hypervisor 技术对比就清楚了，我们之前也提到过，传统的虚拟化（虚拟机）技术，创建环境和部署应用都很麻烦，而且应用的移植性也很繁琐，比如你要把 vmware 里的虚拟机迁移到 KVM 里就很繁琐（需要做镜像格式的转换）。那么有了容器技术就简单了，总结下容器技术主要有三个特点：&lt;/p>
&lt;ul>
&lt;li>极其轻量：只打包了必要的 Bin/Lib；&lt;/li>
&lt;li>秒级部署：根据镜像的不同，容器的部署大概在毫秒与秒之间（比虚拟机强很多）；&lt;/li>
&lt;li>易于移植：一次构建，随处部署；&lt;/li>
&lt;li>弹性伸缩：Kubernetes、Swam、Mesos 这类开源、方便、好使的容器管理平台有着非常强大的弹性管理能力。&lt;/li>
&lt;/ul>
&lt;h2 id="四容器的标准化-open-container-initiativeoci">四、容器的标准化 Open Container Initiative(OCI)&lt;/h2>
&lt;p>当前，docker 几乎是容器的代名词，很多人以为 docker 就是容器。其实，这是错误的认识(docker 只是可以实现容器的引擎, docker 调用 containerd，containerd 再调用 runc 来启动一个容器)。除了 docker 还有 podman 等等。所以，容器世界里并不是只有 docker 一家。既然不是一家就很容易出现分歧。任何技术出现都需要一个标准来规范它，不然各搞各的很容易导致技术实现的碎片化，出现大量的冲突和冗余。因此，在 2015 年，由 Google，Docker、CoreOS、IBM、微软、红帽等厂商联合发起的 &lt;a href="https://www.opencontainers.org/">OCI(Open Container Initiative)&lt;/a> 项目成立了，并于 2016 年 4 月推出了第一个开放容器标准。标准主要包括 runtime(运行时)标准 和 image(镜像)标准。标准的推出，有助于替成长中市场带来稳定性，让企业能放心采用容器技术，用户在打包、部署应用程序后，可以自由选择不同的容器 Runtime；同时，镜像打包、建立、认证、部署、命名也都能按照统一的规范来做。&lt;/p>
&lt;p>两种标准主要包含以下内容：&lt;/p>
&lt;ul>
&lt;li>容器运行时标准 （runtime spec）
&lt;ul>
&lt;li>creating：使用 create 命令创建容器，这个过程称为创建中 b). created：容器创建出来，但是还没有运行，表示镜像和配置没有错误，容器能够运行在当前平台 c).&lt;/li>
&lt;li>running：容器的运行状态，里面的进程处于 up 状态，正在执行用户设定的任务 d)&lt;/li>
&lt;li>stopped：容器运行完成，或者运行出错，或者 stop 命令之后，容器处于暂停状态。这个状态，容器还有很多信息保存在平台中，并没有完全被删除&lt;/li>
&lt;li>&amp;hellip;.等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>容器镜像标准（image spec）
&lt;ul>
&lt;li>文件系统：以 layer 保存的文件系统，每个 layer 保存了和上层之间变化的部分，layer 应该保存哪些文件，怎么表示增加、修改和删除的文件等;&lt;/li>
&lt;li>config 文件：保存了文件系统的层级信息（每个层级的 hash 值，以及历史信息），以及容器运行时需要的一些信息（比如环境变量、工作目录、命令参数、mount 列表），指定了镜像在某个特定平台和系统的配置。比较接近我们使用 docker inspect&lt;/li>
&lt;li>&amp;hellip;.等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="五容器的主要应用场景">五、容器的主要应用场景&lt;/h2>
&lt;p>容器技术的诞生其实主要解决了 PAAS 的层的技术实现。像 OpenStack、Cloudstack 这样的技术是解决 IAAS 层的问题。IAAS 层和 PAAS 层大家估计也听得很多了，关于他们的区别和特性我这里不在描述。那么容器技术主要应用在哪些场景呢？目前主流的有以下几种：&lt;/p>
&lt;ul>
&lt;li>1.容器化传统应用 容器不仅能提高现有应用的安全性和可移植性，还能节约成本。
&lt;ul>
&lt;li>每个企业的环境中都有一套较旧的应用来服务于客户或自动执行业务流程。即使是大规模的单体应用，通过容器隔离的增强安全性、以及可移植性特点，也能从 容器 中获益，从而降低成本。一旦容器化之后，这些应用可以扩展额外的服务或者转变到微服务架构之上。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>持续集成和持续部署 (CI/CD) 通过 Docker 加速应用管道自动化和应用部署，交付速度提高至少 13 倍。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>现代化开发流程快速、持续且具备自动执行能力，最终目标是开发出更加可靠的软件。通过持续集成 (CI) 和持续部署 (CD)，每次开发人员签入代码并顺利测试之后，IT 团队都能够集成新代码。作为开发运维方法的基础，CI/CD 创造了一种实时反馈回路机制，持续地传输小型迭代更改，从而加速更改，提高质量。CI 环境通常是完全自动化的，通过 git 推送命令触发测试，测试成功时自动构建新镜像，然后推送到 Docker 镜像库。通过后续的自动化和脚本，可以将新镜像的容器部署到预演环境，从而进行进一步测试。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ol start="3">
&lt;li>微服务 加速应用架构现代化进程。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>应用架构正在从采用瀑布模型开发法的单体代码库转变为独立开发和部署的松耦合服务。成千上万个这样的服务相互连接就形成了应用。Docker 允许开发人员选择最适合于每种服务的工具或技术栈，隔离服务以消除任何潜在的冲突，从而避免“地狱式的矩阵依赖”。这些容器可以独立于应用的其他服务组件，轻松地共享、部署、更新和瞬间扩展。Docker 的端到端安全功能让团队能够构建和运行最低权限的微服务模型，服务所需的资源（其他应用、涉密信息、计算资源等）会适时被创建并被访问。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ol start="4">
&lt;li>IT 基础设施优化 充分利用基础设施，节省资金。&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Docker 和容器有助于优化 IT 基础设施的利用率和成本。优化不仅仅是指削减成本，还能确保在适当的时间有效地使用适当的资源。容器是一种轻量级的打包和隔离应用工作负载的方法，所以 Docker 允许在同一物理或虚拟服务器上毫不冲突地运行多项工作负载。企业可以整合数据中心，将并购而来的 IT 资源进行整合，从而获得向云端的可迁移性，同时减少操作系统和服务器的维护工作。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="container-的基本核心概念">Container 的基本核心概念&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/lei5gl/1641604740718-d450d041-4e36-462a-844d-d330f0a8715e.png" alt="image.png">&lt;/p>
&lt;h2 id="image镜像">Image(镜像)&lt;/h2>
&lt;p>镜像就是一个只读的模板。&lt;/p>
&lt;p>例如：一个镜像可以包含一个完整的 CentOS 操作系统环境，里面仅安装了 Apache 或用户需要的其他应用程序。&lt;/p>
&lt;p>镜像可以用来创建 Container。&lt;/p>
&lt;h3 id="reference引用">Reference(引用)&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/engine/reference/commandline/images/">https://docs.docker.com/engine/reference/commandline/images/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在互联网上，我们通过 &lt;strong>Reference(引用)&lt;/strong> 表示唯一一个 Image，就像 URL 之于 HTTP 的 Resource 一样，&lt;strong>Reference 就是 Image 的 URL&lt;/strong>。&lt;/p>
&lt;h4 id="syntax语法">Syntax(语法)&lt;/h4>
&lt;p>&lt;strong>Scheme://Registry/[Namespace/]Repository:{Tag|Digest}&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>**Scheme:// **# 访问 Registry 时所使用的协议，比如 HTTP、HTTPS&lt;/li>
&lt;li>&lt;strong>Registry(注册中心)&lt;/strong> # 提供 Image 管理服务的提供商，通常是一个域名
&lt;ul>
&lt;li>现阶段常见的 Registry 有：
&lt;ul>
&lt;li>docker.io&lt;/li>
&lt;li>k8s.gcr.io&lt;/li>
&lt;li>quay.io&lt;/li>
&lt;li>ghcr.io&lt;/li>
&lt;li>&amp;hellip;&amp;hellip; 等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Namespace(名称空间)&lt;/strong> # 在一个 Registry 中可能会有多个同名的 Repository，所以需要通过 Namespace 将这些 Repository 隔开。
&lt;ul>
&lt;li>docker.io 将用户注册的账户名称作为 Namespace，若 Namespace 被省略，则 Image 就是这个 Registry 官方的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Repository(仓库)&lt;/strong> # 顾名思义，存放镜像的仓库&lt;/li>
&lt;li>&lt;strong>Tag(标签)&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>Digest(摘要)&lt;/strong> # Image 内容的 sha256 计算结果。通常是互联网唯一的&lt;/li>
&lt;/ul>
&lt;p>假如我在 docker.io 注册了一个账号 lchdzh 用来存放容器镜像，有一个 k8s-debug 的镜像，版本号是 v1，我想把镜像放在 dd_k8s 仓库中。&lt;/p>
&lt;ul>
&lt;li>那么正常的 Image Reference 是：&lt;code>docker.io/lchdzh/dd_k8s:k8s-debug-v1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>但是，后来人们一般情况 Repository 都存放同一个软件的 Image，把 Tag 仅仅当做了镜像的版本&lt;/p>
&lt;ul>
&lt;li>那么上面例子的 Image Reference 就变成了：&lt;code>docker.io/lchdzh/k8s-debug:v1&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="registry注册中心">Registry(注册中心)&lt;/h3>
&lt;p>Registry 可以理解为一个网站，通过 https 协议与 docker daemon 交互;也可以自己搭建私有单位 registry，提供多个功能&lt;/p>
&lt;ul>
&lt;li>用于存储 image 的 Repository 功能，一个 Registry 上有多个 Repository&lt;/li>
&lt;li>用户来获取 image 时的认证功能&lt;/li>
&lt;li>当前 registry 所有 image 的索引 功能&lt;/li>
&lt;/ul>
&lt;p>Registry 上有多个 Repository，每个 Repository 中又包含了多个 TAG(标签)。一个 registry 中分两种：顶层仓库与用户仓库，顶层仓库里的 Repository 是这个 Registry 官方所创建的，用户仓库里的 Repository 是在该 Registry 创建的用户所创建的。image 名字中有 namespace 的就是用户仓库，没有就是顶层仓库&lt;/p>
&lt;h3 id="repository仓库">Repository(仓库)&lt;/h3>
&lt;p>想要定位一个 Registry 下的一个 Repository，至少需要两部分&lt;/p>
&lt;ul>
&lt;li>Namespace(名称空间) # 有的也称为 ProjectID。
&lt;ul>
&lt;li>Docker 将用户注册的账户名称作为 Namespace，若 Namespace 被省略，则就是这个 Registry 官方的。所以也可以这么理解。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Repository(仓库) # 仓库名称&lt;/li>
&lt;/ul>
&lt;p>很多时候都将 Namespace 和 Repository 合起来，统一称为 Repository&lt;/p>
&lt;p>仓库分为公开仓库(Public)和私有仓库(Private)两种形式。当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下载在另外一台机器上使用这个镜像时候，只需需要从仓库上 pull 下来就可以了。&lt;/p>
&lt;p>注意：Docker 仓库的概念跟 Git 类似，Registry 可以理解为 GitHub 这样的托管服务。&lt;/p>
&lt;h3 id="tag标签">Tag(标签)&lt;/h3>
&lt;p>Repository 可以存放不同的 Image(比如 nginx,redis,centos 等)，通过 Tag 来区分这些 Image。说白了，Tag 就是 Image 的名称。&lt;/p>
&lt;h2 id="container容器">Container(容器)&lt;/h2>
&lt;p>容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的，保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括 root 用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。注意：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。&lt;/p>
&lt;p>Image 与 Container 的关系，就好比是程序与进程之间的关系。Image 类似程序是静态的。Container 类似进程是动态的，是有生命周期的。&lt;/p>
&lt;h2 id="联合文件系统">联合文件系统&lt;/h2>
&lt;ul>
&lt;li>当我们在下载镜像的时候，会发现每一层都有一个 id，这是 &lt;strong>Layer(层)&lt;/strong> 的概念，是 **UnionFS(联合文件系统) **中的重要概念&lt;/li>
&lt;li>联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下&lt;/li>
&lt;li>联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。&lt;/li>
&lt;li>不同容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率。&lt;/li>
&lt;/ul>
&lt;h1 id="rootless-containers">Rootless Containers&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/rootless-containers">GitHub 项目,rootless-containers&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Rootless Containers(无根容器)&lt;/strong> 是指非特权用户能够创建、运行和以各种方式管理容器。这个术语还包括围绕容器的各种工具，这些工具也可以作为非特权用户运行。&lt;/p>
&lt;p>运行 Rootless Containers 通常需要弃用 CGroupV2 来限制 CPU、内存、I/O、PID 这些资源的消耗。&lt;/p></description></item><item><title>Docs: 2.2.实现容器的工具</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/2.2.%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;h1 id="oci-runtime-规范的实现">OCI Runtime 规范的实现&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/NPxLLhRkpNdTgVcKQSLcFA">公众号-k8s 技术圈，Containerd 深度剖析-runtime 篇&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>当人们想到容器运行时，可能会想到一连串的相关概念；runc、runv、lxc、lmctfy、Docker（containerd）、rkt、cri-o。每一个都是基于不同的场景而实现的，均实现了不同的功能。如 containerd 和 cri-o，实际均可使用 runc 来运行容器，但其实现了如镜像管理、容器 API 等功能，可以将这些看作是比 runc 具备的更高级的功能。
可以发现，容器运行时是相当复杂的。每个运行时都涵盖了从低级到高级的不同部分，如下图所示。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ctvy4o/1653965809357-01c7d7f1-81d0-49f1-beaa-15bd63e7acd6.png" alt="">
根据功能范围划分，将其分为 &lt;strong>Low level Container Runtime(低级容器运行时)&lt;/strong> 和 &lt;strong>High level Container Runtime(高级容器运行时)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>低级容器运行时 # 只关注容器的本身运行&lt;/li>
&lt;li>高级容器运行时 # 支持更多高级功能的运行时，如镜像管理及一些 gRPC/Web APIs，通常被称为&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，低级运行时和高级运行时有本质区别，各自解决的问题也不同。&lt;/p>
&lt;h2 id="低级运行时">低级运行时&lt;/h2>
&lt;p>低级运行时的功能有限，通常执行运行容器的低级任务。大多数开发者日常工作中不会使用到。其一般指按照 OCI 规范、能够接收可运行 roofs 文件系统和配置文件并运行隔离进程的实现。这种运行时只负责将进程运行在相对隔离的资源空间里，不提供存储实现和网络实现。但是其他实现可以在系统中预设好相关资源，低级容器运行时可通过 config.json 声明加载对应资源。低级运行时的特点是底层、轻量，限制也很一目了然：&lt;/p>
&lt;ul>
&lt;li>只认识 rootfs 和 config.json，没有其他镜像能力&lt;/li>
&lt;li>不提供网络实现&lt;/li>
&lt;li>不提供持久实现&lt;/li>
&lt;li>无法跨平台等&lt;/li>
&lt;/ul>
&lt;h3 id="runc">RunC&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/opencontainers/runc">GitHub 项目，opencontainers/runc&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>runc 是一个 CLI 工具，用于根据 OCI 规范生成和运行容器。&lt;/p>
&lt;h3 id="youki">youki&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containers/youki">GitHub 项目，containers/youki&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>使用 Rust 语言写的，类似于 Runc 的容器运行时，&lt;/p>
&lt;h3 id="sysbox">Sysbox&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/nestybox/sysbox">GitHub 项目，nestybox/sysbox&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Sysbox 是一个新型的 OCI 容器运行时，对标 runc。相比于 runc，Sysbox 在以下两个方面做了增强：&lt;/p>
&lt;ul>
&lt;li>增强容器隔离性：Sysbox 为所有容器开启 user namespace（即容器中的 root 用户映射为主机中的普通用户），在容器中隐藏宿主机的信息，锁定容器的初始挂载，等等。&lt;/li>
&lt;li>容器不仅可以运行普通进程，还可以运行 systemd、Docker、K8s、K3s 等系统级软件，一定程度上可以替换虚拟机。&lt;/li>
&lt;/ul>
&lt;p>最初 Sysbox 只支持 Docker，但最新版本 v0.4.0 已支持直接作为 Kubernetes 的 CRI 运行时。&lt;/p>
&lt;h3 id="kata-container">Kata Container&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kata-containers/kata-containers">GitHub 项目，kata-containers/kata-containers&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kata Containers 是一个开源项目和社区，致力于构建轻量级虚拟机 (vm) 的标准实现，该虚拟机感觉和性能类似于容器，但提供 vm 的工作负载隔离和安全性优势。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ctvy4o/1616122531941-6b13921a-78c5-45a1-9a38-6695b517bca8.png" alt="">&lt;/p>
&lt;h2 id="高级运行时">高级运行时&lt;/h2>
&lt;p>高级运行时负责容器镜像的传输和管理，解压镜像，并传递给低级运行时来运行容器。通常情况下，高级运行时提供一个守护程序和一个 API，远程应用程序可以使用它来运行容器并监控它们，它们位于低层运行时或其他高级运行时之上。&lt;/p>
&lt;p>高层运行时也会提供一些看似很低级的功能。例如，管理网络命名空间，并允许容器加入另一个容器的网络命名空间。
这里有一个类似逻辑分层图，可以帮助理解这些组件是如何结合在一起工作的。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ctvy4o/1653966607306-f97afdfd-66fd-4d4d-ab04-364b1b60f27e.png" alt="">&lt;/p>
&lt;h3 id="docker">Docker&lt;/h3>
&lt;h3 id="containerd">Containerd&lt;/h3></description></item><item><title>Docs: 2.3.Kubernetes 容器编排系统</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/</guid><description/></item><item><title>Docs: 2.ARP 与 NDP</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/2.arp-%E4%B8%8E-ndp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/2.arp-%E4%B8%8E-ndp/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Address_Resolution_Protocol">Wiki,ARP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol">Wiki,NDP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.rfc-editor.org/rfc/rfc826.html">RFC 826&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/_5Wgsx4mEoDZgwv9-yHYEA">公众号,36 张图详解 ARP&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**Address Resolution Protoco(地址解析协议，简称 ARP) **是一种通信协议，该协议可以通过给定的网络层地址(通常是 IPv4 地址)，发现与之相关联的链路层地址(通常你是 MAC 地址)。ARP 于 1982 年在 &lt;a href="https://www.rfc-editor.org/rfc/rfc826.html">RFC 826&lt;/a> 中定义。说白了，就是根据 IP 地址查询对应 MAC 地址的协议。&lt;/p>
&lt;p>注意：在 IPv6 网络环境下，APR 的功能已经被 NDP 替代&lt;/p>
&lt;p>对应关系：一个 ip 地址对应一个 MAC 地址。多个 ip 地址可以对应一个 MAC 地址(e.g.一个网卡上配置两个 ip)&lt;/p>
&lt;h2 id="arp-报文">ARP 报文&lt;/h2>
&lt;p>在抓包时，可以抓到如下几种 ARP 包&lt;/p>
&lt;ol>
&lt;li>ARP, Request who-has 10.10.100.254 tell 10.10.100.101, length 28
&lt;ol>
&lt;li>在局域网中询问谁有 10.10.100.254，告诉自己，自己就是 10.10.100.101&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>ARP, Reply 10.10.100.254 is-at 00:0f:e2:ff:05:92, length 46
&lt;ol>
&lt;li>当 10.10.100.254 收到 arp 广播包之后，就会进行 reply(响应)，10.10.100.254 的 mac 地址是 00:0f:e2:ff:05:92&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>这时候 10.10.100.101 就会更新自己的 arp 表，记录下来 10.10.100.254 与 00:0f:e2:ff:05:92 的对应关系&lt;/li>
&lt;/ol>
&lt;p>ARP 报文分为 &lt;strong>ARP 请求报文&lt;/strong>和 &lt;strong>ARP 应答报文&lt;/strong>，它们的报文格式相同，但是各个字段的取值不同。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623911614198-6f1cde9d-cf37-4711-b6ce-cd9b8b1ccdd5.png" alt="">ARP 报文格式
ARP 报文中各个字段的含义如下。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623911614222-e8fe87d5-ca7b-494c-99c1-b8b1be472301.png" alt="">&lt;/p>
&lt;h1 id="arp-原理">ARP 原理&lt;/h1>
&lt;p>ARP 是通过 **ARP Request(请求) **和 **ARP Reply(响应) **报文确定 MAC 地址的。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910582328-062e8f26-c6da-4ee5-97e4-857507dbb707.png" alt="">&lt;/p>
&lt;h2 id="同网段交互流程">同网段交互流程&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@lichenhao:~# tcpdump -i any host 172.19.42.202
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tcpdump: verbose output suppressed, use -v or -vv &lt;span style="color:#66d9ef">for&lt;/span> full protocol decode
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>listening on any, link-type LINUX_SLL &lt;span style="color:#f92672">(&lt;/span>Linux cooked v1&lt;span style="color:#f92672">)&lt;/span>, capture size &lt;span style="color:#ae81ff">262144&lt;/span> bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>14:13:38.036170 ARP, Request who-has 172.19.42.202 tell lichenhao.bj-test, length &lt;span style="color:#ae81ff">28&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>14:13:38.036549 ARP, Reply 172.19.42.202 is-at 00:be:d5:ef:24:4e &lt;span style="color:#f92672">(&lt;/span>oui Unknown&lt;span style="color:#f92672">)&lt;/span>, length &lt;span style="color:#ae81ff">46&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>14:13:38.036583 IP lichenhao.bj-test &amp;gt; 172.19.42.202: ICMP echo request, id 3, seq 1, length &lt;span style="color:#ae81ff">64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>14:13:38.036821 IP 172.19.42.202 &amp;gt; lichenhao.bj-test: ICMP echo reply, id 3, seq 1, length &lt;span style="color:#ae81ff">64&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如上所示，当前主机没有 172.19.42.202 主机的 arp 关系表，当我们向 172.19.42.202 发送 icmp 请求时，数据包将会经历如下过程：&lt;/p>
&lt;p>假如主机 A 向同一网段上的主机 B 发送数据。主机 A 的 IP 地址为 &lt;code>10.0.0.1&lt;/code> ，主机 B 的 IP 地址为 &lt;code>10.0.0.2&lt;/code> ，主机 C 的 IP 地址为 &lt;code>10.0.0.3&lt;/code> 。它们都不知道对方的 MAC 地址。ARP 地址解析过程如下：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910652117-bd47d4a3-1fb2-4f50-9b4f-bc8e5c2a1b2b.png" alt="">&lt;/p>
&lt;ol>
&lt;li>&lt;strong>主机 A&lt;/strong> 首先查看自己的 ARP 表（即 ARP 缓存表），确定是否有主机 B 的 IP 地址对应表项。如果有，则直接使用表项中的 MAC 地址进行封装，封装成帧后发送给主机 B 。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669257-9b3f2f80-ddd3-4501-807e-96acfe414c20.png" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>如果&lt;strong>主机 A&lt;/strong> 的 ARP 表没有对应的表项，就发送一个广播帧，源 IP 和源 MAC 地址是主机 A ，目的 IP 地址是主机 B ，目的 MAC 地址是广播 MAC 地址，即 &lt;code>FFFF-FFFF-FFFF&lt;/code> 。这就是 &lt;strong>ARP 请求报文&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669279-c6354c1b-c098-4974-b4cd-22e93fc54efe.png" alt="">&lt;/p>
&lt;ol start="3">
&lt;li>ARP 请求是广播报文，同一个网段的所有主机都能收到。只有主机 B 发现报文中的目的 IP 地址是自己，于是&lt;strong>主机 B&lt;/strong> 发送响应报文给主机 A ，源 MAC 地址和源 IP 地址是主机 B ，目的 MAC 地址和目的 IP 地址是主机 A ，这个报文就叫 &lt;strong>ARP 响应报文&lt;/strong>。同时，主机 B 的 ARP 表记录主机 A 的映射关系，即主机 A 的 IP 地址和 MAC 地址的对应关系。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669203-16790059-e9ef-45f9-9c9b-28f5f835c556.png" alt="">&lt;/p>
&lt;ol start="4">
&lt;li>&lt;strong>主机 C&lt;/strong> 也收到了 ARP 请求报文，但目的 IP 地址不是自己，所以不会进行响应。于是主机 C 添加主机 A 的映射关系到 ARP 表，并丢弃 ARP 请求报文。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669305-20a3b990-6012-45d5-8462-176106ad6dd6.png" alt="">&lt;/p>
&lt;ol start="5">
&lt;li>主机 A 收到 ARP 响应报文后，添加主机 B 的映射关系，同时用主机 B 的 MAC 地址做为目的地址封装成帧，并发送给主机 B 。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669300-be2dccaa-122c-41d2-92b6-3b035e70335d.png" alt="">
如果每发送一个 IP 报文就要进行一次 ARP 请求，来确定 MAC 地址，那将会造成不必要的网络流量，通常的做法是用 ARP 表记录 IP 地址和 MAC 地址的映射关系。主机发送报文时，首先会查看它的 &lt;strong>ARP 表&lt;/strong>，目的是为了确定是否是已知的设备 MAC 地址。如果有，就直接使用；如果没有，就发起 ARP 请求获取。不过，缓存是有一定期限的。ARP 表项在&lt;strong>老化时间&lt;/strong>（ &lt;code>aging time&lt;/code> ）内是有效的，如果老化时间内未被使用，表项就会被删除。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669502-4614b748-fac7-4301-bc85-68ccec45c91a.png" alt="">
ARP 表项分为&lt;strong>动态 ARP 表项&lt;/strong>和&lt;strong>静态 ARP 表项&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>动态 ARP 表项&lt;/strong>由 ARP 动态获取，因此在网络通信中，无需事先知道 MAC 地址，只要有 IP 地址即可。如果老化时间内未被使用，表项就会被自动删除。&lt;/li>
&lt;li>&lt;strong>静态 ARP 表项&lt;/strong>是手工配置，不会老化。静态 ARP 表项的优先级高于动态 ARP 表项，可以将相应的动态 ARP 表项覆盖。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623910669307-374038c8-0ab6-4a06-ad5a-b55240326db1.png" alt="">&lt;/p>
&lt;h2 id="跨网段交互流程">跨网段交互流程&lt;/h2>
&lt;ol>
&lt;li>在 client 向 server 发送数据时，内核会在该数据包外封装 源 IP、源 MAC、目的 IP，由于目的 MAC 未知，所以不填&lt;/li>
&lt;li>数据包经过交换、路由等设备后，到达 server 前的交换机，交换机根据自身的 arp 表，找到目的 IP 与哪个口的 mac 地址相关联，则把数据包交给响应的网口，以便顺利到达 server。&lt;/li>
&lt;li>server 收到数据包后，内核会把最外层的 IP 与 MAC 都玻璃，并交给相对应的用户空间内的程序，处理完成后，再发送数据响应 client&lt;/li>
&lt;li>此时 server 已经有个 client 的 MAC 与 IP，所以在内核封装的时候，会填写源 IP、源 MAC、目的 IP、目的 MAC。&lt;/li>
&lt;li>之后数据包到达 client 前面的交换机时，交换机发现目的 IP 与 MAC，则更新或者保持不变 arp 表，并把数据包交给 MAC 为目的 MAC 的端口，以便数据送达 client&lt;/li>
&lt;li>这样就完成了两台设备之间的互相通信以及数据报文的完整封装&lt;/li>
&lt;li>否则第一次发送数据的时候，如果是不在本网段的地址，则目的 MAC 一般都是未知的，除非已经建立连接之后，才能根据数据包的源 MAC 知道响应的时候目的 MAC 填什么&lt;/li>
&lt;/ol>
&lt;h1 id="免费-arp-广播-与-一般-arp-广播">免费 ARP 广播 与 一般 ARP 广播&lt;/h1>
&lt;ul>
&lt;li>&lt;strong>免费 arp 广播&lt;/strong> # 在设备首次接通网线之后，会进行 arp 广播，告诉大家自己的 IP 与 MAC 对应关系。当局域网内的设备收到这个免费的 arp 广播时，没有该 arp 记录则会添加，如果该 arp 记录改变了则会更新。&lt;/li>
&lt;li>&lt;strong>一般 arp 广播&lt;/strong> # 在对本机未知同网段的设备发送数据时，会进行 arp 广播，询问该设备在哪里。&lt;/li>
&lt;li>“免费 arp 广播”与“一般 arp 广播”的区别：
&lt;ol>
&lt;li>普通的 arp 请求如果目的地址不是本机，则本机就直接丢弃了，但是免费的 arp 广播，则会在本机保留或者更新。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>如果目的 IP 不在本网段，则不会进行 arp 广播，因为目的地址不在本网段的设备，发送出去直接就到网关了，而网关在接入的时候、每隔 N 时间，都会发送 arp 来询问网关在哪。至于交换机测，也是同样的道理，本网段的设备，在需要发送数据包的时候，如果 arp 表里没有，则会先进行 arp 广播再发送。因为这些都是通过 mac 地址来进行二层发送的。&lt;/p>
&lt;p>&lt;strong>免费 ARP&lt;/strong> 是一种特殊的 ARP 请求，它并非通过 IP 找到对应的 MAC 地址，而是当主机启动的时候，发送一个免费 ARP 请求，即请求自己的 IP 地址的 MAC 地址。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623911576682-1a99a60a-4fa9-4480-8061-cec725fe9073.webp" alt="">
与普通 ARP 请求报文的区别在于报文中的目标 IP 地址。普通 ARP 报文中的目标 IP 地址是其它主机的 IP 地址；而免费 ARP 的请求报文中，&lt;strong>目标 IP 地址是自己的 IP 地址&lt;/strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/nguycm/1623911577030-243f8668-60e2-4952-bac9-beef00f9cba4.png" alt="">
免费 ARP 的作用：&lt;/p>
&lt;ul>
&lt;li>起到一个&lt;strong>宣告&lt;/strong>作用。它以广播的形式将数据包发送出去，不需要得到回应，只为了告诉其它主机自己的 IP 地址和 MAC 地址。&lt;/li>
&lt;li>可用于&lt;strong>检测 IP 地址冲突&lt;/strong>。当一台主机发送了免费 ARP 请求报文后，如果收到了 ARP 响应报文，则说明网络内已经存在使用该 IP 地址的主机。&lt;/li>
&lt;li>可用于&lt;strong>更新&lt;/strong>其它主机的 ARP 缓存表。如果该主机更换了网卡，而其它主机的 ARP 缓存表仍然保留着原来的 MAC 地址。这时，通过免费的 ARP 数据包，更新其它主机的 ARP 缓存表。&lt;/li>
&lt;/ul>
&lt;h1 id="ndp">NDP&lt;/h1></description></item><item><title>Docs: 2.Authorization(授权)</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/2.authorization%E6%8E%88%E6%9D%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/7.api-%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/2.authorization%E6%8E%88%E6%9D%83/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">官方文档,参考-API 访问控制-授权&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>在 Kubernetes 中，在 &lt;strong>Authorization(i.e.授予访问权限，简称：授权)&lt;/strong> 之前必须进行过 &lt;a href="https://www.yuque.com/go/doc/33165791">Authenticating(认证)&lt;/a>&lt;/p>
&lt;h2 id="授权流程">授权流程&lt;/h2>
&lt;h3 id="确定是允许还是拒绝请求">确定是允许还是拒绝请求&lt;/h3>
&lt;p>Kubernetes 使用 API 服务器授权 API 请求。它根据所有策略评估所有请求属性来决定允许或拒绝请求。 一个 API 请求的所有部分必须被某些策略允许才能继续。这意味着默认情况下拒绝权限。&lt;/p>
&lt;p>（尽管 Kubernetes 使用 API 服务器，但是依赖于特定种类对象的特定字段的访问控制和策略由准入控制器处理。）&lt;/p>
&lt;p>配置多个授权模块时，将按顺序检查每个模块。 如果任何授权模块批准或拒绝请求，则立即返回该决定，并且不会与其他授权模块协商。 如果所有模块对请求没有意见，则拒绝该请求。一个拒绝响应返回 HTTP 状态代码 403 。&lt;/p>
&lt;h3 id="审查您的请求属性">审查您的请求属性&lt;/h3>
&lt;p>Kubernetes 仅审查以下 API 请求属性：&lt;/p>
&lt;ul>
&lt;li>user - 身份验证期间提供的 user 字符串。&lt;/li>
&lt;li>group - 经过身份验证的用户所属的组名列表。&lt;/li>
&lt;li>extra - 由身份验证层提供的任意字符串键到字符串值的映射。&lt;/li>
&lt;li>API - 指示请求是否针对 API 资源。&lt;/li>
&lt;li>Request path - 各种非资源端点的路径，如 /api 或 /healthz。&lt;/li>
&lt;li>API request verb - API 动词 get，list，create，update，patch，watch，proxy，redirect，delete 和 deletecollection 用于资源请求。要确定资源 API 端点的请求动词，请参阅确定请求动词。&lt;/li>
&lt;li>HTTP request verb - HTTP 动词 get，post，put 和 delete 用于非资源请求。&lt;/li>
&lt;li>Resource - 正在访问的资源的 ID 或名称（仅限资源请求） - 对于使用 get，update，patch 和 delete 动词的资源请求，您必须提供资源名称。&lt;/li>
&lt;li>Subresource - 正在访问的子资源（仅限资源请求）。&lt;/li>
&lt;li>Namespace - 正在访问的对象的名称空间（仅适用于命名空间资源请求）。&lt;/li>
&lt;li>API group - 正在访问的 API 组（仅限资源请求）。空字符串表示核心 API 组。&lt;/li>
&lt;/ul>
&lt;h3 id="确定请求动词">确定请求动词&lt;/h3>
&lt;p>要确定资源 API 端点的请求动词，需要检查所使用的 HTTP 动词以及请求是否对单个资源或资源集合起作用：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>HTTP 动词&lt;/th>
&lt;th>request 动词&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>POST&lt;/td>
&lt;td>create&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GET, HEAD&lt;/td>
&lt;td>get (单个资源)，list (资源集合)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PUT&lt;/td>
&lt;td>update&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PATCH&lt;/td>
&lt;td>patch&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DELETE&lt;/td>
&lt;td>delete (单个资源)，deletecollection (资源集合)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Kubernetes 有时使用专门的动词检查授权以获得额外的权限。例如：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod 安全策略&lt;/a> 检查 policy API 组中 podsecuritypolicies 资源的 use 动词的授权。&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping">RBAC &lt;/a>检查 rbac.authorization.k8s.io API 组中 roles 和 clusterroles 资源的 bind 动词的授权。&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">认证&lt;/a> layer 检查核心 API 组中 users，groups 和 serviceaccounts 的 impersonate 动词的授权，以及 authentication.k8s.io API 组中的 userextras&lt;/li>
&lt;/ul>
&lt;h1 id="授权的实现方式">授权的实现方式&lt;/h1>
&lt;p>在 Kubernetes 中，可以通过多种方式来实现 Authorization(授权) 功能&lt;/p>
&lt;h2 id="rbac-授权">RBAC 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/&lt;/a>&lt;/li>
&lt;li>RBAC 概念：&lt;a href="https://www.yuque.com/go/doc/33177747">https://www.yuque.com/go/doc/33177747&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>RBAC&lt;/strong> # 基于角色的访问控制（RBAC）是一种基于企业内个人用户的角色来管理对计算机或网络资源的访问的方法。在这种语境中，权限是单个用户执行特定任务的能力，例如查看，创建或修改文件。要了解有关使用 RBAC 模式的更多信息，请参阅 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC 模式&lt;/a>。&lt;/p>
&lt;ul>
&lt;li>当指定的 RBAC（基于角色的访问控制）使用 rbac.authorization.k8s.io API 组来驱动授权决策时，允许管理员通过 Kubernetes API 动态配置权限策略。&lt;/li>
&lt;li>要启用 RBAC，请使用 &amp;ndash;authorization-mode = RBAC 启动 apiserver 。&lt;/li>
&lt;/ul>
&lt;p>详见 ：&lt;a href="https://www.yuque.com/go/doc/43619714">RBAC 授权章节&lt;/a>&lt;/p>
&lt;h2 id="abac-授权">ABAC 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">https://kubernetes.io/docs/reference/access-authn-authz/abac/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>ABAC&lt;/strong> # 基于属性的访问控制（ABAC）定义了一种访问控制范例，通过使用将属性组合在一起的策略，将访问权限授予用户。策略可以使用任何类型的属性（用户属性，资源属性，对象，环境属性等）。要了解有关使用 ABAC 模式的更多信息，请参阅 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">ABAC 模式&lt;/a>。&lt;/p>
&lt;h2 id="node-授权">Node 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">https://kubernetes.io/docs/reference/access-authn-authz/node/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Node&lt;/strong> # 一个专用授权程序，根据计划运行的 pod 为 kubelet 授予权限。了解有关使用节点授权模式的更多信息，请参阅&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">节点授权.&lt;/a>&lt;/p>
&lt;h2 id="webhook-授权">Webhook 授权&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/">https://kubernetes.io/docs/reference/access-authn-authz/webhook/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>**Webhook **# WebHook 是一个 HTTP 回调：发生某些事情时调用的 HTTP POST；通过 HTTP POST 进行简单的事件通知。实现 WebHook 的 Web 应用程序会在发生某些事情时将消息发布到 URL。要了解有关使用 Webhook 模式的更多信息，请参阅 Webhook 模式。&lt;/p></description></item><item><title>Docs: 2.CGroup</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/2.cgroup/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Cgroups">Wiki,Cgroups&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">Manual(手册),cgroup(7)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html">Linux Kernel 官方文档,Linux 内核用户和管理员指南-Control Group V1&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux Kernel 官方文档,Linux 内核用户和管理员指南-Control Group V2&lt;/a>&lt;/li>
&lt;li>红帽文档：
&lt;ul>
&lt;li>&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/system_design_guide/using-control-groups-through-a-virtual-file-system_setting-limits-for-applications">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/system_design_guide/using-control-groups-through-a-virtual-file-system_setting-limits-for-applications&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/index">https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/index&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://segmentfault.com/a/1190000009732550">思否，Linux Namespace 和 Cgroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://fuckcloudnative.io/posts/understanding-cgroups-part-1-basics/">https://fuckcloudnative.io/posts/understanding-cgroups-part-1-basics/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Control Groups(控制组，简称 CGroups)&lt;/strong> 是一个 Linux 内核特性，用于限制、隔离一组进程集合的资源使用，资源包括 CPU、内存、磁盘 IO、网络 等。CGroups 由 Google 的两位工程师开发，自 2008 年 1 月发布的 Linux 2.6.24 版本的内核中提供此能力。到目前为止，CGroups 分 v1 和 v2 两个版本，v1 实现较早，功能比较多，但是由于它里面的功能都是零零散散的实现的，所以规划的不是很好，导致了一些使用和维护上的不便，v2 的出现就是为了解决 v1 中这方面的问题，在最新的 4.5 内核中，cgroup v2 声称已经可以用于生产环境了，但它所支持的功能还很有限，随着 v2 一起引入内核的还有 cgroup namespace。v1 和 v2 可以混合使用，但是这样会更复杂，所以一般没人会这样用。&lt;/p>
&lt;p>在 Linux 里，一直以来就有对进程进行分组的概念和需求，比如 session group， progress group 等，后来随着人们对这方面的需求越来越多，比如需要追踪一组进程的内存和 IO 使用情况等，于是出现了 cgroup，用来统一将进程进行分组，并在分组的基础上对进程进行监控和资源控制管理等。&lt;/p>
&lt;p>Cgroup 是 Linux kernel 的一项功能：它是在一个系统中运行的层级制进程组，你可对其进行资源分配（如 CPU 时间、系统内存、网络带宽或者这些资源的组合）。通过使用 cgroup，系统管理员在分配、排序、拒绝、管理和监控系统资源等方面，可以进行精细化控制。硬件资源可以在应用程序和用户间智能分配，从而增加整体效率。&lt;/p>
&lt;p>cgroup 和 namespace 类似，也是将进程进行分组，但它的目的和 namespace 不一样，namespace 是为了隔离进程组之间的资源，而 cgroup 是为了对一组进程进行统一的资源监控和限制。CGroup 还能对进程进行优先级设置、审计、以及将进程挂起和恢复等操作&lt;/p>
&lt;h2 id="术语">术语&lt;/h2>
&lt;p>cgroup 在不同的上下文中代表不同的意思，可以指整个 Linux 的 cgroup 技术，也可以指一个具体进程组。&lt;/p>
&lt;p>cgroup 是 Linux 下的一种将进程按组进行管理的机制，在用户层看来，cgroup 技术就是把系统中的所有进程组织成一颗一颗独立的树，每棵树都包含系统的所有进程，树的每个节点是一个进程组，而每颗树又和一个或者多个 subsystem 关联，树的作用是将进程分组，而 subsystem 的作用就是对这些组进行操作。&lt;/p>
&lt;p>cgroup 主要包括下面两部分：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>subsystem(子系统)&lt;/strong> # 一个 subsystem 就是一个内核模块，他被关联到一颗 cgroup 树之后，就会 在树的每个节点（进程组）上做具体的操作。subsystem 经常被称作 resource controller，因为它主要被用来调度或者限制每个进程组的资源，但是这个说法不完全准确，因为有时我们将进程分组只是为了做一些监控，观察一下他们的状态，比如 perf_event subsystem。到目前为止，Linux 支持 12 种 subsystem，比如限制 CPU 的使用时间，限制使用的内存，统计 CPU 的使用情况，冻结和恢复一组进程等，后续会对它们一一进行介绍。&lt;/li>
&lt;li>&lt;strong>hierarchy(层次结构)&lt;/strong> # 一个 hierarchy 可以理解为一棵 cgroup 树，树的每个节点就是一个进程组，每棵树都会与零到多个 subsystem 关联。在一颗树里面，会包含 Linux 系统中的所有进程，但每个进程只能属于一个节点（进程组）。系统中可以有很多颗 cgroup 树，每棵树都和不同的 subsystem 关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组，只是这些进程组和不同的 subsystem 关联。目前 Linux 支持 12 种 subsystem，如果不考虑不与任何 subsystem 关联的情况（systemd 就属于这种情况），Linux 里面最多可以建 12 颗 cgroup 树，每棵树关联一个 subsystem，当然也可以只建一棵树，然后让这棵树关联所有的 subsystem。当一颗 cgroup 树不和任何 subsystem 关联的时候，意味着这棵树只是将进程进行分组，至于要在分组的基础上做些什么，将由应用程序自己决定，systemd 就是一个这样的例子。&lt;/li>
&lt;/ul>
&lt;h2 id="cgroup-子系统类型">CGroup 子系统类型&lt;/h2>
&lt;p>可以通过 /proc/cgroups 文件查看当前系统支持哪些 subsystem：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/cgroups&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#subsys_name hierarchy num_cgroups enabled&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpuset 6 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu 8 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpuacct 8 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>blkio 4 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>memory 12 236 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>devices 11 95 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>freezer 9 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>net_cls 10 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>perf_event 5 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>net_prio 10 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hugetlb 2 5 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pids 3 103 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rdma 7 1 &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>整理一下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>subsys_name&lt;/th>
&lt;th>hierarchy&lt;/th>
&lt;th>num_cgroups&lt;/th>
&lt;th>enabled&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cpuset&lt;/td>
&lt;td>6&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu&lt;/td>
&lt;td>8&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpuacct&lt;/td>
&lt;td>8&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>blkio&lt;/td>
&lt;td>4&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory&lt;/td>
&lt;td>12&lt;/td>
&lt;td>236&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>devices&lt;/td>
&lt;td>11&lt;/td>
&lt;td>95&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>freezer&lt;/td>
&lt;td>9&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>net_cls&lt;/td>
&lt;td>10&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>perf_event&lt;/td>
&lt;td>5&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>net_prio&lt;/td>
&lt;td>10&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hugetlb&lt;/td>
&lt;td>2&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pids&lt;/td>
&lt;td>3&lt;/td>
&lt;td>103&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>rdma&lt;/td>
&lt;td>7&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>从左到右，字段的含义分别是：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>subsys_name&lt;/strong> # subsystem 的名字
&lt;ul>
&lt;li>blkio # 块设备 IO&lt;/li>
&lt;li>cpu # 基于 CFS 对 CPU 时间配额进行限制的子系统，CFS 概念详见：CPU 管理 章节中的 CFS 调度器。该子系统是 cgroup 对进程使用 CPU 资源进行限制的主要手段&lt;/li>
&lt;li>cpuacct # CPU 资源使用报告&lt;/li>
&lt;li>cpuset # 多处理器平台上的 CPU 集合&lt;/li>
&lt;li>devices # 设备访问&lt;/li>
&lt;li>freezer # 挂载器或恢复任务&lt;/li>
&lt;li>hungetlb #&lt;/li>
&lt;li>memory # 内存用量及报告&lt;/li>
&lt;li>net_cls # cgroup 中的任务创建的数据包的类别标识符&lt;/li>
&lt;li>net_prio #&lt;/li>
&lt;li>perf_event # 对 cgroup 中的任务进行统一性能测试&lt;/li>
&lt;li>pids #&lt;/li>
&lt;li>rdma #&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>hierarchy&lt;/strong> # subsystem 所关联到的 cgroup 树的 ID，如果多个 subsystem 关联到同一颗 cgroup 树，那么他们的这个字段将一样，比如这里的 cpu 和 cpuacct 就一样，表示他们绑定到了同一颗树。如果出现下面的情况，这个字段将为 0：
&lt;ul>
&lt;li>当前 subsystem 没有和任何 cgroup 树绑定&lt;/li>
&lt;li>当前 subsystem 已经和 cgroup v2 的树绑定&lt;/li>
&lt;li>当前 subsystem 没有被内核开启&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>num_cgroups&lt;/strong> # subsystem 所关联的 cgroup 树中进程组的个数，也即树上节点的个数&lt;/li>
&lt;li>&lt;strong>enabled&lt;/strong> # 1 表示开启，0 表示没有被开启(可以通过设置内核的启动参数“cgroup_disable”来控制 subsystem 的开启).&lt;/li>
&lt;/ul>
&lt;h1 id="cgroup-关联文件">CGroup 关联文件&lt;/h1>
&lt;h2 id="sysfscgroup--cgroup-根目录">/sys/fs/cgroup/* # CGroup 根目录。&lt;/h2>
&lt;p>CGroup 的相关操作都是基于内核中的 &lt;strong>CGroup Virtual Filesystem(控制组虚拟文件系统)&lt;/strong>。所以，使用 CGroup 首先需要挂载这个文件系统，通常，现代系统在启动时，都默认会挂载相关的 CGroup 文件系统：&lt;/p>
&lt;ul>
&lt;li>**CGroupV1，**该目录下的每个目录都是 CGroup 子系统的名称。其中包含该子系统中所关联的进程的资源控制信息。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># mount -t cgroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/systemd type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,xattr,name&lt;span style="color:#f92672">=&lt;/span>systemd&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/pids type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,pids&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/blkio type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,blkio&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,cpu,cpuacct&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup on /sys/fs/cgroup/memory type cgroup &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime,memory&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...... 略
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>**CGroupV2，**则只会有一个 cgroup2 on /sys/fs/cgroup type cgroup2 (&amp;hellip;&amp;hellip;) 的挂载项&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># mount -t cgroup2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup2 on /sys/fs/cgroup type cgroup2 &lt;span style="color:#f92672">(&lt;/span>rw,nosuid,nodev,noexec,relatime&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里面的 &lt;strong>/sys/fs/cgroup&lt;/strong> 目录，就称为 **CGroup 的根目录。CGroup 文件系统的 V1 与 V2 的根目录下的内容，各不相同，详见 &lt;strong>[&lt;/strong>《CGroup FS》 **](✏IT 学习笔记/☁️10.云原生/2.1.容器/2.CGroup/CGroup%20FS.md FS.md)&lt;strong>章节&lt;/strong>&lt;/p>
&lt;h2 id="procpidcgroup--进程号为-pid-的进程所属的-cgroup-信息">/proc/PID/cgroup # 进程号为 PID 的进程所属的 cgroup 信息。&lt;/h2>
&lt;p>在** &lt;strong>/proc/PID/cgroup&lt;/strong> **文件中会指定进程所使用的 CGropu 的相对路径。文件中每行都是进程所属的 CGroup 子系统，每行子系统信息由以 &lt;code>:&lt;/code> 分割的三个字段组成&lt;/p>
&lt;ul>
&lt;li>&lt;strong>hierarchy-ID&lt;/strong> # Hierarchy 唯一标识符。与 /proc/cgroups 文件中的 Hierarchy ID 相同。
&lt;ul>
&lt;li>CGroup v2 版本该字段始终为 0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>controller-list&lt;/strong> # 绑定到 Hierarchy ID 的控制器列表。也就是 CGroup 的子系统。
&lt;ul>
&lt;li>CGroup v2 版本该字段为空&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>cgroup-path&lt;/strong> # 进程所属 CGroup 子系统的信息的路径。这是一个相对路径。
&lt;ul>
&lt;li>这里面的 &lt;code>/&lt;/code> 就是指 CGroup 的根节点中对应子系统的目录
&lt;ul>
&lt;li>对于 CGroupV1 来说通常是 /sys/fs/cgroup/SUBSYSTEM。所以，一个完整的 cgroup-path 应该是 &lt;code>/sys/fs/cgroup/SUBSYSTEM/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope/*&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>cgroup 的 v1 和 v2 版本显示的信息不同&lt;/p>
&lt;h3 id="cgroupv1">CGroupV1&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/1185/cgroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>12:memory:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>11:devices:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10:net_cls,net_prio:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>9:freezer:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>8:cpu,cpuacct:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>7:rdma:/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>6:cpuset:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>5:perf_event:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>4:blkio:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3:pids:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2:hugetlb:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>1:name&lt;span style="color:#f92672">=&lt;/span>systemd:/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0::/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>比如，1185 进程的 cpu 子系统的 CGroup 信息，就在 &lt;code>/sys/fs/cgroup/cpu/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope/&lt;/code> 目录中：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls /sys/fs/cgroup/cpu/system.slice/docker-b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460.scope&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.clone_children cpuacct.usage cpuacct.usage_percpu_sys cpuacct.usage_user cpu.shares cpu.uclamp.min
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.procs cpuacct.usage_all cpuacct.usage_percpu_user cpu.cfs_period_us cpu.stat notify_on_release
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpuacct.stat cpuacct.usage_percpu cpuacct.usage_sys cpu.cfs_quota_us cpu.uclamp.max tasks
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="cgropuv2">CGropuV2&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># cat /proc/1277/cgroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0::/system.slice/docker-020cfdfbd4cd43981570f4fa7def9a2b600025b2e60e3150e742a5049562f30f.scope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>比如，1277 进程的 CGroup 信息，就在 &lt;code>/sys/fs/cgroup/system.slice/docker-020cfdfbd4cd43981570f4fa7def9a2b600025b2e60e3150e742a5049562f30f.scope/&lt;/code> 目录中：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># ls /sys/fs/cgroup/system.slice/docker-020cfdfbd4cd43981570f4fa7def9a2b600025b2e60e3150e742a5049562f30f.scope/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.controllers cgroup.procs cpu.max cpuset.mems cpu.weight io.weight memory.low memory.stat rdma.max
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.events cgroup.stat cpu.pressure cpuset.mems.effective cpu.weight.nice memory.current memory.max pids.current
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.freeze cgroup.subtree_control cpuset.cpus cpu.stat io.max memory.events memory.min pids.events
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.max.depth cgroup.threads cpuset.cpus.effective cpu.uclamp.max io.pressure memory.events.local memory.oom.group pids.max
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cgroup.max.descendants cgroup.type cpuset.cpus.partition cpu.uclamp.min io.stat memory.high memory.pressure rdma.current
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="其他文件">其他文件&lt;/h2>
&lt;p>&lt;strong>/proc/cgroups&lt;/strong> # 当前系统支持的所有 CGroup 子系统&lt;/p>
&lt;h1 id="systemd-的-slice-单元">systemd 的 slice 单元&lt;/h1>
&lt;p>在 Systemd 作为 1 号进程的系统中，进程的 CGroup 都可以配置为由 Systemd 管理，其中 Slice 类型的单元就是用来控制 CGroup 的。默认会创建 3 个顶级 Slice&lt;/p>
&lt;ul>
&lt;li>&lt;strong>system.slice&lt;/strong> # 所有 Service Unit 的默认。&lt;/li>
&lt;li>&lt;strong>user.lice&lt;/strong> # 所有用户进程的默认。&lt;/li>
&lt;li>&lt;strong>machine.slice&lt;/strong> # 所有虚拟机和容器的默认。&lt;/li>
&lt;/ul>
&lt;p>&lt;code>systemd-cgls&lt;/code> 命令可以查看 CGroup 的层次结构&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># systemd-cgls&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Control group /:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├─931 bpfilter_umh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├─user.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─user-1000.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─user@1000.service …
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ └─init.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─81271 /lib/systemd/systemd --user
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ └─81276 &lt;span style="color:#f92672">(&lt;/span>sd-pam&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─session-431.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─81902 sshd: lichenhao &lt;span style="color:#f92672">[&lt;/span>priv&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─81998 sshd: lichenhao@pts/1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82001 -bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82100 su - root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82101 -bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ ├─82697 systemd-cgls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ │ └─82698 pager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─session-432.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─82013 sshd: lichenhao &lt;span style="color:#f92672">[&lt;/span>priv&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├─82097 sshd: lichenhao@notty
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─82098 /usr/lib/openssh/sftp-server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├─init.scope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └─1 /sbin/init nospectre_v2 nopti noibrs noibpb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└─system.slice
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├─irqbalance.service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └─524 /usr/sbin/irqbalance --foreground
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├─uniagent.service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └─537 /usr/local/uniagent/bin/uniagent
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├─containerd.service …
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├─ &lt;span style="color:#ae81ff">714&lt;/span> /usr/bin/containerd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├─ &lt;span style="color:#ae81ff">1140&lt;/span> /usr/bin/containerd-shim-runc-v2 -namespace moby -id b8f92f970f0d17377e7ad4c9b75f8316cdb15a6dd7dd81466f415e6fcaed6460 -address /run/containerd/containerd.sock
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├─31778 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 4c5ec4bc9717bb9fd2a2ea7b507ac3c0e16da95fa87974152f0fe3b3a653cef9 -address /run/containerd/containerd.sock
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>......
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>systemd-cgtop&lt;/code> 命令可以查看 CGroup 的动态信息。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Control Group Tasks %CPU Memory Input/s Output/s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/ &lt;span style="color:#ae81ff">221&lt;/span> 1.0 3.1G - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>user.slice &lt;span style="color:#ae81ff">11&lt;/span> 0.7 1.5G - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice &lt;span style="color:#ae81ff">139&lt;/span> 0.4 1.2G - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/containerd.service &lt;span style="color:#ae81ff">46&lt;/span> 0.2 276.0M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/cloudResetPwdUpdateAgent.service &lt;span style="color:#ae81ff">18&lt;/span> 0.2 102.4M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/docker-4c5ec4…d2a2ea7b507ac3c0e16da95fa87974152f0fe3b3a653cef9.scope &lt;span style="color:#ae81ff">1&lt;/span> 0.1 1.3M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/multipathd.service &lt;span style="color:#ae81ff">7&lt;/span> 0.0 13.8M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>init.scope &lt;span style="color:#ae81ff">1&lt;/span> - 7.6M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/ModemManager.service &lt;span style="color:#ae81ff">3&lt;/span> - 6.8M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/NetworkManager.service &lt;span style="color:#ae81ff">3&lt;/span> - 13.6M - -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>system.slice/accounts-daemon.service &lt;span style="color:#ae81ff">3&lt;/span> - 6.5M - -
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="cgroupv2">CGroupV2&lt;/h1>
&lt;h2 id="检查-cgroup-v2-是否已启用">检查 cgroup v2 是否已启用&lt;/h2>
&lt;p>如果 &lt;code>/sys/fs/cgroup/cgroup.controllers&lt;/code> 存在于您的系统上，则您使用的是 v2，否则您使用的是 v1。
已知以下发行版默认使用 cgroup v2：&lt;/p>
&lt;ul>
&lt;li>Fedora（31 起）&lt;/li>
&lt;li>Arch Linux（自 2021 年 4 月起）&lt;/li>
&lt;li>openSUSE Tumbleweed（自 2021 年起）&lt;/li>
&lt;li>Debian GNU/Linux（从 11 开始）&lt;/li>
&lt;li>Ubuntu（自 21.10 起）&lt;/li>
&lt;/ul>
&lt;h2 id="启用-cgroup-v2">启用 cgroup v2&lt;/h2>
&lt;p>为容器启用 cgroup v2 需要内核 4.15 或更高版本。建议使用内核 5.2 或更高版本。
然而，将 cgroup v2 控制器委派给非 root 用户需要最新版本的 systemd。建议使用 systemd 244 或更高版本。
要使用 cgroup v2 引导主机，请将以下字符串添加到 GRUB_CMDLINE_LINUXin 行/etc/default/grub，然后运行 sudo update-grub.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>systemd.unified_cgroup_hierarchy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="启用-cpucpuset-和-io-委派">启用 CPU、CPUSET 和 I/O 委派&lt;/h2>
&lt;p>默认情况下，非 root 用户只能获取 memory 控制器和 pids 要委托的控制器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ cat /sys/fs/cgroup/user.slice/user-&lt;span style="color:#66d9ef">$(&lt;/span>id -u&lt;span style="color:#66d9ef">)&lt;/span>.slice/user@&lt;span style="color:#66d9ef">$(&lt;/span>id -u&lt;span style="color:#66d9ef">)&lt;/span>.service/cgroup.controllers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>memory pids
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>要允许委派其他控制器，例如 cpu、cpuset 和 io，请运行以下命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ sudo mkdir -p /etc/systemd/system/user@.service.d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cat &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF | sudo tee /etc/systemd/system/user@.service.d/delegate.conf
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[Service]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">Delegate=cpu cpuset io memory pids
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo systemctl daemon-reload
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>建议使用委派 cpuset 以及 cpu. 委派 cpuset 需要 systemd 244 或更高版本。
更改 systemd 配置后，您需要重新登录或重新启动主机。建议重启主机。&lt;/p></description></item><item><title>Docs: 2.Kubelet 节点代理</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/2.kubelet-%E8%8A%82%E7%82%B9%E4%BB%A3%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">官方文档，参考-组件工具-kubelet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubelet 是在每个节点上运行的主要“节点代理”。它可以使用以下之一向 APIServer 注册节点：用于覆盖主机名的标志；或云提供商的特定逻辑。&lt;/p>
&lt;p>kubelet 根据 PodSpec 起作用。 PodSpec 是一个描述 Pod 的 YAML 或 JSON 对象。 kubelet 接受通过各种机制（主要是通过 apiserver）提供的一组 PodSpec，并确保这些 PodSpec 中描述的容器正在运行且运行状况良好。 Kubelet 不管理不是 Kubernetes 创建的容器。一般情况， PodSpec 都是由在 k8s 对象的 yaml 文件中定义的。&lt;/p>
&lt;p>kubelet 负责维护容器(CNI)的生命周期，同时也负责 Volume（CVI）和 Network（CNI）的管理。kubernetes 集群的宿主机上，启动的每一个 pod 都有由 kubelet 这个组件管理的。&lt;/p>
&lt;p>kubelet 在每个 Node 上都会启动一个 kubelet daemon 进程，默认监听在 &lt;strong>10250&lt;/strong> 端口。该进程用于处理 Master 节点(主要是 apiserver)下发到本节点的任务，管理 Pod 以及 Pod 中的容器。每个 kubelet 进程会在 APIServer 上注册节点自身信息，定期向 Master 节点汇报节点资源的使用情况，并通过 cAdvisor(kubelet 内部功能) 监控容器和节点资源。10248 为 kubelet 健康检查的 healthz 端口&lt;/p>
&lt;p>Note:如果 master 节点不运行 pod 的话，是不用部署 kubelet 的。&lt;/p>
&lt;p>kubelet 使用 PodSpec 来对其所在节点的 Pod 进行管理，PodSpec(Pod Specification)是描述 pod 的 yaml 或者 json 对象。PodSpec 一般是 yaml 或者 json 格式的文本文件。这些 PodSpecs 有 4 个来源&lt;/p>
&lt;ol>
&lt;li>apiserver：使用最多的方式，通过 kubectl 命令向 apiserver 提交 PodSpec 文件，然后 apiserver 再下发给相应节点的 node。还有一个通过 APIServer 监听 etcd 目录，同步 PodSpec&lt;/li>
&lt;li>File：kubelet 定期监控某个路径(默认路径为/etc/kubernetes/manifests)下所有文件，把这些文件当做 PodSpec，这种方式也就是所谓的 staticPod(静态 Pod)。默认情况下每 20 秒监控一下，可以通过 flag 进行配置，配置时可以指定具体的路径以及监控周期&lt;/li>
&lt;li>HTTP endpoint(URL)：使用&amp;ndash;manifest-url 参数，让 kubelet 每 20 秒检查一次 URL 指定的 endpoint(端点)&lt;/li>
&lt;li>HTTP server：kubelet 监听 HTTP 请求，并响应简单的 API 以提交新的 Pod 清单&lt;/li>
&lt;/ol>
&lt;h2 id="static-pod静态-pod">Static Pod：静态 Pod&lt;/h2>
&lt;p>所有以非 API Server 方式创建的 Pod 都叫 Static Pod。&lt;/p>
&lt;p>kubelet 的工作核心，就是一个控制循环。驱动这个控制循环运行的实践，包括四种&lt;/p>
&lt;ol>
&lt;li>Pod 更新事件&lt;/li>
&lt;li>Pod 生命周期变化&lt;/li>
&lt;li>kubelet 本身设置的执行周期&lt;/li>
&lt;li>定时的清理事件&lt;/li>
&lt;/ol>
&lt;p>注意：kubelet 调用下层容器运行时的执行过程，并不会直接调用 Docker 的 API，而是通过一组叫作 CRI（Container Runtime Interface，容器运行时接口）的 gRPC 接口来间接执行的。gRPC 接口规范详见官网：&lt;a href="https://grpc.io/docs/">https://grpc.io/docs/&lt;/a>&lt;/p>
&lt;p>kubelet 是集群的基础设施，其他主要组件如果不以 daemon 形式运行，则依赖 kubelet 以 pod 方式启动，这样，才可以组成集群最基本的形态。&lt;/p>
&lt;h2 id="kubelet-metrics">Kubelet Metrics&lt;/h2>
&lt;p>详见：k8s 主要组件 metrics 获取指南&lt;/p>
&lt;h1 id="kubelet-部署">Kubelet 部署&lt;/h1>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubelet
&lt;/code>&lt;/pre>
&lt;h1 id="kubelet-关联文件与配置">Kubelet 关联文件与配置&lt;/h1>
&lt;p>kubelet 可以通过多个地方读取其自身的配置并更改自己的行为方式，可以通过指定的 yaml 格式的文件读取配置信息，也可以直接指定命令行参数传递到 kubelet 程序中。&lt;/p>
&lt;p>&lt;strong>/var/lib/kubelet/*&lt;/strong> # kubelet 配置文件目录、以及运行时数据目录，包含基础配置文件、证书、通过 kubelet 启动的容器信息等等&lt;/p>
&lt;ul>
&lt;li>./config.yaml # kubelet 基础配置文件。一般在 kubelet 启动时使用 &amp;ndash;cofnig 参数指定该读取该文件的路径进行加载。
&lt;ul>
&lt;li>Note：该文件内容与 kubectl get configmap -n kube-system kubelet-config-X.XX -o yaml 命令所得结果一样
&lt;ul>
&lt;li>如果想要在 kubelet 运行时动态得更改其配置，则可以修改 configmap 中的内容，详见：&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>./kubeadm-flags.env # 该文件将内容作为 kubelet 参数，在 kubelet 启动时加载，常用来在 kubeadm 初始化时使用&lt;/li>
&lt;li>./pods/* # kubelet 启动的 Pod 的数据保存路径，其内目录名为 Pod 的 uid 。
&lt;ul>
&lt;li>./${POD_UID}/volumes/* # 对应 pod 挂载的 volume 保存路径，其内目录为 kubernetes.io~TYPE ，其中 TYPE 为 volume 的类型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>./pki/* # kubelet 与 apiserver 交互时所用到的证书存放目录。
&lt;ul>
&lt;li>./kubelet.crt # 在 kubelet 完成 TLS bootstrapping 后并且没有配置 &amp;ndash;feature-gates=RotateKubeletServerCertificate=true 时生成；这种情况下该文件为一个独立于 apiserver CA 的自签 CA 证书，有效期为 1 年；被用作 kubelet 10250 api 端口。当其他东西需要访问 kubelet 的 api 时，需要使用该证书作为认证。&lt;/li>
&lt;li>./kubelet-client-current.pem # 与 API server 通讯所用到的证书，与 apiserver 交互后生成。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/kubernetes/*&lt;/strong> # Kubernetes 系统组件运行时目录。&lt;/p>
&lt;ul>
&lt;li>./manifests/* # Kubelet 默认从该目录中读取 Pod 的 Manifests 文件，以运行静态类型 Pod。&lt;/li>
&lt;li>kubelet 在 k8s 集群交互时认证文件所在目录，kubelet 需要读取认证配置，用来与 apiserver 进行交互。
&lt;ul>
&lt;li>./bootstrap-kubelet.conf # 用于 TLS 引导程序的 KubeConfig 文件。该 kubeconfig 文件的用户信息为 token。该文件用于 kubelet 所在节点不在集群中时，向集群发起注册请求所用，如果节点已在集群中，则会自动生成 kubelet.conf 文件&lt;/li>
&lt;li>./kubelet.conf # 具有唯一 kubelet 标识的 KubeConfig 文件(与 kubectl 的 config 文件一样，用于 kubelet 与 apiserver 交互时提供认证信息)。该 kubeconfig 文件的用户信息为客户端证书和私钥，一般在 kubelet 启动时由 bootstrap-kubelet.conf 文件生成。
&lt;ul>
&lt;li>当该文件不存在时，会在 kubelet 启动时，由 bootstrap-kubelet.confg 生成&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/sysconfig/kubelet&lt;/strong> # 与 /var/lib/kubelet/kubeadm-flags.env 文件作用一样 ，将内容作为 kubelet 参数，在 kubelet 启动时加载。一般用于让用户指定 kubelet 的运行时参数 。 KUBELET_EXTRA_ARGS 在标志链中排在最后，并且在其他设置冲突时具有最高优先级。&lt;/p>
&lt;ul>
&lt;li>Note：对于 DEB 系统，配置文件位于：/etc/default/kubelet&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.confg&lt;/strong> #与 kubelet 守护进程运行参数&lt;/p>
&lt;h1 id="kubelet-的启动过程--kubelet-与-apiserver-的交互说明">Kubelet 的启动过程 &amp;amp;&amp;amp; Kubelet 与 APIServer 的交互说明&lt;/h1>
&lt;h2 id="kubelet-启动过程">kubelet 启动过程：&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>kubelet 启动流程源码分析&lt;/li>
&lt;li>&lt;a href="https://xiaohanliang.gitbook.io/notes/k8s/zhi-shi-yu-ding-yi/jie-dian-zu-jian-kubelet/kubelet-qi-dong-liu-cheng">https://xiaohanliang.gitbook.io/notes/k8s/zhi-shi-yu-ding-yi/jie-dian-zu-jian-kubelet/kubelet-qi-dong-liu-cheng&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.jianshu.com/p/e07d84cce9f9">https://www.jianshu.com/p/e07d84cce9f9&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ol>
&lt;li>读取配置 kubelet 配置文件。kubelet 启动时首先会根据 &lt;code>--config=PATH&lt;/code> 参数指定路径(默认 /var/lib/kubelet/config.yaml)读取配置文件，并根据其内配置加载 kubelet 相关参数，
&lt;ol>
&lt;li>根据 ca 配置路径加载 ca.crt 文件，并在 /var/lib/kubelet/pki 目录下生成关于 kubelet 的 10250 私有 api 端口所需的 crt 与 key 文件。&lt;/li>
&lt;li>如果该文件不存在或有问题，则启动失败。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>配置与 apiserver 通信的 kubeconfig 文件。根据 &amp;ndash;bootstrap-kubeconfig=PATH 参数加载 /etc/kubernetes/bootstrap-kubelet.conf 文件(如果不存在则根据 &amp;ndash;kubeconfig=PATH 参数加载 /etc/kubernetes/kubelet.conf 文件)，两个文件都不存在则报错
&lt;ol>
&lt;li>如果当前节点不在集群中，则会执行证书申请操作
&lt;ol>
&lt;li>首先 kubelet 向 bootstrap-kubelet.conf 文件内配置的 apiserver(文件中 server 的配置) 发送加入集群的申请申，同时 kubelet 会根据 bootstrap-kubelet.conf 文件生成 kubelet.conf 文件，将该 kubeconfig 文件中的 user 认证方式改为证书认证方式，并指定证书路径为/var/lib/kubelet/pki/kubelet-client-current.pem。&lt;/li>
&lt;li>在集群 master 上执行 kubectl get csr 命令获取当前申请列表，并使用 kubectl certificate approve XXXX 命令通过该节点的申请&lt;/li>
&lt;li>master 节点的 controller-manager 处理完该请求后，本地 kubelet 将生成 kubelet.conf 文件所需证书，并保存在 /var/lib/kubelet/pki/ 目录下，以 /var/lib/kubelet/pki/kubelet-client-current.pem-TIME 命名，并建立软件链接指向该文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>如果当前节点已在集群中，则会根据 bootstrap-kubelet.conf 文件生成 kubelet.conf 文件，且根据 kubelet.conf 文件中的证书信息与 apiserver 进行通信。&lt;/li>
&lt;li>如果当前节点已在集群且存在 kubelet.conf 文件，则使用该 kubeconfig 文件与 apiserver 进行交互后执行后续操作&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>初始化 kubelet 组件内部模块
&lt;ol>
&lt;li>会在 /var/lib/kubelet 目录下添加相关文件，用以驱动 pod 及 kubelet 相关功能。&lt;/li>
&lt;li>检查 cgroup 驱动设置与 CRI 的 cgroup 驱动设置是否一致，如果不一致则启动失败&lt;/li>
&lt;li>等等操作，后续有发现再补充&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>启动 kubelet 内部模块及服务(Note：当 csr 申请发送后，即可执行后续工作，无需等待申请审批通过)&lt;/li>
&lt;/ol>
&lt;h2 id="kubelet-启动后的工作原理">kubelet 启动后的工作原理&lt;/h2>
&lt;p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。如下图所示：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rf1qwi/1616120074647-12f084c2-e91c-475d-93d4-f4e3fd3e61ef.png" alt="">&lt;/p>
&lt;h3 id="kubelet-创建-pod-流程">kubelet 创建 pod 流程&lt;/h3>
&lt;p>参考：
&lt;a href="https://www.jianshu.com/p/5e0c9d1dbe95">https://www.jianshu.com/p/5e0c9d1dbe95&lt;/a>
&lt;a href="https://www.kubernetes.org.cn/6766.html">https://www.kubernetes.org.cn/6766.html&lt;/a>
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rf1qwi/1616120074625-d25d5fad-d58b-4142-958d-b1aee16d8e71.png" alt="">&lt;/p>
&lt;p>Note：注意 14，,15 步，kubelet 会先将生成配置(volume 挂载、配置主机名等等)，才会去启动 pod，哪怕 pod 启动失败，挂载依然存在。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rf1qwi/1616120074636-5f1f460b-6cf2-4190-ab01-6284178805e6.png" alt="">&lt;/p>
&lt;h1 id="kubelet-所管理三大板块">Kubelet 所管理三大板块&lt;/h1>
&lt;h2 id="kubelet-负责所在节点-containercri-的管理">kubelet 负责所在节点 Container(CRI) 的管理&lt;/h2>
&lt;p>CRI 的起源：
官方介绍：&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&lt;/a>&lt;/p>
&lt;p>Kubernetes 项目之所以要在 kubelet 中引入这样一层单独的抽象，是为了对 Kubernetes 屏蔽下层容器运行时的差异。因为 Kubernets 是一个编排工具，不只可以编排 Docker，还可以编排除 Docker 以外的其余的容器项目。而每种容器项目的实现方式不尽相同，为了解决这个问题，那么可以把 kubelet 对容器的操作，统一抽象成一个借口，这样，kubelet 就只需要跟这个借口打交道，而作为具体的容器项目，它们只需要自己提供一个该接口的实现，然后对 kubelet 暴露出 gRPC 服务即可(docker shim 现在集成在了 kubelet 中，以后会单独拿出来甚至废弃)&lt;/p>
&lt;p>CRI 的运作方式：&lt;/p>
&lt;ul>
&lt;li>当 kubernetes 通过编排能力声明一个 Pod 后，调度器会为这个 pod 选择一个具体的 Node 来运行，这时候，该 Node 上的 kubelet 会通过 SyncLoop 判断需要执行的具体操作，这个时候 kubelet 会调用一个叫做 GenericRuntime 的通用组件来发起创建 Pod 的 CRI 请求。&lt;/li>
&lt;li>CRI shim(CRI 垫片，宿主机与 kubelet 之间的东西)来响应 CRI 请求，然后把请求“翻译”成对后端容器项目的请求或者操作。&lt;/li>
&lt;li>每个容器项目都会自己实现一个 CRI shim，然后 CRI shim 收到的请求会转给对应的容器守护进程(e.g.docker 项目里的 dockerd)，由该守护进程进行容器的创建、更改、删除、exec 等操作&lt;/li>
&lt;/ul>
&lt;p>当前主流的 CRI 有如下几种：&lt;/p>
&lt;ul>
&lt;li>Docker(kubelet 默认的 CRI)
&lt;ul>
&lt;li>Note：kubelet 内置 dockershim，在启动或，会生成 dockersim.sock 文件，kubelet 与 crictl 都会默认与该文件关联&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CRI-O&lt;/li>
&lt;li>Containerd(通常与 docker 同时安装)&lt;/li>
&lt;li>Frakti(kata OCI 的实现)&lt;/li>
&lt;/ul>
&lt;p>kubelet 与 CRI 对接的方式&lt;/p>
&lt;ul>
&lt;li>kubelet 根据参数 &amp;ndash;container-runtime-endpoint 来决定其所绑定的 CRI sock。默认使用 docker 的 sock，路径为：/var/run/dockershim.sock
&lt;ul>
&lt;li>可以通过 crictl 工具来测试目标 sock 是否可用，crictl 用法详见：crictl 命令行工具&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>如果使用不同的 CRI 运行时，则需要为 kubelet 指定不同的 flag。 例如，当使用非 docker CRI 时， 则需要使用 &amp;ndash;container-runtime=remote 与 &amp;ndash;container-runtime-path-endpoint=&lt;!-- raw HTML omitted --> 指定 CRI 端点。endpoint 的值为指定 CRI 的 sock 文件。&lt;/li>
&lt;li>各个 CRI 需要进行配置才可与 kubelet 对接成功，如果不进行初始化配置，则 kubelet 无法获取到该 CRI 对于 k8s 的相关配置参数&lt;/li>
&lt;/ul>
&lt;p>kubelet 通过如下两个命令行标志来指定要使用的 CRI&lt;/p>
&lt;pre>&lt;code>--container-runtime=remote
--container-runtime-endpoint=unix:///run/containerd/containerd.sock
&lt;/code>&lt;/pre>
&lt;h2 id="kubelet-负责所在节点-networkcni-的管理">kubelet 负责所在节点 Network(CNI) 的管理&lt;/h2>
&lt;p>kubelet 对 cni(Container Network Interface 容器网络接口)的调用详见另一片关于网络介绍的文章：&lt;a href="https://www.yuque.com/go/doc/33164217">Kubernetes 的网络&lt;/a>&lt;/p>
&lt;p>kubelet 配置 pod 网络时，首先会读取下 /etc/cni/net.d/_ 目录下的配置，查看当前所使用的 CNI 插件及插件参数，比如现在是 flannel ，那么 flannel 会将 /run/flannel/subnet.env 文件的配置信息传递给 kubelet ，然后 kubelet 使用 /opt/cni/bin/_ 目录中的二进制文件，来处理处理 pod 的网络信息。&lt;/p>
&lt;h2 id="kubelet-负责所在节点-volumecvi-的管理">kubelet 负责所在节点 Volume(CVI) 的管理&lt;/h2></description></item><item><title>Docs: 2.Linux 上抽象网络设备的原理及使用</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/2.linux-%E4%B8%8A%E6%8A%BD%E8%B1%A1%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/network-virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/2.linux-%E4%B8%8A%E6%8A%BD%E8%B1%A1%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/</guid><description>
&lt;h1 id="linux-抽象网络设备简介">Linux 抽象网络设备简介&lt;/h1>
&lt;p>和磁盘设备类似，Linux 用户想要使用网络功能，不能通过直接操作硬件完成，而需要直接或间接的操作一个 Linux 为我们抽象出来的设备，既通用的 Linux 网络设备来完成。一个常见的情况是，系统里装有一个硬件网卡，Linux 会在系统里为其生成一个网络设备实例，如 eth0，用户需要对 eth0 发出命令以配置或使用它了。更多的硬件会带来更多的设备实例，虚拟的硬件也会带来更多的设备实例。随着网络技术，虚拟化技术的发展，更多的高级网络设备被加入了到了 Linux 中，使得情况变得更加复杂。在以下章节中，将一一分析在虚拟化技术中经常使用的几种 Linux 网络设备抽象类型：Bridge、802.1.q VLAN device、VETH、TAP，详细解释如何用它们配合 Linux 中的 Route table、IP table 简单的创建出本地虚拟网络。&lt;/p>
&lt;h2 id="相关网络设备工作原理">相关网络设备工作原理&lt;/h2>
&lt;h3 id="bridge">Bridge&lt;/h3>
&lt;p>Bridge（桥）是 Linux 上用来做 TCP/IP 二层协议交换的设备，与现实世界中的交换机功能相似。Bridge 设备实例可以和 Linux 上其他网络设备实例连接，既 attach 一个从设备，类似于在现实世界中的交换机和一个用户终端之间连接一根网线。当有数据到达时，Bridge 会根据报文中的 MAC 信息进行广播、转发、丢弃处理。&lt;/p>
&lt;p>图 1.Bridge 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257424-79688f5a-dd7b-4f17-9afe-18d24df26e54.png" alt="">&lt;/p>
&lt;p>如图所示，Bridge 的功能主要在内核里实现。当一个从设备被 attach 到 Bridge 上时，相当于现实世界里交换机的端口被插入了一根连有终端的网线。这时在内核程序里，netdev_rx_handler_register()被调用，一个用于接受数据的回调函数被注册。以后每当这个从设备收到数据时都会调用这个函数可以把数据转发到 Bridge 上。当 Bridge 接收到此数据时，br_handle_frame()被调用，进行一个和现实世界中的交换机类似的处理过程：判断包的类别（广播/单点），查找内部 MAC 端口映射表，定位目标端口号，将数据转发到目标端口或丢弃，自动更新内部 MAC 端口映射表以自我学习。&lt;/p>
&lt;p>Bridge 和现实世界中的二层交换机有一个区别，图中左侧画出了这种情况：数据被直接发到 Bridge 上，而不是从一个端口接受。这种情况可以看做 Bridge 自己有一个 MAC 可以主动发送报文，或者说 Bridge 自带了一个隐藏端口和寄主 Linux 系统自动连接，Linux 上的程序可以直接从这个端口向 Bridge 上的其他端口发数据。所以当一个 Bridge 拥有一个网络设备时，如 bridge0 加入了 eth0 时，实际上 bridge0 拥有两个有效 MAC 地址，一个是 bridge0 的，一个是 eth0 的，他们之间可以通讯。由此带来一个有意思的事情是，Bridge 可以设置 IP 地址。通常来说 IP 地址是三层协议的内容，不应该出现在二层设备 Bridge 上。但是 Linux 里 Bridge 是通用网络设备抽象的一种，只要是网络设备就能够设定 IP 地址。当一个 bridge0 拥有 IP 后，Linux 便可以通过路由表或者 IP 表规则在三层定位 bridge0，此时相当于 Linux 拥有了另外一个隐藏的虚拟网卡和 Bridge 的隐藏端口相连，这个网卡就是名为 bridge0 的通用网络设备，IP 可以看成是这个网卡的。当有符合此 IP 的数据到达 bridge0 时，内核协议栈认为收到了一包目标为本机的数据，此时应用程序可以通过 Socket 接收到它。一个更好的对比例子是现实世界中的带路由的交换机设备，它也拥有一个隐藏的 MAC 地址，供设备中的三层协议处理程序和管理程序使用。设备里的三层协议处理程序，对应名为 bridge0 的通用网络设备的三层协议处理程序，即寄主 Linux 系统内核协议栈程序。设备里的管理程序，对应 bridge0 寄主 Linux 系统里的应用程序。&lt;/p>
&lt;p>Bridge 的实现当前有一个限制：当一个设备被 attach 到 Bridge 上时，那个设备的 IP 会变的无效，Linux 不再使用那个 IP 在三层接受数据。举例如下：如果 eth0 本来的 IP 是 192.168.1.2，此时如果收到一个目标地址是 192.168.1.2 的数据，Linux 的应用程序能通过 Socket 操作接受到它。而当 eth0 被 attach 到一个 bridge0 时，尽管 eth0 的 IP 还在，但应用程序是无法接受到上述数据的。此时应该把 IP 192.168.1.2 赋予 bridge0。&lt;/p>
&lt;p>另外需要注意的是数据流的方向。对于一个被 attach 到 Bridge 上的设备来说，只有它收到数据时，此包数据才会被转发到 Bridge 上，进而完成查表广播等后续操作。当请求是发送类型时，数据是不会被转发到 Bridge 上的，它会寻找下一个发送出口。用户在配置网络时经常忽略这一点从而造成网络故障。&lt;/p>
&lt;h3 id="vlan-device-for-8021q">VLAN device for 802.1.q&lt;/h3>
&lt;p>VLAN 又称虚拟网络，是一个被广泛使用的概念，有些应用程序把自己的内部网络也称为 VLAN。此处主要说的是在物理世界中存在的，需要协议支持的 VLAN。它的种类很多，按照协议原理一般分为：MACVLAN、802.1.q VLAN、802.1.qbg VLAN、802.1.qbh VLAN。其中出现较早，应用广泛并且比较成熟的是 802.1.q VLAN，其基本原理是在二层协议里插入额外的 VLAN 协议数据（称为 802.1.q VLAN Tag)，同时保持和传统二层设备的兼容性。Linux 里的 VLAN 设备是对 802.1.q 协议的一种内部软件实现，模拟现实世界中的 802.1.q 交换机。&lt;/p>
&lt;p>图 2 .VLAN 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257463-a0ff1a57-bfc0-40ea-b486-e6b2a4081b2e.png" alt="">&lt;/p>
&lt;p>如图所示，Linux 里 802.1.q VLAN 设备是以母子关系成对出现的，母设备相当于现实世界中的交换机 TRUNK 口，用于连接上级网络，子设备相当于普通接口用于连接下级网络。当数据在母子设备间传递时，内核将会根据 802.1.q VLAN Tag 进行对应操作。母子设备之间是一对多的关系，一个母设备可以有多个子设备，一个子设备只有一个母设备。当一个子设备有一包数据需要发送时，数据将被加入 VLAN Tag 然后从母设备发送出去。当母设备收到一包数据时，它将会分析其中的 VLAN Tag，如果有对应的子设备存在，则把数据转发到那个子设备上并根据设置移除 VLAN Tag，否则丢弃该数据。在某些设置下，VLAN Tag 可以不被移除以满足某些监听程序的需要，如 DHCP 服务程序。举例说明如下：eth0 作为母设备创建一个 ID 为 100 的子设备 eth0.100。此时如果有程序要求从 eth0.100 发送一包数据，数据将被打上 VLAN 100 的 Tag 从 eth0 发送出去。如果 eth0 收到一包数据，VLAN Tag 是 100，数据将被转发到 eth0.100 上，并根据设置决定是否移除 VLAN Tag。如果 eth0 收到一包包含 VLAN Tag 101 的数据，其将被丢弃。上述过程隐含以下事实：对于寄主 Linux 系统来说，母设备只能用来收数据，子设备只能用来发送数据。和 Bridge 一样，母子设备的数据也是有方向的，子设备收到的数据不会进入母设备，同样母设备上请求发送的数据不会被转到子设备上。可以把 VLAN 母子设备作为一个整体想象为现实世界中的 802.1.q 交换机，下级接口通过子设备连接到寄主 Linux 系统网络里，上级接口同过主设备连接到上级网络，当母设备是物理网卡时上级网络是外界真实网络，当母设备是另外一个 Linux 虚拟网络设备时上级网络仍然是寄主 Linux 系统网络。&lt;/p>
&lt;p>需要注意的是母子 VLAN 设备拥有相同的 MAC 地址，可以把它当成现实世界中 802.1.q 交换机的 MAC，因此多个 VLAN 设备会共享一个 MAC。当一个母设备拥有多个 VLAN 子设备时，子设备之间是隔离的，不存在 Bridge 那样的交换转发关系，原因如下：802.1.q VLAN 协议的主要目的是从逻辑上隔离子网。现实世界中的 802.1.q 交换机存在多个 VLAN，每个 VLAN 拥有多个端口，同一 VLAN 端口之间可以交换转发，不同 VLAN 端口之间隔离，所以其包含两层功能：交换与隔离。Linux VLAN device 实现的是隔离功能，没有交换功能。一个 VLAN 母设备不可能拥有两个相同 ID 的 VLAN 子设备，因此也就不可能出现数据交换情况。如果想让一个 VLAN 里接多个设备，就需要交换功能。在 Linux 里 Bridge 专门实现交换功能，因此将 VLAN 子设备 attach 到一个 Bridge 上就能完成后续的交换功能。总结起来，Bridge 加 VLAN device 能在功能层面完整模拟现实世界里的 802.1.q 交换机。&lt;/p>
&lt;p>Linux 支持 VLAN 硬件加速，在安装有特定硬件情况下，图中所述内核处理过程可以被放到物理设备上完成。&lt;/p>
&lt;h3 id="tap-设备与-veth-设备">TAP 设备与 VETH 设备&lt;/h3>
&lt;p>TUN/TAP 设备是一种让用户态程序向内核协议栈注入数据的设备，一个工作在三层，一个工作在二层，使用较多的是 TAP 设备。VETH 设备出现较早，它的作用是反转通讯数据的方向，需要发送的数据会被转换成需要收到的数据重新送入内核网络层进行处理，从而间接的完成数据的注入。&lt;/p>
&lt;p>图 3 .TAP 设备和 VETH 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257472-c3161ee0-667a-4db3-b478-6d4d3e6c1682.png" alt="">&lt;/p>
&lt;p>如图所示，当一个 TAP 设备被创建时，在 Linux 设备文件目录下将会生成一个对应 char 设备，用户程序可以像打开普通文件一样打开这个文件进行读写。当执行 write()操作时，数据进入 TAP 设备，此时对于 Linux 网络层来说，相当于 TAP 设备收到了一包数据，请求内核接受它，如同普通的物理网卡从外界收到一包数据一样，不同的是其实数据来自 Linux 上的一个用户程序。Linux 收到此数据后将根据网络配置进行后续处理，从而完成了用户程序向 Linux 内核网络层注入数据的功能。当用户程序执行 read()请求时，相当于向内核查询 TAP 设备上是否有需要被发送出去的数据，有的话取出到用户程序里，完成 TAP 设备的发送数据功能。针对 TAP 设备的一个形象的比喻是：使用 TAP 设备的应用程序相当于另外一台计算机，TAP 设备是本机的一个网卡，他们之间相互连接。应用程序通过 read()/write()操作，和本机网络核心进行通讯。(可以这么说，一台虚拟机的网卡就是一个物理机上的 tap 设备)&lt;/p>
&lt;p>VETH 设备总是成对出现，送到一端请求发送的数据总是从另一端以请求接受的形式出现。该设备不能被用户程序直接操作，但使用起来比较简单。创建并配置正确后，向其一端输入数据，VETH 会改变数据的方向并将其送入内核网络核心，完成数据的注入。在另一端能读到此数据。&lt;/p>
&lt;h2 id="网络设置举例说明">网络设置举例说明&lt;/h2>
&lt;p>为了更好的说明 Linux 网络设备的用法，下面将用一系列的例子，说明在一个复杂的 Linux 网络元素组合出的虚拟网络里，数据的流向。网络设置简介如下：一个中心 Bridge：bridge0 下 attach 了 4 个网络设备，包括 2 个 VETH 设备，1 个 TAP 设备 tap0，1 个物理网卡 eth0。在 VETH 的另外一端又创建了 VLAN 子设备。Linux 上共存在 2 个 VLAN 网络，既 vlan100 与 vlan200。物理网卡和外部网络相连，并且在它之下创建了一个 VLAN ID 为 200 的 VLAN 子设备。&lt;/p>
&lt;h3 id="从-vlan100-子设备发送-arp-报文">从 vlan100 子设备发送 ARP 报文&lt;/h3>
&lt;p>图 4 .ARP from vlan100 child device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257520-663ccc4b-26bd-4784-bda1-7b8dd04c75a6.png" alt="">&lt;/p>
&lt;p>如图所示，当用户尝试 ping 192.168.100.3 时，Linux 将会根据路由表，从 vlan100 子设备发出 ARP 报文，具体过程如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>用户 ping 192.168.100.3&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Linux 向 vlan100 子设备发送 ARP 信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ARP 报文被打上 VLAN ID 100 的 Tag 成为 ARP@vlan100，转发到母设备上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VETH 设备将这一发送请求转变方向，成为一个需要接受处理的报文送入内核网络模块。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>由于对端的 VETH 设备被加入到了 bridge0 上，并且内核发现它收到一个报文，于是报文被转发到 bridge0 上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bridge0 处理此 ARP@vlan100 信息，根据 TCP/IP 二层协议发现是一个广播请求，于是向它所知道的所有端口广播此报文，其中一路进入另一对 VETH 设备的一端，一路进入 TAP 设备 tap0，一路进入物理网卡设备 eth0。此时在 tap0 上，用户程序可以通过 read()操作读到 ARP@vlan100，eth0 将会向外界发送 ARP@vlan100，但 eth0 的 VLAN 子设备不会收到它，因为此数据方向为请求发送而不是请求接收。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VETH 将请求方向转换，此时在另一端得到请求接受的 ARP@vlan100 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对端 VETH 设备发现有数据需要接受，并且自己有两个 VLAN 子设备，于是执行 VLAN 处理逻辑。其中一个子设备是 vlan100，与 ARP@vlan100 吻合，于是去除 VLAN ID 100 的 Tag 转发到这个子设备上，重新成为标准的以太网 ARP 报文。另一个子设备由于 ID 不吻合，不会得到此报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此 VLAN 子设备又被 attach 到另一个桥 bridge1 上，于是转发自己收到的 ARP 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bridge1 广播 ARP 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最终另外一个 TAP 设备 tap1 收到此请求发送报文，用户程序通过 read()可以得到它。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="从-vlan200-子设备发送-arp-报文">从 vlan200 子设备发送 ARP 报文&lt;/h3>
&lt;p>图 5 .ARP from vlan200 child device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257503-55a18ca0-f189-4987-9a14-be02dff09d03.png" alt="">&lt;/p>
&lt;p>和前面情况类似，区别是 VLAN ID 是 200，对端的 vlan200 子设备设置为 reorder_hdr = 0，表示此设备被要求保留收到的报文中的 VLAN Tag。此时子设备会收到 ARP 报文，但是带了 VLAN ID 200 的 Tag，既 ARP@vlan200。&lt;/p>
&lt;h3 id="从中心-bridge-发送-arp-报文">从中心 bridge 发送 ARP 报文&lt;/h3>
&lt;p>图 5 .ARP from central bridge&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257514-72135ddd-006c-417e-a0b2-0a62c5906609.png" alt="">&lt;/p>
&lt;p>当 bridge0 拥有 IP 时，通过 Linux 路由表用户程序可以直接将 ARP 报文发向 bridge0。这时 tap0 和外部网络都能收到 ARP，但 VLAN 子设备由于 VLAN ID 过滤的原因，将收不到 ARP 信息。&lt;/p>
&lt;h3 id="从外部网络向物理网卡发送-arpvlan200-报文">从外部网络向物理网卡发送 ARP@vlan200 报文&lt;/h3>
&lt;p>图 6 .ARP from external network&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257531-ba4cbf5b-4940-4019-b46c-6e9a47946cbc.png" alt="">&lt;/p>
&lt;p>当外部网络连接在一个支持 VLAN 并且对应端口为 vlan200 时，此情况会发生。此时所有的 VLAN ID 为 200 的 VLAN 子设备都将接受到报文，如果设置 reorder_hdr=0 则会收到带 Tag 的 ARP@vlan200。&lt;/p>
&lt;h3 id="从-tap-设备以-ping-方式发送-arp">从 TAP 设备以 ping 方式发送 ARP&lt;/h3>
&lt;p>图 7 .ping from TAP device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257520-73844bea-b56f-413c-8688-ea99573c2b2b.png" alt="">&lt;/p>
&lt;p>给 tap0 赋予 IP 并加入路由，此时再 Ping 其对应网段的未知 IP 会产生 ARP 发送请求。需要注意的是此时由于 tap0 上存在的是发送而不是接收请求，因此 ARP 报文不会被转发到桥上，从而什么也不会发生。图中右边画了一个类似情况：从 vlan200 子设备发送 ARP 请求。由于缺少 VETH 设备反转请求方向，因此报文也不会被转发到桥上，而是直接通过物理网卡发往外部网络。&lt;/p>
&lt;h3 id="以文件操作方式从-tap-设备发送报文">以文件操作方式从 TAP 设备发送报文&lt;/h3>
&lt;p>图 8 .file operation on TAP device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257536-d6ac7a20-331e-421f-a7d0-58862524405e.png" alt="">&lt;/p>
&lt;p>用户程序指定 tap0 设备发送报文有两种方式：socket 和 file operation。当用 socket_raw 标志新建 socket 并指定设备编号时，可以要求内核将报文从 tap0 发送。但和前面的 ping from tap0 情况类似，由于报文方向问题，消息并不会被转发到 bridge0 上。当用 open()方式打开 tap 设备文件时，情况有所不同。当执行 write()操作时，内核认为 tap0 收到了报文，从而会触发转发动作，bridge0 将收到它。如果发送的报文如图所示，是一个以 A 为目的地的携带 VLAN ID 100 Tag 的单点报文，bridge0 将会找到对应的设备进行转发，对应的 VLAN 子设备将收到没有 VLAN ID 100 Tag 的报文。&lt;/p>
&lt;h2 id="linux-上配置网络设备命令举例">Linux 上配置网络设备命令举例&lt;/h2>
&lt;p>以 Redhat6.2 红帽 Linux 发行版为例，如果已安装 VLAN 内核模块和管理工具 vconfig，TAP/TUN 设备管理工具 tunctl，那么可以用以下命令设置前述网络设备：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>创建 Bridge：brctl addbr [BRIDGE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 Bridge：brctl delbr [BRIDGE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>attach 设备到 Bridge：brctl addif [BRIDGE NAME] [DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从 Bridge detach 设备：brctl delif [BRIDGE NAME] [DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查询 Bridge 情况：brctl show&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 VLAN 设备：vconfig add [PARENT DEVICE NAME] [VLAN ID]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 VLAN 设备：vconfig rem [VLAN DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设置 VLAN 设备 flag：vconfig set_flag [VLAN DEVICE NAME] [FLAG] [VALUE]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设置 VLAN 设备 qos：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>vconfig set_egress_map [VLAN DEVICE NAME] [SKB_PRIORITY] [VLAN_QOS]&lt;/p>
&lt;p>vconfig set_ingress_map [VLAN DEVICE NAME] [SKB_PRIORITY] [VLAN_QOS]&lt;/p>
&lt;ul>
&lt;li>
&lt;p>查询 VLAN 设备情况：cat /proc/net/vlan/[VLAN DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 VETH 设备：ip link add link [DEVICE NAME] type veth&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 TAP 设备：tunctl -p [TAP DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 TAP 设备：tunctl -d [TAP DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查询系统里所有二层设备，包括 VETH/TAP 设备：ip link show&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除普通二层设备：ip link delete [DEVICE NAME] type [TYPE]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>综上所述，Linux 已经提供一套基本工具供用户创建出各种内部网络，利用这些工具可以方便的创建出特定网络给应用程序使用，包括云计算中的初级内部虚拟网络。&lt;/p>
&lt;p>相关主题&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.linuxcommand.org/man_pages/vconfig8.html">Vconfig Man Page&lt;/a>，vconfig 工具帮助文档。&lt;/li>
&lt;li>&lt;a href="http://candelatech.com/~greear/vlan.html">802.1Q VLAN implementation for Linux&lt;/a>，Linux 中 VLAN 模块如何实现的文档说明。&lt;/li>
&lt;li>&lt;a href="http://www.policyrouting.org/iproute2.doc.html">IPROUTE2 Utility Suite Howto&lt;/a>，Linux 里的 IP 工具使用说明。&lt;/li>
&lt;li>&lt;a href="http://www.ibm.com/developerworks/linux/library/l-virtual-networking">Virtual networking in Linux&lt;/a>，以虚拟化应用为中心讲述主流的虚拟网络技术，主要以 openvswith 为例。&lt;/li>
&lt;li>&lt;a href="http://tldp.org/HOWTO/BRIDGE-STP-HOWTO/index.html">Linux BRIDGE-STP-HOWTO&lt;/a>，Linux 中的 bridge 设备使用说明。&lt;/li>
&lt;li>&lt;a href="http://www.linuxfoundation.org/collaborate/workgroups/networking/networkoverview">Linux Kernel Networking (Network Overview) by Rami Rosen&lt;/a>，Linux 内核里的各种网络概念的含义，目的及用法简单介绍。&lt;/li>
&lt;li>在 &lt;a href="http://www.ibm.com/developerworks/cn/linux/">developerWorks Linux &lt;/a>专区寻找为 Linux 开发人员（包括 &lt;a href="http://www.ibm.com/developerworks/cn/linux/newto/">Linux 新手入门&lt;/a>）准备的更多参考资料。&lt;/li>
&lt;/ul></description></item><item><title>Docs: 2.Network File System</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/network-attached-storage%E7%BD%91%E7%BB%9C%E9%99%84%E5%8A%A0%E5%AD%98%E5%82%A8/2.network-file-system/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/1.%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84/network-attached-storage%E7%BD%91%E7%BB%9C%E9%99%84%E5%8A%A0%E5%AD%98%E5%82%A8/2.network-file-system/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;p>&lt;strong>Network File System(网络文件系统，简称 NFS)&lt;/strong> 是让客户端通过网络访问不同主机上磁盘里的数据，主要用在类 Unix 系统上实现文件共享的一种方法。 本例演示 CentOS 7 下安装和配置 NFS 的基本步骤。&lt;/p></description></item><item><title>Docs: 2.neutron</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/2.neutron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.3.openstack-%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/2.neutron/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>语法格式：neutron [OPTIONS] [SubCommand [OPTIONS]]&lt;/p>
&lt;p>直接输入 neutron 可以进入 neutron 的 shell 模式，在 neutron 的 shell 中再执行相关命令&lt;/p>
&lt;p>net-list #列出网络信息&lt;/p>
&lt;p>neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.&lt;/p>
&lt;p>usage: neutron [&amp;ndash;version] [-v] [-q] [-h] [-r NUM]&lt;/p>
&lt;pre>&lt;code> [--os-service-type ]
[--os-endpoint-type ]
[--service-type ]
[--endpoint-type ]
[--os-auth-strategy ] [--os-cloud ]
[--os-auth-url ]
[--os-tenant-name | --os-project-name ]
[--os-tenant-id | --os-project-id ]
[--os-username ] [--os-user-id ]
[--os-user-domain-id ]
[--os-user-domain-name ]
[--os-project-domain-id ]
[--os-project-domain-name ]
[--os-cert ] [--os-cacert ]
[--os-key ] [--os-password ]
[--os-region-name ] [--os-token ]
[--http-timeout ] [--os-url ] [--insecure]
&lt;/code>&lt;/pre>
&lt;p>Command-line interface to the Neutron APIs (neutron CLI version: 6.7.0)&lt;/p>
&lt;p>optional arguments:&lt;/p>
&lt;p>--version show program&amp;rsquo;s version number and exit&lt;/p>
&lt;p>-v, &amp;ndash;verbose, &amp;ndash;debug&lt;/p>
&lt;pre>&lt;code> Increase verbosity of output and show tracebacks on
errors. You can repeat this option.
&lt;/code>&lt;/pre>
&lt;p>-q, &amp;ndash;quiet Suppress output except warnings and errors.&lt;/p>
&lt;p>-h, &amp;ndash;help Show this help message and exit.&lt;/p>
&lt;p>-r NUM, &amp;ndash;retries NUM&lt;/p>
&lt;pre>&lt;code> How many times the request to the Neutron server
should be retried if it fails.
&lt;/code>&lt;/pre>
&lt;p>--os-service-type&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_NETWORK_SERVICE_TYPE] or network.
&lt;/code>&lt;/pre>
&lt;p>--os-endpoint-type&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_ENDPOINT_TYPE] or public.
&lt;/code>&lt;/pre>
&lt;p>--service-type&lt;/p>
&lt;pre>&lt;code> DEPRECATED! Use --os-service-type.
&lt;/code>&lt;/pre>
&lt;p>--endpoint-type&lt;/p>
&lt;pre>&lt;code> DEPRECATED! Use --os-endpoint-type.
&lt;/code>&lt;/pre>
&lt;p>--os-auth-strategy&lt;/p>
&lt;pre>&lt;code> DEPRECATED! Only keystone is supported.
&lt;/code>&lt;/pre>
&lt;p>--os-cloud Defaults to env[OS_CLOUD].&lt;/p>
&lt;p>--os-auth-url&lt;/p>
&lt;pre>&lt;code> Authentication URL, defaults to env[OS_AUTH_URL].
&lt;/code>&lt;/pre>
&lt;p>--os-tenant-name&lt;/p>
&lt;pre>&lt;code> Authentication tenant name, defaults to
env[OS_TENANT_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-project-name&lt;/p>
&lt;pre>&lt;code> Another way to specify tenant name. This option is
mutually exclusive with --os-tenant-name. Defaults to
env[OS_PROJECT_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-tenant-id&lt;/p>
&lt;pre>&lt;code> Authentication tenant ID, defaults to
env[OS_TENANT_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-project-id&lt;/p>
&lt;pre>&lt;code> Another way to specify tenant ID. This option is
mutually exclusive with --os-tenant-id. Defaults to
env[OS_PROJECT_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-username&lt;/p>
&lt;pre>&lt;code> Authentication username, defaults to env[OS_USERNAME].
&lt;/code>&lt;/pre>
&lt;p>--os-user-id&lt;/p>
&lt;pre>&lt;code> Authentication user ID (Env: OS_USER_ID)
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-id&lt;/p>
&lt;pre>&lt;code> OpenStack user domain ID. Defaults to
env[OS_USER_DOMAIN_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-user-domain-name&lt;/p>
&lt;pre>&lt;code> OpenStack user domain name. Defaults to
env[OS_USER_DOMAIN_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-id&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_PROJECT_DOMAIN_ID].
&lt;/code>&lt;/pre>
&lt;p>--os-project-domain-name&lt;/p>
&lt;pre>&lt;code> Defaults to env[OS_PROJECT_DOMAIN_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-cert&lt;/p>
&lt;pre>&lt;code> Path of certificate file to use in SSL connection.
This file can optionally be prepended with the private
key. Defaults to env[OS_CERT].
&lt;/code>&lt;/pre>
&lt;p>--os-cacert&lt;/p>
&lt;pre>&lt;code> Specify a CA bundle file to use in verifying a TLS
(https) server certificate. Defaults to
env[OS_CACERT].
&lt;/code>&lt;/pre>
&lt;p>--os-key Path of client key to use in SSL connection. This&lt;/p>
&lt;pre>&lt;code> option is not necessary if your key is prepended to
your certificate file. Defaults to env[OS_KEY].
&lt;/code>&lt;/pre>
&lt;p>--os-password&lt;/p>
&lt;pre>&lt;code> Authentication password, defaults to env[OS_PASSWORD].
&lt;/code>&lt;/pre>
&lt;p>--os-region-name&lt;/p>
&lt;pre>&lt;code> Authentication region name, defaults to
env[OS_REGION_NAME].
&lt;/code>&lt;/pre>
&lt;p>--os-token Authentication token, defaults to env[OS_TOKEN].&lt;/p>
&lt;p>--http-timeout&lt;/p>
&lt;pre>&lt;code> Timeout in seconds to wait for an HTTP response.
Defaults to env[OS_NETWORK_TIMEOUT] or None if not
specified.
&lt;/code>&lt;/pre>
&lt;p>--os-url Defaults to env[OS_URL].&lt;/p>
&lt;p>--insecure Explicitly allow neutronclient to perform &amp;ldquo;insecure&amp;rdquo;&lt;/p>
&lt;pre>&lt;code> SSL (https) requests. The server's certificate will
not be verified against any certificate authorities.
This option should be used with caution.
&lt;/code>&lt;/pre>
&lt;p>Commands for API v2.0:&lt;/p>
&lt;p>address-scope-create Create an address scope for a given tenant.&lt;/p>
&lt;p>address-scope-delete Delete an address scope.&lt;/p>
&lt;p>address-scope-list List address scopes that belong to a given tenant.&lt;/p>
&lt;p>address-scope-show Show information about an address scope.&lt;/p>
&lt;p>address-scope-update Update an address scope.&lt;/p>
&lt;p>agent-delete Delete a given agent.&lt;/p>
&lt;p>agent-list List agents.&lt;/p>
&lt;p>agent-show Show information of a given agent.&lt;/p>
&lt;p>agent-update Updates the admin status and description for a specified agent.&lt;/p>
&lt;p>auto-allocated-topology-delete Delete the auto-allocated topology of a given tenant.&lt;/p>
&lt;p>auto-allocated-topology-show Show the auto-allocated topology of a given tenant.&lt;/p>
&lt;p>availability-zone-list List availability zones.&lt;/p>
&lt;p>bash-completion Prints all of the commands and options for bash-completion.&lt;/p>
&lt;p>bgp-dragent-list-hosting-speaker List Dynamic Routing agents hosting a BGP speaker.&lt;/p>
&lt;p>bgp-dragent-speaker-add Add a BGP speaker to a Dynamic Routing agent.&lt;/p>
&lt;p>bgp-dragent-speaker-remove Removes a BGP speaker from a Dynamic Routing agent.&lt;/p>
&lt;p>bgp-peer-create Create a BGP Peer.&lt;/p>
&lt;p>bgp-peer-delete Delete a BGP peer.&lt;/p>
&lt;p>bgp-peer-list List BGP peers.&lt;/p>
&lt;p>bgp-peer-show Show information of a given BGP peer.&lt;/p>
&lt;p>bgp-peer-update Update BGP Peer&amp;rsquo;s information.&lt;/p>
&lt;p>bgp-speaker-advertiseroute-list List routes advertised by a given BGP speaker.&lt;/p>
&lt;p>bgp-speaker-create Create a BGP Speaker.&lt;/p>
&lt;p>bgp-speaker-delete Delete a BGP speaker.&lt;/p>
&lt;p>bgp-speaker-list List BGP speakers.&lt;/p>
&lt;p>bgp-speaker-list-on-dragent List BGP speakers hosted by a Dynamic Routing agent.&lt;/p>
&lt;p>bgp-speaker-network-add Add a network to the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-network-remove Remove a network from the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-peer-add Add a peer to the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-peer-remove Remove a peer from the BGP speaker.&lt;/p>
&lt;p>bgp-speaker-show Show information of a given BGP speaker.&lt;/p>
&lt;p>bgp-speaker-update Update BGP Speaker&amp;rsquo;s information.&lt;/p>
&lt;p>dhcp-agent-list-hosting-net List DHCP agents hosting a network.&lt;/p>
&lt;p>dhcp-agent-network-add Add a network to a DHCP agent.&lt;/p>
&lt;p>dhcp-agent-network-remove Remove a network from a DHCP agent.&lt;/p>
&lt;p>ext-list List all extensions.&lt;/p>
&lt;p>ext-show Show information of a given resource.&lt;/p>
&lt;p>firewall-create Create a firewall.&lt;/p>
&lt;p>firewall-delete Delete a given firewall.&lt;/p>
&lt;p>firewall-list List firewalls that belong to a given tenant.&lt;/p>
&lt;p>firewall-policy-create Create a firewall policy.&lt;/p>
&lt;p>firewall-policy-delete Delete a given firewall policy.&lt;/p>
&lt;p>firewall-policy-insert-rule Insert a rule into a given firewall policy.&lt;/p>
&lt;p>firewall-policy-list List firewall policies that belong to a given tenant.&lt;/p>
&lt;p>firewall-policy-remove-rule Remove a rule from a given firewall policy.&lt;/p>
&lt;p>firewall-policy-show Show information of a given firewall policy.&lt;/p>
&lt;p>firewall-policy-update Update a given firewall policy.&lt;/p>
&lt;p>firewall-rule-create Create a firewall rule.&lt;/p>
&lt;p>firewall-rule-delete Delete a given firewall rule.&lt;/p>
&lt;p>firewall-rule-list List firewall rules that belong to a given tenant.&lt;/p>
&lt;p>firewall-rule-show Show information of a given firewall rule.&lt;/p>
&lt;p>firewall-rule-update Update a given firewall rule.&lt;/p>
&lt;p>firewall-show Show information of a given firewall.&lt;/p>
&lt;p>firewall-update Update a given firewall.&lt;/p>
&lt;p>flavor-associate Associate a Neutron service flavor with a flavor profile.&lt;/p>
&lt;p>flavor-create Create a Neutron service flavor.&lt;/p>
&lt;p>flavor-delete Delete a given Neutron service flavor.&lt;/p>
&lt;p>flavor-disassociate Disassociate a Neutron service flavor from a flavor profile.&lt;/p>
&lt;p>flavor-list List Neutron service flavors.&lt;/p>
&lt;p>flavor-profile-create Create a Neutron service flavor profile.&lt;/p>
&lt;p>flavor-profile-delete Delete a given Neutron service flavor profile.&lt;/p>
&lt;p>flavor-profile-list List Neutron service flavor profiles.&lt;/p>
&lt;p>flavor-profile-show Show information about a given Neutron service flavor profile.&lt;/p>
&lt;p>flavor-profile-update Update a given Neutron service flavor profile.&lt;/p>
&lt;p>flavor-show Show information about a given Neutron service flavor.&lt;/p>
&lt;p>flavor-update Update a Neutron service flavor.&lt;/p>
&lt;p>floatingip-associate Create a mapping between a floating IP and a fixed IP.&lt;/p>
&lt;p>floatingip-create Create a floating IP for a given tenant.&lt;/p>
&lt;p>floatingip-delete Delete a given floating IP.&lt;/p>
&lt;p>floatingip-disassociate Remove a mapping from a floating IP to a fixed IP.&lt;/p>
&lt;p>floatingip-list List floating IPs that belong to a given tenant.&lt;/p>
&lt;p>floatingip-show Show information of a given floating IP.&lt;/p>
&lt;p>help print detailed help for another command&lt;/p>
&lt;p>ipsec-site-connection-create Create an IPsec site connection.&lt;/p>
&lt;p>ipsec-site-connection-delete Delete a given IPsec site connection.&lt;/p>
&lt;p>ipsec-site-connection-list List IPsec site connections that belong to a given tenant.&lt;/p>
&lt;p>ipsec-site-connection-show Show information of a given IPsec site connection.&lt;/p>
&lt;p>ipsec-site-connection-update Update a given IPsec site connection.&lt;/p>
&lt;p>l3-agent-list-hosting-router List L3 agents hosting a router.&lt;/p>
&lt;p>l3-agent-router-add Add a router to a L3 agent.&lt;/p>
&lt;p>l3-agent-router-remove Remove a router from a L3 agent.&lt;/p>
&lt;p>lb-agent-hosting-pool Get loadbalancer agent hosting a pool.&lt;/p>
&lt;p>lb-healthmonitor-associate Create a mapping between a health monitor and a pool.&lt;/p>
&lt;p>lb-healthmonitor-create Create a health monitor.&lt;/p>
&lt;p>lb-healthmonitor-delete Delete a given health monitor.&lt;/p>
&lt;p>lb-healthmonitor-disassociate Remove a mapping from a health monitor to a pool.&lt;/p>
&lt;p>lb-healthmonitor-list List health monitors that belong to a given tenant.&lt;/p>
&lt;p>lb-healthmonitor-show Show information of a given health monitor.&lt;/p>
&lt;p>lb-healthmonitor-update Update a given health monitor.&lt;/p>
&lt;p>lb-member-create Create a member.&lt;/p>
&lt;p>lb-member-delete Delete a given member.&lt;/p>
&lt;p>lb-member-list List members that belong to a given tenant.&lt;/p>
&lt;p>lb-member-show Show information of a given member.&lt;/p>
&lt;p>lb-member-update Update a given member.&lt;/p>
&lt;p>lb-pool-create Create a pool.&lt;/p>
&lt;p>lb-pool-delete Delete a given pool.&lt;/p>
&lt;p>lb-pool-list List pools that belong to a given tenant.&lt;/p>
&lt;p>lb-pool-list-on-agent List the pools on a loadbalancer agent.&lt;/p>
&lt;p>lb-pool-show Show information of a given pool.&lt;/p>
&lt;p>lb-pool-stats Retrieve stats for a given pool.&lt;/p>
&lt;p>lb-pool-update Update a given pool.&lt;/p>
&lt;p>lb-vip-create Create a vip.&lt;/p>
&lt;p>lb-vip-delete Delete a given vip.&lt;/p>
&lt;p>lb-vip-list List vips that belong to a given tenant.&lt;/p>
&lt;p>lb-vip-show Show information of a given vip.&lt;/p>
&lt;p>lb-vip-update Update a given vip.&lt;/p>
&lt;p>lbaas-agent-hosting-loadbalancer Get lbaas v2 agent hosting a loadbalancer.&lt;/p>
&lt;p>lbaas-healthmonitor-create LBaaS v2 Create a healthmonitor.&lt;/p>
&lt;p>lbaas-healthmonitor-delete LBaaS v2 Delete a given healthmonitor.&lt;/p>
&lt;p>lbaas-healthmonitor-list LBaaS v2 List healthmonitors that belong to a given tenant.&lt;/p>
&lt;p>lbaas-healthmonitor-show LBaaS v2 Show information of a given healthmonitor.&lt;/p>
&lt;p>lbaas-healthmonitor-update LBaaS v2 Update a given healthmonitor.&lt;/p>
&lt;p>lbaas-l7policy-create LBaaS v2 Create L7 policy.&lt;/p>
&lt;p>lbaas-l7policy-delete LBaaS v2 Delete a given L7 policy.&lt;/p>
&lt;p>lbaas-l7policy-list LBaaS v2 List L7 policies that belong to a given listener.&lt;/p>
&lt;p>lbaas-l7policy-show LBaaS v2 Show information of a given L7 policy.&lt;/p>
&lt;p>lbaas-l7policy-update LBaaS v2 Update a given L7 policy.&lt;/p>
&lt;p>lbaas-l7rule-create LBaaS v2 Create L7 rule.&lt;/p>
&lt;p>lbaas-l7rule-delete LBaaS v2 Delete a given L7 rule.&lt;/p>
&lt;p>lbaas-l7rule-list LBaaS v2 List L7 rules that belong to a given L7 policy.&lt;/p>
&lt;p>lbaas-l7rule-show LBaaS v2 Show information of a given rule.&lt;/p>
&lt;p>lbaas-l7rule-update LBaaS v2 Update a given L7 rule.&lt;/p>
&lt;p>lbaas-listener-create LBaaS v2 Create a listener.&lt;/p>
&lt;p>lbaas-listener-delete LBaaS v2 Delete a given listener.&lt;/p>
&lt;p>lbaas-listener-list LBaaS v2 List listeners that belong to a given tenant.&lt;/p>
&lt;p>lbaas-listener-show LBaaS v2 Show information of a given listener.&lt;/p>
&lt;p>lbaas-listener-update LBaaS v2 Update a given listener.&lt;/p>
&lt;p>lbaas-loadbalancer-create LBaaS v2 Create a loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-delete LBaaS v2 Delete a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-list LBaaS v2 List loadbalancers that belong to a given tenant.&lt;/p>
&lt;p>lbaas-loadbalancer-list-on-agent List the loadbalancers on a loadbalancer v2 agent.&lt;/p>
&lt;p>lbaas-loadbalancer-show LBaaS v2 Show information of a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-stats Retrieve stats for a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-status Retrieve status for a given loadbalancer.&lt;/p>
&lt;p>lbaas-loadbalancer-update LBaaS v2 Update a given loadbalancer.&lt;/p>
&lt;p>lbaas-member-create LBaaS v2 Create a member.&lt;/p>
&lt;p>lbaas-member-delete LBaaS v2 Delete a given member.&lt;/p>
&lt;p>lbaas-member-list LBaaS v2 List members that belong to a given pool.&lt;/p>
&lt;p>lbaas-member-show LBaaS v2 Show information of a given member.&lt;/p>
&lt;p>lbaas-member-update LBaaS v2 Update a given member.&lt;/p>
&lt;p>lbaas-pool-create LBaaS v2 Create a pool.&lt;/p>
&lt;p>lbaas-pool-delete LBaaS v2 Delete a given pool.&lt;/p>
&lt;p>lbaas-pool-list LBaaS v2 List pools that belong to a given tenant.&lt;/p>
&lt;p>lbaas-pool-show LBaaS v2 Show information of a given pool.&lt;/p>
&lt;p>lbaas-pool-update LBaaS v2 Update a given pool.&lt;/p>
&lt;p>meter-label-create Create a metering label for a given tenant.&lt;/p>
&lt;p>meter-label-delete Delete a given metering label.&lt;/p>
&lt;p>meter-label-list List metering labels that belong to a given tenant.&lt;/p>
&lt;p>meter-label-rule-create Create a metering label rule for a given label.&lt;/p>
&lt;p>meter-label-rule-delete Delete a given metering label.&lt;/p>
&lt;p>meter-label-rule-list List metering labels that belong to a given label.&lt;/p>
&lt;p>meter-label-rule-show Show information of a given metering label rule.&lt;/p>
&lt;p>meter-label-show Show information of a given metering label.&lt;/p>
&lt;p>net-create Create a network for a given tenant.&lt;/p>
&lt;p>net-delete Delete a given network.&lt;/p>
&lt;p>net-external-list List external networks that belong to a given tenant.&lt;/p>
&lt;p>net-ip-availability-list List IP usage of networks&lt;/p>
&lt;p>net-ip-availability-show Show IP usage of specific network&lt;/p>
&lt;p>net-list List networks that belong to a given tenant.&lt;/p>
&lt;p>net-list-on-dhcp-agent List the networks on a DHCP agent.&lt;/p>
&lt;p>net-show Show information of a given network.&lt;/p>
&lt;p>net-update Update network&amp;rsquo;s information.&lt;/p>
&lt;p>port-create Create a port for a given tenant.&lt;/p>
&lt;p>port-delete Delete a given port.&lt;/p>
&lt;p>port-list List ports that belong to a given tenant.&lt;/p>
&lt;p>port-show Show information of a given port.&lt;/p>
&lt;p>port-update Update port&amp;rsquo;s information.&lt;/p>
&lt;p>purge Delete all resources that belong to a given tenant.&lt;/p>
&lt;p>qos-available-rule-types List available qos rule types.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-create Create a qos bandwidth limit rule.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-delete Delete a given qos bandwidth limit rule.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-list List all qos bandwidth limit rules belonging to the specified policy.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-show Show information about the given qos bandwidth limit rule.&lt;/p>
&lt;p>qos-bandwidth-limit-rule-update Update the given qos bandwidth limit rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-create Create a QoS DSCP marking rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-delete Delete a given qos dscp marking rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-list List all QoS DSCP marking rules belonging to the specified policy.&lt;/p>
&lt;p>qos-dscp-marking-rule-show Show information about the given qos dscp marking rule.&lt;/p>
&lt;p>qos-dscp-marking-rule-update Update the given QoS DSCP marking rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-create Create a qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-delete Delete a given qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-list List all qos minimum bandwidth rules belonging to the specified policy.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-show Show information about the given qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-minimum-bandwidth-rule-update Update the given qos minimum bandwidth rule.&lt;/p>
&lt;p>qos-policy-create Create a qos policy.&lt;/p>
&lt;p>qos-policy-delete Delete a given qos policy.&lt;/p>
&lt;p>qos-policy-list List QoS policies that belong to a given tenant connection.&lt;/p>
&lt;p>qos-policy-show Show information of a given qos policy.&lt;/p>
&lt;p>qos-policy-update Update a given qos policy.&lt;/p>
&lt;p>quota-default-show Show default quotas for a given tenant.&lt;/p>
&lt;p>quota-delete Delete defined quotas of a given tenant.&lt;/p>
&lt;p>quota-list List quotas of all tenants who have non-default quota values.&lt;/p>
&lt;p>quota-show Show quotas for a given tenant.&lt;/p>
&lt;p>quota-update Update a given tenant&amp;rsquo;s quotas.&lt;/p>
&lt;p>rbac-create Create a RBAC policy for a given tenant.&lt;/p>
&lt;p>rbac-delete Delete a RBAC policy.&lt;/p>
&lt;p>rbac-list List RBAC policies that belong to a given tenant.&lt;/p>
&lt;p>rbac-show Show information of a given RBAC policy.&lt;/p>
&lt;p>rbac-update Update RBAC policy for given tenant.&lt;/p>
&lt;p>router-create Create a router for a given tenant.&lt;/p>
&lt;p>router-delete Delete a given router.&lt;/p>
&lt;p>router-gateway-clear Remove an external network gateway from a router.&lt;/p>
&lt;p>router-gateway-set Set the external network gateway for a router.&lt;/p>
&lt;p>router-interface-add Add an internal network interface to a router.&lt;/p>
&lt;p>router-interface-delete Remove an internal network interface from a router.&lt;/p>
&lt;p>router-list List routers that belong to a given tenant.&lt;/p>
&lt;p>router-list-on-l3-agent List the routers on a L3 agent.&lt;/p>
&lt;p>router-port-list List ports that belong to a given tenant, with specified router.&lt;/p>
&lt;p>router-show Show information of a given router.&lt;/p>
&lt;p>router-update Update router&amp;rsquo;s information.&lt;/p>
&lt;p>security-group-create Create a security group.&lt;/p>
&lt;p>security-group-delete Delete a given security group.&lt;/p>
&lt;p>security-group-list List security groups that belong to a given tenant.&lt;/p>
&lt;p>security-group-rule-create Create a security group rule.&lt;/p>
&lt;p>security-group-rule-delete Delete a given security group rule.&lt;/p>
&lt;p>security-group-rule-list List security group rules that belong to a given tenant.&lt;/p>
&lt;p>security-group-rule-show Show information of a given security group rule.&lt;/p>
&lt;p>security-group-show Show information of a given security group.&lt;/p>
&lt;p>security-group-update Update a given security group.&lt;/p>
&lt;p>service-provider-list List service providers.&lt;/p>
&lt;p>subnet-create Create a subnet for a given tenant.&lt;/p>
&lt;p>subnet-delete Delete a given subnet.&lt;/p>
&lt;p>subnet-list List subnets that belong to a given tenant.&lt;/p>
&lt;p>subnet-show Show information of a given subnet.&lt;/p>
&lt;p>subnet-update Update subnet&amp;rsquo;s information.&lt;/p>
&lt;p>subnetpool-create Create a subnetpool for a given tenant.&lt;/p>
&lt;p>subnetpool-delete Delete a given subnetpool.&lt;/p>
&lt;p>subnetpool-list List subnetpools that belong to a given tenant.&lt;/p>
&lt;p>subnetpool-show Show information of a given subnetpool.&lt;/p>
&lt;p>subnetpool-update Update subnetpool&amp;rsquo;s information.&lt;/p>
&lt;p>tag-add Add a tag into the resource.&lt;/p>
&lt;p>tag-remove Remove a tag on the resource.&lt;/p>
&lt;p>tag-replace Replace all tags on the resource.&lt;/p>
&lt;p>vpn-endpoint-group-create Create a VPN endpoint group.&lt;/p>
&lt;p>vpn-endpoint-group-delete Delete a given VPN endpoint group.&lt;/p>
&lt;p>vpn-endpoint-group-list List VPN endpoint groups that belong to a given tenant.&lt;/p>
&lt;p>vpn-endpoint-group-show Show a specific VPN endpoint group.&lt;/p>
&lt;p>vpn-endpoint-group-update Update a given VPN endpoint group.&lt;/p>
&lt;p>vpn-ikepolicy-create Create an IKE policy.&lt;/p>
&lt;p>vpn-ikepolicy-delete Delete a given IKE policy.&lt;/p>
&lt;p>vpn-ikepolicy-list List IKE policies that belong to a tenant.&lt;/p>
&lt;p>vpn-ikepolicy-show Show information of a given IKE policy.&lt;/p>
&lt;p>vpn-ikepolicy-update Update a given IKE policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-create Create an IPsec policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-delete Delete a given IPsec policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-list List IPsec policies that belong to a given tenant connection.&lt;/p>
&lt;p>vpn-ipsecpolicy-show Show information of a given IPsec policy.&lt;/p>
&lt;p>vpn-ipsecpolicy-update Update a given IPsec policy.&lt;/p>
&lt;p>vpn-service-create Create a VPN service.&lt;/p>
&lt;p>vpn-service-delete Delete a given VPN service.&lt;/p>
&lt;p>vpn-service-list List VPN service configurations that belong to a given tenant.&lt;/p>
&lt;p>vpn-service-show Show information of a given VPN service.&lt;/p>
&lt;p>vpn-service-update Update a given VPN service.&lt;/p></description></item><item><title>Docs: 2.Redis 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/2.redis-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/2.redis-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;a href="https://redis.io/topics/config">官方文档&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Redis 可以在不使用配置文件的情况下使用内置的默认配置启动。但是一般情况，都会使用一个 Redis 的配置文件(文件名通常是 redis.conf)来启动 Redis。Redis 启动后，会将 redis.conf 文件的内容加载到内存中，通过 Redis 客户端的 &lt;strong>config get *&lt;/strong> 命令，即可获取当前已经加载到内存中的配置。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>127.0.0.1:6379&amp;gt; config get *
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 1&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#e6db74">&amp;#34;dbfilename&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 2&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#e6db74">&amp;#34;dump.rdb&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 3&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#e6db74">&amp;#34;requirepass&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 4&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 5&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#e6db74">&amp;#34;masterauth&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 6&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>....... 后略
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>配置文件的写法非常简单。redis.conf 由 &lt;strong>Directives(指令)&lt;/strong> 组成，每条指令一行。而 Directives 分为两部分&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Keyword(关键字)&lt;/strong> # 该指令的含义&lt;/li>
&lt;li>&lt;strong>Arguments(参数)&lt;/strong> # Redis 执行该指令时的行为&lt;/li>
&lt;/ul>
&lt;p>格式如下：&lt;/p>
&lt;pre>&lt;code># 关键字 参数(多个参数以空格分隔)
Keyword Argument1 Argument2 ... ArugmentN
&lt;/code>&lt;/pre>
&lt;h2 id="通过命令函参数传递配置">通过命令函参数传递配置&lt;/h2>
&lt;p>通过命令行传递参数的格式与 redis.conf 文件中配置格式完全相同，只不过关键字前面有个 &lt;code>--&lt;/code> 前缀。比如：&lt;/p>
&lt;pre>&lt;code>redis-server --port 6380 --replicaof 127.0.0.1 6379
&lt;/code>&lt;/pre>
&lt;p>生成的内存中配置如下：&lt;/p>
&lt;pre>&lt;code>127.0.0.1:6380&amp;gt; config get &amp;quot;replicaof&amp;quot;
1) &amp;quot;replicaof&amp;quot;
2) &amp;quot;127.0.0.1 6379&amp;quot;
127.0.0.1:6380&amp;gt; config get &amp;quot;port&amp;quot;
1) &amp;quot;port&amp;quot;
2) &amp;quot;6380&amp;quot;
&lt;/code>&lt;/pre>
&lt;h3 id="其他基本示例">其他基本示例&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Usage: ./redis-server &lt;span style="color:#f92672">[&lt;/span>/path/to/redis.conf&lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#f92672">[&lt;/span>options&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server - &lt;span style="color:#f92672">(&lt;/span>read config from stdin&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server -v or --version
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server -h or --help
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server --test-memory &amp;lt;megabytes&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Examples:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server &lt;span style="color:#f92672">(&lt;/span>run the server with default conf&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server /etc/redis/6379.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server --port &lt;span style="color:#ae81ff">7777&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server --port &lt;span style="color:#ae81ff">7777&lt;/span> --replicaof 127.0.0.1 &lt;span style="color:#ae81ff">8888&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./redis-server /etc/myredis.conf --loglevel verbose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="启动-redis-后修改配置">启动 Redis 后修改配置&lt;/h2>
&lt;p>Redis 支持在线热更新配置，可以通过 config set ARGUMENT 命令来更改当前配置，若想要配置永久生效，则使用 config rewrite 命令将内存中的配置重写到启动 Redis 时指定的 redis.conf 文件中。但是，并非配置中的所有指令都支持这种方式。&lt;/p>
&lt;h2 id="简单的配置文件示例">简单的配置文件示例&lt;/h2>
&lt;pre>&lt;code>dir &amp;quot;/data&amp;quot;
port 6379
maxmemory 1G
maxmemory-policy volatile-lru
min-replicas-max-lag 5
min-replicas-to-write 1
rdbchecksum yes
rdbcompression yes
repl-diskless-sync yes
save 900 1
requirepass redis
masterauth &amp;quot;redis&amp;quot;
replica-announce-port 6379
replica-announce-ip &amp;quot;10.105.180.122&amp;quot;
&lt;/code>&lt;/pre>
&lt;h1 id="redisconf-文件详解">redis.conf 文件详解&lt;/h1>
&lt;h2 id="includes-配置环境">Includes 配置环境&lt;/h2>
&lt;ul>
&lt;li>**include /PATH/TO/FILE **# Redis 启动时，除了加载 redis.conf 文件外，还会加载 include 指令指定的文件。&lt;/li>
&lt;/ul>
&lt;h2 id="network-配置环境">Network 配置环境&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>bind 127.0.0.1&lt;/strong> # 监听的地址&lt;/li>
&lt;li>&lt;strong>port 6379&lt;/strong> # redis 监听的端口，默认 6379&lt;/li>
&lt;li>&lt;strong>tcp-backlog 511&lt;/strong> # tcp 的等待队列&lt;/li>
&lt;li>&lt;strong>timeout 0&lt;/strong> # 客户端连接超时时长。&lt;code>默认值：0&lt;/code>，不会超时&lt;/li>
&lt;/ul>
&lt;h2 id="general-配置环境">General 配置环境&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>daemonize yes|no&lt;/strong> # 指定是否在后台运行&lt;/li>
&lt;li>&lt;strong>databases 16&lt;/strong> # 可使用的 databases，默认 16 个&lt;/li>
&lt;li>&lt;strong>logfile /PATH/TO/FILE&lt;/strong> # 指定 redis 记录日志文件位置&lt;/li>
&lt;/ul>
&lt;h2 id="snapshotting-配置环境-rdb-功能">Snapshotting 配置环境 RDB 功能&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>save TIME NUM&lt;/strong> # 在 TIME 秒内有 NUM 个键改变，就做一次快照&lt;/li>
&lt;li>&lt;strong>stop-writes-on-bgsave-error yes|no&lt;/strong> # 当 RDB 持久化出现错误后，是否依然进行继续进行工作，yes：不能进行工作，no：可以继续进行工作，可以通过 info 中的 rdb_last_bgsave_status 了解 RDB 持久化是否有错误&lt;/li>
&lt;li>&lt;strong>rdbcompression yes|no&lt;/strong> # 是否压缩 rdb 文件，rdb 文件压缩使用 LZF 压缩算法。压缩需要一些 cpu 的消耗；不压缩需要更多的磁盘空间&lt;/li>
&lt;li>&lt;strong>rdbchecksum yes|no&lt;/strong> # 是否校验 rdb 文件。从 rdb 格式的第五个版本开始，在 rdb 文件的末尾会带上 CRC64 的校验和。这跟有利于文件的容错性，但是在保存 rdb 文件的时候，会有大概 10%的性能损耗，所以如果你追求高性能，可以关闭该配置。&lt;/li>
&lt;li>&lt;strong>dbfilename FileName&lt;/strong> # 指定 snapshot 文件的文件名。默认为 dump.rbd&lt;/li>
&lt;li>&lt;strong>dir PATH&lt;/strong> # 指定 snapshot 文件的保存路径(注意：PATH 是目录，不是文件，具体文件名通过 dbfilename 关键字配置)。默认为 /var/lib/redis/ 目录&lt;/li>
&lt;/ul>
&lt;h2 id="replication-配置环境">Replication 配置环境&lt;/h2>
&lt;ul>
&lt;li>**replicof 192.168.1.2 6379 **# 启动主从模式，并设定自己为从服务器，主服务器 IP 为 192.168.1.2，主服务器端口为 6379&lt;/li>
&lt;li>&lt;strong>slave-read-only no&lt;/strong> # 作为从服务器是否只读，默认不只读&lt;/li>
&lt;/ul>
&lt;h2 id="security-配置环境">Security 配置环境&lt;/h2>
&lt;ul>
&lt;li>**requirepass PASSWORD **# 配置认证密码为 PASSWORD&lt;/li>
&lt;/ul>
&lt;h2 id="limits-配置环境">Limits 配置环境&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>maxmemory BYTES&lt;/strong> # 指定 redis 可使用的最大内存量，单位是 bytes。如果达到限额，则需要配合 maxmemory-policy 配置指定的策略删除 key。note：slave 的输出缓冲区是不计算在 maxmemory 内的。所以为了防止主机内存使用完，建议设置的 maxmemory 需要更小一些。&lt;/li>
&lt;li>&lt;strong>maxmemory-policy POLICY&lt;/strong> # 指定 redis 超过内存限额之后的策略，包括以下几种
&lt;ul>
&lt;li>volatile-lru：利用 LRU 算法移除设置过过期时间的 key。&lt;/li>
&lt;li>volatile-random：随机移除设置过过期时间的 key。&lt;/li>
&lt;li>volatile-ttl：移除即将过期的 key，根据最近过期时间来删除（辅以 TTL）&lt;/li>
&lt;li>allkeys-lru：利用 LRU 算法移除任何 key。&lt;/li>
&lt;li>allkeys-random：随机移除任何 key。&lt;/li>
&lt;li>noeviction：不移除任何 key，只是返回一个写错误。&lt;/li>
&lt;li>Note:上面的这些驱逐策略，如果 redis 没有合适的 key 驱逐，对于写命令，还是会返回错误。redis 将不再接收写请求，只接收 get 请求。写命令包括：set setnx setex append incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby getset mset msetnx exec sort。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="append-only-mode-配置环境-aof-功能配置">Append Only Mode 配置环境 AOF 功能配置&lt;/h2>
&lt;h2 id="lua-scripting-配置环境">LUA Scripting 配置环境&lt;/h2>
&lt;h2 id="redis-cluster-配置环境">Redis Cluster 配置环境&lt;/h2>
&lt;h2 id="slow-log-配置环境">SLOW LOG 配置环境&lt;/h2>
&lt;h2 id="latency-monitor-配置环境">LATENCY MONITOR 配置环境&lt;/h2>
&lt;h2 id="event-notification-配置环境">EVENT NOTIFICATION 配置环境&lt;/h2>
&lt;h2 id="advanced-config-配置环境">ADVANCED CONFIG 配置环境&lt;/h2></description></item><item><title>Docs: 2.数据库</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://db-engines.com/en/article/Database">DB-Engines&lt;/a>(所有数据库的排名、状态等信息的观察网站)&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>数据库是由特定软件（所谓的数据库管理系统或 DBMS）管理的数据的逻辑集合。数据库和 DBMS 共同构成数据库系统。&lt;/p>
&lt;p>A database is a logical collection of data which is managed by a specific software (the so-called &lt;a href="https://db-engines.com/en/article/Database+Management+System">database management system&lt;/a> or DBMS). Database and DBMS together form the database system.&lt;/p>
&lt;p>数据库不仅包括用户数据，还包括对其进行管理所需的对象（例如索引或日志文件）。&lt;/p>
&lt;p>A database includes not only user data but also the objects necessary for its management (e.g. indexes or logfiles).&lt;/p>
&lt;p>数据库的类型&lt;/p>
&lt;ol>
&lt;li>RDBMS：关系型数据库
&lt;ol>
&lt;li>Oracle&lt;/li>
&lt;li>MariaDB/MySQL&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>NoSQL：非关系型数据库
&lt;ol>
&lt;li>Key/Val NoSQL：redis,etcd&lt;/li>
&lt;li>Column Family NoSQL 列族：HBase&lt;/li>
&lt;li>Documentation NoSQL：MongoDB&lt;/li>
&lt;li>Graph  NoSQL：Neo4j&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>NewSQL：分布式数据库&lt;/li>
&lt;/ol>
&lt;h1 id="relational-dbms">Relational DBMS&lt;/h1>
&lt;p>&lt;strong>Relational database management systems(关系数据库管理系统，简称 RDBMS)&lt;/strong> support the relational (=table-oriented) data model. The schema of a table (=relation schema) is defined by the table name and a fixed number of attributes with fixed data types. A record (=entity) corresponds to a row in the table and consists of the values of each attribute. A relation thus consists of a set of uniform records.&lt;/p>
&lt;p>The table schemas are generated by normalization in the process of data modeling.&lt;/p>
&lt;p>Certain basic operations are defined on the relations:&lt;/p>
&lt;ul>
&lt;li>classical set operations (union, intersection and difference)&lt;/li>
&lt;li>Selection (selection of a subset of records according to certain filter criteria for the attribute values)&lt;/li>
&lt;li>Projection (selecting a subset of attributes / columns of the table)&lt;/li>
&lt;li>Join: special conjunction of multiple tables as a combination of the Cartesian product with selection and projection.&lt;/li>
&lt;/ul>
&lt;p>These basic operations, as well as operations for creation, modification and deletion of table schemas, operations for controlling transactions and user management are performed by means of database languages, with SQL being a well established standard for such languages.&lt;/p>
&lt;p>The first relational database management systems appeared on the market at the beginning of the 1980s and since have been the most commonly used &lt;a href="https://db-engines.com/en/article/DBMS">DBMS&lt;/a> type.&lt;/p>
&lt;p>Over the years, many RDBMS have been expanded with non-relational concepts such as user-defined data types, not atomic attributes, inheritance and hierarchies, which is why they are sometimes referred to as object-relational DBMS.&lt;/p>
&lt;h2 id="most-popular-examples">Most popular examples&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://db-engines.com/en/system/Oracle">Oracle&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/system/MySQL">MySQL&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/system/Microsoft+SQL+Server">Microsoft SQL Server&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/system/PostgreSQL">PostgreSQL&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/system/IBM+Db2">IBM Db2&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Find more systems in our &lt;a href="https://db-engines.com/en/ranking/relational+dbms">relational DBMS ranking&lt;/a>.&lt;/p>
&lt;h1 id="nosql">NoSQL&lt;/h1>
&lt;p>NoSQL Database Systems are an alternative to the mainstream &lt;a href="https://db-engines.com/en/article/Relational+DBMS">Relational DBMS&lt;/a>. They don&amp;rsquo;t use a relational data model and typically have no SQL interface.&lt;/p>
&lt;p>Although this type of systems exists for many years (some even longer than relational systems), the term NoSQL was first introduced in 2009 when many new systems were developed in order to cope with the new requirements for database management systems at that time. E.g.  Big Data, scalability and fault tolerance for large web applications.&lt;/p>
&lt;p>The acronym NoSQL is often understood as &amp;ldquo;Not Only SQL&amp;rdquo;, implying that relational systems are a proven technology but not necessarily the optimal choice for each kind of intended use.&lt;/p>
&lt;h2 id="classification分类">Classification(分类)&lt;/h2>
&lt;p>NoSQL systems are a heterogenous group of very different database systems. Therefore each attempt for a classification fails in classifying one or another system. However, the following categegories are well accepted:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://db-engines.com/en/article/Key-value+Stores">Key-Value Stores&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/Wide+Column+Stores">Wide Column Stores&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/Document+Stores">Document Stores&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/Graph+DBMS">Graph DBMS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/RDF+Stores">RDF Stores&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/Native+XML+DBMS">Native XML DBMS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/Content+Stores">Content Stores&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://db-engines.com/en/article/Search+Engines">Search Engines&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="advantages优势">Advantages(优势)&lt;/h2>
&lt;p>Not all of the above mentioned classes have the same general advantages, but they benefit from a combination of the following aspects.&lt;/p>
&lt;ul>
&lt;li>higher performance&lt;/li>
&lt;li>easy distribution of data on different nodes (e.g. sharding), thereby achieving scalability and fault tolerance&lt;/li>
&lt;li>higher flexibility by using a schema-free data model.&lt;/li>
&lt;li>simpler administration&lt;/li>
&lt;/ul>
&lt;h3 id="methods">Methods&lt;/h3>
&lt;p>These advantages are achieved by means of one or more of the following approaches:&lt;/p>
&lt;ul>
&lt;li>No normalized relational data model&lt;/li>
&lt;li>Abandoning one or more of the ACID criteria&lt;/li>
&lt;li>Less powerful possibilities for querying the data&lt;/li>
&lt;/ul></description></item><item><title>Docs: 3.1.Replication(复制) 模式详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/3.1.replication%E5%A4%8D%E5%88%B6-%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2.%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE/redis/redis-%E9%AB%98%E5%8F%AF%E7%94%A8/3.1.replication%E5%A4%8D%E5%88%B6-%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/</guid><description>
&lt;p>参考：&lt;a href="https://redis.io/topics/replication">官方文档&lt;/a>&lt;/p>
&lt;ol>
&lt;li>基本原理&lt;/li>
&lt;/ol>
&lt;p>主从复制模式中包含 一个主数据库实例(master) 与 一个或多个从数据库实例(slave)，如下图&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/equ0le/1616134781068-8b8f1a94-6405-4bea-98b3-5627a9d8ff17.png" alt="">&lt;/p>
&lt;p>客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库。&lt;/p>
&lt;p>具体工作机制为：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>slave 启动后，向 master 发送 SYNC 命令，master 接收到 SYNC 命令后通过 bgsave 保存快照(即上文所介绍的 RDB 持久化)，并使用缓冲区记录保存快照这段时间内执行的写命令&lt;/p>
&lt;/li>
&lt;li>
&lt;p>master 将保存的快照文件发送给 slave，并继续记录执行的写命令&lt;/p>
&lt;/li>
&lt;li>
&lt;p>slave 接收到快照文件后，加载快照文件，载入数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>master 快照发送完后开始向 slave 发送缓冲区的写命令，slave 接收命令并执行，完成复制初始化&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此后 master 每次执行一个写命令都会同步发送给 slave，保持 master 与 slave 之间数据的一致性&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>2. 部署示例&lt;/strong>&lt;/p>
&lt;p>redis.conf 的主要配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">###网络相关###&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># bind 127.0.0.1 # 绑定监听的网卡IP，注释掉或配置成0.0.0.0可使任意IP均可访问&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>protected-mode no &lt;span style="color:#75715e"># 关闭保护模式，使用密码访问&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>port &lt;span style="color:#ae81ff">6379&lt;/span> &lt;span style="color:#75715e"># 设置监听端口，建议生产环境均使用自定义端口&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>timeout &lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#75715e"># 客户端连接空闲多久后断开连接，单位秒，0表示禁用&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">###通用配置###&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>daemonize yes &lt;span style="color:#75715e"># 在后台运行&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pidfile /var/run/redis_6379.pid &lt;span style="color:#75715e"># pid进程文件名&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logfile /usr/local/redis/logs/redis.log &lt;span style="color:#75715e"># 日志文件的位置&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">###RDB持久化配置###&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>save &lt;span style="color:#ae81ff">900&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#75715e"># 900s内至少一次写操作则执行bgsave进行RDB持久化&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>save &lt;span style="color:#ae81ff">300&lt;/span> &lt;span style="color:#ae81ff">10&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>save &lt;span style="color:#ae81ff">60&lt;/span> &lt;span style="color:#ae81ff">10000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 如果禁用RDB持久化，可在这里添加 save &amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rdbcompression yes &lt;span style="color:#75715e">#是否对RDB文件进行压缩，建议设置为no，以（磁盘）空间换（CPU）时间&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dbfilename dump.rdb &lt;span style="color:#75715e"># RDB文件名称&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dir /usr/local/redis/datas &lt;span style="color:#75715e"># RDB文件保存路径，AOF文件也保存在这里&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">###AOF配置###&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>appendonly yes &lt;span style="color:#75715e"># 默认值是no，表示不使用AOF增量持久化的方式，使用RDB全量持久化的方式&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>appendfsync everysec &lt;span style="color:#75715e"># 可选值 always， everysec，no，建议设置为everysec&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">###设置密码###&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>requirepass &lt;span style="color:#ae81ff">123456&lt;/span> &lt;span style="color:#75715e"># 设置复杂一点的密码&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>部署主从复制模式只需稍微调整 slave 的配置，在 redis.conf 中添加&lt;/p>
&lt;pre>&lt;code>replicaof 127.0.0.1 6379 # master的ip，port
masterauth 123456 # master的密码
replica-serve-stale-data no # 如果slave无法与master同步，设置成slave不可读，方便监控脚本发现问题
&lt;/code>&lt;/pre>
&lt;p>本示例在单台服务器上配置 master 端口 6379，两个 slave 端口分别为 7001,7002，启动 master，再启动两个 slave&lt;/p>
&lt;pre>&lt;code>[root@dev-server-1 master-slave]# redis-server master.conf
[root@dev-server-1 master-slave]# redis-server slave1.conf
[root@dev-server-1 master-slave]# redis-server slave2.conf
&lt;/code>&lt;/pre>
&lt;p>进入 master 数据库，写入一个数据，再进入一个 slave 数据库，立即便可访问刚才写入 master 数据库的数据。如下所示&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@dev-server-1 master-slave&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># redis-cli&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:6379&amp;gt; auth &lt;span style="color:#ae81ff">123456&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>OK
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:6379&amp;gt; set site blog.jboost.cn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>OK
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:6379&amp;gt; get site
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;blog.jboost.cn&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:6379&amp;gt; info replication
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Replication&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>role:master
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>connected_slaves:2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>slave0:ip&lt;span style="color:#f92672">=&lt;/span>127.0.0.1,port&lt;span style="color:#f92672">=&lt;/span>7001,state&lt;span style="color:#f92672">=&lt;/span>online,offset&lt;span style="color:#f92672">=&lt;/span>13364738,lag&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>slave1:ip&lt;span style="color:#f92672">=&lt;/span>127.0.0.1,port&lt;span style="color:#f92672">=&lt;/span>7002,state&lt;span style="color:#f92672">=&lt;/span>online,offset&lt;span style="color:#f92672">=&lt;/span>13364738,lag&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:6379&amp;gt; exit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@dev-server-1 master-slave&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># redis-cli -p 7001&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:7001&amp;gt; auth &lt;span style="color:#ae81ff">123456&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>OK
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>127.0.0.1:7001&amp;gt; get site
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#34;blog.jboost.cn&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>执行 info replication 命令可以查看连接该数据库的其它库的信息，如上可看到有两个 slave 连接到 master&lt;/p>
&lt;p>&lt;strong>主从复制的优缺点&lt;/strong>&lt;/p>
&lt;p>优点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>master 能自动将数据同步到 slave，可以进行读写分离，分担 master 的读压力&lt;/p>
&lt;/li>
&lt;li>
&lt;p>master、slave 之间的同步是以非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>缺点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>不具备自动容错与恢复功能，master 或 slave 的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端 IP 才能恢复&lt;/p>
&lt;/li>
&lt;li>
&lt;p>master 宕机，如果宕机前数据没有同步完，则切换 IP 后会存在数据不一致的问题&lt;/p>
&lt;/li>
&lt;li>
&lt;p>难以支持在线扩容，Redis 的容量受限于单机配置&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 3.1.RKE(Rancher Kubernetes Engine) 介绍</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.1.rkerancher-kubernetes-engine-%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.1.rkerancher-kubernetes-engine-%E4%BB%8B%E7%BB%8D/</guid><description>
&lt;h1 id="rke-介绍">RKE 介绍&lt;/h1>
&lt;p>官方文档：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://rancher.com/docs/rke/latest/en/">https://rancher.com/docs/rke/latest/en/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://rancher2.docs.rancher.cn/docs/rke/_index/">https://rancher2.docs.rancher.cn/docs/rke/_index/&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Rancher Kubernetes Engine(RKE)，是经过 CNCF 认证的 Kubernetes 发行版，完全在 Docker 容器内运行。它适用于裸机和虚拟机。RKE 解决了安装复杂性的问题，这是 Kubernetes 社区中的常见问题。借助 RKE，Kubernetes 的安装和操作既简单又自动化，并且完全独立于所运行的操作系统和平台。只要服务器可以运行受支持的 Docker 版本，就可以使用 RKE 部署和运行 Kubernetes。&lt;/p>
&lt;p>使用 rke 工具，仅需通过一个 yaml 的配置文件以及 docker 环境，即可启动一个功能完全的 kubernetes 集群。其中所有系统组件(包括 kubelet)都是以容器的方式运行的。通过 Rancher 创建的 kubernetes 集群，就是 RKE 集群。&lt;/p>
&lt;h2 id="rke-集群与原生-k8s-集群的区别">RKE 集群与原生 K8S 集群的区别&lt;/h2>
&lt;p>RKE 与 sealos 实现高可用的方式类似。不同点是 RKE 集群的 node 节点是通过 ngxin 来连接 API Server。&lt;/p>
&lt;h1 id="rke-集群部署">RKE 集群部署&lt;/h1>
&lt;p>参考：RKE 部署与清理&lt;/p>
&lt;ol>
&lt;li>
&lt;p>下载 rke 二进制文件。(在 github 上下载 rke 命令行工具)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建集群配置文件。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>RKE 默认使用名为 cluster.yml 的集群配置文件来确定集群中应该包含哪些节点以及如何部署 Kubernetes。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>下面是一个单节点 cluster.yml 文件示例，&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;pre>&lt;code>cat &amp;gt; cluster.yml &amp;lt;&amp;lt;
# 指定要部署集群的节点信息
nodes:
# 指定该节点的IP
- address: 1.2.3.4
# 指定部署集群时，所使用的用户
user: ubuntu
# 指定该集群的角色，controlplane运行k8s主要组件，etcd运行etcd，worker运行用户创建的非k8s主要组件的pod。
role:
- controlplane # 对应 k8s master 节点
- etcd
- worker # 对应 k8s node 节点
EOF
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>在 cluster.yml 执行** rke up** 命令&lt;/li>
&lt;/ol>
&lt;h2 id="rke-清理">RKE 清理&lt;/h2>
&lt;p>参考：&lt;a href="https://rancher.com/docs/rke/latest/en/managing-clusters/">https://rancher.com/docs/rke/latest/en/managing-clusters/&lt;/a>。中文：&lt;a href="https://rancher2.docs.rancher.cn/docs/rke/managing-clusters/_index">https://rancher2.docs.rancher.cn/docs/rke/managing-clusters/_index&lt;/a>&lt;/p>
&lt;p>在 cluster.yaml 文件所在目录&lt;/p>
&lt;h1 id="rke-配置">RKE 配置&lt;/h1>
&lt;p>RKE 默认通过一个名为 cluster.yml 的文件配置集群参数。可以通过 &amp;ndash;config 选项来指定其他的 yaml 格式的文件&lt;/p>
&lt;p>cluster.yml #&lt;/p></description></item><item><title>Docs: 3.3.RKE 配置详解</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.3.rke-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.3.rke-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid><description/></item><item><title>Docs: 3.4.RKE yaml 示例</title><link>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.4.rke-yaml-%E7%A4%BA%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/it%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/kubernetes-%E8%A1%8D%E7%94%9F%E5%93%81/rancher/3.4.rke-yaml-%E7%A4%BA%E4%BE%8B/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;pre>&lt;code># Cluster Config
#
answers: {}
docker_root_dir: /var/lib/docker
enable_cluster_alerting: false
enable_cluster_monitoring: false
enable_network_policy: false
local_cluster_auth_endpoint:
enabled: true
name: kubernetes-test
#
# Rancher Config
#
rancher_kubernetes_engine_config:
addon_job_timeout: 30
authentication:
strategy: x509|webhook
authorization: {}
bastion_host:
ssh_agent_auth: false
cloud_provider: {}
dns:
linear_autoscaler_params:
cores_per_replica: 128
max: 0
min: 1
nodes_per_replica: 4
prevent_single_point_failure: true
node_selector: null
nodelocal:
ip_address: ''
node_selector: null
update_strategy:
rolling_update: {}
reversecidrs: null
stubdomains: null
update_strategy:
rolling_update: {}
upstreamnameservers: null
ignore_docker_version: true
#
# # 当前仅支持nginx的ingress
# # 设置`provider: none`禁用ingress控制器
# # 通过node_selector可以指定在某些节点上运行ingress控制器，例如:
# provider: nginx
# node_selector:
# app: ingress
#
ingress:
provider: nginx
kubernetes_version: v1.18.6-rancher1-1
monitoring:
provider: metrics-server
replicas: 1
#
# # 如果您在AWS上使用calico
#
# network:
# plugin: calico
# calico_network_provider:
# cloud_provider: aws
#
# # 指定flannel网络接口
#
# network:
# plugin: flannel
# flannel_network_provider:
# iface: eth1
#
# # 指定canal网络插件的flannel网络接口
#
# network:
# plugin: canal
# canal_network_provider:
# iface: eth1
#
network:
mtu: 0
options:
flannel_backend_type: vxlan
plugin: flannel
restore:
restore: false
#
# # 自定义服务参数，仅适用于Linux环境
# services:
# kube-api:
# service_cluster_ip_range: 10.43.0.0/16
# extra_args:
# watch-cache: true
# kube-controller:
# cluster_cidr: 10.42.0.0/16
# service_cluster_ip_range: 10.43.0.0/16
# extra_args:
# # 修改每个节点子网大小(cidr掩码长度)，默认为24，可用IP为254个；23，可用IP为510个；22，可用IP为1022个；
# node-cidr-mask-size: 24
# # 控制器定时与节点通信以检查通信是否正常，周期默认5s
# node-monitor-period: '5s'
# # 当节点通信失败后，再等一段时间kubernetes判定节点为notready状态。这个时间段必须是kubelet的nodeStatusUpdateFrequency(默认10s)的N倍，其中N表示允许kubelet同步节点状态的重试次数，默认40s。
# node-monitor-grace-period: '20s'
# # 再持续通信失败一段时间后，kubernetes判定节点为unhealthy状态，默认1m0s。
# node-startup-grace-period: '30s'
# # 再持续失联一段时间，kubernetes开始迁移失联节点的Pod，默认5m0s。
# pod-eviction-timeout: '1m'
# kubelet:
# cluster_domain: cluster.local
# cluster_dns_server: 10.43.0.10
# # 扩展变量
# extra_args:
# # 与apiserver会话时的并发数，默认是10
# kube-api-burst: '30'
# # 与apiserver会话时的 QPS,默认是5
# kube-api-qps: '15'
# # 修改节点最大Pod数量
# max-pods: '250'
# # secrets和configmaps同步到Pod需要的时间，默认一分钟
# sync-frequency: '3s'
# # kubelet默认一次拉取一个镜像，设置为false可以同时拉取多个镜像，前提是存储驱动要为overlay2，对应的Docker也需要增加下载并发数
# serialize-image-pulls: false
# # 拉取镜像的最大并发数，registry-burst不能超过registry-qps ，仅当registry-qps大于0(零)时生效，(默认10)。如果registry-qps为0则不限制(默认5)。
# registry-burst: '10'
# registry-qps: '0'
# # 以下配置用于配置节点资源预留和限制
# cgroups-per-qos: 'true'
# cgroup-driver: cgroupfs
# # 以下两个参数指明为相关服务预留多少资源，仅用于调度，不做实际限制
# system-reserved: 'memory=300Mi'
# kube-reserved: 'memory=2Gi'
# enforce-node-allocatable: 'pods'
# # 硬驱逐阈值，当节点上的可用资源少于这个值时，就会触发强制驱逐。强制驱逐会强制kill掉POD，不会等POD自动退出。
# eviction-hard: 'memory.available&amp;lt;300Mi,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;15%,nodefs.inodesFree&amp;lt;5%'
# # 软驱逐阈值
# ## 以下四个参数配套使用，当节点上的可用资源少于这个值时但大于硬驱逐阈值时候，会等待eviction-soft-grace-period设置的时长；
# ## 等待中每10s检查一次，当最后一次检查还触发了软驱逐阈值就会开始驱逐，驱逐不会直接Kill POD，先发送停止信号给POD，然后等待eviction-max-pod-grace-period设置的时长；
# ## 在eviction-max-pod-grace-period时长之后，如果POD还未退出则发送强制kill POD
# eviction-soft: 'memory.available&amp;lt;500Mi,nodefs.available&amp;lt;50%,imagefs.available&amp;lt;50%,nodefs.inodesFree&amp;lt;10%'
# eviction-soft-grace-period: 'memory.available=1m30s'
# eviction-max-pod-grace-period: '30'
# ## 当处于驱逐状态的节点不可调度，当节点恢复正常状态后
# eviction-pressure-transition-period: '5m0s'
# extra_binds:
# - &amp;quot;/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins&amp;quot;
# - &amp;quot;/etc/iscsi:/etc/iscsi&amp;quot;
# - &amp;quot;/sbin/iscsiadm:/sbin/iscsiadm&amp;quot;
# etcd:
# # 修改空间配额为$((4*1024*1024*1024))，默认2G,最大8G
# extra_args:
# quota-backend-bytes: '4294967296'
# auto-compaction-retention: 240 #(单位小时)
# kubeproxy:
# extra_args:
# # 默认使用iptables进行数据转发
# proxy-mode: &amp;quot;&amp;quot; # 如果要启用ipvs，则此处设置为`ipvs`
#
services:
etcd:
backup_config:
enabled: true
interval_hours: 12
retention: 6
safe_timestamp: false
creation: 12h
extra_args:
election-timeout: '5000'
heartbeat-interval: '500'
listen-metrics-urls: 'http://0.0.0.0:2381'
gid: 0
retention: 72h
snapshot: false
uid: 0
kube-api:
always_pull_images: false
pod_security_policy: false
service_node_port_range: 30000-60000
kube-controller: {}
kubelet:
extra_args:
cgroup-driver: systemd
fail_swap_on: false
generate_serving_certificate: false
kubeproxy:
extra_args:
proxy-mode: ipvs
scheduler: {}
ssh_agent_auth: false
upgrade_strategy:
drain: false
max_unavailable_controlplane: '1'
max_unavailable_worker: 10%%
node_drain_input:
delete_local_data: false
force: false
grace_period: -1
ignore_daemon_sets: true
timeout: 120
scheduled_cluster_scan:
enabled: false
scan_config:
cis_scan_config:
debug_master: false
debug_worker: false
override_benchmark_version: rke-cis-1.4
profile: permissive
schedule_config:
cron_schedule: 0 0 * * *
retention: 24
&lt;/code>&lt;/pre></description></item></channel></rss>