<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Pod 的资源管理</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.Pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</link><description>Recent content in Pod 的资源管理 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.Pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Pod 的资源管理</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.Pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.Pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">官方文档，概念-配置-Pod 和容器的资源管理&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="pod-中-container-的资源需求与资源限制">Pod 中 Container 的资源需求与资源限制&lt;/h1>
&lt;p>可以在 Pod 的 yaml 中定义该 Pod 中各个 Container 对内存与 CPU 的最低需求量和最大使用量&lt;/p>
&lt;ul>
&lt;li>requests：资源需求，最低保障，资源最少需要多少&lt;/li>
&lt;li>limits：限制，硬限制，限额，资源最大不能超过多少&lt;/li>
&lt;/ul>
&lt;p>当对 Container 进行资源制定后，会出现 QoS(服务质量)的属性，下列 3 个属性从上往下优先级下降；当节点资源不够时，优先级越高，越会保证其正常运行，其余不够提供资源的 Container 则不再运行&lt;/p>
&lt;ul>
&lt;li>Guarateed：有保证的，Pod 中每个 Container 同时设置 CPU 和内存的 requests 和 limits，且 request 和 limits 的值相同&lt;/li>
&lt;li>Burstable：超频，Pod 中至少有一个 Container 设置了 CPU 或内存资源的 requests 属性&lt;/li>
&lt;li>BestEffort：尽力努力(尽力而为)没有任何一个 Container 设置了 requests 和 limits 属性&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/xu9mbw/1617283210163-cdf33748-8346-40d3-96b1-c3da2cf90df5.jpeg" alt="image.jpeg">&lt;/p>
&lt;p>关于在 yaml 中如何写资源限制中数值的说明：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>kubernetes 中的一个 CPU 是一个逻辑 CPU，1CPU 的核心数=1000millicores 毫核心(也就是说 500m 相当于 0.5 个 CPU)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>限制 cpu 可以用整数写，1 就是 1 个 cpu、0.5 就是 0.5 个 cpu。也可以带单位，1000m 就是 1 个 cpu，500m 就是 0.5 个 cpu。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>限制内存需要使用单位(IEC 标准、公制标准都可以)，即 Mi、Gi 或者 M、G 等。例如 1024Mi、1Gi 等&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Note：polinux/stress 这是一个非常好用的压测容器，可以对容器指定其所使用的内存和 cpu 等资源的大小。当创建完资源配合等资源限制的对象后，可以通过该容器来测试资源限制是否生效。&lt;/p>
&lt;p>分配内存资源给 容器 和 Pods&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">memory-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">mem-example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">memory-demo-ctr&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">polinux/stress&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;200Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;100Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">command&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;stress&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">args&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;--vm&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;--vm-bytes&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;150M&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;--vm-hang&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>分配 CPU 资源给 容器 和 Pods&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">cpu-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">cpu-example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">cpu-demo-ctr&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">vish/stress&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0.5&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">args&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - -&lt;span style="color:#ae81ff">cpus&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: CPU资源的调度和管理(CFS)</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.Pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/CPU%E8%B5%84%E6%BA%90%E7%9A%84%E8%B0%83%E5%BA%A6%E5%92%8C%E7%AE%A1%E7%90%86CFS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/3.Pod-%E9%9B%86%E7%BE%A4%E6%9C%80%E5%B0%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8D%95%E5%85%83/Pod-%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/CPU%E8%B5%84%E6%BA%90%E7%9A%84%E8%B0%83%E5%BA%A6%E5%92%8C%E7%AE%A1%E7%90%86CFS/</guid><description>
&lt;h1 id="一前言">一、前言&lt;/h1>
&lt;p>在使用 Kubernetes 的过程中，我们看到过这样一个告警信息：&lt;/p>
&lt;p>[K8S]告警主题: CPUThrottlingHigh&lt;/p>
&lt;p>告警级别: warning&lt;/p>
&lt;p>告警类型: CPUThrottlingHigh&lt;/p>
&lt;p>故障实例:&lt;/p>
&lt;p>告警详情: 27% throttling of CPU in namespace kube-system for container kube-proxy in pod kube-proxy-9pj9j.&lt;/p>
&lt;p>触发时间: 2020-05-08 17:34:17&lt;/p>
&lt;p>这个告警信息说明 kube-proxy 容器被 throttling 了，然而查看该容器的资源使用历史信息，发现该容器以及容器所在的节点的 CPU 资源使用率都不高：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119568683-4d3d60e5-d34f-4e0c-914b-203298c7c642.png" alt="image.png">
告警期间容器所在节点 CPU 使用率&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119576905-621581d5-8e07-49cf-9588-11e12445e976.png" alt="image.png">
告警期间 kube-proxy 的资源使用率&lt;/p>
&lt;p>经过我们的分析，发现该告警实际上是和 Kubernetes 对于 CPU 资源的限制和管控机制有关。Kubernetes 依赖于容器的 runtime 进行 CPU 资源的调度，而容器 runtime 以 Docker 为例，是借助于 cgroup 和 CFS 调度机制进行资源管控。本文基于这个告警案例，首先分析了 CFS 的基本原理，然后对于 Kubernetes 借助 CFS 进行 CPU 资源的调度和管控方法进行了介绍，最后使用一个例子来分析 CFS 的一些调度特性来解释这个告警的 root cause 和解决方案。&lt;/p>
&lt;p>转载自&lt;a href="https://blog.csdn.net/cloudvtech">https://blog.csdn.net/cloudvtech&lt;/a>&lt;/p>
&lt;p>二、CFS 基本原理&lt;/p>
&lt;p>参考：CPU 管理&lt;/p>
&lt;p>2.3 运行和观察&lt;/p>
&lt;p>部署这样一个 yaml POD：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>apiVersion: v1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kind: Pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>metadata:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>name: busybox&lt;/p>
&lt;/li>
&lt;li>
&lt;p>labels:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>app: busybox&lt;/p>
&lt;/li>
&lt;li>
&lt;p>spec:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>containers:&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>image: busybox&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>resources:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>requests:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>memory: &amp;ldquo;64Mi&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cpu: &amp;ldquo;250m&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>limits:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>memory: &amp;ldquo;128Mi&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cpu: &amp;ldquo;500m&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>command:&lt;/p>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&amp;ldquo;/bin/sh&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&amp;ldquo;-c&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;ul>
&lt;li>&amp;ldquo;while true; do sleep 10; done&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>imagePullPolicy: IfNotPresent&lt;/p>
&lt;/li>
&lt;li>
&lt;p>name: busybox&lt;/p>
&lt;/li>
&lt;li>
&lt;p>restartPolicy: Always&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>可以看到该容器内部的进程对应的 CPU 调度信息变化如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>[root@k8s-node-04 ~]# cat /proc/121133/sched&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sh (121133, # threads: 1)&lt;/p>
&lt;/li>
&lt;li>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>se.exec_start : 20229360324.308323&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.vruntime : 0.179610&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.sum_exec_runtime : 31.190620&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.nr_migrations : 12&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_switches : 79&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_voluntary_switches : 78&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_involuntary_switches : 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.load.weight : 1024&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prio : 120&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clock-delta : 26&lt;/p>
&lt;/li>
&lt;li>
&lt;p>mm-&amp;gt;numa_scan_seq : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_migrations, 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 1, 1, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;li>&lt;/li>
&lt;li>
&lt;p>[root@k8s-node-04 ~]# cat /proc/121133/sched&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sh (121133, # threads: 1)&lt;/p>
&lt;/li>
&lt;li>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>se.exec_start : 20229480327.896307&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.vruntime : 0.149504&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.sum_exec_runtime : 33.325310&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.nr_migrations : 17&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_switches : 91&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_voluntary_switches : 90&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_involuntary_switches : 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.load.weight : 1024&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prio : 120&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clock-delta : 31&lt;/p>
&lt;/li>
&lt;li>
&lt;p>mm-&amp;gt;numa_scan_seq : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_migrations, 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 0, 1, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;li>&lt;/li>
&lt;li>
&lt;p>[root@k8s-node-04 ~]# cat /proc/121133/sched&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sh (121133, # threads: 1)&lt;/p>
&lt;/li>
&lt;li>
&lt;hr>
&lt;/li>
&lt;li>
&lt;p>se.exec_start : 20229520328.862396&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.vruntime : 1.531536&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.sum_exec_runtime : 34.053116&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.nr_migrations : 18&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_switches : 95&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_voluntary_switches : 94&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nr_involuntary_switches : 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>se.load.weight : 1024&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prio : 120&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clock-delta : 34&lt;/p>
&lt;/li>
&lt;li>
&lt;p>mm-&amp;gt;numa_scan_seq : 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_migrations, 0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 0, 0, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 0, 1, 1, 0, -1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>numa_faults_memory, 1, 1, 0, 0, -1&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>其中 sum_exec_runtime 表示实际运行的物理时间。&lt;/p>
&lt;p>转载自&lt;a href="https://blog.csdn.net/cloudvtech">https://blog.csdn.net/cloudvtech&lt;/a>&lt;/p>
&lt;p>三、Kubernetes 借助 CFS 进行 CPU 管理&lt;/p>
&lt;p>3.1 CFS 进行 CPU 资源限流(throtting)的原理&lt;/p>
&lt;p>根据文章《Kubernetes 生产实践系列之三十：Kubernetes 基础技术之集群计算资源管理》的描述，Kubernetes 的资源定义：&lt;/p>
&lt;ol>
&lt;li>resources:&lt;/li>
&lt;li>requests:&lt;/li>
&lt;li>memory: &amp;ldquo;64Mi&amp;rdquo;&lt;/li>
&lt;li>cpu: &amp;ldquo;250m&amp;rdquo;&lt;/li>
&lt;li>limits:&lt;/li>
&lt;li>memory: &amp;ldquo;128Mi&amp;rdquo;&lt;/li>
&lt;li>cpu: &amp;ldquo;500m&amp;rdquo;&lt;/li>
&lt;/ol>
&lt;p>比如里面的 CPU 需求，会被翻译成容器 runtime 的运行时参数，并最终变成 cgroups 和 CFS 的参数配置：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cat cpu.shares&lt;/p>
&lt;/li>
&lt;li>
&lt;p>256&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_quota_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>50000&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_period_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>100000&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这里有一个默认的参数：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cat /proc/sys/kernel/sched_latency_ns&lt;/p>
&lt;/li>
&lt;li>
&lt;p>24000000&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>所以在这个节点上，正常压力下，系统的 CFS 调度周期是 24ms，CFS 重分配周期是 100ms，而该 POD 在一个重分配周期最多占用 50ms 的时间，在有压力的情况下，POD 可以占据的 CPU share 比例是 256。&lt;/p>
&lt;p>下面一个例子可以说明不同资源需求的 POD 容器是如何在 CFS 的调度下占用 CPU 资源的：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119602382-f95c9175-9b27-4d95-9746-2c7725bca2ef.png" alt="image.png">CPU 资源配置和 CFS 调度&lt;/p>
&lt;p>在这个例子中，有如下系统配置情况：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>CFS 调度周期为 10ms，正常负载情况下，进程 ready 队列里面的进程在每 10ms 的间隔内都会保证被执行一次&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CFS 重分配周期为 100ms，用于保证一个进程的 limits 设置会被反映在每 100ms 的重分配周期内可以占用的 CPU 时间数，在多核系统中，limit 最大值可以是 CFS 重分配周期*CPU 核数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>该执行进程队列只有进程 A 和进程 B 两个进程&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进程 A 和 B 定义的 CPU share 占用都一样，所以在系统资源紧张的时候可以保证 A 和 B 进程都可以占用可用 CPU 资源的一半&lt;/p>
&lt;/li>
&lt;li>
&lt;p>定义的 CFS 重分配周期都是 100ms&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进程 A 在 100ms 内最多占用 50ms，进程 B 在 100ms 内最多占用 20ms&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>所以在一个 CFS 重分配周期(相当于 10 个 CFS 调度周期)内，进程队列的执行情况如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在前面的 4 个 CFS 调度周期内，进程 A 和 B 由于 share 值是一样的，所以每个 CFS 调度内(10ms)，进程 A 和 B 都会占用 5ms&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在第 4 个 CFS 调度周期结束的时候，在本 CFS 重分配周期内，进程 B 已经占用了 20ms，在剩下的 8 个 CFS 调度周期即 80ms 内，进程 B 都会被限流，一直到下一个 CFS 重分配周期内，进程 B 才可以继续占用 CPU&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在第 5-7 这 3 个 CFS 调度周期内，由于进程 B 被限流，所以进程 A 可以完全拥有这 3 个 CFS 调度的 CPU 资源，占用 30ms 的执行时间，这样在本 CFS 重分配周期内，进程 A 已经占用了 50ms 的 CPU 时间，在后面剩下的 3 个 CFS 调度周期即后面的 30ms 内，进程 A 也会被限流，一直到下一个 CFS 重分配周期内，进程 A 才可以继续占用 CPU&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>如果进程被限流了，可以在如下的路径看到：&lt;/p>
&lt;p>cat /sys/fs/cgroup/cpu/kubepods/pod5326d6f4-789d-11ea-b093-fa163e23cb69/69336c973f9f414c3f9fdfbd90200b7083b35f4d54ce302a4f5fc330f2889846/cpu.stat&lt;/p>
&lt;p>nr_periods 14001693&lt;/p>
&lt;p>nr_throttled 2160435&lt;/p>
&lt;p>throttled_time 570069950532853&lt;/p>
&lt;p>3.2 本文开头问题的原因分析&lt;/p>
&lt;p>根据 3.1 描述的原理，很容易理解本文开通的告警信息的出现，是由于在某些特定的 CFS 重分配周期内，kube-proxy 的 CPU 占用率超过了给它分配的 limits，而参看 kube-proxy daemonset 的配置，确实它的 limits 配置只有 200ms，这就意味着在默认的 100ms 的 CFS 重调度周期内，它只能占用 20ms，所以在特定繁忙场景会有问题：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119635719-1bd824ee-7535-423f-8316-293a4f5b4daf.png" alt="image.png">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>cat cpu.shares&lt;/p>
&lt;/li>
&lt;li>
&lt;p>204&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_period_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>100000&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cat cpu.cfs_quota_us&lt;/p>
&lt;/li>
&lt;li>
&lt;p>20000&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>注：这里 cpu.shares 的计算方法如下：200x1024/1000~=204&lt;/p>
&lt;p>而这个问题的解决方案就是将 CPU limits 提高。&lt;/p>
&lt;p>Zalando 公司有一个分享《Optimizing Kubernetes Resource Requests/Limits for Cost-Efficiency and Latency / Henning Jacobs》很好的讲述了 CPU 资源管理的问题，可以参考，这个演讲的 PPT 在这里可以找到。&lt;/p>
&lt;p>更具体问题分析和讨论还可以参考如下文章：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>CPUThrottlingHigh false positives #108&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CFS quotas can lead to unnecessary throttling #67577&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CFS Bandwidth Control&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Overly aggressive CFS&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>其中《Overly aggressive CFS》里面还有几个小实验可以帮助大家更好的认识到 CFS 进行 CPU 资源管控的特点：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/qt8tld/1616119625964-b27b0e72-58fd-441f-81eb-c7d543aa9733.png" alt="image.png">&lt;/p>
&lt;p>转载自&lt;a href="https://blog.csdn.net/cloudvtech">https://blog.csdn.net/cloudvtech&lt;/a>&lt;/p></description></item></channel></rss>