<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 集群与分布式</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/</link><description>Recent content in 集群与分布式 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 分布式算法</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/</guid><description/></item><item><title>Docs: 分布式算法</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/</guid><description/></item><item><title>Docs: 分布式与集群的区别是什么？</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/</guid><description>
&lt;h3 id="大白话讲解分布式与集群的区别是什么">大白话讲解：分布式与集群的区别是什么？&lt;/h3>
&lt;p>参考：&lt;a href="https://blog.csdn.net/chenchunlin526/article/details/86077674">原文链接&lt;/a>&lt;/p>
&lt;h3 id="前言">前言：&lt;/h3>
&lt;p>参考了一些其他文章的见解，也补充了一些自己的见解。&lt;/p>
&lt;h3 id="一大白话解说用生活中的例子来说明">&lt;strong>一、大白话解说，用生活中的例子来说明：&lt;/strong>&lt;/h3>
&lt;p>小饭店原来只有一个厨师，切菜洗菜备料炒菜全干。后来客人多了，厨房一个厨师忙不过来，又请了个厨师，两个厨师都能炒一样的菜，两个厨师的关系是&lt;strong>集群&lt;/strong>。&lt;/p>
&lt;p>为了让厨师专心炒菜，把菜做到极致，再请了个配菜师负责切菜，备菜，备料 &amp;hellip; ， 厨师和配菜师的关系是&lt;strong>分布式&lt;/strong>。&lt;/p>
&lt;p>一个配菜师也忙不过来了，又请了个配菜师，两个配菜师关系是&lt;strong>集群&lt;/strong>。&lt;/p>
&lt;p>一个配菜师因故请假了，但是其余的配菜师还是该啥就干啥，只是没请假的配菜师任务均匀的加量了，但他们的任务和职责是不变的，这是&lt;strong>集群&lt;/strong>。&lt;/p>
&lt;p>店里生意很好，当店长接到订单后，看哪个厨师活儿不重，就将新的订单分给谁，这就是&lt;strong>负载均衡&lt;/strong>。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>集群：多个人在一起做同样的事 。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>分布式 ：多个人在一起做不同的事 。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>负载均衡：决定将任务以某种规则分给谁做。&lt;/strong>&lt;/p>
&lt;hr>
&lt;h3 id="二图解">&lt;strong>二、图解：&lt;/strong>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pcfi0a/1616132706039-3a4bf2af-22c7-4086-bd0e-8947e769d090.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/pcfi0a/1616132706053-725564b4-aed6-487f-b845-4413dffa8ed1.png" alt="">&lt;/p>
&lt;h3 id="三区别联系-其实上面的内容应该已经让你理解-2-者了">&lt;strong>三、区别联系 （其实上面的内容应该已经让你理解 2 者了）&lt;/strong>&lt;/h3>
&lt;p>1）我记得在一本讲 TCP/IP 的书上有这样一句话：&lt;strong>分布式是指多个系统协同合作完成一个特定任务的系统&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>分布式是解决中心化管理的问题，把所有的任务叠加到一个节点处理，太慢了。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>所以**&lt;strong>把一个大的问题拆分为多个小的问题，并分别解决，最终协同合作*&lt;/strong>*。分布式的主要工作是分解任务，将职能拆解&lt;/strong>。&lt;/p>
&lt;p>中心化带来的主要问题是可靠性，若中心节点宕机则整个系统不可用，分布式除了解决部分中心化问题，也倾向于分散负载，&lt;strong>但分布式会带来很多的其他问题，最主要的就是一致性&lt;/strong>。&lt;/p>
&lt;p>这些年吵得很热的&lt;strong>云计算&lt;/strong>实际上只是包装在分布式之外的“新”概念。&lt;/p>
&lt;p>2） &lt;strong>集群&lt;/strong>主要的使用场景是&lt;strong>为了分担请求的压力&lt;/strong>，也就是在几个服务器上部署相同的应用程序，配合负载均衡来分担客户端请求。&lt;/p>
&lt;p>当压力进一步增大的时候，可能在需要存储的部分，mysql 无法面对很多的写压力。因为在 mysql 做成集群之后，主要的写压力还是在 master 的机器上面，其他 slave 机器无法分担写压力，从而这个时候，也就引出来分布式。&lt;/p>
&lt;p>&lt;strong>分布式&lt;/strong>的主要应用场景是&lt;strong>单台机器已经无法满足这种性能的要求，必须要融合多个节点，并且节点之间是相关之间有交互的&lt;/strong>。相当于在写 mysql 的时候，每个节点存储部分数据，也就是&lt;strong>分布式存储&lt;/strong>的由来。在存储一些非结构化数据：静态文件、图片、pdf、小视频 &amp;hellip; 这些也就是分布式文件系统的由来。&lt;/p>
&lt;p>如：现在的 Spring Cloud 的分布式微服务架构，一个系统分解成了多个 Spring Boot 的微服务，各个微服务协同合作完成特定的任务。同个微服务又可以部署多台服务器形成微服务集群，从而提供高可用服务。&lt;/p>
&lt;p>3）&lt;strong>集群主要是简单加机器解决问题，对于问题本身不做任何分解&lt;/strong>；&lt;/p>
&lt;p>分布式处理里必然包含任务分解与答案归并。&lt;strong>分布式&lt;/strong>中的某个子任务节点，可能由一个集群来代替；&lt;strong>集群&lt;/strong>中任一节点，都是做一个完整的任务。&lt;/p>
&lt;p>集群和分布式都是由多个节点组成，但是集群之间的通信协调基本不需要；而分布式各个节点的通信协调必不可少。&lt;/p>
&lt;h3 id="总结">总结：&lt;/h3>
&lt;hr>
&lt;p>&lt;strong>将一套系统拆分成不同子系统部署在不同服务器上（这叫分布式），然后部署多个相同的子系统在不同的服务器上（这叫集群），部署在不同服务器上的同一个子系统需要做负载均衡处理。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>集群是个物理形态，分布式是个工作方式。&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>**分布式：一个业务拆分为多个子业务，部署在多个服务器上 。 **&lt;/p>
&lt;p>&lt;strong>集群：同一个业务，部署在多个服务器上 。&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>分布式&lt;/strong>：不同的业务模块部署在不同的服务器上或者同一个业务模块分拆多个子业务，部署在不同的服务器上，解决高并发的问题。&lt;/p>
&lt;p>&lt;strong>集群&lt;/strong>：同一个业务部署在多台机器上，提高系统可用性。&lt;/p>
&lt;hr>
&lt;p>【参考】：&lt;a href="https://www.zhihu.com/question/20004877">https://www.zhihu.com/question/20004877&lt;/a>，&lt;a href="https://www.zhihu.com/question/20004877">https://www.zhihu.com/question/20004877&lt;/a>，&lt;a href="https://blog.csdn.net/jiangyu1013/article/details/80417961">https://blog.csdn.net/jiangyu1013/article/details/80417961&lt;/a>&lt;/p></description></item><item><title>Docs: 负载均衡</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid><description>
&lt;p>四层、七层负载均衡的区别&lt;/p>
&lt;h1 id="一什么是负载均衡load-balancing">一，什么是负载均衡（Load balancing）&lt;/h1>
&lt;p>在网站创立初期，我们一般都使用单台机器对台提供集中式服务，但是随着业务量越来越大，无论是性能上还是稳定性上都有了更大的挑战。这时候我们就会想到通过扩容的方式来提供更好的服务。&lt;/p>
&lt;p>我们一般会把多台机器组成一个集群对外提供服务。然而，我们的网站对外提供的访问入口都是一个的，比如www.taobao.com。那么当用户在浏览器输入www.taobao.com的时候如何将用户的请求分发到集群中不同的机器上呢，这就是负载均衡在做的事情。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132229006-9cc152b6-68bc-4541-b75c-ef38c72b6991.jpeg" alt="">&lt;/p>
&lt;h1 id="二负载均衡按网络七层模型分类">二，负载均衡按网络七层模型分类&lt;/h1>
&lt;p>现在我们知道，负载均衡就是一种计算机网络技术，用来在多个计算机（计算机集群）、网络连接、CPU、磁碟驱动器或其他资源中分配负载，以达到最佳化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的。那么，这种计算机技术的实现方式有多种。大致可以分为以下几种，其中最常用的是四层和七层负载均衡：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>二层负载均衡&lt;/p>
&lt;/li>
&lt;li>
&lt;p>负载均衡服务器对外依然提供一个 VIP（虚 IP），集群中不同的机器采用相同 IP 地址，但是机器的 MAC 地址不一样。当负载均衡服务器接受到请求之后，通过改写报文的目标 MAC 地址的方式将请求转发到目标机器实现负载均衡。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>三层负载均衡&lt;/p>
&lt;/li>
&lt;li>
&lt;p>和二层负载均衡类似，负载均衡服务器对外依然提供一个 VIP（虚 IP），但是集群中不同的机器采用不同的 IP 地址。当负载均衡服务器接受到请求之后，根据不同的负载均衡算法，通过 IP 将请求转发至不同的真实服务器。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>四层负载均衡：TCP 层的 Load Balance,转发请求&lt;/p>
&lt;/li>
&lt;li>
&lt;p>四层负载均衡工作在 OSI 模型的传输层，由于在传输层，只有 TCP/UDP 协议，这两种协议中除了包含源 IP、目标 IP 以外，还包含源端口号及目的端口号。四层负载均衡服务器在接受到客户端请求后，以后通过修改数据包的地址信息（IP+端口号）将流量转发到应用服务器。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>应用场景：对于用户请求一个网站的图片，会根据用户请求中 URL 的地址:端口，转发到后端的服务器上，再由后端服务器处理该请求，这时候要求运维人员记住用户请求图片所在的服务器是哪台&lt;/p>
&lt;/li>
&lt;li>
&lt;p>七层负载均衡：HTTP 协议层的反向代理，代理请求&lt;/p>
&lt;/li>
&lt;li>
&lt;p>七层负载均衡工作在 OSI 模型的应用层，应用层协议较多，常用 http、radius、dns 等。七层负载就可以基于这些协议来负载。这些应用层协议中会包含很多有意义的内容。比如同一个 Web 服务器的负载均衡，除了根据 IP 加端口进行负载外，还可根据七层的 URL、浏览器类别、语言来决定是否要进行负载均衡。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>应用场景：对于用户请求一个网站的图片，会根据用户请求的 URL 来代理用户的该请求，重新构建请求报文，根据自身的缓存规则，比如一致性哈希算法，找到该图片的位置，然后把请求发送给该设备。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132229033-ae331c0a-6081-46ae-a118-89ff2490cb1c.jpeg" alt="">&lt;/p>
&lt;p>对于一般的应用来说，有了 Nginx 就够了。Nginx 可以用于七层负载均衡。但是对于一些大的网站，一般会采用 DNS+四层负载+七层负载的方式进行多层次负载均衡。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132229056-e7197895-718e-4c41-8470-de541575e410.jpeg" alt="">&lt;/p>
&lt;h1 id="三四层七层负载均衡对比">三、四层、七层负载均衡对比&lt;/h1>
&lt;p>所谓四层即运输层，就是基于 IP + 端口的负载均衡；七层即应用层，就是基于 URL 等应用层信息的负载均衡；&lt;/p>
&lt;p>同理，还有基于 MAC 地址的二层负载均衡和基于 IP 地址的三层负载均衡。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132228994-d0c7ede4-3016-4a5b-9ae2-088fd7fc699c.jpeg" alt="">&lt;/p>
&lt;p>换句换说，&lt;/p>
&lt;p>二层负载均衡会通过一个虚拟 MAC 地址接收请求，然后再分配到真实的 MAC 地址；&lt;/p>
&lt;p>三层负载均衡会通过一个虚拟 IP 地址接收请求，然后再分配到真实的 IP 地址；&lt;/p>
&lt;p>四层通过虚拟 IP + 端口接收请求，然后再分配到真实的服务器；&lt;/p>
&lt;p>七层通过虚拟的 URL 或主机名接收请求，然后再分配到真实的服务器。&lt;/p>
&lt;p>所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。&lt;/p>
&lt;p>比如四层的负载均衡，就是通过发布三层的 IP 地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，&lt;/p>
&lt;p>对需要处理的流量进行 NAT 处理，转发至后台服务器，并记录下这个 TCP 或者 UDP 的流量是由哪台服务器处理的，&lt;/p>
&lt;p>后续这个连接的所有流量都同样转发到同一台服务器处理。&lt;/p>
&lt;p>七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征， 比如同一个 Web 服务器的负载均衡，除了根据 VIP 加 80 端口辨别是否需要处理的流量， 还可根据七层的 URL、浏览器类别、语言来决定是否要进行负载均衡。&lt;/p>
&lt;p>举个例子，如果你的 Web 服务器分成两组，一组是中文语言的，一组是英文语言的，&lt;/p>
&lt;p>那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132229038-0c054f8b-f050-46b3-9d1d-bf0523bf4501.jpeg" alt="">&lt;/p>
&lt;p>负载均衡器通常称为四层交换机或七层交换机。&lt;/p>
&lt;p>四层交换机主要分析 IP 层及 TCP/UDP 层，实现四层流量负载均衡。&lt;/p>
&lt;p>七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如 HTTP 协议 URI 或 Cookie 信息。&lt;/p>
&lt;p>负载均衡分为 L4 Switch（四层交换），即在 OSI 第 4 层工作，就是 TCP 层啦。&lt;/p>
&lt;p>此种 Load Balancer 不理解应用协议（如 HTTP/FTP/MySQL 等等）。例子：LVS，F5。&lt;/p>
&lt;p>另一种叫做 L7 Switch（七层交换），OSI 的最高层，应用层。&lt;/p>
&lt;p>此时，该 Load Balancer 能理解应用协议。例子： HAProxy，MySQL Proxy。&lt;/p>
&lt;p>注意：上面的很多 Load Balancer 既可以做四层交换，也可以做七层交换。&lt;/p>
&lt;p>当前可以看到对于 F5, Array 等硬件负载均衡设备本身也是支持 7 层负载均衡的，&lt;/p>
&lt;p>同时在 4 层负载均衡的时候我们还可以设置是否进行会话保持等高级特性。&lt;/p>
&lt;p>要明白 4 层负载均衡本质是转发，而 7 层负载本质是内容交换和代理，具体说明如下：&lt;/p>
&lt;h1 id="四四层七层技术原理上的区别">四、四层，七层技术原理上的区别&lt;/h1>
&lt;p>所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。&lt;/p>
&lt;p>以常见的 TCP 为例，负载均衡设备在接收到第一个来自客户端的 SYN 请求时，即通过上述方式选择一个最佳的服务器， 并对报文中的目标 IP 地址进行修改(改为后端服务器 IP），直接转发给该服务器。 TCP 的连接建立，即三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作。&lt;/p>
&lt;p>在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132229034-860a5b39-2224-4d37-bbe6-c2011888f728.jpeg" alt="">&lt;/p>
&lt;p>所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，&lt;/p>
&lt;p>再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。&lt;/p>
&lt;p>以常见的　 TCP 　为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，&lt;/p>
&lt;p>只能先代理最终的服务器和客户端建立连接（TCP 三次握手）后，才可能接收到客户端发送的真正应用层内容的报文， 然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。&lt;/p>
&lt;p>负载均衡设备在这种情况下，更类似于一个代理服务器。负载均衡和前端的客户端以及后端的服务器会分别建立 TCP 连接。&lt;/p>
&lt;p>所以从这个技术原理上来看，七层负载均衡明显地对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。&lt;/p>
&lt;h1 id="五-应用场景的需求">五、 应用场景的需求&lt;/h1>
&lt;p>七层应用负载均衡的好处，是使得整个网络更“智能化”, 例如访问一个网站的用户流量，可以通过七层的方式，&lt;/p>
&lt;p>将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。&lt;/p>
&lt;p>当然这只是七层应用的一个小案例，从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，极大的提升了应用系统在网络层的灵活性。 很多在后台(例如 Nginx 或者 Apache )上部署的功能可以前移到负载均衡设备上，例如客户请求中的 Header 重写，服务器响应中的关键字过滤或者内容插入等功能。&lt;/p>
&lt;p>另外一个常常被提到功能就是安全性。网络中最常见的 SYN Flood 攻击，即黑客控制众多源客户端，使用虚假 IP 地址对同一目标发送 SYN 攻击，&lt;/p>
&lt;p>通常这种攻击会大量发送 SYN 报文，耗尽服务器上的相关资源，以达到 Denial of Service(DoS) 的目的。&lt;/p>
&lt;p>从技术原理上也可以看出，四层模式下这些 SYN 攻击都会被转发到后端的服务器上；&lt;/p>
&lt;p>而七层模式下这些 SYN 攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营。&lt;/p>
&lt;p>另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文，例如 SQL Injection 等应用层面的特定攻击手段，从应用层面进一步提高系统整体安全。&lt;/p>
&lt;p>现在的 7 层负载均衡，主要还是着重于应用广泛的 HTTP 协议，所以其应用范围主要是众多的网站或者内部信息平台等基于 B/S 开发的系统。&lt;/p>
&lt;p>4 层负载均衡则对应其他 TCP 应用，例如基于 C/S 开发的 ERP 等系统。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/awekiu/1616132229056-b0531047-5625-432f-ab76-8cef95f5fdb1.jpeg" alt="">&lt;/p>
&lt;h1 id="六七层应用需要考虑的问题">六、七层应用需要考虑的问题&lt;/h1>
&lt;p>是否真的必要，七层应用的确可以提高流量智能化，同时必不可免的带来设备配置复杂，负载均衡压力增高以及故障排查上的复杂性等问题。&lt;/p>
&lt;p>在设计系统时需要考虑四层七层同时应用的混杂情况。&lt;/p>
&lt;p>是否真的可以提高安全性。例如 SYN Flood 攻击，七层模式的确将这些流量从服务器屏蔽，但负载均衡设备本身要有强大的抗 DDoS 能力，&lt;/p>
&lt;p>否则即使服务器正常而作为中枢调度的负载均衡设备故障也会导致整个应用的崩溃。&lt;/p>
&lt;p>是否有足够的灵活度。七层应用的优势是可以让整个应用的流量智能化，但是负载均衡设备需要提供完善的七层功能，满足客户根据不同情况的基于应用的调度。&lt;/p>
&lt;p>最简单的一个考核就是能否取代后台 Nginx 或者 Apache 等服务器上的调度功能。&lt;/p>
&lt;p>能够提供一个七层应用开发接口的负载均衡设备，可以让客户根据需求任意设定功能，才真正有可能提供强大的灵活性和智能性。&lt;/p>
&lt;h1 id="七常用负载均衡工具">七、常用负载均衡工具&lt;/h1>
&lt;p>Nginx/LVS/HAProxy 是目前使用最广泛的三种负载均衡软件。&lt;/p>
&lt;p>LVS&lt;/p>
&lt;p>LVS（Linux Virtual Server），也就是 Linux 虚拟服务器, 是一个由章文嵩博士发起的自由软件项目。使用 LVS 技术要达到的目标是：通过 LVS 提供的负载均衡技术和 Linux 操作系统实现一个高性能、高可用的服务器群集，它具有良好可靠性、可扩展性和可操作性。从而以低廉的成本实现最优的服务性能。&lt;/p>
&lt;p>LVS 主要用来做四层负载均衡。&lt;/p>
&lt;p>Nginx&lt;/p>
&lt;p>Nginx（发音同 engine x）是一个网页服务器，它能反向代理 HTTP, HTTPS, SMTP, POP3, IMAP 的协议链接，以及一个负载均衡器和一个 HTTP 缓存。&lt;/p>
&lt;p>Nginx 主要用来做七层负载均衡。&lt;/p>
&lt;p>HAProxy&lt;/p>
&lt;p>HAProxy 是一个使用 C 语言编写的自由及开放源代码软件，其提供高可用性、负载均衡，以及基于 TCP 和 HTTP 的应用程序代理。&lt;/p>
&lt;p>Haproxy 主要用来做七层负载均衡。&lt;/p>
&lt;h1 id="八常见负载均衡算法">八，常见负载均衡算法&lt;/h1>
&lt;p>上面介绍负载均衡技术的时候提到过，负载均衡服务器在决定将请求转发到具体哪台真实服务器的时候，是通过负载均衡算法来实现的。负载均衡算法可以分为两类：静态负载均衡算法和动态负载均衡算法。&lt;/p>
&lt;p>静态负载均衡算法包括：轮询，比率，优先权&lt;/p>
&lt;p>动态负载均衡算法包括: 最少连接数,最快响应速度，观察方法，预测法，动态性能分配，动态服务器补充，服务质量，服务类型，规则模式。&lt;/p>
&lt;p>轮询（Round Robin）：顺序循环将请求一次顺序循环地连接每个服务器。当其中某个服务器发生第二到第 7 层的故障，BIG-IP 就把其从顺序循环队列中拿出，不参加下一次的轮询，直到其恢复正常。&lt;/p>
&lt;p>比率（Ratio）：给每个服务器分配一个加权值为比例，根椐这个比例，把用户的请求分配到每个服务器。当其中某个服务器发生第二到第 7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配, 直到其恢复正常。&lt;/p>
&lt;p>优先权（Priority）：给所有服务器分组,给每个组定义优先权，BIG-IP 用户的请求，分配给优先级最高的服务器组（在同一组内，采用轮询或比率算法，分配用户的请求）；当最高优先级中所有服务器出现故障，BIG-IP 才将请求送给次优先级的服务器组。这种方式，实际为用户提供一种热备份的方式。&lt;/p>
&lt;p>最少的连接方式（Least Connection）：传递新的连接给那些进行最少连接处理的服务器。当其中某个服务器发生第二到第 7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配, 直到其恢复正常。&lt;/p>
&lt;p>最快模式（Fastest）：传递连接给那些响应最快的服务器。当其中某个服务器发生第二到第 7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。&lt;/p>
&lt;p>观察模式（Observed）：连接数目和响应时间以这两项的最佳平衡为依据为新的请求选择服务器。当其中某个服务器发生第二到第 7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。&lt;/p>
&lt;p>预测模式（Predictive）：BIG-IP 利用收集到的服务器当前的性能指标，进行预测分析，选择一台服务器在下一个时间片内，其性能将达到最佳的服务器相应用户的请求。(被 BIG-IP 进行检测)&lt;/p>
&lt;p>动态性能分配(Dynamic Ratio-APM):BIG-IP 收集到的应用程序和应用服务器的各项性能参数，动态调整流量分配。&lt;/p>
&lt;p>动态服务器补充(Dynamic Server Act.):当主服务器群中因故障导致数量减少时，动态地将备份服务器补充至主服务器群。&lt;/p>
&lt;p>服务质量(QoS）:按不同的优先级对数据流进行分配。&lt;/p>
&lt;p>服务类型(ToS): 按不同的服务类型（在 Type of Field 中标识）负载均衡对数据流进行分配。&lt;/p>
&lt;p>规则模式：针对不同的数据流设置导向规则，用户可自行。&lt;/p>
&lt;p>文章参考链接：hollischuang.com/archives/1844；jaminzhang.github.io/lb/L4-L7-Load-Balancer-Difference/&lt;/p></description></item><item><title>Docs: 集群与分布式</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/</guid><description>
&lt;h1 id="cluster集群概述">Cluster(集群)概述&lt;/h1>
&lt;p>当单独一台主机无法承载现有的用户请求量；或者一台主机因为单一故障导致业务中断的时候，就可以增加服务主机数，这些主机在一起提供服务，就叫集群，而用户所看到的依然是单个的主机，用户并不用知道具体是集群内哪台设备为我提供服务，只需要知道访问集群的入口即可。&lt;/p>
&lt;p>集群类型包括：&lt;/p>
&lt;ol>
&lt;li>LB # Load Balancing 负载均衡&lt;/li>
&lt;li>HA # High Availability 高可用，双机主备&lt;/li>
&lt;li>HP # High Performancing 高性能&lt;/li>
&lt;/ol>
&lt;p>构建高可扩展性系统的重要原则：在系统内部尽量避免串行化和交互&lt;/p>
&lt;h2 id="load-balancing负载均衡-集群">Load Balancing(负载均衡) 集群&lt;/h2>
&lt;p>根据请求报文的目标 IP:PORT 将其转发至后端主机集群中的某一台主机&lt;/p>
&lt;p>LB 的作用：将业务请求分摊到多个后端设备进行执行，例如 Web 服务器、FTP 服务器等，当一台 web 服务器为 1W 人提供服务的时候，为了减少单台服务器的压力，可以把 1W 人分成 4 份，增加三台服务器，四台服务器每台 2500 人，共同完成工作任务，就算一台坏掉了，其余三台还能正常提供服务。也是变相实现了高可用，可以解决单点故障&lt;/p>
&lt;p>类似于交换机的转发以及路由器的转发，都是把收到的请求转发到另一个地方，LB 可以称为 4 层交换或 4 层路由，工作在 4 网络 7 层模型中 4 层及以上，主要是对协议请求报文进行广播，转发，广播是对于同一个区域来进行的（LB 整个架构中的每一台设备都相当于交换机一个端口，其中一个端口(调度器)收到请求报文，广播给其余的 RS 或选择一个 RS 进行接收该请求）&lt;/p>
&lt;p>负载均衡实现方式从软硬件来区分，分两种：&lt;/p>
&lt;ol>
&lt;li>硬件负载均衡：通过硬件设备来实现负载均衡功能，国内常用前三家的设备
&lt;ol>
&lt;li>F5 厂家的 BIG-IP，最好的，并发承载能力最高，价格也是最好的&lt;/li>
&lt;li>Citrix(思捷)厂家的 NetScaleer&lt;/li>
&lt;li>A10 厂家的 A10&lt;/li>
&lt;li>Array 厂家&lt;/li>
&lt;li>Redware&lt;/li>
&lt;li>等等等&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>软件负载均衡：通过软件技术来实现负载均衡功能
&lt;ol>
&lt;li>LVS(Linux virtual server)，Linux 自带的功能 2.LB 之 LVS 概念、类型、调度方法、命令操作与实际配置.note&lt;/li>
&lt;li>haProxy&lt;/li>
&lt;li>Nginx&lt;/li>
&lt;li>ATS(apache traffic service)&lt;/li>
&lt;li>等等。软件负载均衡基于工作的协议层次划分：
&lt;ol>
&lt;li>传输层：lvs，haproxy&lt;/li>
&lt;li>应用层：haproxy，nginx，ats&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="lb-的实现方式">LB 的实现方式&lt;/h3>
&lt;p>在很久很久以前，只有一台服务器来为用户提供服务，用户直接访问这台服务器的 IP 即可，如上文描述，当一台服务器不够用的时候，想让多台服务器同时为用户提供服务，这就是负载均衡的技术，那么这么多台服务器，每个服务器都有自己的 IP，用户如何知道自己访问哪个呢？&lt;/p>
&lt;p>这时候在这些服务器前面就需要有一个调度人员，来处理用户的请求，用户具体访问哪台服务器，由这个调度人员来决定，这个调度人员的工作，也可以同通过一台服务器来实现，这台调度服务器，就是实现 LB 技术的设备。而调度器(Director)的叫法，也是由此而来&lt;/p>
&lt;h2 id="high-availability高可用-集群">High Availability(高可用) 集群&lt;/h2>
&lt;p>为提升系统可用性，组合多台主机构建的集群称为 &lt;strong>High Availability(高可用，简称 HA)&lt;/strong> 集群&lt;/p>
&lt;p>HA 的作用：为避免单一资源损坏导致业务终端，那么就需要增加备用资源，当主资源坏了之后，备用资源可以立刻接替已坏资源的工作继续提供服务。两个资源中间需要交换数据以确认对方是否是正常运行，是否需要把备用资源启动成主资源提供服务，这种检测机制就是 heartbeat 心跳检测。（注意：资源包括但不限于真实物理机，虚拟机，操作系统，系统中的一个应用程序，系统中的一个进程等等。比如，当一台主机宕机了，可以由另一台主机接管；当系统中的 web 进程终止了，由另一台设备的 web 进程提供服务）&lt;/p>
&lt;h3 id="ha-的实现方式">HA 的实现方式&lt;/h3>
&lt;ul>
&lt;li>keepalived&lt;/li>
&lt;/ul>
&lt;h1 id="distributed分布式系统概述">Distributed(分布式)系统概述&lt;/h1>
&lt;blockquote>
&lt;p>参考文章：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.cnblogs.com/xybaby/p/7787034.html">https://www.cnblogs.com/xybaby/p/7787034.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是利用更多的机器，处理更多的数据。&lt;/p>
&lt;ol>
&lt;li>系统的各个组件分布于网络上的多个 NODE 上&lt;/li>
&lt;li>各组件之间仅仅通过消息传递来通信并协调行动&lt;/li>
&lt;/ol>
&lt;p>分布式系统存在的意义&lt;/p>
&lt;ol>
&lt;li>向上扩展的性价比越来越低&lt;/li>
&lt;li>单机扩展存在性能上升临界点&lt;/li>
&lt;li>处于稳定性以及可用性考虑，单机会存在多方面的问题&lt;/li>
&lt;/ol>
&lt;p>分布式计算：YARN&lt;/p>
&lt;p>batch：MapReduce&lt;/p>
&lt;p>in-memory:spark&lt;/p>
&lt;p>stream：storm&lt;/p>
&lt;h2 id="分布式系统的实现">分布式系统的实现&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>负载均衡：&lt;/p>
&lt;ul>
&lt;li>Nginx：高性能、高并发的 web 服务器；功能包括负载均衡、反向代理、静态内容缓存、访问控制；工作在应用层&lt;/li>
&lt;li>LVS： Linux virtual server，基于集群技术和 Linux 操作系统实现一个高性能、高可用的服务器；工作在网络层&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>webserver：&lt;/p>
&lt;ul>
&lt;li>Java：Tomcat，Apache，Jboss&lt;/li>
&lt;li>Python：gunicorn、uwsgi、twisted、webpy、tornado&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>service：&lt;/p>
&lt;ul>
&lt;li>SOA、微服务、spring boot，django&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>容器：&lt;/p>
&lt;ul>
&lt;li>docker，kubernetes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>cache：&lt;/p>
&lt;ul>
&lt;li>memcache、redis 等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>协调中心、仲裁系统：分布式协作协议，分布式系统中只要存在主备节点这种角色，则必须要有一套系统来选举出来哪个是主的，这就是仲裁系统的由来&lt;/p>
&lt;ul>
&lt;li>zookeeper、etcd、VRRP、corosync 等&lt;/li>
&lt;li>zookeeper 使用了 Paxos 协议 Paxos 是强一致性，高可用的去中心化分布式。zookeeper 的使用场景非常广泛，之后细讲。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>rpc 框架：&lt;/p>
&lt;ul>
&lt;li>grpc、dubbo、brpc&lt;/li>
&lt;li>dubbo 是阿里开源的 Java 语言开发的高性能 RPC 框架，在阿里系的诸多架构中，都使用了 dubbo + spring boot&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>消息队列：&lt;/p>
&lt;ul>
&lt;li>kafka、rabbitMQ、rocketMQ、QSP&lt;/li>
&lt;li>消息队列的应用场景：异步处理、应用解耦、流量削锋和消息通讯&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>实时数据平台：&lt;/p>
&lt;ul>
&lt;li>storm、akka&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>离线数据平台：(分布式存储)&lt;/p>
&lt;ul>
&lt;li>hadoop(HDFS)、spark&lt;/li>
&lt;li>PS: apark、akka、kafka 都是 scala 语言写的，看到这个语言还是很牛逼的• dbproxy：&lt;/li>
&lt;li>cobar 也是阿里开源的，在阿里系中使用也非常广泛，是关系型数据库的 sharding + replica 代理&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>db：&lt;/p>
&lt;ul>
&lt;li>mysql、oracle、MongoDB、HBase&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>搜索：&lt;/p>
&lt;ul>
&lt;li>elasticsearch、solr&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>日志：&lt;/p>
&lt;ul>
&lt;li>rsyslog、elk、flume&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="分布式系统的基本问题可用性与一致性">分布式系统的基本问题：可用性与一致性&lt;/h2>
&lt;p>分布式系统的挑战&lt;/p>
&lt;p>一致性可理解为所有节点都能访问到最新版本的数据，这在单机场景下非常容易实现，使用共享内存和锁即可解决，但数据存储在单机会有两个限制：&lt;/p>
&lt;p>1）单机不可用系统整体将不可用；&lt;/p>
&lt;p>2）系统吞吐量受限于单机的计算能力。&lt;/p>
&lt;p>消除这两个限制的方法是用多机来存储数据的多个副本，负责更新的客户端会同时更新数据的多个副本，于是问题就来了，多机之间的网络可能无法连接，当负责更新的客户端无法同时到连接多个机器时，如何能保证所有客户端都能读到最新版本的数据？&lt;/p>
&lt;p>如下图 1 中所示，Client A 负责更新数据，为了保证 Server 1 和 Server 2 上的数据是一致的，Client A 会将 X=1 的写操作同时发给 Server 1 和 Server 2，但是当 Client A 和 Server 2 之间发生网络分区（网络无法连接）时，此时如果让 write X=1 的写操作在 Server 1 上成功，那 Client B 和 Client C 将从 Server 1 和 Server 2 上读取到不一致的 X 值；此时如果要保持 X 值的一致性，那么 write X=1 的写操作在 Server 1 和 Server 2 上都必须失败，这就是著名的 CAP 理论：在容忍网络分区的前提下，要么牺牲数据的一致性，要么牺牲写操作的可用性。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ge97s8/1616132737152-16151a6a-007b-46d9-9f95-30f279dce9c9.jpeg" alt="">&lt;/p>
&lt;p>图 1：CAP 理论示意图&lt;/p>
&lt;p>解决这个问题你可能会想到让 Client C 同时读取 Server 1 和 Server 2 上的 X 值和版本信息，然后取 Server 1 和 Server 2 最新版本的 X 值，如下图 2 所示。但 Client C 和 Server 1 之间也可能发生网络分区，这本质上是牺牲读可用性换取写可用性，并没有突破 CAP 理论。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ge97s8/1616132737170-045ed97b-a9dc-4ff6-8c3b-c3f55b86be73.jpeg" alt="">&lt;/p>
&lt;p>图 2：对图 1 中可用性的优化&lt;/p>
&lt;h2 id="cap-理论">CAP 理论&lt;/h2>
&lt;p>CAP 理论由加州大学伯克利分校的计算机教授 Eric Brewer 在 2000 年提出，其核心思想是任何基于网络的数据共享系统最多只能满足数据一致性(Consistency)、可用性(Availability)和网络分区容忍(Partition Tolerance)三个特性中的两个，三个特性的定义如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Consistency(数据一致性)&lt;/strong>：等同于所有节点拥有数据的最新版本&lt;/li>
&lt;li>&lt;strong>Availability(可用性)&lt;/strong>：数据具备高可用性&lt;/li>
&lt;li>&lt;strong>Partition Tolerance(分区容忍)&lt;/strong>：容忍网络出现分区，分区之间网络不可达&lt;/li>
&lt;/ul>
&lt;p>在大规模的分布式环境下，网络分区是必须容忍的现实，于是只能在可用性和一致性两者间做出选择，CAP 理论似乎给分布式系统定义了一个悲观的结局，一时间大家都按照 CAP 理论在对热门的分布式系统进行判定，譬如认为 HBase 是一个 CP 系统、Cassandra 是 AP 系统。&lt;/p>
&lt;p>我个人认为这是不严谨的，理由是 CAP 理论是对分布式系统中一个数据无法同时达到可用性和一致性的断言，而一个系统中往往存在很多类型的数据，部分数据（譬如银行账户中的余额）是需要强一致性的，而另外一部分数据（譬如银行的总客户数）并不要求强一致性，所以拿 CAP 理论来划分整个系统是不严谨的， CAP 理论带来的价值是指引我们在设计分布式系统时需要区分各种数据的特点，并仔细考虑在小概率的网络分区发生时究竟为该数据选择可用性还是一致性。&lt;/p>
&lt;p>对 CAP 理论的另外一种误读是系统设计时选择其一而完全不去优化另外一项，可用性和一致性的取值范围并不是只有 0 和 1，可用性的值域可以定义成 0 到 100%的连续区间，而一致性也可分为强一致性、弱一致性、读写一致性、最终一致性等多个不同的强弱等级，细想下去 CAP 理论定义的其实是在容忍网络分区的条件下，“强一致性”和“极致可用性”无法同时达到。&lt;/p>
&lt;p>（注：这里用“极致可用性”而不用“100%可用性”是因为即使不考虑一致性，多台 server 组成的分布式系统也达不到 100%的可用性，如果单个 server 的可用性是 P，那 n 台 server 的极致可用性是&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ge97s8/1616132737170-80fe6908-dc56-498a-af88-7f6772b44251.jpeg" alt="">&lt;/p>
&lt;p>，公式的意思是只要任何一台或多台 server 可用就认为系统都是可用的）&lt;/p>
&lt;p>虽然无法达到同时达到强一致性和极致可用性，但我们可以根据数据类型在二者中选择其一后去优化另外一个，Paxos 协议就是一种在保证强一致性前提下把可用性优化到极限的算法。&lt;/p>
&lt;p>Paxos 协议&lt;/p>
&lt;p>Paxos 协议由 Leslie Lamport 最早在 1990 年提出，由于 Paxos 在云计算领域的广泛应用 Leslie Lamport 因此获得了 2013 年度图灵奖。&lt;/p>
&lt;p>Paxos 协议提出只要系统中 2f+1 个节点中的 f+1 个节点可用，那么系统整体就可用并且能保证数据的强一致性，它对于可用性的提升是极大的，仍然假设单节点的可用性是 P，那么 2f+1 个节点中任意组合的 f+1 以上个节点正常的可用性 P(total)=&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ge97s8/1616132737159-28e7a301-dfb4-412a-92ad-6c68f3331af1.jpeg" alt="">&lt;/p>
&lt;p>，又假设 P=0.99，f=2，P(total)=0.9999901494，可用性将从单节点的 2 个 9 提升到了 5 个 9，这意味着系统每年的宕机时间从 87.6 小时降到 0.086 小时，这已经可以满足地球上 99.99999999%的应用需求。&lt;/p>
&lt;p>Leslie 写的两篇论文：《The Part-Time Parliament》和《Paxos Made Simple》比较完整的阐述了 Paxos 的工作流程和证明过程，Paxos 协议把每个数据写请求比喻成一次提案（proposal），每个提案都有一个独立的编号，提案会转发到提交者（Proposer）来提交，提案必须经过 2f+1 个节点中的 f+1 个节点接受才会生效，2f+1 个节点叫做这次提案的投票委员会(Quorum)，投票委员会中的节点叫做 Acceptor，Paxos 协议流程还需要满足两个约束条件：&lt;/p>
&lt;p>a）Acceptor 必须接受它收到的第一个提案；&lt;/p>
&lt;p>b）如果一个提案的 v 值被大多数 Acceptor 接受过，那后续的所有被接受的提案中也必须包含 v 值（v 值可以理解为提案的内容，提案由一个或多个 v 和提案编号组成）。&lt;/p>
&lt;p>Paxos 协议流程划分为两个阶段，第一阶段是 Proposer 学习提案最新状态的准备阶段；第二阶段是根据学习到的状态组成正确提案提交的阶段，完整的协议过程如下：&lt;/p>
&lt;p>阶段 1&lt;/p>
&lt;ol>
&lt;li>Proposer 选择一个提案编号 n ，然后向半数以上的 Acceptors 发送编号为 n 的 prepare 请求。&lt;/li>
&lt;li>如果一个 Acceptor 收到一个编号为 n 的 prepare 请求，且 n 大于它已经响应的所有 prepare 请求的编号，那么它就会保证不会再通过(accept)任何编号小于 n 的提案，同时将它已经通过的最大编号的提案(如果存在的话)作为响应。&lt;/li>
&lt;/ol>
&lt;p>阶段 2&lt;/p>
&lt;ol>
&lt;li>
&lt;p>如果 Proposer 收到来自半数以上的 Acceptor 对于它的 prepare 请求(编号为 n )的响应，那么它就会发送一个针对编号为 n ，value 值为 v 的提案的 accept 请求给 Acceptors，在这里 v 是收到的响应中编号最大的提案的值，如果响应中不包含提案，那么它就是任意值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果 Acceptor 收到一个针对编号 n 的提案的 accept 请求，只要它还未对编号大于 n 的 prepare 请求作出响应，它就可以通过这个提案。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>用时序图来描述 Paxos 协议如图 3 所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ge97s8/1616132737201-958df2f3-6832-438a-b517-9622d3409615.jpeg" alt="">&lt;/p>
&lt;p>图 3：Paxos 协议流程的时序图&lt;/p>
&lt;p>上述 Paxos 协议流程看起来比较复杂，是因为要保证很多边界条件下的协议完备性，譬如初试值为空、两个 Proposer 同时提交提案等情况，但 Paxos 协议的核心可以简单描述为：Proposer 先从大多数 Acceptor 那里学习提案的最新内容，然后根据学习到的编号最大的提案内容组成新的提案提交，如果提案获得大多数 Acceptor 的投票通过就意味着提案被通过。由于学习提案和通过提案的 Acceptor 集合都超过了半数，所以一定能学到最新通过的提案值，两次提案通过的 Acceptor 集合中也一定存在一个公共的 Acceptor，在满足约束条件 b 时这个公共的 Acceptor 时保证了数据的一致性，于是 Paxos 协议又被称为多数派协议。&lt;/p>
&lt;p>Paxos 协议的真正伟大之处在于它的简洁性，Paxos 协议流程中任何消息都是可以丢失的，一致性保证并不依赖某个特殊消息传递的成功，这极大的简化了分布式系统的设计，极其匹配分布式环境下网络可能分区的特点，相比较在 Paxos 协议之前的“两阶段提交（2PC）”也能保证数据强一致性，但复杂度相当高且依赖单个协调者的可用性。&lt;/p>
&lt;p>那既然 Paxos 如此强大，那为什么还会出现 ZAB 协议？&lt;/p>
&lt;p>ZAB 协议&lt;/p>
&lt;p>Paxos 协议虽然是完备的，但要把它应用到实际的分布式系统中还有些问题要解决：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在多个 Proposer 的场景下，Paxos 不保证先提交的提案先被接受，实际应用中要保证多提案被接受的先后顺序怎么办？&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Paxos 允许多个 Proposer 提交提案，那有可能出现活锁问题，出现场景是这样的：提案 n 在第二阶段还没有完成时，新的提案 n+1 的第一阶段 prepare 请求到达 Acceptor，按协议规定 Acceptor 将响应新提案的 prepare 请求并保证不会接受小于 n+1 的任何请求，这可能导致提案 n 将不会被通过，同样在 n+1 提案未完成第二阶段时，假如提案 n 的提交者又提交了 n+2 提案，这可能导致 n+1 提案也无法通过。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Paxos 协议规定提案的值 v 只要被大多数 Acceptor 接受过，后续的所有提案不能修改值 v，那现实情况下我还要修改 v 值怎么办？&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ZooKeeper 的核心算法 ZAB 通过一个简单的约束解决了前 2 个问题：所有提案都转发到唯一的 Leader（通过 Leader 选举算法从 Acceptor 中选出来的）来提交，由 Leader 来保证多个提案之间的先后顺序，同时也避免了多 Proposer 引发的活锁问题。&lt;/p>
&lt;p>ZAB 协议的过程用时序图描述如图 4 所示，相比 Paxos 协议省略了 Prepare 阶段，因为 Leader 本身就有提案的最新状态，不需要有提案内容学习的过程,图中的 Follower 对应 Paxos 协议中的 Acceptor，Observer 对应 Paxos 中的 Learner。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ge97s8/1616132737174-2acbb000-a721-40b7-bc06-4c671a70bb72.jpeg" alt="">&lt;/p>
&lt;p>图 4：ZAB 协议的工作过程&lt;/p>
&lt;p>ZAB 引入 Leader 后也会带来一个新问题： Leader 宕机了怎么办？其解决方案是选举出一个新的 Leader，选举 Leader 的过程也是一个 Paxos 提案决议过程，这里不展开讨论。&lt;/p>
&lt;p>那如何做到提案的值 v 可以修改呢？这不是 ZAB 协议的范畴，研究 ZooKeeper 源码后发现它是这么做的：ZooKeeper 提供了一个 znode 的概念，znode 可以被修改，ZooKeeper 对每个 znode 都记录了一个自增且连续的版本号，对 znode 的任何修改操作（create/set/setAcl）都会促发一次 Paxos 多数派投票过程，投票通过后 znode 版本号加 1，这相当于用 znode 不同版本的多次 Paxos 协议来破除单次 Paxos 协议无法修改提案值的限制。&lt;/p>
&lt;p>从保证一致性的算法核心角度看 ZAB 确实是借鉴了 Paxos 的多数派思想，但它提供的全局时序保证以及 ZooKeeper 提供给用户可修改的 znode 才让 Paxos 在开源界大放异彩，所以 ZAB 的价值不仅仅是提供了 Paxos 算法的优化实现，也难怪 ZAB 的作者一直强调 ZAB 和 Paxos 是不一样的算法。&lt;/p>
&lt;p>总结&lt;/p>
&lt;p>CAP 理论告诉我们在分布式环境下网络分区无法避免，需要去权衡选择数据的一致性和可用性，Paxos 协议提出了一种极其简单的算法在保障数据一致性时最大限度的优化了可用性，ZooKeeper 的 ZAB 协议把 Paxos 更加简化，并提供全局时序保证，使得 Paxos 能够广泛应用到工业场景。&lt;/p></description></item><item><title>Docs: 可能是把 ZooKeeper 概念讲的最清楚的一篇文章</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%8F%AF%E8%83%BD%E6%98%AF%E6%8A%8A-ZooKeeper-%E6%A6%82%E5%BF%B5%E8%AE%B2%E7%9A%84%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%8F%AF%E8%83%BD%E6%98%AF%E6%8A%8A-ZooKeeper-%E6%A6%82%E5%BF%B5%E8%AE%B2%E7%9A%84%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;blockquote>
&lt;p>该文已加入开源文档：JavaGuide（一份涵盖大部分 Java 程序员所需要掌握的核心知识）。地址:&lt;a href="https://github.com/Snailclimb/JavaGuide">https://github.com/Snailclimb/JavaGuide&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>1. 前言&lt;/p>
&lt;p>相信大家对 ZooKeeper 应该不算陌生。但是你真的了解 ZooKeeper 到底有啥用不？如果别人/面试官让你给他讲讲对于 ZooKeeper 的认识，你能回答到什么地步呢？&lt;/p>
&lt;p>拿我自己来说吧！我本人曾经使用 Dubbo 来做分布式项目的时候，使用了 ZooKeeper 作为注册中心。为了保证分布式系统能够同步访问某个资源，我还使用 ZooKeeper 做过分布式锁。另外，我在学习 Kafka 的时候，知道 Kafka 很多功能的实现依赖了 ZooKeeper。&lt;/p>
&lt;p>前几天，总结项目经验的时候，我突然问自己 ZooKeeper 到底是个什么东西？想了半天，脑海中只是简单的能浮现出几句话：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ZooKeeper 可以被用作注册中心、分布式锁；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ZooKeeper 是 Hadoop 生态系统的一员；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>构建 ZooKeeper 集群的时候，使用的服务器最好是奇数台。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>由此可见，我对于 ZooKeeper 的理解仅仅是停留在了表面。&lt;/p>
&lt;p>所以，通过本文，希望带大家稍微详细的了解一下 ZooKeeper 。如果没有学过 ZooKeeper ，那么本文将会是你进入 ZooKeeper 大门的垫脚砖。如果你已经接触过 ZooKeeper ，那么本文将带你回顾一下 ZooKeeper 的一些基础概念。&lt;/p>
&lt;p>另外，本文不光会涉及到 ZooKeeper 的一些概念，后面的文章会介绍到 ZooKeeper 常见命令的使用以及使用 Apache Curator 作为 ZooKeeper 的客户端。&lt;/p>
&lt;p>&lt;em>如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！&lt;/em>&lt;/p>
&lt;h2 id="2-zookeeper-介绍">2. ZooKeeper 介绍&lt;/h2>
&lt;h3 id="21-zookeeper-由来">2.1. ZooKeeper 由来&lt;/h3>
&lt;p>正式介绍 ZooKeeper 之前，我们先来看看 ZooKeeper 的由来，还挺有意思的。&lt;/p>
&lt;p>下面这段内容摘自《从 Paxos 到 ZooKeeper 》第四章第一节，推荐大家阅读一下：&lt;/p>
&lt;blockquote>
&lt;p>ZooKeeper 最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的 Pig 项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家 RaghuRamakrishnan 开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而 ZooKeeper 正好要用来进行分布式环境的协调一一于是，ZooKeeper 的名字也就由此诞生了。&lt;/p>
&lt;/blockquote>
&lt;h3 id="22-zookeeper-概览">2.2. ZooKeeper 概览&lt;/h3>
&lt;p>ZooKeeper 是一个开源的&lt;strong>分布式协调服务&lt;/strong>，它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>原语：&lt;/strong> 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>ZooKeeper 为我们提供了高可用、高性能、稳定的分布式数据一致性解决方案，通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。&lt;/strong>&lt;/p>
&lt;p>另外，&lt;strong>ZooKeeper 将数据保存在内存中，性能是非常棒的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景）。&lt;/strong>&lt;/p>
&lt;h3 id="23-zookeeper-特点">2.3. ZooKeeper 特点&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>顺序一致性：&lt;/strong> 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>原子性：&lt;/strong> 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>单一系统映像 ：&lt;/strong> 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>可靠性：&lt;/strong> 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="24-zookeeper-典型应用场景">2.4. ZooKeeper 典型应用场景&lt;/h3>
&lt;p>ZooKeeper 概览中，我们介绍到使用其通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。&lt;/p>
&lt;p>下面选 3 个典型的应用场景来专门说说：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>分布式锁&lt;/strong> ： 通过创建唯一节点获得分布式锁，当获得锁的一方执行完相关代码或者是挂掉之后就释放锁。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>命名服务&lt;/strong> ：可以通过 ZooKeeper 的顺序节点生成全局唯一 ID&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>数据发布/订阅&lt;/strong> ：通过 &lt;strong>Watcher 机制&lt;/strong> 可以很方便地实现数据发布/订阅。当你将数据发布到 ZooKeeper 被监听的节点上，其他机器可通过监听 ZooKeeper 上节点的变化来实现配置的动态更新。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>实际上，这些功能的实现基本都得益于 ZooKeeper 可以保存数据的功能，但是 ZooKeeper 不适合保存大量数据，这一点需要注意。&lt;/p>
&lt;h3 id="25-有哪些著名的开源项目用到了-zookeeper">2.5. 有哪些著名的开源项目用到了 ZooKeeper?&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Kafka&lt;/strong> : ZooKeeper 主要为 Kafka 提供 Broker 和 Topic 的注册以及多个 Partition 的负载均衡等功能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Hbase&lt;/strong> : ZooKeeper 为 Hbase 提供确保整个集群只有一个 Master 以及保存和提供 regionserver 状态信息（是否在线）等功能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Hadoop&lt;/strong> : ZooKeeper 为 Namenode 提供高可用支持。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="3-zookeeper-重要概念解读">3. ZooKeeper 重要概念解读&lt;/h2>
&lt;p>&lt;em>破音：拿出小本本，下面的内容非常重要哦！&lt;/em>&lt;/p>
&lt;h3 id="31-data-model数据模型">3.1. Data model（数据模型）&lt;/h3>
&lt;p>ZooKeeper 数据模型采用层次化的多叉树形结构，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二级制序列。并且。每个节点还可以拥有 N 个子节点，最上层是根节点以“/”来代表。每个数据节点在 ZooKeeper 中被称为 &lt;strong>znode&lt;/strong>，它是 ZooKeeper 中数据的最小单元。并且，每个 znode 都一个唯一的路径标识。&lt;/p>
&lt;p>强调一句：&lt;strong>ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。&lt;/strong>&lt;/p>
&lt;p>从下图可以更直观地看出：ZooKeeper 节点路径标识方式和 Unix 文件系统路径非常相似，都是由一系列使用斜杠&amp;quot;/&amp;ldquo;进行分割的路径表示，开发人员可以向这个节点中写人数据，也可以在节点下面创建子节点。这些操作我们后面都会介绍到。&lt;/p>
&lt;h3 id="32-znode数据节点">3.2. znode（数据节点）&lt;/h3>
&lt;p>介绍了 ZooKeeper 树形数据模型之后，我们知道每个数据节点在 ZooKeeper 中被称为 &lt;strong>znode&lt;/strong>，它是 ZooKeeper 中数据的最小单元。你要存放的数据就放在上面，是你使用 ZooKeeper 过程中经常需要接触到的一个概念。&lt;/p>
&lt;p>3.2.1. znode 4 种类型&lt;/p>
&lt;p>我们通常是将 znode 分为 4 大类：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>持久（PERSISTENT）节点&lt;/strong> ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>临时（EPHEMERAL）节点&lt;/strong> ：临时节点的生命周期是与 &lt;strong>客户端会话（session）&lt;/strong> 绑定的，&lt;strong>会话消失则节点消失&lt;/strong> 。并且，&lt;strong>临时节点只能做叶子节点&lt;/strong> ，不能创建子节点。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>持久顺序（PERSISTENT_SEQUENTIAL）节点&lt;/strong> ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 &lt;code>/node1/app0000000001&lt;/code> 、&lt;code>/node1/app0000000002&lt;/code> 。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>临时顺序（EPHEMERAL_SEQUENTIAL）节点&lt;/strong> ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。&lt;/p>
&lt;p>3.2.2. znode 数据结构&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>每个 znode 由 2 部分组成:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>stat&lt;/strong> ：状态信息&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>data&lt;/strong> ： 节点存放的数据的具体内容&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>如下所示，我通过 get 命令来获取 根目录下的 dubbo 节点的内容。（get 命令在下面会介绍到）。&lt;/p>
&lt;pre>&lt;code>[zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo
# 该数据节点关联的数据内容为空
null
# 下面是该数据节点的一些状态信息，其实就是 Stat 对象的格式化输出
cZxid = 0x2
ctime = Tue Nov 27 11:05:34 CST 2018
mZxid = 0x2
mtime = Tue Nov 27 11:05:34 CST 2018
pZxid = 0x3
cversion = 1
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 1
&lt;/code>&lt;/pre>
&lt;p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Plain Text&lt;/p>
&lt;p>Stat 类中包含了一个数据节点的所有状态信息的字段，包括事务 ID-cZxid、节点创建时间-ctime 和子节点个数-numChildren 等等。&lt;/p>
&lt;p>下面我们来看一下每个 znode 状态信息究竟代表的是什么吧！（下面的内容来源于《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》，因为 Guide 确实也不是特别清楚，要学会参考资料的嘛！ ） ：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>znode 状态信息&lt;/td>
&lt;td>解释&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cZxid&lt;/td>
&lt;td>create ZXID，即该数据节点被创建时的事务 id&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ctime&lt;/td>
&lt;td>create time，即该节点的创建时间&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mZxid&lt;/td>
&lt;td>modified ZXID，即该节点最终一次更新时的事务 id&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>mtime&lt;/td>
&lt;td>modified time，即该节点最后一次的更新时间&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pZxid&lt;/td>
&lt;td>该节点的子节点列表最后一次修改时的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cversion&lt;/td>
&lt;td>子节点版本号，当前节点的子节点每次变化时值增加 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dataVersion&lt;/td>
&lt;td>数据节点内容版本号，节点创建时为 0，每更新一次节点内容(不管内容有无变化)该版本号的值增加 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>aclVersion&lt;/td>
&lt;td>节点的 ACL 版本号，表示该节点 ACL 信息变更次数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ephemeralOwner&lt;/td>
&lt;td>创建该临时节点的会话的 sessionId；如果当前节点为持久节点，则 ephemeralOwner=0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dataLength&lt;/td>
&lt;td>数据节点内容长度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>numChildren&lt;/td>
&lt;td>当前节点的子节点个数&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="33-版本version">3.3. 版本（version）&lt;/h3>
&lt;p>在前面我们已经提到，对应于每个 znode，ZooKeeper 都会为其维护一个叫作 &lt;strong>Stat&lt;/strong> 的数据结构，Stat 中记录了这个 znode 的三个相关的版本：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>dataVersion&lt;/strong> ：当前 znode 节点的版本号&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>cversion&lt;/strong> ： 当前 znode 子节点的版本&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>aclVersion&lt;/strong> ： 当前 znode 的 ACL 的版本。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="34-acl权限控制">3.4. ACL（权限控制）&lt;/h3>
&lt;p>ZooKeeper 采用 ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。&lt;/p>
&lt;p>对于 znode 操作的权限，ZooKeeper 提供了以下 5 种：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>CREATE&lt;/strong> : 能创建子节点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>READ&lt;/strong> ：能获取节点数据和列出其子节点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>WRITE&lt;/strong> : 能设置/更新节点数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>DELETE&lt;/strong> : 能删除子节点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ADMIN&lt;/strong> : 能设置节点 ACL 的权限&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>其中尤其需要注意的是，&lt;strong>CREATE&lt;/strong> 和 &lt;strong>DELETE&lt;/strong> 这两种权限都是针对 &lt;strong>子节点&lt;/strong> 的权限控制。&lt;/p>
&lt;p>对于身份认证，提供了以下几种方式：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>world&lt;/strong> ： 默认方式，所有用户都可无条件访问。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>auth&lt;/strong> :不使用任何 id，代表任何已认证的用户。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>digest&lt;/strong> :用户名:密码认证方式： &lt;em>username:password&lt;/em> 。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ip&lt;/strong> : 对指定 ip 进行限制。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="35-watcher事件监听器">3.5. Watcher（事件监听器）&lt;/h3>
&lt;p>Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gn3rcr/1616130259256-38dfceb6-4eff-4fb1-a41c-f2383f96b8e9.png" alt="">&lt;/p>
&lt;p>&lt;em>破音：非常有用的一个特性，都能出小本本记好了，后面用到 ZooKeeper 基本离不开 Watcher（事件监听器）机制。&lt;/em>&lt;/p>
&lt;h3 id="36-会话session">3.6. 会话（Session）&lt;/h3>
&lt;p>Session 可以看作是 ZooKeeper 服务器与客户端的之间的一个 TCP 长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watcher 事件通知。&lt;/p>
&lt;p>Session 有一个属性叫做：&lt;code>sessionTimeout&lt;/code> ，&lt;code>sessionTimeout&lt;/code> 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在&lt;code>sessionTimeout&lt;/code>规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。另外，在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 &lt;code>sessionID&lt;/code>。由于 &lt;code>sessionID&lt;/code>是 ZooKeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 &lt;code>sessionID&lt;/code> 的，因此，无论是哪台服务器为客户端分配的 &lt;code>sessionID&lt;/code>，都务必保证全局唯一。&lt;/p>
&lt;h2 id="4-zookeeper-集群">4. ZooKeeper 集群&lt;/h2>
&lt;p>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。ZooKeeper 官方提供的架构图就是一个 ZooKeeper 集群整体对外提供服务。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gn3rcr/1616130259317-567b57ec-3ff9-408d-81ac-fb1c74b69c18.png" alt="">&lt;/p>
&lt;p>上图中每一个 Server 代表一个安装 ZooKeeper 服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 ZAB 协议（ZooKeeper Atomic Broadcast）来保持数据的一致性。&lt;/p>
&lt;p>&lt;strong>最典型集群模式： Master/Slave 模式（主备模式）&lt;/strong>。在这种模式中，通常 Master 服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。&lt;/p>
&lt;h3 id="41-zookeeper-集群角色">4.1. ZooKeeper 集群角色&lt;/h3>
&lt;p>但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了 Leader、Follower 和 Observer 三种角色。如下图所示&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/gn3rcr/1616130259179-822f082a-f721-4744-b57a-eab7e0da264b.png" alt="">&lt;/p>
&lt;p>ZooKeeper 集群中的所有机器通过一个 &lt;strong>Leader 选举过程&lt;/strong> 来选定一台称为 “&lt;strong>Leader&lt;/strong>” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，&lt;strong>Follower&lt;/strong> 和 &lt;strong>Observer&lt;/strong> 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>角色&lt;/td>
&lt;td>说明&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Leader&lt;/td>
&lt;td>为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Follower&lt;/td>
&lt;td>为客户端提供读服务，如果是写服务则转发给 Leader。在选举过程中参与投票。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Observer&lt;/td>
&lt;td>为客户端提供读服务器，如果是写服务则转发给 Leader。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。此角色于 ZooKeeper3.3 系列新增的角色。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，就会进入 Leader 选举过程，这个过程会选举产生新的 Leader 服务器。&lt;/p>
&lt;p>这个过程大致是这样的：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Leader election（选举阶段）&lt;/strong>：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Discovery（发现阶段）&lt;/strong> ：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Synchronization（同步阶段）&lt;/strong> :同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Broadcast（广播阶段）&lt;/strong> :到了这个阶段，ZooKeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="42-zookeeper-集群中的服务器状态">4.2. ZooKeeper 集群中的服务器状态&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>LOOKING&lt;/strong> ：寻找 Leader。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>LEADING&lt;/strong> ：Leader 状态，对应的节点为 Leader。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>FOLLOWING&lt;/strong> ：Follower 状态，对应的节点为 Follower。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>OBSERVING&lt;/strong> ：Observer 状态，对应节点为 Observer，该节点不参与 Leader 选举。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="43-zookeeper-集群为啥最好奇数台">4.3. ZooKeeper 集群为啥最好奇数台？&lt;/h3>
&lt;p>ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。&lt;/p>
&lt;p>综上，何必增加那一个不必要的 ZooKeeper 呢？&lt;/p>
&lt;h2 id="5-zab-协议和-paxos-算法">5. ZAB 协议和 Paxos 算法&lt;/h2>
&lt;p>Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos 算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在 ZooKeeper 的官方文档中也指出，ZAB 协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为 Zookeeper 设计的崩溃可恢复的原子消息广播算法。&lt;/p>
&lt;h3 id="51-zab-协议介绍">5.1. ZAB 协议介绍&lt;/h3>
&lt;p>ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。&lt;/p>
&lt;h3 id="52-zab-协议两种基本的模式崩溃恢复和消息广播">5.2. ZAB 协议两种基本的模式：崩溃恢复和消息广播&lt;/h3>
&lt;p>ZAB 协议包括两种基本的模式，分别是&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>崩溃恢复&lt;/strong> ：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。其中，&lt;strong>所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>消息广播&lt;/strong> ：&lt;strong>当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。&lt;/strong> 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>关于 &lt;strong>ZAB 协议&amp;amp;Paxos 算法&lt;/strong> 需要讲和理解的东西太多了，具体可以看下面这两篇文章：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>图解 Paxos 一致性协议&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Zookeeper ZAB 协议分析&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="6-总结">6. 总结&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持 znode 中存储的数据量较小的进一步原因）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地明显，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ZooKeeper 有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个 znode 被创建了，除非主动进行 znode 的移除操作，否则这个 znode 将一直保存在 ZooKeeper 上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ZooKeeper 底层其实只提供了两个功能：① 管理（存储、读取）用户程序提交的数据；② 为用户程序提供数据节点监听服务。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="7-参考">7. 参考&lt;/h2>
&lt;ol>
&lt;li>《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》&lt;/li>
&lt;/ol></description></item></channel></rss>