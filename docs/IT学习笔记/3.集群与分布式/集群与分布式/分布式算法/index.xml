<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 分布式算法</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/</link><description>Recent content in 分布式算法 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 共识算法</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/130332285">知乎大佬&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>分布式系统中，如何保证集群中所有节点中的数据完全相同并且能够对某个提案(Proposal)达成一致，是分布式系统正常工作的核心问题，而 共识算法 就是用来保证分布式系统一致性的方法。有时候，共识算法也称为“一致性算法”。&lt;/p>
&lt;p>Note：
提案(Proposal) # 在分布式系统中，Proposal 指分布式系统的修改请求，比如当一个客户端将一个数据写入到分布式系统中，负责接收数据的节点就会发起提案，让集群中其余节点写入该数据。&lt;/p>
&lt;p>共识算法的实现：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Paxos&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>ZAB(muti-paxos)&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>Raft(muti-paxos)&lt;/strong> # Paxos 算法不易实现，Raft 算法是对 Paxos 算法的简化和改进&lt;/li>
&lt;li>&lt;strong>Gossip&lt;/strong> # 这是一个协议，与 Raft 等公式算法不太一样&lt;/li>
&lt;/ul>
&lt;p>由于共识算法的特点(下文会详细介绍，可以看完算法介绍再来看这段描述)，所以一般保证一致性的分布式系统的节点数都是都是奇数个。&lt;/p>
&lt;h1 id="raft-算法">Raft 算法&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://thesecretlivesofdata.com/raft/">动画演示&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>一个节点一般具有三种状态：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Leader&lt;/strong> # 领导者，负责发出提案&lt;/li>
&lt;li>** Candidate** # 候选人，负责争夺 Leader&lt;/li>
&lt;li>&lt;strong>Follower&lt;/strong> # 追随者，负责同意 Leader 发出的提案&lt;/li>
&lt;/ol>
&lt;p>名词解释：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>voted&lt;/strong> # 投票&lt;/li>
&lt;li>&lt;strong>term&lt;/strong> # 期限、任期。用来代指 Leader 产生到结束这一时间段&lt;/li>
&lt;/ol>
&lt;p>Leader 的产生&lt;/p>
&lt;ol>
&lt;li>所有节点的初始状态都是以 Follower，并且持有一个定时器(各节点定时器时间随机，以保证不会同时触发选举)。
&lt;ol>
&lt;li>该定时器被随机分配在 150ms 和 300ms 之间&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>如果 Followers 在定时器时间到了，而没有收到 Leader 信息的话。那么 Follower 则声明自己是 Candidate 并参与 Leader 选举(此时为自己投票)，同时将请求发给其他节点来争取他们的投票，若其他节点长时间没有响应 Candidate，将重新发送选举信息。&lt;/li>
&lt;li>如果接收投票信息的节点在这个 term 中还没有投过票，那么该节点将对发起该请求的 Candidate 投票，并且该节点重置定时器。&lt;/li>
&lt;li>Candidate 计算总票数(自己+其他节点)，得票数大于 (n+1)/2 时(n 为节点数，除不开向下取整)，则该 Candidate 将称为第 M 任 Leader (M 任是最新的 term)
&lt;ol>
&lt;li>Note：这里面所谓的投票与现实意义中的投票不太一样，没有投同意或者不同意这种说法。&lt;/li>
&lt;li>值要对目标投票，目标就会计算一份得票数&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Leader 在 term 内会不断发送心跳信息给集群内其他节点证明自己还活着，其他节点在收到心跳后会清空自己的定时器并回复 Leader 的心跳。这个机制保证其他节点不会在 Leader 任期内参加 Leader 选举。&lt;/li>
&lt;li>当 Leader 节点出现故障而导致 Leader 失联，没有接收到心跳的 Follower 节点将准备成为 Candidate 进入下一轮 Leader 选举(回到步骤 1)。&lt;/li>
&lt;li>若出现两个 Candidate 同时选举并获得了相同的票数，那么这两个 Candidate 将随机推迟一段时间后再向其他节点发出投票请求，这保证了再次发送投票请求以后不冲突。&lt;/li>
&lt;/ol>
&lt;p>状态复制&lt;/p>
&lt;ol>
&lt;li>Leader 负责接收来自 Client 的提案请求。
&lt;ol>
&lt;li>比如提案内容是：写入一个数值 5 。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>提案内容将包含在 Leader 发出的下一个心跳中。&lt;/li>
&lt;li>Follower 接收到心跳以后回复 Leader 的心跳&lt;/li>
&lt;li>Leader 接收到多数 Follower 的回复以后，确认提案并写入自己的存储空间并回复 Client 。&lt;/li>
&lt;li>Leader 通知 Follower 节点确认提案并向存储空间写入内容，随后所有的节点都拥有相同的数据。&lt;/li>
&lt;li>若集群中出现网络异常，导致集群被分割，将出现多个 Leader。(比如 5 节点集群被分割成两个集群，一个 2 节点，一个 3 节点)&lt;/li>
&lt;li>被分割出的少数节点的集群将无法达到共识，即脑裂。
&lt;ol>
&lt;li>在状态复制时，&amp;hellip;&amp;hellip;&amp;hellip;.未知&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>当网络恢复，集群恢复之后，将只听从最新任期 Leader 的指挥，旧 Leader 将&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;.未知&lt;/li>
&lt;/ol></description></item><item><title>Docs: 一致性哈希算法 consistent hashing</title><link>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95-consistent-hashing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95-consistent-hashing/</guid><description>
&lt;p>白话解析：一致性哈希算法 consistent hashing&lt;/p>
&lt;p>2017 年 10 月 22 日 21:46:15&lt;/p>
&lt;p>阅读数：1180&lt;/p>
&lt;p>摘要：&lt;/p>
&lt;p>本文首先以一个经典的分布式缓存的应用场景为铺垫，在了解了这个应用场景之后，生动而又不失风趣地介绍了一致性哈希算法，同时也明确给出了一致性哈希算法的优点、存在的问题及其解决办法。&lt;/p>
&lt;hr>
&lt;p>声明与致谢：&lt;/p>
&lt;p>本文转载于朱双印博主的个人日志《白话解析：一致性哈希算法 consistent hashing》一文。&lt;/p>
&lt;hr>
&lt;p>一. 引子&lt;/p>
&lt;p>在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。&lt;/p>
&lt;hr>
&lt;p>1、场景描述&lt;/p>
&lt;p>假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0 号、1 号、2 号，现在，有 3 万张图片需要缓存，我们希望这些图片被均匀的缓存到这 3 台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存 1 万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将 3 万张图片平均的缓存在 3 台服务器上，可以满足我们的要求吗？可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历 3 台缓存服务器，从 3 万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接收的，也就失去了缓存的意义，缓存的目的就是 提高速度，改善用户体验，减轻后端服务器压力，如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？原始的做法是对缓存项的键进行哈希，将 hash 后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的 key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。&lt;/p>
&lt;hr>
&lt;p>hash（图片名称）% N&lt;/p>
&lt;p>因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有 3 台服务器，使用哈希后的结果对 3 求余，那么余数一定是 0、1 或者 2，没错，正好与我们之前的服务器编号相同，如果求余的结果为 0， 我们就把当前图片名称对应的图片缓存在 0 号服务器上，如果余数为 1，就把当前图片名对应的图片缓存在 1 号服务器上，如果余数为 2，同理，那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将 3 万张图片随机的分布到 3 台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为 HASH 算法或者取模算法，取模算法的过程可以用下图表示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202555-a60e8446-0c60-472c-84a4-e164dc32c2eb.jpeg" alt="">&lt;/p>
&lt;p>但是，使用上述 HASH 算法进行缓存时，会出现一些缺陷，试想一下，如果 3 台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两台缓存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由 3 台变成了 4 台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来 3 台服务器时所在的服务器编号不同，因为除数由 3 变为了 4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设 3 台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从 3 台变为 2 台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了 缓存雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述 HASH 算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。&lt;/p>
&lt;p>我们来回顾一下使用上述算法会出现的问题：&lt;/p>
&lt;p>问题 1：当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。&lt;/p>
&lt;p>问题 2：当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，怎样才能尽量减少受影响的缓存呢？&lt;/p>
&lt;p>其实，上面两个问题是一个问题，那么，一致性哈希算法能够解决上述问题吗？我们现在就来了解一下一致性哈希算法。&lt;/p>
&lt;hr>
&lt;p>二. 一致性哈希算法的基本概念&lt;/p>
&lt;p>其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模，什么意思呢？我们慢慢聊。&lt;/p>
&lt;p>首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 2^32 个点组成的圆，示意图如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202586-97ba41e2-bd8d-4bcb-ab83-7b07b1876d76.jpeg" alt="">&lt;/p>
&lt;p>圆环的正上方的点代表 0，0 点右侧的第一个点代表 1，以此类推，2、3、4、5、6……直到 2^32-1,也就是说 0 点左侧的第一个点代表 2^32-1&lt;/p>
&lt;p>我们把这个由 2 的 32 次方个点组成的圆环称为 hash 环。&lt;/p>
&lt;p>那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有 3 台缓存服务器，服务器 A、服务器 B、服务器 C，那么，在生产环境中，这三台服务器肯定有自己的 IP 地址，我们使用它们各自的 IP 地址进行哈希计算，使用哈希后的结果对 2^32 取模，可以使用如下公式示意。&lt;/p>
&lt;hr>
&lt;p>hash（服务器 A 的 IP 地址） % 2^32&lt;/p>
&lt;p>通过上述公式算出的结果一定是一个 0 到 2^32-1 之间的一个整数，我们就用算出的这个整数，代表服务器 A，既然这个整数肯定处于 0 到 2^32-1 之间，那么，上图中的 hash 环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器 A，那么，服务器 A 就可以映射到这个环上，用下图示意：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202585-14af29fb-df19-4b5d-a9a8-54b069ac66cb.jpeg" alt="">&lt;/p>
&lt;p>同理，服务器 B 与服务器 C 也可以通过相同的方法映射到上图中的 hash 环中：&lt;/p>
&lt;p>hash（服务器 B 的 IP 地址） % 2^32&lt;/p>
&lt;p>hash（服务器 C 的 IP 地址） % 2^32&lt;/p>
&lt;p>通过上述方法，可以将服务器 B 与服务器 C 映射到上图中的 hash 环上，示意图如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202603-15e04eaa-c965-4854-84fa-b342412db159.jpeg" alt="">&lt;/p>
&lt;p>假设 3 台服务器映射到 hash 环上以后如上图所示（当然，这是理想的情况，我们慢慢聊）。&lt;/p>
&lt;p>好了，到目前为止，我们已经把缓存服务器与 hash 环联系在了一起，我们通过上述方法，把缓存服务器映射到了 hash 环上，那么使用同样的方法，我们也可以将需要缓存的对象映射到 hash 环上。&lt;/p>
&lt;p>假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的 key，那么我们使用如下公式可以将图片映射到上图中的 hash 环上。&lt;/p>
&lt;hr>
&lt;p>hash（图片名称） % 2^32&lt;/p>
&lt;p>映射后的示意图如下，下图中的橘黄色圆形表示图片：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202605-8c3c063d-9556-434e-b6fa-cbeed21e5a3c.jpeg" alt="">&lt;/p>
&lt;p>好了，现在服务器与图片都被映射到了 hash 环上，那么上图中的这个图片到底应该被缓存到哪一台服务器上呢？上图中的图片将会被缓存到服务器 A 上，为什么呢？因为从图片的位置开始，沿顺时针方向遇到的第一个服务器 就是 A 服务器，所以，上图中的图片将会被缓存到服务器 A 上，如下图所示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202623-dbd970a7-40c9-4f77-bf0a-af258f3bf6a5.jpeg" alt="">&lt;/p>
&lt;p>没错，一致性哈希算法就是通过这种方法，判断一个对象应该被缓存到哪台服务器上的，将缓存服务器与被缓存对象都映射到 hash 环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器，由于被缓存对象与服务器 hash 后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。/font&amp;gt;&lt;/p>
&lt;p>刚才的示例只使用了一张图片进行演示，假设有四张图片需要缓存，示意图如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202612-68280120-c095-4f3a-80fe-8f42cf7e71f8.jpeg" alt="">&lt;/p>
&lt;p>1 号、2 号图片将会被缓存到服务器 A 上，3 号图片将会被缓存到服务器 B 上，4 号图片将会被缓存到服务器 C 上。&lt;/p>
&lt;hr>
&lt;p>三. 一致性哈希算法的优点&lt;/p>
&lt;p>经过上述描述，我想兄弟你应该已经明白了一致性哈希算法的原理了，但是话说回来，一致性哈希算法能够解决之前出现的问题吗，我们说过，如果简单的对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，那么使用一致性哈希算法，能够避免这个问题吗？我们来模拟一遍，即可得到答案。&lt;/p>
&lt;p>假设，服务器 B 出现了故障，我们现在需要将服务器 B 移除，那么，我们将上图中的服务器 B 从 hash 环上移除即可，移除服务器 B 以后示意图如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202653-039f84ad-a042-419b-ae51-5e181853f80d.jpeg" alt="">&lt;/p>
&lt;p>在服务器 B 未移除时，图片 3 应该被缓存到服务器 B 中，可是当服务器 B 移除以后，按照之前描述的一致性哈希算法的规则，图片 3 应该被缓存到服务器 C 中，因为从图片 3 的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器 C，也就是说，如果服务器 B 出现故障被移除时，图片 3 的缓存位置会发生改变。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202621-d4a2c0fd-f096-4e13-9839-7f50dfe7b430.jpeg" alt="">&lt;/p>
&lt;p>但是，图片 4 仍然会被缓存到服务器 C 中，图片 1 与图片 2 仍然会被缓存到服务器 A 中，这与服务器 B 移除之前并没有任何区别，这就是一致性哈希算法的优点，如果使用之前的 hash 算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分缓存会失效，前端的缓存仍然能分担整个系统的压力，而不至于所有压力都在同一时间集中到后端服务器上。&lt;/p>
&lt;p>这就是一致性哈希算法所体现出的优点。&lt;/p>
&lt;hr>
&lt;p>四. hash 环的偏斜&lt;/p>
&lt;p>在介绍一致性哈希的概念时，我们理想化的将 3 台服务器均匀的映射到了 hash 环上，如下图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202636-2bc6be5c-d885-49aa-a228-3c7a6d13efe6.jpeg" alt="">&lt;/p>
&lt;p>但是，理想很丰满，现实很骨感，我们想象的与实际情况往往不一样。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202623-28068a4f-a82e-4dcf-833b-50a5b63b446d.jpeg" alt="">&lt;/p>
&lt;p>在实际的映射中，服务器可能会被映射成如下模样。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202670-68b20340-9bdd-4ef1-955a-4bfa4a413343.jpeg" alt="">&lt;/p>
&lt;p>聪明如你一定想到了，如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上，如下图所示。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202641-e7413fb2-fab4-46fa-9bf7-ca879555b484.jpeg" alt="">&lt;/p>
&lt;p>上图中，1 号、2 号、3 号、4 号、6 号图片均被缓存在了服务器 A 上，只有 5 号图片被缓存在了服务器 B 上，服务器 C 上甚至没有缓存任何图片，如果出现上图中的情况，A、B、C 三台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器 A 出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引起系统的崩溃，上图中的情况则被称之为 hash 环的偏斜，那么，我们应该怎样防止 hash 环的偏斜呢？一致性 hash 算法中使用 “虚拟节点” 解决了这个问题，我们继续聊。&lt;/p>
&lt;hr>
&lt;p>五.虚拟节点&lt;/p>
&lt;p>话接上文，由于我们只有 3 台服务器，当我们把服务器映射到 hash 环上的时候，很有可能出现 hash 环偏斜的情况，当 hash 环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，聪明如你一定已经想到了，如果想要均衡的将缓存分布到 3 台服务器上，最好能让这 3 台服务器尽量多的、均匀的出现在 hash 环上，但是，真实的服务器资源只有 3 台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为”虚拟节点”。加入虚拟节点以后的 hash 环如下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/hgulx6/1616132202654-66fcca21-b967-41c3-8ebf-aa501e1f51f9.jpeg" alt="">&lt;/p>
&lt;p>“虚拟节点”是”实际节点”（实际的物理服务器）在 hash 环上的复制品,一个实际节点可以对应多个虚拟节点。&lt;/p>
&lt;p>从上图可以看出，A、B、C 三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上图中，1 号、3 号图片被缓存在服务器 A 中，5 号、4 号图片被缓存在服务器 B 中，6 号、2 号图片被缓存在服务器 C 中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小 hash 环偏斜所带来的影响，虚拟节点越多，hash 环上的节点就越多，缓存被均匀分布的概率就越大。&lt;/p></description></item></channel></rss>