<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – 自然语言处理</title><link>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link><description>Recent content in 自然语言处理 on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 自然语言处理</title><link>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Natural_language_processing">Wiki，Natural_language_processing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Natural Language Processing(自然语言处理，简称 NLP)&lt;/strong> 是语言学、计算机科学和人工智能的跨学科领域，主要关注计算机与人类语言之间的交互，特别是如何编写程序来处理和分析大量的自然语言数据。目标是让计算机能够“理解”文档的内容，包括其中的语言上下文细微差别。技术可以准确提取文档中包含的信息和见解，以及对文档本身进行分类和组织。&lt;/p>
&lt;h1 id="语言模型">语言模型&lt;a class="td-heading-self-link" href="#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Language_model">Wiki，Language_model&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Language model(语言模型)&lt;/strong> 是单词序列的概率分布。&lt;/p>
&lt;p>通过语言模型，才可以实现自然语言处理。NLP 程序都会使用语言模型，我们将自然语言作为输入，传递给语言模型，语言模型将会预测其将要输出的每一个单词的出现概率，然后逐一输出这些单词。&lt;/p>
&lt;p>想要训练出来一个良好的语言模型，通常会需要类似 &lt;a href="https://desistdaydream.github.io/docs/12.AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Transformers.md">Transformers&lt;/a> 这种机器学习模型。&lt;/p>
&lt;h2 id="large-language-model">Large language model&lt;a class="td-heading-self-link" href="#large-language-model" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;strong>Large language model(大语言模型，简称 LLM)&lt;/strong> 是由具有许多参数（通常为数十亿或更多权重）的神经网络组成的&lt;a href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">语言模型&lt;/a>，使用自我监督学习对大量未标记文本进行训练。LLM 在 2018 年左右出现，并在各种任务中表现出色。这已经将自然语言处理研究的重点从之前为特定任务训练专门的监督模型的范式转移了。&lt;/p>
&lt;blockquote>
&lt;p>我们经常看到 LLM 实现的模型后面有 XB 的样式，其中 B 表示 Billions(十亿)，这个 XB 指的就是参数的数量。比如 6B 表示 60 亿参数。&lt;/p>
&lt;/blockquote>
&lt;h3 id="常见模型">常见模型&lt;a class="td-heading-self-link" href="#%e5%b8%b8%e8%a7%81%e6%a8%a1%e5%9e%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Large_language_model#List_of_large_language_models">Wiki，Large_language_model-大语言模型列表&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/wa55CHRNMeBUXl91WFJVpA">公众号-OSC 开源社区，大预言模型精选开源项目&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>我们常见的语言模型在现阶段（2023.5）可以简单分为如下几大类&lt;/p>
&lt;ul>
&lt;li>&lt;strong>BERT 系&lt;/strong>
&lt;ul>
&lt;li>类似完形填空，联系上下文直接给出空中的内容&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>GPT 系&lt;/strong>
&lt;ul>
&lt;li>一字一字推测的自回归模型。有a推测b，然后根据ab推测c，根据abc推测d，以此类推&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>其他系&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bidirectional Encoder Representations from Transformers(来自 Transformers 的双向编码器表示，简称 BERT)&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Generative Pre-trained Transformer(生成式预训练 Transformer，简称 GPT)&lt;/strong>&lt;/p>
&lt;p>&lt;strong>General Language Model(通用语言模型，简称 GLM)&lt;/strong> # 清华开源&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/THUDM/GLM">GitHub 项目，THUDM/GLM&lt;/a> 框架&lt;/li>
&lt;li>&lt;a href="https://github.com/THUDM/ChatGLM3">GitHub 项目，THUDM/ChatGLM3&lt;/a> # 已经衍化到第三代。&lt;/li>
&lt;li>实践项目
&lt;ul>
&lt;li>&lt;a href="https://github.com/THUDM/ChatGLM-6B">THUDM/ChatGLM-6B&lt;/a>
&lt;ul>
&lt;li>量化后的模型需要使用到 CPU 处理一些数据，需要安装 gcc 与 openmp(有的 &lt;a href="https://desistdaydream.github.io/docs/2.%E7%BC%96%E7%A8%8B/%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/C/C%20%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/GCC.md">GCC&lt;/a> 的整合包中包含了 openmp)。&lt;/li>
&lt;li>TODO: 有没有办法不用 CPU？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://github.com/THUDM/GLM-130B">THUDM/GLM-130B&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>具体应用: &lt;a href="https://www.chatglm.cn/">智谱清言&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Large Language Model Meta AI(简称 LLaMA)&lt;/strong>&lt;/p>
&lt;p>&lt;strong>MOSS&lt;/strong> # 复旦开源，已停止更新&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/OpenLMLab/MOSS">GitHub 项目，OpenLMLab/MOSS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.zhihu.com/question/596908242/answer/2994534005">https://www.zhihu.com/question/596908242/answer/2994534005&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: 不通过数学解释 LLMs 是如何工作的</title><link>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%8D%E9%80%9A%E8%BF%87%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A-LLMs-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%8D%E9%80%9A%E8%BF%87%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A-LLMs-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math">https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/z93ZEVnjdpKM__-_0HvjRQ">中文翻译：公众号 - 云原生实验室，大模型到底有没有智能？一篇文章给你讲明明白白&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>生成式人工智能 (&lt;a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence">GenAI&lt;/a>) 和大语言模型 (LLM&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>)，这两个词汇想必已在大家的耳边萦绕多时。它们如惊涛骇浪般席卷了整个科技界，登上了各大新闻头条。ChatGPT，这个神奇的对话助手，也许已成为你形影不离的良师益友。&lt;/p>
&lt;p>然而，在这场方兴未艾的 GenAI 革命背后，有一个谜题久久萦绕在人们心头：&lt;strong>这些模型的智能究竟从何而来&lt;/strong>？本文将为您揭开谜底，解析生成式文本模型的奥秘。我们将抛开晦涩艰深的数学，用通俗易懂的语言，带您走进这个神奇的算法世界。让我们撕下 “魔法” 的面纱，看清其中的计算机科学本质。&lt;/p>
&lt;h2 id="llm-的真面目">LLM 的真面目&lt;a class="td-heading-self-link" href="#llm-%e7%9a%84%e7%9c%9f%e9%9d%a2%e7%9b%ae" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>首先，我们要破除一个常见的误区。许多人误以为，这些模型是真的能够与人对话，回答人们的各种问题。然而，它们真正的能力远没有想象的那么复杂——&lt;strong>它们所做的，不过是根据输入的文本，预测下一个词语&lt;/strong> (更准确地说，是下一个 token)。&lt;/p>
&lt;p>Token，这个看似简单的概念，却是揭开 LLM 神秘面纱的钥匙。让我们由此出发，步步深入，一探究竟。&lt;/p>
&lt;p>Token，这些文本的积木、语言的原子，正是 LLM 理解世界的基石。对我们而言，token 不过是单词、标点、空格的化身，但在 LLM 的眼中，它们是精简而高效的信息编码。有时，一个 token 可能代表一串字符，长短不一；有时，它可能是孤零零的一个标点符号。&lt;/p>
&lt;p>&lt;strong>LLM 的词汇表，就是这些 token 的集合&lt;/strong>，啥都有，样样全。这其中的奥秘，要追溯到 &lt;a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE 算法&lt;/a>。BPE 算法是如何炼制出这些 tokens 的？这个问题，值得我们细细探究。但在此之前，只需记住：&lt;a href="https://github.com/openai/gpt-2">GPT-2 模型&lt;/a>，这个自然语言处理界的明星，它的词汇表中有 50,257 个 token。&lt;/p>
&lt;p>在 LLM 的世界里，每个 token 都有一个独一无二的数字身份证。而 Tokenizer，就是文本和 token 之间的 “翻译官”，将人类的语言转化为 LLM 能理解的编码，也将 LLM 的思维解码为人类的文字。如果你熟悉 Python，不妨亲自与 token 打个照面。只需安装 OpenAI 的 &lt;code>tiktoken&lt;/code> 包：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ pip install tiktoken
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>`&lt;/p>
&lt;p>然后在 Python 中尝试以下操作：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">tiktoken&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">tiktoken&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">encoding_for_model&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;gpt-2&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">encode&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;The quick brown fox jumps over the lazy dog.&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">464&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2068&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">7586&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">21831&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">18045&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">625&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">262&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">16931&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">3290&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">13&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">decode&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">464&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2068&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">7586&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">21831&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">18045&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">625&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">262&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">16931&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">3290&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">13&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;The quick brown fox jumps over the lazy dog.&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">decode&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">464&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;The&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">decode&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2068&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39; quick&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">decode&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">13&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;.&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在这个实验中，我们可以看到，对于 GPT-2 而言，token 464 表示单词 “The”，token 2068 表示 “quick” (含前导空格)，token 13 则表示句点。&lt;/p>
&lt;p>由于 token 是算法生成的，有时会出现一些奇怪的现象。比如，同一个单词 “the” 的三个变体在 GPT-2 中被编码成了不同的 token：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">encode&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;The&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">464&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">encode&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;the&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1169&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">encode&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39; the&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">262&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>BPE 算法并不总是将完整的单词映射为 token。事实上，不太常用的单词可能无法成为单独的 token，需要用多个 token 组合编码，比如这个 “Payment”，就要化身为 “Pay” 和 “ment” 的组合：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">encode&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;Payment&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">19197&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">434&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">decode&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">19197&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;Pay&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#000">encoding&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">decode&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">434&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;ment&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>`&lt;/p>
&lt;h3 id="预测下一个-token">预测下一个 Token&lt;a class="td-heading-self-link" href="#%e9%a2%84%e6%b5%8b%e4%b8%8b%e4%b8%80%e4%b8%aa-token" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>语言模型就像一个 “水晶球”，给它一串文字，它就能预言下一个最可能出现的词语。这是它的看家本领。但模型并非真的手眼通天，它的预言能力其实基于扎实的概率计算。让我们一起掀开这层神秘的面纱，看看背后的真相。&lt;/p>
&lt;p>如果你懂一点 Python，我们可以用几行代码来窥探语言模型的预言过程：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">predictions&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">get_token_predictions&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;The&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39; quick&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39; brown&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39; fox&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>`&lt;/p>
&lt;p>这个 &lt;code>get_token_predictions&lt;/code> 函数就是我们的 “水晶球”。它接受一个 token 列表作为输入，这些 token 来自用户提供的 prompt。在这个例子中，我们假设每个单词都是一个独立的 token。当然，在实际使用中，每个 token 都有一个对应的数字 ID，但为了简单起见，我们这里直接用单词的文本形式。&lt;/p>
&lt;p>函数的返回结果是一个庞大的数据结构，里面记录了词汇表中每个 token 出现在输入文本之后的概率。以 GPT-2 模型为例，它的词汇表包含 50,257 个 token，因此返回值就是一个 50,257 维的概率分布。&lt;/p>
&lt;p>现在再来重新审视一下这个例子。如果我们的语言模型训练有素，面对 “&lt;a href="https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog">The quick brown fox&lt;/a>” 这样一个烂大街的句子片段，它很可能会预测下一个词是 “jumps”，而不是 “potato” 之类风马牛不相及的词。在这个概率分布中，“jumps” 的概率值会非常高，而 “potato” 的概率值则接近于零。&lt;/p>
&lt;blockquote>
&lt;p>[!Tip]&lt;/p>
&lt;p>&lt;strong>The quick brown fox jumps over the lazy dog&lt;/strong> (相应中文可简译为 “快狐跨懒狗”，完整翻译则是 “敏捷的棕色狐狸跨过懒狗”) 是一个著名的英语全字母句，常用于测试字体显示效果和键盘是否故障。此句也常以 “quick brown fox” 做为指代简称。&lt;/p>
&lt;/blockquote>
&lt;p>当然，语言模型的预测能力并非与生俱来，而是通过日积月累的训练得来的。在漫长的训练过程中，模型如饥似渴地汲取海量文本的营养，逐渐茁壮成长。训练结束时，它已经具备了应对各种文本输入的能力，可以利用积累的知识和经验，计算出任意 token 序列的下一个 token 概率。&lt;/p>
&lt;p>现在是不是觉得语言模型的预测过程没那么神奇了？它与其说是魔法，不如说是一个基于概率的计算过程。这个过程虽然复杂，但并非不可理解。我们只要掌握了基本原理，就可以揭开它的神秘面纱，走近它，了解它。&lt;/p>
&lt;h3 id="长文本生成的奥秘">长文本生成的奥秘&lt;a class="td-heading-self-link" href="#%e9%95%bf%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90%e7%9a%84%e5%a5%a5%e7%a7%98" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>由于语言模型每次只能预测下一个 token 会是什么，因此生成完整句子的唯一方法就是在循环中多次运行该模型。每一轮迭代都会从模型返回的概率分布中选择一个新的 token，生成新的内容。然后将这个新 token 附加到下一轮中输入给模型的文本序列末尾，如此循环往复，直到生成足够长度的文本。&lt;/p>
&lt;p>我们来看一个更完整的 Python 伪代码，展示具体的实现逻辑：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">def&lt;/span> &lt;span style="color:#000">generate_text&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">prompt&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">num_tokens&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">hyperparameters&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">tokens&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">tokenize&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">prompt&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#000">i&lt;/span> &lt;span style="color:#204a87;font-weight:bold">in&lt;/span> &lt;span style="color:#204a87">range&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">num_tokens&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">predictions&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">get_token_predictions&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">tokens&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">next_token&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">select_next_token&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">predictions&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">hyperparameters&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">tokens&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">append&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">next_token&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">return&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">join&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">tokens&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>其中，&lt;code>generate_text()&lt;/code> 函数接受一个用户输入的提示词 (prompt) 文本作为参数，这可以是一个问题或其他任意文本。&lt;/p>
&lt;p>&lt;code>tokenize()&lt;/code> 辅助函数使用类似 &lt;code>tiktoken&lt;/code> 的分词库将提示文本转换成一系列等效的 &lt;code>token(token)&lt;/code> 序列。在 for 循环内部，get_token_predictions() 函数调用语言模型来获取下一个 token 的概率分布，这一步与前面的示例类似。&lt;/p>
&lt;p>&lt;code>select_next_token()&lt;/code> 函数根据上一步得到的下个 token 概率分布，选择最合适的 token 来延续当前的文本序列。最简单的做法是选择概率最高的 token，在机器学习中被称为贪婪选择 (greedy selection)。更好的做法是用符合模型给出概率分布的随机数生成器来选词，这样可以让生成的文本更丰富多样。如果用同样的输入多次运行模型，这种方法还可以让每次产生的回应都略有不同。&lt;/p>
&lt;p>为了让 token 选择过程更加灵活可控，可以用一些超参数 (hyperparameter) 来调整语言模型返回的概率分布，这些超参数作为参数传递给文本生成函数。通过调整超参数，你可以控制 token 选择的 “贪婪程度”。如果你用过大语言模型，你可能比较熟悉名为 &lt;code>temperature&lt;/code> 的超参数。提高 temperature 的值可以让 token 的概率分布变得更加平缓，增加选中概率较低 token 的机会，从而让生成的文本显得更有创意和变化。此外，常用的还有 &lt;code>top_p&lt;/code> 和 &lt;code>top_k&lt;/code> 两个超参数，它们限定从概率最高的前 k 个或概率超过阈值 p 的 token 中进行选择，以平衡多样性和连贯性。&lt;/p>
&lt;p>选定了一个新 token 后，循环进入下一轮迭代，将新 token 添加到原有文本序列的末尾，作为新一轮的输入，再接着生成下一个 token。&lt;code>num_tokens&lt;/code> 参数控制循环的迭代轮数，决定要生成的文本长度。但需要注意的是，由于语言模型是逐词预测，没有句子或段落的概念，生成的文本常常会在句子中途意外结束。为了避免这种情况，我们可以把 &lt;code>num_tokens&lt;/code> 参数视为生成长度的上限而非确切值，当遇到句号、问号等标点符号时提前结束生成过程，以保证文本在语义和语法上的完整性。&lt;/p>
&lt;p>如果你已经读到这里且充分理解了以上内容，那么恭喜你！现在你对大语言模型的基本工作原理有了一个高层次的认识。如果你想进一步了解更多细节，我在下一节会深入探讨一些更加技术性的话题，但会尽量避免过多涉及晦涩难懂的数学原理。&lt;/p>
&lt;h2 id="模型训练">模型训练&lt;a class="td-heading-self-link" href="#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>遗憾的是，不借助数学语言来讨论模型训练实际上是很困难的。这里先展示一种非常简单的训练方法。&lt;/p>
&lt;p>既然 LLM 的任务是预测某些词后面跟随的词，那么一个简单的模型训练方式就是从训练数据集中提取所有连续的词对，并用它们来构建一张概率表。&lt;/p>
&lt;p>让我们用一个小型词表和数据集来演示这个过程。假设模型的词表包含以下 5 个词：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;I&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;you&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;like&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;apples&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;bananas&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>为了保持示例简洁，我不打算将空格和标点符号视为独立的词。&lt;/p>
&lt;p>我们使用由三个句子组成的训练数据集：&lt;/p>
&lt;ul>
&lt;li>I like apples&lt;/li>
&lt;li>I like bananas&lt;/li>
&lt;li>you like bananas&lt;/li>
&lt;/ul>
&lt;p>我们可以构建一个 5x5 的表格，在每个单元格中记录 “该单元格所在行的词” 后面跟随 “该单元格所在列的词” 的次数。下面是根据数据集中三个句子得到的表格：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>-&lt;/th>
&lt;th>I&lt;/th>
&lt;th>you&lt;/th>
&lt;th>like&lt;/th>
&lt;th>apples&lt;/th>
&lt;th>bananas&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>I&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>2&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>you&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>1&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>like&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>apples&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>bananas&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>这个表格应该不难理解。数据集中包含两个 “I like” 实例，一个 “you like” 实例，一个 “like apples” 实例和两个 “like bananas” 实例。&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>现在我们知道了每对词在训练集中出现的次数，就可以计算每个词后面跟随其他词的概率了。为此，我们将表中每一行的数字转换为概率值。例如，表格中间行的 “like” 后面有一次跟随 “apples”，两次跟随 “bananas”。这意味着在 33.3%的情况下 “like” 后面是 “apples”，剩下 66.7%的情况下是 “bananas”。&lt;/p>
&lt;p>下面是计算出所有概率后的完整表格。空单元格代表 0%的概率。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>-&lt;/th>
&lt;th>I&lt;/th>
&lt;th>you&lt;/th>
&lt;th>like&lt;/th>
&lt;th>apples&lt;/th>
&lt;th>bananas&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>I&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>100%&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>you&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>100%&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>like&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>33.3%&lt;/td>
&lt;td>66.7%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>apples&lt;/strong>&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>&lt;/td>
&lt;td>25%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>bananas&lt;/strong>&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>25%&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>“I”、“you” 和 “like” 这几行的概率很容易计算，但 “apples” 和 “bananas” 带来了问题。由于数据集中没有这两个词后面接其他词的例子，它们存在训练数据的空白。为了确保模型即使面对未见过的词也能做出预测，我决定将 “apples” 和 “bananas” 的后续词概率平均分配给其他四个可能的词。这种做法虽然可能产生不自然的结果，但至少能防止模型在遇到这两个词时陷入死循环。&lt;/p>
&lt;p>训练数据存在 “空洞” 的问题对语言模型的影响不容忽视。&lt;strong>在真实的大语言模型中，由于训练语料极其庞大，这些空洞通常表现为局部覆盖率偏低，而不是整体性的缺失，因而不太容易被发现。语言模型在这些训练不足的领域或话题上会产生片面、错误或前后不一致的预测结果，但通常会以一种难以感知的形式表现出来。这就是语言模型有时会产生 “&lt;a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">Hallucination(幻觉)&lt;/a>” 的原因之一，所谓幻觉，就是指生成的文本表面上读起来通顺流畅，但实际包含了事实错误或前后矛盾的内容。&lt;/strong>&lt;/p>
&lt;p>借助上面给出的概率表，你现在可以自己想象一下 &lt;code>get_token_predictions()&lt;/code> 函数会如何实现。用 Python 伪代码表示大致如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">def&lt;/span> &lt;span style="color:#000">get_token_predictions&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">input_tokens&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">last_token&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">input_tokens&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">return&lt;/span> &lt;span style="color:#000">probabilities_table&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">last_token&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>是不是比想象的要简单？该函数接受一个单词序列作为输入，也就是用户提示。它取这个序列的最后一个单词，然后返回概率表中与之对应的那一行。&lt;/p>
&lt;p>举个例子，如果用 &lt;code>['you', 'like']&lt;/code> 来调用这个函数，它会返回 “like” 所在的行，其中 “apples” 有 33.3%的概率接在后面组成句子，而 “bananas” 占剩下的 66.7%。有了这些概率信息，之前展示的 &lt;code>select_next_token()&lt;/code> 函数在三分之一的情况下应该选择 “apples”。&lt;/p>
&lt;p>当 “apples” 被选为 “you like” 的续词时，“you like apples” 这个句子就形成了。&lt;strong>这是一个在训练数据中不存在的全新句子，但它却非常合理&lt;/strong>。希望这个例子能帮你认识到，&lt;strong>语言模型其实只是在重复使用和拼凑它在训练过程中学到的各种模式碎片，就能组合出一些看似原创的想法或概念&lt;/strong>。&lt;/p>
&lt;h3 id="上下文窗口">上下文窗口&lt;a class="td-heading-self-link" href="#%e4%b8%8a%e4%b8%8b%e6%96%87%e7%aa%97%e5%8f%a3" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>上一节内容我使用 &lt;a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain(马尔可夫链)&lt;/a> 的方法训练了一个小语言模型。这种方法存在一个问题：它的上下文窗口只有一个标记，也就是说，&lt;strong>模型在预测下一个词时，只考虑了输入序列的最后一个词，而忽略了之前的所有内容&lt;/strong>。这导致生成的文本缺乏连贯性和一致性，常常前后矛盾，逻辑跳跃。&lt;/p>
&lt;p>为了提高模型的预测质量，一种直观的思路是&lt;strong>扩大上下文窗口的大小，比如增加到 2 个标记&lt;/strong>。但这样做会导致概率表的规模急剧膨胀。以我之前使用的 5 个标记的简单词表为例，将上下文窗口增加到 2 个标记，就需要在原有的 5 行概率表基础上，额外增加 25 行来覆盖所有可能的双词组合。如果进一步扩大到 3 个标记，额外的行数将达到 125 行。可以预见，&lt;strong>随着上下文窗口的增大，概率表的规模将呈指数级爆炸式增长&lt;/strong>。&lt;/p>
&lt;p>更重要的是，即使将上下文窗口扩大到 2 个或 3 个标记，其改进效果仍然非常有限。要使语言模型生成的文本真正做到前后连贯、逻辑通顺，实际上需要一个远大于此的上下文窗口。&lt;strong>只有足够大的上下文，新生成的词才能与之前较远处提及的概念、思想产生联系，从而赋予文本连续的语义和逻辑&lt;/strong>。&lt;/p>
&lt;p>举个实际的例子，OpenAI 开源的 GPT-2 模型采用了 1024 个标记的上下文窗口。如果仍然沿用马尔可夫链的思路来实现这一尺度的上下文，&lt;strong>以 5 个标记的词表为例，仅覆盖 1024 个词长度的所有可能序列，就需要高达 $5^{1024}$ 行的概率表&lt;/strong>。这是一个天文数字，我在 Python 中计算了这个值的具体大小，读者可以向右滚动来查看完整的数字：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#204a87">pow&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1024&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">55626846462680034577255817933310101605480399511558295763833185422180110870347954896357078975312775514101683493275895275128810854038836502721400309634442970528269449838300058261990253686064590901798039126173562593355209381270166265416453973718012279499214790991212515897719252957621869994522193843748736289511290126272884996414561770466127838448395124802899527144151299810833802858809753719892490239782222290074816037776586657834841586939662825734294051183140794537141608771803070715941051121170285190347786926570042246331102750604036185540464179153763503857127117918822547579033069472418242684328083352174724579376695971173152319349449321466491373527284227385153411689217559966957882267024615430273115634918212890625&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这段 Python 代码示例生成了一个庞大的表格，但即便如此，它也只是整个表格的一小部分。因为除了当前的 1024 个 token 长度的序列，我们还需要生成更短的序列，譬如 1023 个、1022 个 token 的序列，一直到只包含 1 个 token 的序列。这样做是为了确保在输入数据 token 数量不足的情况下，模型也能妥善处理较短的序列。马尔可夫链虽然是一个有趣的文本生成方法，但在可扩展性方面确实存在很大的问题。&lt;/p>
&lt;p>如今，1024 个 token 的上下文窗口已经不那么出色了。GPT-3 将其扩大到了 2048 个 token，GPT-3.5 进一步增加到 4096 个。GPT-4 一开始支持 8192 个 token 的上下文，后来增加到 32000 个，再后来甚至达到了 128000 个 token！目前，开始出现支持 100 万以上 token 的超大上下文窗口模型，使得模型在进行 token 预测时，能够拥有更好的一致性和更强的记忆能力。&lt;/p>
&lt;p>总而言之，尽管马尔可夫链为我们提供了一种正确的思路来思考文本生成问题，但其固有的可扩展性不足，使其难以成为一个可行的、能够满足实际需求的解决方案。面对海量文本数据，我们需要寻求更加高效和可扩展的文本生成方法。&lt;/p>
&lt;h3 id="从马尔可夫链到神经网络">从马尔可夫链到神经网络&lt;a class="td-heading-self-link" href="#%e4%bb%8e%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be%e5%88%b0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>显然，我们必须摒弃使用概率表的想法。对于一个合理大小的上下文窗口，所需的表格大小将远超内存限制。&lt;strong>我们可以用一个函数来代替这个表格，该函数能够通过算法生成近似的下一个词出现概率，而无需将其存储在一个巨大的表格中&lt;/strong>。这正是神经网络擅长的领域。&lt;/p>
&lt;p>神经网络是一种特殊的函数，它接收一些输入，经过计算后给出输出。对于语言模型而言，输入是代表提示信息的词，输出是下一个可能出现的词及其概率列表。神经网络之所以特殊，是因为除了函数逻辑之外，它们对输入进行计算的方式还受到许多外部定义参数的控制。&lt;/p>
&lt;p>最初，神经网络的参数是未知的，因此其输出毫无意义。神经网络的训练过程就是要找到那些能让函数在训练数据集上表现最佳的参数，并假设如果函数在训练数据上表现良好，它在其他数据上的表现也会相当不错。&lt;/p>
&lt;p>在训练过程中，参数会使用一种叫做 &lt;a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation(反向传播)&lt;/a> 的算法进行迭代调整，每次调整的幅度都很小。这个算法涉及大量数学计算，我们在这里就不展开了。&lt;strong>每次参数调整后，神经网络的预测都会变得更准一些&lt;/strong>。参数更新后，网络会用训练数据集重新评估，结果为下一轮调整提供参考。这个过程会反复进行，直到函数能够在训练数据上很好地预测下一个词。&lt;/p>
&lt;p>为了让你对神经网络的规模有个概念，&lt;strong>GPT-2 模型有大约 15 亿个参数，GPT-3 增加到了 1750 亿，而 GPT-4 据说有 1.76 万亿个参数&lt;/strong>。在当前硬件条件下，训练如此规模的神经网络通常需要几周或几个月的时间。&lt;/p>
&lt;p>有趣的是，由于参数数量巨大，并且都是通过漫长的迭代过程自动计算出来的，我们很难理解模型的工作原理。&lt;strong>训练完成的大语言模型就像一个难以解释的黑箱，因为模型的大部分 “思考” 过程都隐藏在海量参数之中。即使是训练它的人，也很难说清其内部的运作机制。&lt;/strong>&lt;/p>
&lt;h3 id="层transformer-与-attention-机制">层、Transformer 与 Attention 机制&lt;a class="td-heading-self-link" href="#%e5%b1%82transformer-%e4%b8%8e-attention-%e6%9c%ba%e5%88%b6" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>你可能好奇神经网络函数内部进行了哪些神秘的计算。在精心调校的参数帮助下，它可以接收一系列输入标记，并以某种方式输出下一个标记出现的合理概率。&lt;/p>
&lt;p>神经网络被配置为执行一系列操作，&lt;strong>每个操作称为一个 “层”&lt;/strong>。第一层接收输入并对其进行转换。转换后的输入进入下一层，再次被转换。这一过程持续进行，直到数据到达最后一层并完成最终转换，生成输出或预测结果。&lt;/p>
&lt;p>机器学习专家设计出不同类型的层，对输入数据进行数学转换。他们还探索了组织和分组层的方法，以实现期望的结果。有些层是通用的，而另一些则专门处理特定类型的输入数据，如图像，或者在大语言模型中的标记化文本。&lt;/p>
&lt;p>&lt;strong>目前在大语言模型的文本生成任务中最流行的神经网络架构被称为 “[Transformer](&lt;a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)&lt;/a>”。使用这种架构的模型被称为 GPT，也就是 [Generative Pre-Trained Transformers(生成式预训练 Transformer)](&lt;a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer">https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&lt;/a>。&lt;/strong>&lt;/p>
&lt;p>Transformer 模型的独特之处在于其执行的 “[Attention](&lt;a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">https://en.wikipedia.org/wiki/Attention_(machine_learning)&lt;/a>” 层计算。这种计算允许模型&lt;strong>在上下文窗口内的标记之间找出关系和模式，并将其反映在下一个标记出现的概率中&lt;/strong>。Attention 机制最初被用于语言翻译领域，作为一种找出输入序列中对理解句子意义最重要的标记的方法。这种机制赋予了现代语言模型在基本层面上 “理解” 句子的能力，它可以关注 (或集中 “注意力” 于) 重要词汇或标记，从而更好地把握句子的整体意义。正是这一机制，使 Transformer 模型在各种自然语言处理任务中取得了巨大成功。&lt;/p>
&lt;h2 id="大语言模型到底有没有智能">大语言模型到底有没有智能？&lt;a class="td-heading-self-link" href="#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%88%b0%e5%ba%95%e6%9c%89%e6%b2%a1%e6%9c%89%e6%99%ba%e8%83%bd" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>通过上面的分析，你心中可能已经有了一个初步的判断：大语言模型在生成文本时是否表现出了某种形式的智能？&lt;/p>
&lt;p>我个人并不认为大语言模型具备推理或提出原创想法的能力，但这并不意味着它们一无是处。得益于对上下文窗口中 token 进行的精妙计算，大语言模型能够捕捉用户输入中的模式，并将其与训练过程中学习到的相似模式匹配。&lt;strong>它们生成的文本大部分来自训练数据的片段，但将词语 (实际上是 token) 组合在一起的方式非常复杂，在许多情况下产生了感觉原创且有用的结果&lt;/strong>。&lt;/p>
&lt;p>不过，考虑到大语言模型容易产生幻觉，我不会信任任何将其输出直接提供给最终用户而不经过人工验证的工作流程。&lt;/p>
&lt;p>未来几个月或几年内出现的更大规模语言模型是否能实现类似真正智能的能力？鉴于 GPT 架构的诸多局限性，我觉得这不太可能发生，但谁又说的准呢，也许将来出现一些创新手段，我们就能实现这一目标。&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>LLM: &lt;a href="https://en.wikipedia.org/wiki/Large_language_model">https://en.wikipedia.org/wiki/Large_language_model&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>