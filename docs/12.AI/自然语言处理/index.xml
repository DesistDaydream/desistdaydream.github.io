<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>自然语言处理 on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link><description>Recent content in 自然语言处理 on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>自然语言处理</title><link>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</guid><description>概述 参考：
Wiki, Natural_language_processing Natural Language Processing(自然语言处理，简称 NLP) 是语言学、计算机科学和人工智能的跨学科领域，主要关注计算机与人类语言之间的交互，特别是如何编写程序来处理和分析大量的自然语言数据。目标是让计算机能够“理解”文档的内容，包括其中的语言上下文细微差别。技术可以准确提取文档中包含的信息和见解，以及对文档本身进行分类和组织。
语言模型 参考：
Wiki, Language_model Language model(语言模型) 是单词序列的概率分布。
通过语言模型，才可以实现自然语言处理。NLP 程序都会使用语言模型，我们将自然语言作为输入，传递给语言模型，语言模型将会预测其将要输出的每一个单词的出现概率，然后逐一输出这些单词。
想要训练出来一个良好的语言模型，通常会需要类似 Transformers 这种机器学习模型。
Large language model Large language model(大语言模型，简称 LLM) 是由具有许多参数（通常为数十亿或更多权重）的神经网络组成的语言模型，使用自我监督学习对大量未标记文本进行训练。LLM 在 2018 年左右出现，并在各种任务中表现出色。这已经将自然语言处理研究的重点从之前为特定任务训练专门的监督模型的范式转移了。
我们经常看到 LLM 实现的模型后面有 XB 的样式，其中 B 表示 Billions(十亿)，这个 XB 指的就是参数的数量。比如 6B 表示 60 亿参数。
常见模型 参考：
Wiki, Large_language_model-大语言模型列表 公众号-OSC 开源社区，大预言模型精选开源项目 我们常见的语言模型在现阶段（2023.5）可以简单分为如下几大类
BERT 系 类似完形填空，联系上下文直接给出空中的内容 GPT 系 一字一字推测的自回归模型。有a推测b，然后根据ab推测c，根据abc推测d，以此类推 其他系 Bidirectional Encoder Representations from Transformers(来自 Transformers 的双向编码器表示，简称 BERT)
Generative Pre-trained Transformer(生成式预训练 Transformer，简称 GPT)</description></item><item><title>不通过数学解释 LLMs 是如何工作的</title><link>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%8D%E9%80%9A%E8%BF%87%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A-LLMs-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/12.AI/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%B8%8D%E9%80%9A%E8%BF%87%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A-LLMs-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/</guid><description>概述 参考：
https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math 中文翻译：公众号 - 云原生实验室，大模型到底有没有智能？一篇文章给你讲明明白白 生成式人工智能 (GenAI) 和大语言模型 (LLM1)，这两个词汇想必已在大家的耳边萦绕多时。它们如惊涛骇浪般席卷了整个科技界，登上了各大新闻头条。ChatGPT，这个神奇的对话助手，也许已成为你形影不离的良师益友。
然而，在这场方兴未艾的 GenAI 革命背后，有一个谜题久久萦绕在人们心头：这些模型的智能究竟从何而来？本文将为您揭开谜底，解析生成式文本模型的奥秘。我们将抛开晦涩艰深的数学，用通俗易懂的语言，带您走进这个神奇的算法世界。让我们撕下 “魔法” 的面纱，看清其中的计算机科学本质。
LLM 的真面目 首先，我们要破除一个常见的误区。许多人误以为，这些模型是真的能够与人对话，回答人们的各种问题。然而，它们真正的能力远没有想象的那么复杂——它们所做的，不过是根据输入的文本，预测下一个词语 (更准确地说，是下一个 token)。
Token，这个看似简单的概念，却是揭开 LLM 神秘面纱的钥匙。让我们由此出发，步步深入，一探究竟。
Token，这些文本的积木、语言的原子，正是 LLM 理解世界的基石。对我们而言，token 不过是单词、标点、空格的化身，但在 LLM 的眼中，它们是精简而高效的信息编码。有时，一个 token 可能代表一串字符，长短不一；有时，它可能是孤零零的一个标点符号。
LLM 的词汇表，就是这些 token 的集合，啥都有，样样全。这其中的奥秘，要追溯到 BPE 算法。BPE 算法是如何炼制出这些 tokens 的？这个问题，值得我们细细探究。但在此之前，只需记住：GPT-2 模型，这个自然语言处理界的明星，它的词汇表中有 50,257 个 token。
在 LLM 的世界里，每个 token 都有一个独一无二的数字身份证。而 Tokenizer，就是文本和 token 之间的 “翻译官”，将人类的语言转化为 LLM 能理解的编码，也将 LLM 的思维解码为人类的文字。如果你熟悉 Python，不妨亲自与 token 打个照面。只需安装 OpenAI 的 tiktoken 包：
pip install tiktoken `
然后在 Python 中尝试以下操作：</description></item></channel></rss>