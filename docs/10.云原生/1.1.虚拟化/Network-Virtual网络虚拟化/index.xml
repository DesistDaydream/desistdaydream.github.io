<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Network Virtual(网络虚拟化)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</link><description>Recent content in Network Virtual(网络虚拟化) on 断念梦</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 2.Linux 上抽象网络设备的原理及使用</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/2.Linux-%E4%B8%8A%E6%8A%BD%E8%B1%A1%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/2.Linux-%E4%B8%8A%E6%8A%BD%E8%B1%A1%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/</guid><description>
&lt;h1 id="linux-抽象网络设备简介">Linux 抽象网络设备简介&lt;/h1>
&lt;p>和磁盘设备类似，Linux 用户想要使用网络功能，不能通过直接操作硬件完成，而需要直接或间接的操作一个 Linux 为我们抽象出来的设备，既通用的 Linux 网络设备来完成。一个常见的情况是，系统里装有一个硬件网卡，Linux 会在系统里为其生成一个网络设备实例，如 eth0，用户需要对 eth0 发出命令以配置或使用它了。更多的硬件会带来更多的设备实例，虚拟的硬件也会带来更多的设备实例。随着网络技术，虚拟化技术的发展，更多的高级网络设备被加入了到了 Linux 中，使得情况变得更加复杂。在以下章节中，将一一分析在虚拟化技术中经常使用的几种 Linux 网络设备抽象类型：Bridge、802.1.q VLAN device、VETH、TAP，详细解释如何用它们配合 Linux 中的 Route table、IP table 简单的创建出本地虚拟网络。&lt;/p>
&lt;h2 id="相关网络设备工作原理">相关网络设备工作原理&lt;/h2>
&lt;h3 id="bridge">Bridge&lt;/h3>
&lt;p>Bridge（桥）是 Linux 上用来做 TCP/IP 二层协议交换的设备，与现实世界中的交换机功能相似。Bridge 设备实例可以和 Linux 上其他网络设备实例连接，既 attach 一个从设备，类似于在现实世界中的交换机和一个用户终端之间连接一根网线。当有数据到达时，Bridge 会根据报文中的 MAC 信息进行广播、转发、丢弃处理。&lt;/p>
&lt;p>图 1.Bridge 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257424-79688f5a-dd7b-4f17-9afe-18d24df26e54.png" alt="">&lt;/p>
&lt;p>如图所示，Bridge 的功能主要在内核里实现。当一个从设备被 attach 到 Bridge 上时，相当于现实世界里交换机的端口被插入了一根连有终端的网线。这时在内核程序里，netdev_rx_handler_register()被调用，一个用于接受数据的回调函数被注册。以后每当这个从设备收到数据时都会调用这个函数可以把数据转发到 Bridge 上。当 Bridge 接收到此数据时，br_handle_frame()被调用，进行一个和现实世界中的交换机类似的处理过程：判断包的类别（广播/单点），查找内部 MAC 端口映射表，定位目标端口号，将数据转发到目标端口或丢弃，自动更新内部 MAC 端口映射表以自我学习。&lt;/p>
&lt;p>Bridge 和现实世界中的二层交换机有一个区别，图中左侧画出了这种情况：数据被直接发到 Bridge 上，而不是从一个端口接受。这种情况可以看做 Bridge 自己有一个 MAC 可以主动发送报文，或者说 Bridge 自带了一个隐藏端口和寄主 Linux 系统自动连接，Linux 上的程序可以直接从这个端口向 Bridge 上的其他端口发数据。所以当一个 Bridge 拥有一个网络设备时，如 bridge0 加入了 eth0 时，实际上 bridge0 拥有两个有效 MAC 地址，一个是 bridge0 的，一个是 eth0 的，他们之间可以通讯。由此带来一个有意思的事情是，Bridge 可以设置 IP 地址。通常来说 IP 地址是三层协议的内容，不应该出现在二层设备 Bridge 上。但是 Linux 里 Bridge 是通用网络设备抽象的一种，只要是网络设备就能够设定 IP 地址。当一个 bridge0 拥有 IP 后，Linux 便可以通过路由表或者 IP 表规则在三层定位 bridge0，此时相当于 Linux 拥有了另外一个隐藏的虚拟网卡和 Bridge 的隐藏端口相连，这个网卡就是名为 bridge0 的通用网络设备，IP 可以看成是这个网卡的。当有符合此 IP 的数据到达 bridge0 时，内核协议栈认为收到了一包目标为本机的数据，此时应用程序可以通过 Socket 接收到它。一个更好的对比例子是现实世界中的带路由的交换机设备，它也拥有一个隐藏的 MAC 地址，供设备中的三层协议处理程序和管理程序使用。设备里的三层协议处理程序，对应名为 bridge0 的通用网络设备的三层协议处理程序，即寄主 Linux 系统内核协议栈程序。设备里的管理程序，对应 bridge0 寄主 Linux 系统里的应用程序。&lt;/p>
&lt;p>Bridge 的实现当前有一个限制：当一个设备被 attach 到 Bridge 上时，那个设备的 IP 会变的无效，Linux 不再使用那个 IP 在三层接受数据。举例如下：如果 eth0 本来的 IP 是 192.168.1.2，此时如果收到一个目标地址是 192.168.1.2 的数据，Linux 的应用程序能通过 Socket 操作接受到它。而当 eth0 被 attach 到一个 bridge0 时，尽管 eth0 的 IP 还在，但应用程序是无法接受到上述数据的。此时应该把 IP 192.168.1.2 赋予 bridge0。&lt;/p>
&lt;p>另外需要注意的是数据流的方向。对于一个被 attach 到 Bridge 上的设备来说，只有它收到数据时，此包数据才会被转发到 Bridge 上，进而完成查表广播等后续操作。当请求是发送类型时，数据是不会被转发到 Bridge 上的，它会寻找下一个发送出口。用户在配置网络时经常忽略这一点从而造成网络故障。&lt;/p>
&lt;h3 id="vlan-device-for-8021q">VLAN device for 802.1.q&lt;/h3>
&lt;p>VLAN 又称虚拟网络，是一个被广泛使用的概念，有些应用程序把自己的内部网络也称为 VLAN。此处主要说的是在物理世界中存在的，需要协议支持的 VLAN。它的种类很多，按照协议原理一般分为：MACVLAN、802.1.q VLAN、802.1.qbg VLAN、802.1.qbh VLAN。其中出现较早，应用广泛并且比较成熟的是 802.1.q VLAN，其基本原理是在二层协议里插入额外的 VLAN 协议数据（称为 802.1.q VLAN Tag)，同时保持和传统二层设备的兼容性。Linux 里的 VLAN 设备是对 802.1.q 协议的一种内部软件实现，模拟现实世界中的 802.1.q 交换机。&lt;/p>
&lt;p>图 2 .VLAN 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257463-a0ff1a57-bfc0-40ea-b486-e6b2a4081b2e.png" alt="">&lt;/p>
&lt;p>如图所示，Linux 里 802.1.q VLAN 设备是以母子关系成对出现的，母设备相当于现实世界中的交换机 TRUNK 口，用于连接上级网络，子设备相当于普通接口用于连接下级网络。当数据在母子设备间传递时，内核将会根据 802.1.q VLAN Tag 进行对应操作。母子设备之间是一对多的关系，一个母设备可以有多个子设备，一个子设备只有一个母设备。当一个子设备有一包数据需要发送时，数据将被加入 VLAN Tag 然后从母设备发送出去。当母设备收到一包数据时，它将会分析其中的 VLAN Tag，如果有对应的子设备存在，则把数据转发到那个子设备上并根据设置移除 VLAN Tag，否则丢弃该数据。在某些设置下，VLAN Tag 可以不被移除以满足某些监听程序的需要，如 DHCP 服务程序。举例说明如下：eth0 作为母设备创建一个 ID 为 100 的子设备 eth0.100。此时如果有程序要求从 eth0.100 发送一包数据，数据将被打上 VLAN 100 的 Tag 从 eth0 发送出去。如果 eth0 收到一包数据，VLAN Tag 是 100，数据将被转发到 eth0.100 上，并根据设置决定是否移除 VLAN Tag。如果 eth0 收到一包包含 VLAN Tag 101 的数据，其将被丢弃。上述过程隐含以下事实：对于寄主 Linux 系统来说，母设备只能用来收数据，子设备只能用来发送数据。和 Bridge 一样，母子设备的数据也是有方向的，子设备收到的数据不会进入母设备，同样母设备上请求发送的数据不会被转到子设备上。可以把 VLAN 母子设备作为一个整体想象为现实世界中的 802.1.q 交换机，下级接口通过子设备连接到寄主 Linux 系统网络里，上级接口同过主设备连接到上级网络，当母设备是物理网卡时上级网络是外界真实网络，当母设备是另外一个 Linux 虚拟网络设备时上级网络仍然是寄主 Linux 系统网络。&lt;/p>
&lt;p>需要注意的是母子 VLAN 设备拥有相同的 MAC 地址，可以把它当成现实世界中 802.1.q 交换机的 MAC，因此多个 VLAN 设备会共享一个 MAC。当一个母设备拥有多个 VLAN 子设备时，子设备之间是隔离的，不存在 Bridge 那样的交换转发关系，原因如下：802.1.q VLAN 协议的主要目的是从逻辑上隔离子网。现实世界中的 802.1.q 交换机存在多个 VLAN，每个 VLAN 拥有多个端口，同一 VLAN 端口之间可以交换转发，不同 VLAN 端口之间隔离，所以其包含两层功能：交换与隔离。Linux VLAN device 实现的是隔离功能，没有交换功能。一个 VLAN 母设备不可能拥有两个相同 ID 的 VLAN 子设备，因此也就不可能出现数据交换情况。如果想让一个 VLAN 里接多个设备，就需要交换功能。在 Linux 里 Bridge 专门实现交换功能，因此将 VLAN 子设备 attach 到一个 Bridge 上就能完成后续的交换功能。总结起来，Bridge 加 VLAN device 能在功能层面完整模拟现实世界里的 802.1.q 交换机。&lt;/p>
&lt;p>Linux 支持 VLAN 硬件加速，在安装有特定硬件情况下，图中所述内核处理过程可以被放到物理设备上完成。&lt;/p>
&lt;h3 id="tap-设备与-veth-设备">TAP 设备与 VETH 设备&lt;/h3>
&lt;p>TUN/TAP 设备是一种让用户态程序向内核协议栈注入数据的设备，一个工作在三层，一个工作在二层，使用较多的是 TAP 设备。VETH 设备出现较早，它的作用是反转通讯数据的方向，需要发送的数据会被转换成需要收到的数据重新送入内核网络层进行处理，从而间接的完成数据的注入。&lt;/p>
&lt;p>图 3 .TAP 设备和 VETH 设备工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257472-c3161ee0-667a-4db3-b478-6d4d3e6c1682.png" alt="">&lt;/p>
&lt;p>如图所示，当一个 TAP 设备被创建时，在 Linux 设备文件目录下将会生成一个对应 char 设备，用户程序可以像打开普通文件一样打开这个文件进行读写。当执行 write()操作时，数据进入 TAP 设备，此时对于 Linux 网络层来说，相当于 TAP 设备收到了一包数据，请求内核接受它，如同普通的物理网卡从外界收到一包数据一样，不同的是其实数据来自 Linux 上的一个用户程序。Linux 收到此数据后将根据网络配置进行后续处理，从而完成了用户程序向 Linux 内核网络层注入数据的功能。当用户程序执行 read()请求时，相当于向内核查询 TAP 设备上是否有需要被发送出去的数据，有的话取出到用户程序里，完成 TAP 设备的发送数据功能。针对 TAP 设备的一个形象的比喻是：使用 TAP 设备的应用程序相当于另外一台计算机，TAP 设备是本机的一个网卡，他们之间相互连接。应用程序通过 read()/write()操作，和本机网络核心进行通讯。(可以这么说，一台虚拟机的网卡就是一个物理机上的 tap 设备)&lt;/p>
&lt;p>VETH 设备总是成对出现，送到一端请求发送的数据总是从另一端以请求接受的形式出现。该设备不能被用户程序直接操作，但使用起来比较简单。创建并配置正确后，向其一端输入数据，VETH 会改变数据的方向并将其送入内核网络核心，完成数据的注入。在另一端能读到此数据。&lt;/p>
&lt;h2 id="网络设置举例说明">网络设置举例说明&lt;/h2>
&lt;p>为了更好的说明 Linux 网络设备的用法，下面将用一系列的例子，说明在一个复杂的 Linux 网络元素组合出的虚拟网络里，数据的流向。网络设置简介如下：一个中心 Bridge：bridge0 下 attach 了 4 个网络设备，包括 2 个 VETH 设备，1 个 TAP 设备 tap0，1 个物理网卡 eth0。在 VETH 的另外一端又创建了 VLAN 子设备。Linux 上共存在 2 个 VLAN 网络，既 vlan100 与 vlan200。物理网卡和外部网络相连，并且在它之下创建了一个 VLAN ID 为 200 的 VLAN 子设备。&lt;/p>
&lt;h3 id="从-vlan100-子设备发送-arp-报文">从 vlan100 子设备发送 ARP 报文&lt;/h3>
&lt;p>图 4 .ARP from vlan100 child device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257520-663ccc4b-26bd-4784-bda1-7b8dd04c75a6.png" alt="">&lt;/p>
&lt;p>如图所示，当用户尝试 ping 192.168.100.3 时，Linux 将会根据路由表，从 vlan100 子设备发出 ARP 报文，具体过程如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>用户 ping 192.168.100.3&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Linux 向 vlan100 子设备发送 ARP 信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ARP 报文被打上 VLAN ID 100 的 Tag 成为 ARP@vlan100，转发到母设备上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VETH 设备将这一发送请求转变方向，成为一个需要接受处理的报文送入内核网络模块。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>由于对端的 VETH 设备被加入到了 bridge0 上，并且内核发现它收到一个报文，于是报文被转发到 bridge0 上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bridge0 处理此 ARP@vlan100 信息，根据 TCP/IP 二层协议发现是一个广播请求，于是向它所知道的所有端口广播此报文，其中一路进入另一对 VETH 设备的一端，一路进入 TAP 设备 tap0，一路进入物理网卡设备 eth0。此时在 tap0 上，用户程序可以通过 read()操作读到 ARP@vlan100，eth0 将会向外界发送 ARP@vlan100，但 eth0 的 VLAN 子设备不会收到它，因为此数据方向为请求发送而不是请求接收。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VETH 将请求方向转换，此时在另一端得到请求接受的 ARP@vlan100 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对端 VETH 设备发现有数据需要接受，并且自己有两个 VLAN 子设备，于是执行 VLAN 处理逻辑。其中一个子设备是 vlan100，与 ARP@vlan100 吻合，于是去除 VLAN ID 100 的 Tag 转发到这个子设备上，重新成为标准的以太网 ARP 报文。另一个子设备由于 ID 不吻合，不会得到此报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此 VLAN 子设备又被 attach 到另一个桥 bridge1 上，于是转发自己收到的 ARP 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bridge1 广播 ARP 报文。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最终另外一个 TAP 设备 tap1 收到此请求发送报文，用户程序通过 read()可以得到它。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="从-vlan200-子设备发送-arp-报文">从 vlan200 子设备发送 ARP 报文&lt;/h3>
&lt;p>图 5 .ARP from vlan200 child device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257503-55a18ca0-f189-4987-9a14-be02dff09d03.png" alt="">&lt;/p>
&lt;p>和前面情况类似，区别是 VLAN ID 是 200，对端的 vlan200 子设备设置为 reorder_hdr = 0，表示此设备被要求保留收到的报文中的 VLAN Tag。此时子设备会收到 ARP 报文，但是带了 VLAN ID 200 的 Tag，既 ARP@vlan200。&lt;/p>
&lt;h3 id="从中心-bridge-发送-arp-报文">从中心 bridge 发送 ARP 报文&lt;/h3>
&lt;p>图 5 .ARP from central bridge&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257514-72135ddd-006c-417e-a0b2-0a62c5906609.png" alt="">&lt;/p>
&lt;p>当 bridge0 拥有 IP 时，通过 Linux 路由表用户程序可以直接将 ARP 报文发向 bridge0。这时 tap0 和外部网络都能收到 ARP，但 VLAN 子设备由于 VLAN ID 过滤的原因，将收不到 ARP 信息。&lt;/p>
&lt;h3 id="从外部网络向物理网卡发送-arpvlan200-报文">从外部网络向物理网卡发送 ARP@vlan200 报文&lt;/h3>
&lt;p>图 6 .ARP from external network&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257531-ba4cbf5b-4940-4019-b46c-6e9a47946cbc.png" alt="">&lt;/p>
&lt;p>当外部网络连接在一个支持 VLAN 并且对应端口为 vlan200 时，此情况会发生。此时所有的 VLAN ID 为 200 的 VLAN 子设备都将接受到报文，如果设置 reorder_hdr=0 则会收到带 Tag 的 ARP@vlan200。&lt;/p>
&lt;h3 id="从-tap-设备以-ping-方式发送-arp">从 TAP 设备以 ping 方式发送 ARP&lt;/h3>
&lt;p>图 7 .ping from TAP device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257520-73844bea-b56f-413c-8688-ea99573c2b2b.png" alt="">&lt;/p>
&lt;p>给 tap0 赋予 IP 并加入路由，此时再 Ping 其对应网段的未知 IP 会产生 ARP 发送请求。需要注意的是此时由于 tap0 上存在的是发送而不是接收请求，因此 ARP 报文不会被转发到桥上，从而什么也不会发生。图中右边画了一个类似情况：从 vlan200 子设备发送 ARP 请求。由于缺少 VETH 设备反转请求方向，因此报文也不会被转发到桥上，而是直接通过物理网卡发往外部网络。&lt;/p>
&lt;h3 id="以文件操作方式从-tap-设备发送报文">以文件操作方式从 TAP 设备发送报文&lt;/h3>
&lt;p>图 8 .file operation on TAP device&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/szbbas/1616124257536-d6ac7a20-331e-421f-a7d0-58862524405e.png" alt="">&lt;/p>
&lt;p>用户程序指定 tap0 设备发送报文有两种方式：socket 和 file operation。当用 socket_raw 标志新建 socket 并指定设备编号时，可以要求内核将报文从 tap0 发送。但和前面的 ping from tap0 情况类似，由于报文方向问题，消息并不会被转发到 bridge0 上。当用 open()方式打开 tap 设备文件时，情况有所不同。当执行 write()操作时，内核认为 tap0 收到了报文，从而会触发转发动作，bridge0 将收到它。如果发送的报文如图所示，是一个以 A 为目的地的携带 VLAN ID 100 Tag 的单点报文，bridge0 将会找到对应的设备进行转发，对应的 VLAN 子设备将收到没有 VLAN ID 100 Tag 的报文。&lt;/p>
&lt;h2 id="linux-上配置网络设备命令举例">Linux 上配置网络设备命令举例&lt;/h2>
&lt;p>以 Redhat6.2 红帽 Linux 发行版为例，如果已安装 VLAN 内核模块和管理工具 vconfig，TAP/TUN 设备管理工具 tunctl，那么可以用以下命令设置前述网络设备：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>创建 Bridge：brctl addbr [BRIDGE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 Bridge：brctl delbr [BRIDGE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>attach 设备到 Bridge：brctl addif [BRIDGE NAME] [DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从 Bridge detach 设备：brctl delif [BRIDGE NAME] [DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查询 Bridge 情况：brctl show&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 VLAN 设备：vconfig add [PARENT DEVICE NAME] [VLAN ID]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 VLAN 设备：vconfig rem [VLAN DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设置 VLAN 设备 flag：vconfig set_flag [VLAN DEVICE NAME] [FLAG] [VALUE]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>设置 VLAN 设备 qos：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>vconfig set_egress_map [VLAN DEVICE NAME] [SKB_PRIORITY] [VLAN_QOS]&lt;/p>
&lt;p>vconfig set_ingress_map [VLAN DEVICE NAME] [SKB_PRIORITY] [VLAN_QOS]&lt;/p>
&lt;ul>
&lt;li>
&lt;p>查询 VLAN 设备情况：cat /proc/net/vlan/[VLAN DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 VETH 设备：ip link add link [DEVICE NAME] type veth&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 TAP 设备：tunctl -p [TAP DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除 TAP 设备：tunctl -d [TAP DEVICE NAME]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>查询系统里所有二层设备，包括 VETH/TAP 设备：ip link show&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除普通二层设备：ip link delete [DEVICE NAME] type [TYPE]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>综上所述，Linux 已经提供一套基本工具供用户创建出各种内部网络，利用这些工具可以方便的创建出特定网络给应用程序使用，包括云计算中的初级内部虚拟网络。&lt;/p>
&lt;p>相关主题&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.linuxcommand.org/man_pages/vconfig8.html">Vconfig Man Page&lt;/a>，vconfig 工具帮助文档。&lt;/li>
&lt;li>&lt;a href="http://candelatech.com/~greear/vlan.html">802.1Q VLAN implementation for Linux&lt;/a>，Linux 中 VLAN 模块如何实现的文档说明。&lt;/li>
&lt;li>&lt;a href="http://www.policyrouting.org/iproute2.doc.html">IPROUTE2 Utility Suite Howto&lt;/a>，Linux 里的 IP 工具使用说明。&lt;/li>
&lt;li>&lt;a href="http://www.ibm.com/developerworks/linux/library/l-virtual-networking">Virtual networking in Linux&lt;/a>，以虚拟化应用为中心讲述主流的虚拟网络技术，主要以 openvswith 为例。&lt;/li>
&lt;li>&lt;a href="http://tldp.org/HOWTO/BRIDGE-STP-HOWTO/index.html">Linux BRIDGE-STP-HOWTO&lt;/a>，Linux 中的 bridge 设备使用说明。&lt;/li>
&lt;li>&lt;a href="http://www.linuxfoundation.org/collaborate/workgroups/networking/networkoverview">Linux Kernel Networking (Network Overview) by Rami Rosen&lt;/a>，Linux 内核里的各种网络概念的含义，目的及用法简单介绍。&lt;/li>
&lt;li>在 &lt;a href="http://www.ibm.com/developerworks/cn/linux/">developerWorks Linux &lt;/a>专区寻找为 Linux 开发人员（包括 &lt;a href="http://www.ibm.com/developerworks/cn/linux/newto/">Linux 新手入门&lt;/a>）准备的更多参考资料。&lt;/li>
&lt;/ul></description></item><item><title>Docs: Network Virtual(网络虚拟化)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;/blockquote>
&lt;p>在传统网络环境中，一台物理主机包含一个或多个网卡（NIC），要实现与其他物理主机之间的通信，需要通过自身的 NIC 连接到外部的网络设施，如交换机上；为了对应用进行隔离，往往是将一个应用部署在一台物理设备上，这样会存在两个问题：&lt;/p>
&lt;ol>
&lt;li>是某些应用大部分情况可能处于空闲状态，&lt;/li>
&lt;li>是当应用增多的时 候，只能通过增加物理设备来解决扩展性问题。不管怎么样，这种架构都会对物理资源造成极大的浪费。&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，可以借助虚拟化技术对一台物理资源进行抽象，将一张物理网卡虚拟成多张虚拟网卡（vNIC），通过虚拟机来隔离不同的应用。&lt;/p>
&lt;ol>
&lt;li>针对问题 1），可以利用虚拟化层 Hypervisor 的调度技术，将资源从空闲的应用上调度到繁忙的应用上，达到资源的合理利用；&lt;/li>
&lt;li>针对问题 2），可以根据物理设备的资源使用情况进行横向扩容，除非设备资源已经用尽，否则没有必要新增设备。&lt;/li>
&lt;/ol>
&lt;p>综上所述：SDN 主要是通过系统的功能，模拟出网络设备中的路由器，交换机，端口，网线等等，这些现实中的数通设备都可以通过软件来模拟实现&lt;/p>
&lt;p>网络虚拟化的几种最基础模型：&lt;/p>
&lt;ol>
&lt;li>隔离模型：在 host 上创建一个 vSwitch(bridge device)：每个 VM 的 TAP 设备直接添加至 vswitch 上，VM 通过 vSwitch 互相通信，与外界隔离&lt;/li>
&lt;li>路由模型：基于隔离模型，在 vSwitch 添加一个端口，作为 host 上的虚拟网卡使用(就是 VMware workstation 中创建的那些虚拟网卡，其中的 IP 作为虚拟机的网关)，并打开 host 的核心转发功能，使数据从 VM 发送到 host；该模型数据包可以从 VM 上出去，但是外界无法回到 VM，如果想让外部访问 VM，需要添加 NAT 功能，变成 NAT 模型&lt;/li>
&lt;li>NAT 模型：配置 Linux 自带的 NAT(可通过 iptables 定义)功能，所有 VM 的 IP 被 NAT 成物理网卡 IP，这是一种常用的虚拟网络模型&lt;/li>
&lt;li>桥接模型：可以想象成把物理机网卡变成一台 vSwitch，然后给物理机创建一个虚拟网卡，虚拟机和物理机都连接到 vSwitch，相当于把虚拟机直接接入到网络中，从网络角度看，VM 相当于同网段的一台 host&lt;/li>
&lt;li>隧道模型：VM 的数据包在经过某个具备隧道功能的虚拟网络设备时，可以在数据包外层再封装一层 IP，以 IP 套 IP 的隧道方式，与对方互通&lt;/li>
&lt;/ol>
&lt;p>网络虚拟化术语&lt;/p>
&lt;ol>
&lt;li>Network Stack：网络栈，包括网卡（Network Interface）、回环设备（LoopbackDevice）、路由表（Routing Table）和 iptables 规则。对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。&lt;/li>
&lt;li>port：当成虚拟交换机上的端口，可以打 VLAN TAG&lt;/li>
&lt;li>interface：当成接口，类似于连到端口的网线，可以设置 TYPE。&lt;a href="https://blog.csdn.net/number1killer/article/details/79226772">注意端口与接口的概念&lt;/a>&lt;/li>
&lt;li>bridge，port，interface：一个 BRIDGE 上可以配置多个 PORT，一个 PORT 上可以配置多个 INTERFACE&lt;/li>
&lt;/ol>
&lt;h1 id="网路虚拟化的解决方案">网路虚拟化的解决方案&lt;/h1>
&lt;p>基于 Linux 本身的网络虚拟化方案&lt;/p>
&lt;ol>
&lt;li>Linux Bridge # 虚拟网络基础资源，用于二层网络虚拟化&lt;/li>
&lt;li>Namespace # 网络名称空间，用于三层网络虚拟化&lt;/li>
&lt;/ol>
&lt;p>高级虚拟化方案&lt;/p>
&lt;ol>
&lt;li>Open vSwitch # 开源的虚拟交换机，用于二层网络虚拟化&lt;/li>
&lt;/ol>
&lt;h2 id="linux-bridgelinux-网桥">Linux Bridge(Linux 网桥)&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9d2b14eaa119000192206f">Linux 上抽象网络设备的原理及使用&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9d2a2712d5ba000162d3e2">云计算底层技术-虚拟网络设备(Bridge,VLAN)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.teambition.com/project/5f90e312755d8a00446050eb/app/5eba5fba6a92214d420a3219/workspaces/5f90e312c800160016ea22fb/docs/5f9d294eeaa11900019215fc">云计算底层技术-虚拟网络设备(tun tap,veth)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>每台 VM 都有一套独立的网络栈，如果想让两台 VM 互相通信怎么办呢？最直接的办法就是把两台 VM 用一根网线连接起来，而如果想要实现多台 VM 通信，那就需要用把它们连接到一台交换机上。这个&lt;strong>简单的虚拟交换机&lt;/strong>的功能就叫 &lt;code>Bridge(网桥)&lt;/code>，而&lt;strong>虚拟交换机想要实现普通物理交换机的&lt;/strong>上的功能，比如接口，网线，端口，Vlan 等 都有专门对应的虚拟网络设备来模拟实现。而交换机中的 VLAN 则是 Linux 自己本身实现的。&lt;/p>
&lt;h3 id="虚拟网络设备的类型">虚拟网络设备的类型&lt;/h3>
&lt;p>&lt;strong>Veth Pair：&lt;/strong>(Virtual Ethernet Pair)虚拟以太网设备对，可以理解为一跟网线。是 Linux 内核由模块实现的虚拟网络设备，该设备被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。创建方式详见 ip link add 命令。可以把 Veth 连接到 vSwich 与 vSwitch、vSwitch 与 nameSpace、namespace 与 namespace 上。&lt;/p>
&lt;p>veth 的作用是反转数据流量，从一段接收到数据后，会把该数据流进行反转变成发送，发送到对端后，对端接收到数据流之后，再次反转会发送给绑定到该端的 namesapce 中。ethtool -S VethNAME 可以使用该命令通过 Veth 的一半查看另一半网卡的序号&lt;/p>
&lt;p>&lt;strong>TAP/TUN&lt;/strong>：该设备会创建一个 /dev/tunX 的文件并作用在内核空间，与用户空间的 APP 相连(比如 VM)，当这个 VM 通过其 Hypervisor 通信时，会把数据写入该/dev/tunX 文件，并交给内核，内核会处理这些数据并通过网卡发送。TAP 工作在二层，TUN 工作在三层。详见 TUN TAP 设备浅析(一) &amp;ndash; 原理浅析&lt;/p>
&lt;ul>
&lt;li>Veth Pari 与 TAP/TUN 设备在 VM 与 Container 中的使用注意事项以及原因
&lt;ul>
&lt;li>为什么 VM 要使用 TAP/TUN,而 Container 不用？因为 VM 数据包在从其进程发送到 Host 的时候，由于 VM 有自己的内核，那么这个数据包相当于已经经过了一个 VM 的网络栈，这时候就不能直接发送给 Host 的网络栈再进行处理了，所以需要 TAP/TUN 设备来作为一个转折点，接收 VM 的数据包，并以一个已经经过网络栈处理过的姿态直接进入内核的网络设备。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bridge(网桥)&lt;/strong>：在 Linux 中能够起到虚拟交换机作用的网络设备，但不同于 TAP/TUN 这种单端口的设备，Bridge 实现虚拟为多端口，本质上是一个虚拟交换机，具备和物理交换机类似的功能。Bridge 可以绑定其他 Linux 网络设备作为从设备，并将这些从设备虚拟化为端口，当一个从设备被绑定到 Bridge 上时，就相当于真实网络中的交换机端口上插入了一根连有终端的网线。&lt;/p>
&lt;p>注意：一旦一块虚拟网卡被连接到 Bridge 上，该设备会变成 该 Bridge 的“从设备”。从设备会被剥夺调用网络协议栈处理数据包的资格，从而降级成网桥上的一个端口。而这个端口的唯一作用就是接收流入的数据包，然后把这些数据包的“生杀大权”(比如转发或者丢弃等)全部交给对应的 Bridge 进行处理&lt;/p>
&lt;h3 id="linux-如何实现-vlan">Linux 如何实现 VLAN&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kinqyh/1616124322231-b84ec223-407c-41fd-b0bf-0a1c61faa7c9.jpeg" alt="">&lt;/p>
&lt;ol>
&lt;li>eth0 是宿主机上的物理网卡，有一个命名为 eth0.10 的子设备与之相连。 eth0.10 就是 VLAN 设备了，其 VLAN ID 就是 VLAN 10。 eth0.10 挂在命名为 brvlan10 的 Linux Bridge 上，虚机 VM1 的虚拟网卡 vent0 也挂在 brvlan10 上。&lt;/li>
&lt;li>这样的配置其效果就是： 宿主机用软件实现了一个交换机（当然是虚拟的），上面定义了一个 VLAN10。 eth0.10，brvlan10 和 vnet0 都分别接到 VLAN10 的 Access 口上。而 eth0 就是一个 Trunk 口。VM1 通过 vnet0 发出来的数据包会被打上 VLAN10 的标签。&lt;/li>
&lt;li>eth0.10 的作用是：定义了 VLAN10&lt;/li>
&lt;li>brvlan10 的作用是：Bridge 上的其他网络设备自动加入到 VLAN10 中&lt;/li>
&lt;li>再增加一个 VLAN20&lt;/li>
&lt;li>样虚拟交换机就有两个 VLAN 了，VM1 和 VM2 分别属于 VLAN10 和 VLAN20。 对于新创建的虚机，只需要将其虚拟网卡放入相应的 Bridge，就能控制其所属的 VLAN。&lt;/li>
&lt;li>VLAN 设备总是以母子关系出现，母子设备之间是一对多的关系。 一个母设备（eth0）可以有多个子设备（eth0.10，eth0.20 ……），而一个子设备只有一个母设备。&lt;/li>
&lt;/ol>
&lt;p>Linux Bridge + VLAN = 虚拟交换机&lt;/p>
&lt;ol>
&lt;li>物理交换机存在多个 VLAN，每个 VLAN 拥有多个端口。 同一 VLAN 端口之间可以交换转发，不同 VLAN 端口之间隔离。 所以交换机其包含两层功能：交换与隔离。&lt;/li>
&lt;li>Linux 的 VLAN 设备实现的是隔离功能，但没有交换功能。 一个 VLAN 母设备（比如 eth0）不能拥有两个相同 ID 的 VLAN 子设备，因此也就不可能出现数据交换情况。&lt;/li>
&lt;li>Linux Bridge 专门实现交换功能。 将同一 VLAN 的子设备都挂载到一个 Bridge 上，设备之间就可以交换数据了。&lt;/li>
&lt;/ol>
&lt;p>总结起来，Linux Bridge 加 VLAN 在功能层面完整模拟现实世界里的二层交换机。eth0 相当于虚拟交换机上的 trunk 口，允许 vlan10 和 vlan20 的数据通过。&lt;/p>
&lt;p>eth0.10，vent0 和 brvlan10 都可以看着 vlan10 的 access 口。&lt;/p>
&lt;p>eth0.20，vent1 和 brvlan20 都可以看着 vlan20 的 access 口。&lt;/p>
&lt;h2 id="open-vswitch--开放的虚拟交换机">Open vSwitch # 开放的虚拟交换机&lt;/h2>
&lt;p>详见 &lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network%20Virtual(%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96)/Open%20vSwitch.md">Open vSwitch&lt;/a>&lt;/p>
&lt;h2 id="network-namespace--网络名称空间">Network Namespace # 网络名称空间&lt;/h2>
&lt;p>Network Namespace 可以简单得理解为 Linux 上的 &lt;strong>虚拟路由器(vRouter)&lt;/strong>。详见：&lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/1.Namespaces/Network%20Namespace/Network%20Namespace.md">Network Namespace&lt;/a>&lt;/p>
&lt;p>Network Namespace 在逻辑上是网络栈的另一个副本，具有自己的路由、防火墙规则、网络设备等功能。默认情况下，每个进程从其父进程继承其网络名称空间。在最初的时候，所有进程都共享来自系统启动的 PID 为 1 的父进程的名称空间。整个系统 PID 为 1 的进程的 Network Namespace 就是整台机器和系统的网络栈。Linux 内核可以通过 clone()函数的功能在默认的网络名称空间中，克隆出来一个具备相同功能的网络栈，该克隆出来的 Network Namespace 为绑定上来的进程提供一个完全独立的网络协议栈，多个进程也可同时共享同一个 Network Namespace。Host 上一个网络设备只能存在于一个 Network Namespace 当中，而不同的 Network Namespace 之间要想通信，可以通过虚拟网络设备来提供一种类似于管道的抽象，在不同的 Network Namespace 中建立隧道实现通信。当一个 Network Namespace 被销毁时，插入到该 Netwrok Namespace 上的虚拟网络设备会被自动移回最开始默认整台设备的 Network Namespace&lt;/p>
&lt;p>Network Namespace 的应用场景&lt;/p>
&lt;ol>
&lt;li>可以把 Net Namepace 就相当于在物理机上创建了一台 vRouter，这台 vRouter 就是一块 namespace，把与 VM 连接的 vSwitch 连接到这台 vRouter，然后 VM 通过 vRouter 与外部或者另一部分被隔离的网络通信，这样即可实现对这台 vSwitch 以及与之关联的 VM 进行网络隔离（如果要与外部通信，那么需要使用桥接模型，把物理网卡模拟成 vSwitch，然后把该 vSwitch 关联到该 vRouter）&lt;/li>
&lt;li>Network Namespace 还可用于承载 Container 技术的网络功能，一个 Container 占据一个 Namespace，通过使用 Veth 设备连接 Namespace 与 Bridge 相连来实现各 Namespace 中的 Container 之间互相通信。具体详见 &lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.1.%E5%AE%B9%E5%99%A8/1.Namespaces/Network%20Namespace/Network%20Namespace.md">Network Namespace&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>管理 Network Namesapce 的方式：&lt;/p>
&lt;ol>
&lt;li>通过 ip netns 命令来管理，该命令的用法详见 &lt;a href="https://desistdaydream.github.io/docs/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/X.Linux%20%E7%AE%A1%E7%90%86/Linux%20%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/Iproute%20%E5%B7%A5%E5%85%B7%E5%8C%85/Iproute%20%E5%B7%A5%E5%85%B7%E5%8C%85.md">Iproute 工具包&lt;/a> 中的 netns 子命令&lt;/li>
&lt;/ol>
&lt;h1 id="overlay-network-叠加网络">Overlay Network 叠加网络&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kinqyh/1616124322243-b9ecf172-ef2b-4452-8c3c-364af95526aa.jpeg" alt="">
Overlay Network 产生的原因&lt;/p>
&lt;p>在网桥的概念中，各个 VM 可以通过 Host 上的 vSwitch 来进行通信，那么当需要访问另外一台 Host 上不同网段的 VM 的时候呢？&lt;/p>
&lt;p>实现 overlay network 的方式有 gre 等&lt;/p>
&lt;p>VXLAN&lt;/p>
&lt;p>概念详见 flannel.note 中的 vxlan 模型&lt;/p></description></item><item><title>Docs: Open vSwitch</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/Open-vSwitch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/Open-vSwitch/</guid><description>
&lt;h1 id="open-vswitch--开放的虚拟交换机">Open vSwitch # 开放的虚拟交换机&lt;/h1>
&lt;p>特性：支持 802.1q，trunk，access；支持网卡绑定技术(NIC Bonding);支持 QoS 配置及策略；支持 GRE 通用路由封装；支持 VxLAN；等等&lt;/p>
&lt;p>OVS 的组成部分：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ovs-vswitchd # 守护进程,实现数据报文交换功能，和 Linux 内核兼容模块一同实现了基于流的交换技术&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovsdb-server # ovs 的数据库，轻量级的数据库服务器，主要保存了 OVS 的配置信息，EXP 接口、交换、VLAN 等，ovs-vswitchd 的交换功能基于此库实现,相关数据信息保存在这个文件中：/etc/openvswitch/conf.db&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl # 命令行工具，用于获取或更改 ovs-vswitchd 的配置信息，其修改操作会保存至 ovsdb-server 中&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-dpctl #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-appctl #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovsdbmonitor #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-controller #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-ofctl #&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-pki #&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>一般情况下都是对于一台物理机上的几个 vSwithc 上的 VM 进行同行进行的配置，比如两个 VM 各连接一个 vSwitch，这时候可以对物理机使用 ip link add veth1.1 type veth peer name veth1.2 命令俩创建一对虚拟接口然后使用 ovs-vsctl add-port BRIDGE PORT 命令分别把这两个虚拟接口绑定在两个 vSwitch 上，实现俩个 vSwitch 之间互联并且能够通信。还有就是如图所示，由于 OVS 有 DB，各 NODE 之间的 OVS 数据都互相共享，那么可以直接把 VM 连接到 vSwitch 上，然后再连接到物理网络就可以互通了相当于只是几个交换机互联而已，如果进行隔离后，使得隔离的 VM 可以通信，那么使用 namespace 功能创建一个 vRouter，通过 vRouter 实现被隔离的网络间互相通信&lt;/p>
&lt;p>安装 OVS：yum install openvswitch -y&lt;/p>
&lt;h3 id="ovs-的几种-interface-类型">OVS 的几种 INTERFACE 类型&lt;/h3>
&lt;ol>
&lt;li>patch：用于两个 vSwitch 的互联，需要一对，在每一个上面的选项配置对端接口&lt;/li>
&lt;/ol>
&lt;h3 id="ovs-命令行工具">OVS 命令行工具&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>ovs-vsctl [OPTIONS] COMMAND [ARGS(arguments 参数)&amp;hellip;..] # Open vSwitch-vSwitch control，OVS 的虚拟交换机控制&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EXAMPLE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl show # 显示 ovsdb 中的内容概况，即显示 ovs 创建的相关网络信息&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>创建命令&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ovs-vsctl add-br BRIDGE # 创建一个名为 BRIDGE 的桥设备(即创建一个 vSwitch)，创建 BRIDGE 完成后会自动创建一个名字一样的 PORT 和 INTERFACE&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl add-port BRIDGE PORT # 给 BRIDGE 这个桥设备添加一个 PORT(PORT 也可以是物理机的网卡，相当于把物理机的网卡连了根虚拟的网线到这个 BRIDGE 的虚拟交换机上)，创建完 PORT 后会自动创建一个名字一样的 INTERFACE&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>删除命令&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ovs-vsctl del-br BRIDGE # 删除一个名为 BRIDGE 的桥设备&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl del-port [BRIDGE] PORT # 从指定 BRIDGE 删除指定的 PORT，由于一个 PORT 只能绑定在一个 BRIDGE 上，所有 BRIDGE 可省略&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>查询命令&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ovs-vsctl list-br # 显示所有创建了的桥设备的名字(仅显示名字)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl list-ports BRIDGE # 显示 BRIDGE 这个桥设备上添加的所有 PORT 接口&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl list&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>DB 命令，设置、更改等，由于有 ovs 信息全部写在自己的 DB 中，所以这个命令就是以数据库的模式按列显示信息&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ovs-vsctl list br|port|interface [NAME] # 显示 BRIDGE 或者 PORT 或者 INTERFACE 的详细信息[具体某个的]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl find # 查找&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl set port PORT tag=NUM # 设定 PORT 的 vlan tag 号为 NUM&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl set port PORT vlan_mode=trunk|access # 设定该 PORT 的 vlan 模式为 trunk 还是 access&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl set port PORT trunks=NUM,NUM&amp;hellip;. # 设定该 PORT 在 trunk 模式下允许通过的 vlan NUM 号有哪些&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl set interface INTERFACE type=TYPE options:OPT=VAL # 设定 INTERFACE 的类型为 TYPE，某个选项=该选项值.TYPE 默认是 internal&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ovs-vsctl set interface gre0 type=gre options:remote_ip=192.268.20.2 # 设定 gre0 接口的类型为 gre，gre 类型中有个选项是远端地址，IP 为 192.168.20.2(gre 为隧道技术，所以设定该接口的时候必须要指明对端 IP，否则无法进行 IP 上套 IP 的封装操作)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ovs-vsctl set interface vx0 type=vxlan options:remote_ip=192.268.20.2 # 设定 gre0 接口的类型为 vxlan,vxlan 类型中有个选项是远端地址，IP 为 192.168.20.2&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: TUN 与 TAP设备浅析(一) -- 原理浅析</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/TUN-%E4%B8%8E-TAP%E8%AE%BE%E5%A4%87%E6%B5%85%E6%9E%90%E4%B8%80--%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/TUN-%E4%B8%8E-TAP%E8%AE%BE%E5%A4%87%E6%B5%85%E6%9E%90%E4%B8%80--%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90/</guid><description>
&lt;h1 id="tuntap-设备浅析">TUN/TAP 设备浅析&lt;/h1>
&lt;h2 id="tun-设备">TUN 设备&lt;/h2>
&lt;p>TUN 设备是一种虚拟网络设备，通过此设备，程序可以方便地模拟网络行为。TUN 模拟的是一个三层设备,也就是说,通过它可以处理来自网络层的数据，更通俗一点的说，通过它，我们可以处理 IP 数据包。&lt;/p>
&lt;p>先来看看物理设备是如何工作的：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/xu2gy9/1616124241972-bd04293d-efc8-45da-8088-314f83bf47f9.png" alt="">&lt;/p>
&lt;p>上图中的 eth0 表示我们主机已有的真实的网卡接口 (interface)。&lt;/p>
&lt;p>网卡接口 eth0 所代表的真实网卡通过网线(wire)和外部网络相连，该物理网卡收到的数据包会经由接口 eth0 传递给内核的网络协议栈(Network Stack)。然后协议栈对这些数据包进行进一步的处理。&lt;/p>
&lt;p>对于一些错误的数据包,协议栈可以选择丢弃；对于不属于本机的数据包，协议栈可以选择转发；而对于确实是传递给本机的数据包,而且该数据包确实被上层的应用所需要，协议栈会通过 Socket API 告知上层正在等待的应用程序。&lt;/p>
&lt;p>下面看看 TUN 的工作方式：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/xu2gy9/1616124241917-45f4416e-f176-4d02-8ad5-0a31cc478397.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/xu2gy9/1616124241956-edea8b9d-7c2c-463d-9206-f508ac7e8241.png" alt="">&lt;/p>
&lt;p>我们知道，普通的网卡是通过网线来收发数据包的话，而 TUN 设备比较特殊，它通过一个文件收发数据包。&lt;/p>
&lt;p>如上图所示，tunX 和上面的 eth0 在逻辑上面是等价的， tunX 也代表了一个网络接口,虽然这个接口是系统通过软件所模拟出来的.&lt;/p>
&lt;p>网卡接口 tunX 所代表的虚拟网卡通过文件 /dev/tunX 与我们的应用程序(App) 相连，应用程序每次使用 write 之类的系统调用将数据写入该文件，这些数据会以网络层数据包的形式，通过该虚拟网卡，经由网络接口 tunX 传递给网络协议栈，同时该应用程序也可以通过 read 之类的系统调用，经由文件 /dev/tunX 读取到协议栈向 tunX 传递的所有数据包。&lt;/p>
&lt;p>此外，协议栈可以像操纵普通网卡一样来操纵 tunX 所代表的虚拟网卡。比如说，给 tunX 设定 IP 地址，设置路由，总之，在协议栈看来，tunX 所代表的网卡和其他普通的网卡区别不大，当然，硬要说区别，那还是有的,那就是 tunX 设备不存在 MAC 地址，这个很好理解，tunX 只模拟到了网络层，要 MAC 地址没有任何意义。当然，如果是 tapX 的话，在协议栈的眼中，tapX 和真是网卡没有任何区别。&lt;/p>
&lt;p>如果我们使用 TUN 设备搭建一个基于 UDP 的 VPN ，那么整个处理过程可能是这幅样子：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/xu2gy9/1616124241974-9a5fa9c2-7881-4919-b735-9ad361d649df.png" alt="">&lt;/p>
&lt;p>首先，我们的应用程序通过 eth0 和远程的 UDP 程序相连,对方传递过来的 UDP 数据包经由左边的协议栈传递给了应用程序，UDP 数据包的内容其实是一个网络层的数据包，比如说 IP 数据报，应用程序接收到该数据包的数据（剥除了各种头部之后的 UDP 数据）之后，然后进行一定的处理，处理完成后将处理后的数据写入文件 /dev/tunX，这样，数据会第二次到达协议栈。需要注意的是，上图中绘制的两个协议栈其实是同一个协议栈，之所以这么画是为了叙述的方便。&lt;/p>
&lt;h2 id="tap-设备">TAP 设备&lt;/h2>
&lt;p>TAP 设备与 TUN 设备工作方式完全相同，区别在于：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>TUN 设备是一个三层设备，它只模拟到了 IP 层，即网络层 我们可以通过 /dev/tunX 文件收发 IP 层数据包，它无法与物理网卡做 bridge，但是可以通过三层交换（如 ip_forward）与物理网卡连通。可以使用 ifconfig 之类的命令给该设备设定 IP 地址。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>TAP 设备是一个二层设备，它比 TUN 更加深入，通过 /dev/tapX 文件可以收发 MAC 层数据包，即数据链路层，拥有 MAC 层功能，可以与物理网卡做 bridge，支持 MAC 层广播。同样的，我们也可以通过 ifconfig 之类的命令给该设备设定 IP 地址，你如果愿意，我们可以给它设定 MAC 地址。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>最后，关于文章中出现的二层，三层，我这里说明一下，第一层是物理层，第二层是数据链路层，第三层是网络层，第四层是传输层。&lt;/p>
&lt;p>参考文章:&lt;/p>
&lt;p>[1]. &lt;a href="https://blog.kghost.info/2013/03/27/linux-network-tun/">https://blog.kghost.info/2013/03/27/linux-network-tun/&lt;/a>&lt;/p></description></item><item><title>Docs: 云计算底层技术-虚拟网络设备(Bridge,VLAN) opengers</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%BA%95%E5%B1%82%E6%8A%80%E6%9C%AF-%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87BridgeVLAN-opengers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%BA%95%E5%B1%82%E6%8A%80%E6%9C%AF-%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87BridgeVLAN-opengers/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;h1 id="云计算底层技术-虚拟网络设备bridgevlan">云计算底层技术-虚拟网络设备(Bridge,VLAN)&lt;/h1>
&lt;p>Posted on September 24, 2017 by opengers in openstack&lt;/p>
&lt;p>原文链接：&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-bridge-and-vlan/">openstack 底层技术-各种虚拟网络设备一(Bridge,VLAN)&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>IBM 网站上有一篇高质量文章。本文会参考文章部分内容，本系列介绍 OpenStack 使用的这些网络设备包括 Bridge，VLAN，tun/tap, veth，vxlan/gre。本篇先介绍 Bridge 和 VLAN 相关，其它在下一篇中介绍&lt;/p>
&lt;/blockquote>
&lt;p>OpenStack 一般分为计算，存储，网络三部分。考虑构建一个灵活的可扩展的云网络环境，而物理网络架构一般是固定和难于扩展的，因此虚拟网络将更有优势。Linux 平台上实现了各种不同功能的虚拟网络设备，包括&lt;code>Bridge,Vlan,tun/tap,veth pair,vxlan/gre，...&lt;/code>，这些虚拟设备就像一个个积木块一样，被 OpenStack 组合用于构建虚拟网络。 还有火热的 Docker，docker 容器的隔离技术实现脱胎于 Linux 平台上的&lt;code>namspace&lt;/code>,以及更早的&lt;code>chroot&lt;/code>。&lt;/p>
&lt;p>文中会牵涉虚拟机，所以文中出现的”主机”一词明确表示一台物理机，”接口”指挂载到网桥上的网络设备，环境如下：&lt;/p>
&lt;pre>&lt;code>CentOS Linux release 7.3.1611 (Core)
Linux controller 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
OpenStack社区版 Newton
&lt;/code>&lt;/pre>
&lt;p>Linux Bridge&lt;/p>
&lt;p>内核模块&lt;code>bridge&lt;/code>&lt;/p>
&lt;pre>&lt;code>[root@controller ~]# modinfo bridge
filename: /lib/modules/3.10.0-514.16.1.el7.x86_64/kernel/net/bridge/bridge.ko
&lt;/code>&lt;/pre>
&lt;p>Bridge 是 Linux 上工作在内核协议栈二层的虚拟交换机，虽然是软件实现的，但它与普通的二层物理交换机功能一样。可以添加若干个网络设备(em1,eth0,tap,..)到 Bridge 上(&lt;code>brctl addif&lt;/code>)作为其接口，添加到 Bridge 上的设备被设置为只接受二层数据帧并且转发所有收到的数据包到 Bridge 中(bridge 内核模块)，在 Bridge 中会进行一个类似物理交换机的查 MAC 端口映射表，转发，更新 MAC 端口映射表这样的处理逻辑，从而数据包可以被转发到另一个接口/丢弃/广播/发往上层协议栈，由此 Bridge 实现了数据转发的功能。如果使用&lt;code>tcpdump&lt;/code>在 Bridge 接口上抓包，是可以抓到桥上所有接口进出的包跟物理交换机不同的是，运行 Bridge 的是一个 Linux 主机，Linux 主机本身也需要 IP 地址与其它设备通信。但被添加到 Bridge 上的网卡是不能配置 IP 地址的，他们工作在数据链路层，对路由系统不可见。不过 Bridge 本身可以设置 IP 地址，可以认为当使用&lt;code>brctl addbr br0&lt;/code>新建一个&lt;code>br0&lt;/code>网桥时，系统自动创建了一个同名的隐藏&lt;code>br0&lt;/code>网络设备。&lt;code>br0&lt;/code>一旦设置 IP 地址，就意味着&lt;code>br0&lt;/code>可以作为路由接口设备，参与 IP 层的路由选择(可以使用&lt;code>route -n&lt;/code>查看最后一列&lt;code>Iface&lt;/code>)。因此只有当&lt;code>br0&lt;/code>设置 IP 地址时，Bridge 才有可能将数据包发往上层协议栈。&lt;/p>
&lt;p>根据下图来具体分析下 Bridge 工作过程&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rg120l/1616124222069-97b6dbd5-5adf-4cd6-811e-db15669b7bba.png" alt="">&lt;/p>
&lt;p>上图主机有 em1 和 em2 两块网卡，有网桥&lt;code>br0&lt;/code>。用户空间进程有 app1，app2 等普通网络应用，还有 OpenVPN 进程 P1，以及一台或多台 kvm 虚拟机 P2(kvm 虚拟机实现为主机上的一个&lt;code>qemu-kvm&lt;/code>进程，下文用&lt;code>qemu-kvm&lt;/code>进程表示虚拟机)。此主机上使用到了多种虚拟网络设备，在具体介绍某个虚拟网络设备时，我们可以忽略其它网络设备工作细节，只专注于当前网络设备。下面来具体分析网桥&lt;code>br0&lt;/code>&lt;/p>
&lt;p>&lt;strong>Bridge 处理数据包流程&lt;/strong>&lt;/p>
&lt;p>图中可以看到&lt;code>br0&lt;/code>有 N 个&lt;code>tap&lt;/code>类型接口(tap0,..,tapN)，tap 设备名称可能不同，例如&lt;code>tap45400fa0-9c&lt;/code>或&lt;code>vnet*&lt;/code>，但都是 tap 设备。一个”隐藏”的&lt;code>br0&lt;/code>接口(可设置 IP)，以及物理网卡 em2 的一个 VLAN 子设备&lt;code>em2.100&lt;/code>(这里简单看作有一个网卡桥接到 br0 上即可，VLAN 下面会讲)，他们都工作在链路层(Link Layer)。来看数据从外部网络(A)发往虚拟机(P2)&lt;code>qemu-kvm&lt;/code>这一过程，首先数据包从 em2(B)物理网卡进入，之后 em2 将数据包转发给其 vlan 子设备 em2.100，经过&lt;code>Bridge check&lt;/code>(L)发现子设备&lt;code>em2.100&lt;/code>属于网桥接口设备，因此数据包不会发往协议栈上层(T),而是进入 bridge 代码处理逻辑，从而数据包从&lt;code>em2.100&lt;/code>接口(C)进入&lt;code>br0&lt;/code>，经过&lt;code>Bridging decision&lt;/code>(D)发现数据包应当从&lt;code>tap0&lt;/code>(E)接口发出，此时数据包离开主机网络协议栈(G)，发往被用户空间进程&lt;code>qemu-kvm&lt;/code>打开的字符设备&lt;code>/dev/net/tun&lt;/code>(N)，&lt;code>qemu-kvm&lt;/code>进程执行系统调用&lt;code>read(fd,...)&lt;/code>从字符设备读取数据。 这个过程中，外部网络 A 发出的数据包是不会也没必要进入主机上层协议栈的，因为 A 是与主机上的 P2 虚拟机通信，&lt;strong>主机只是起到一个网桥转发的作用&lt;/strong>作为网桥的对比，如果是从网卡 em1(M)进入主机的数据包，经过&lt;code>Bridge check&lt;/code>(L)后，发现 em1 非网桥接口，则数据包会直接发往(T)协议栈 IP 层,从而在&lt;code>Routing decision&lt;/code>环节决定数据包的去向(A –&amp;gt; M –&amp;gt; T –&amp;gt; K)&lt;/p>
&lt;p>&lt;strong>Bridging decision&lt;/strong>&lt;/p>
&lt;p>上图中网桥&lt;code>br0&lt;/code>收到数据包后，根据数据包目的 MAC 的不同，&lt;code>Bridging decision&lt;/code>环节(D)对数据包的处理有以下几种：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>包目的 MAC 为 Bridge 本身 MAC 地址(当&lt;code>br0&lt;/code>设置有 IP 地址)，从 MAC 地址这一层来看，收到发往主机自身的数据包，交给上层协议栈(D –&amp;gt; J)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>广播包，转发到 Bridge 上的所有接口(br0,tap0,tap1,tap…)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>单播&amp;amp;&amp;amp;存在于 MAC 端口映射表，查表直接转发到对应接口(比如 D –&amp;gt; E)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>单播&amp;amp;&amp;amp;不存在于 MAC 端口映射表，泛洪到 Bridge 连接的所有接口(br0,tap0,tap1,tap…)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据包目的地址接口不是网桥接口，桥不处理，交给上层协议栈(D –&amp;gt; J)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="bridge-与-netfilter">Bridge 与 netfilter&lt;/h1>
&lt;p>Linux 防火墙是通过&lt;code>netfiler&lt;/code>这个内核框架实现，&lt;code>netfiler&lt;/code>用于管理网络数据包。不仅具有网络地址转换(NAT)的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。利用运作于用户空间的应用软件，如 iptables/firewalld/ebtables 等来控制&lt;code>netfilter&lt;/code>。Netfilter 在内核协议栈中指定了五个处理数据包的钩子(hook)，分别是 PRE_ROUTING、INPUT、OUTPUT、FORWARD 与 POST_ROUTING，通过 iptables/firewalld/ebtables 等用户层工具向这些 hook 点注入一些数据包处理函数，这样当数据包经过相应的 hook 时，处理函数就被调用，从而实现包过滤功能。这些用户层工具中，iptables 工作在 IP 层，只能过滤 IP 数据包；ebtables 工作在数据链路层，只能过滤以太网帧(比如更改源或目的 MAC 地址)当主机上没有 Bridge 存在时，从网卡进入主机的数据包会依次穿过主机内核协议栈，最后到达应用层交给某个应用程序处理。这样我们可以很方便的使用 iptables 设置本主机的防火墙规则。进入数据包流向对应下图路径&lt;code>A --&amp;gt; L --&amp;gt; T --&amp;gt; ...&lt;/code>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/rg120l/1616124222054-993994b9-971c-4bdd-96ef-ad7ee716c329.png" alt="">&lt;/p>
&lt;p>Bridge 的出现使 Linux 上设置防火墙变得复杂，因为此时从物理网卡进入主机的数据包目的地可能是其上运行的一台虚拟机。上图是上面介绍的&lt;code>数据从外部网络(A)发往虚拟机(P2)&lt;/code>这一过程中数据包所经过的防火墙链(文中的两张图可以对比来看)。物理网卡 em2 子设备 em2.100 从外部网络 A 收到二层数据包，经过&lt;code>bridge check&lt;/code>后进入 br0 并穿越一系列防火墙链&lt;code>L --&amp;gt; D --&amp;gt; E&lt;/code>，最终从 Bridge 上的另一个接口&lt;code>tap0&lt;/code>发出。上图红色导向线可以很清楚看到整个过程中数据包是没有进入主机内核协议栈的，因此位于主机 IP 层(Network Layer)的 iptables 根本无法过滤&lt;code>L --&amp;gt; D --&amp;gt; E&lt;/code>这一路径的数据包。那有没有办法&lt;strong>使 iptables 能够过滤 Bridge 中的数据包呢?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ebtables 只可以简单过滤二层以太网帧，无法过滤 ipv4 数据包。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当然也可以在虚拟机内使用 iptables，但是一般不会这么玩，特别是在云平台环境。 一个原因是如果一台主机上运行有 10 台虚拟机，那就需要分别登录这 10 台虚拟机设置其 iptables 规则，工作量会多很多。而且这么做意味着云平台必须要能够登录用户的虚拟机来设置 iptables 规则。 OpenStack 中安全组也是 iptables 实现，我们在虚拟机内部并没有发现有 iptables 规则存在。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>解决办法就是下文要讲的&lt;code>bridge_netfilter&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>来自：&lt;a href="http://ebtables.netfilter.org/misc/brnf-faq.html">Bridge-nf Frequently Asked Questions&lt;/a>&lt;/p>
&lt;p>为了解决上面提到的问题，Linux 内核引入了&lt;code>bridge_netfilter&lt;/code>，简称&lt;code>bridge_nf&lt;/code>。&lt;code>bridge_netfilter&lt;/code>在链路层 Bridge 代码中插入了几个能够被 iptables 调用的钩子函数，Bridge 中数据包在经过这些钩子函数时，iptables 规则被执行(上图中最下层 Link Layer 中的绿色方框即是 iptables 插入到链路层的 chain,蓝色方框为 ebtables chain)。这就使得{ip,ip6,arp}tables 能够”看见”Bridge 中的 IPv4,ARP 等数据包。这样不管此数据包是发给主机本身，还是通过 Bridge 转发给虚拟机，iptables 都能完成过滤。&lt;/p>
&lt;p>&lt;strong>如何使用 bridge_nf&lt;/strong>&lt;/p>
&lt;p>从 Linux 2.6.1 内核开始，可以通过设置内核参数开启&lt;code>bridge_netfilter&lt;/code>机制。看名字就很容易知道具体作用&lt;/p>
&lt;pre>&lt;code>[root@controller ~]# sysctl -a |grep 'bridge-nf-'
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
...
&lt;/code>&lt;/pre>
&lt;p>然后在 iptables 中使用&lt;code>-m physdev&lt;/code>引入相应模块，以文中第一张图上的虚拟机 P2 为例，它的虚拟网卡&lt;code>tap0&lt;/code>桥接在&lt;code>br0&lt;/code>上。我们在主机上设置如下 iptables 规则：丢弃从网桥&lt;code>br0&lt;/code>的&lt;code>tap0&lt;/code>接口进入的数据包。&lt;/p>
&lt;pre>&lt;code>#查看网桥
# brctl show
bridge name bridge id STP enabled interfaces
br0 8000.f8bc1212c3a0 no em1
tap
#操作对象是tap0
#ptables -t raw -A PREROUTING -m physdev --physdev-in tap0 -j DROP
&lt;/code>&lt;/pre>
&lt;p>注意 iptables &lt;code>-m physdev&lt;/code>操作对象是 Bridge 上的某个接口，因此规则的有效范围是针对从此接口进出 Bridge 的数据包还需要注意一点，这条命令是在主机上执行的，从主机角度看，主机收到从&lt;code>tap0&lt;/code>接口进入的数据包，因此使用&lt;code>--physdev-in&lt;/code>。但是从虚拟机 P2 角度来看，它发出了数据包，发给主机上的网桥 br0。因此上面这条 iptables 命令实际的作用是丢弃虚拟机 P2 发出的数据包，也就是禁止虚拟机 P2 访问外网。方向要分清，后面讲到/tun/tap 设备时会细说&lt;/p>
&lt;p>上面介绍了使用 iptables 过滤 Bridge 中数据包的方法，实际中如果直接使用 iptables 命令显然太繁琐，而且如果主机上有多台虚拟机的话，网桥接口就会变多导致容易出错，需要依靠工具去做这些。&lt;/p>
&lt;p>&lt;strong>使用 libvirt 提供的 virsh 工具&lt;/strong>&lt;/p>
&lt;p>libvirt 的&lt;code>virsh nwfilter-*&lt;/code>系列命令提供了设置虚拟机防火墙的功能，它其实是封装了 iptables 过滤 Bridge 中数据包的命令(&lt;code>-m physdev&lt;/code>)。它使用多个 xml 文件，每个 xml 文件中都可以定义一系列防火墙规则，然后把某个 xml 文件应用到某虚拟机的某张网卡(Bridge 中的接口)，这样就完成了对此虚拟机的这张网卡的防火墙设置。当然可以把一个定义好防火墙规则的 xml 文件应用到多台虚拟机。&lt;/p>
&lt;pre>&lt;code>#查看用于设置
# virsh --help |grep nwfilter
nwfilter-define define or update a network filter from an XML file
nwfilter-dumpxml network filter information in XML
nwfilter-edit edit XML configuration for a network filter
nwfilter-list list network filters
nwfilter-undefine undefine a network filter
# 定义有防火墙规则的xml文件
# virsh nwfilter-dumpxml centos6.3_filter
&amp;lt;filter name='centos6.3_filter' chain='root'&amp;gt;
&amp;lt;uuid&amp;gt;b1fdd87c-a44c-48fb-9a9d-e30f1466b720&amp;lt;/uuid&amp;gt;
&amp;lt;rule action='accept' direction='in' priority='400'&amp;gt;
&amp;lt;tcp dstportstart='8000' dstportend='8002'/&amp;gt;
&amp;lt;/rule&amp;gt;
&amp;lt;rule action='accept' direction='in' priority='400'&amp;gt;
&amp;lt;tcp srcipaddr='172.16.1.0' srcipmask='24'/&amp;gt;
&amp;lt;/rule&amp;gt;
&amp;lt;/filter&amp;gt;
#查看定义的防火墙xml文件
# virsh nwfilter-list
UUID Name
------------------------------------------------------------------
69754f43-0325-453f-bd53-4a6e3ab5d456 centos6.3_filter
# 在虚拟机xml文件中应用centos6.3_filter
&amp;lt;interface type='bridge'&amp;gt;
&amp;lt;mac address='f8:c9:79:ce:60:01'/&amp;gt;
&amp;lt;source bridge='br0'/&amp;gt;
&amp;lt;model type='virtio'/&amp;gt;
&amp;lt;filterref filter='centos6.3_filter'/&amp;gt;
&amp;lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&amp;gt;
&amp;lt;/interface&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>openstack 中的安全组&lt;/strong>&lt;/p>
&lt;p>像 openstack 等很多云平台在 web 控制台中都会提供有设置虚拟机防火墙功能(安全组)，可以很方便的添加应用防火墙规则到云主机&lt;/p>
&lt;p>在 OpenStack 部署中，若使用 Bridge 实现虚拟网络，其安全组功能就是依靠&lt;code>bridge_nf&lt;/code>实现，计算节点上 iptables 才能”看见”并过滤发往其上 instance 的数据包，如下是 OpenStack 计算节点上部分 iptables 规则，当启用安全组时，OpenStack 会自动设置&lt;code>net.bridge.bridge-nf-call-iptables = 1&lt;/code>等内核参数，不用再明确设置。 &lt;code>tap10f15e45-aa&lt;/code>为该计算节点上某 instance 网卡(tap 设备)&lt;/p>
&lt;pre>&lt;code>...
#针对虚拟网卡tap10f15e45-aa的部分规则
-A neutron-filter-top -j neutron-linuxbri-local
-A neutron-linuxbri-FORWARD -m physdev --physdev-out tap10f15e45-aa --physdev-is-bridged -m comment --comment &amp;quot;Direct traffic from the VM interface to the security group chain.&amp;quot; -j neutron-linuxbri-sg-chain
-A neutron-linuxbri-FORWARD -m physdev --physdev-in tap10f15e45-aa --physdev-is-bridged -m comment --comment &amp;quot;Direct traffic from the VM interface to the security group chain.&amp;quot; -j neutron-linuxbri-sg-chain
...
&lt;/code>&lt;/pre>
&lt;p>Bridge+netfilter 内容很多，下次有时间会专门用一篇文章介绍 OpenStack 中的安全组实现，关键字 &lt;code>iptables+bridge+netfilter+conntrack&lt;/code>&lt;/p>
&lt;p>Linux 上还有一款虚拟交换机 OVS，主要区别是 OVS 支持 vlan tag 以及流表(例如 openflow)等一些高级特性，Bridge 只是单纯二层交换机也不支持 vlan tag，OVS 具体介绍参考这里&lt;a href="https://opengers.github.io/openstack/openstack-base-use-openvswitch/">openstack 底层技术-使用 openvswitch&lt;/a>&lt;/p>
&lt;h1 id="vlan">VLAN&lt;/h1>
&lt;p>上面简单介绍过 Bridge 和 OVS 区别，要在 Linux 上实现一个带 VLAN 功能的虚拟交换机，OVS 可以通过给不同 port 打不同 tag 实现 vlan 功能，而 Bridge 需要结合 VLAN 设备才能实现。&lt;/p>
&lt;p>这部分先介绍 VLAN 设备原理及配置，然后介绍 VLAN 在 openstack 中的应用&lt;/p>
&lt;h3 id="vlan-设备原理及配置">VLAN 设备原理及配置&lt;/h3>
&lt;p>VLAN 又称虚拟网络，其基本原理是在二层协议里插入额外的 VLAN 协议数据（称为 802.1.q VLAN Tag)，同时保持和传统二层设备的兼容性。Linux 里的 VLAN 设备是对 802.1.q 协议的一种内部软件实现，模拟现实世界中的 802.1.q 交换机。详细介绍参考文章开头给出的 IBM 文章中”VLAN device for 802.1.q”部分，这里不再重复&lt;/p>
&lt;p>下面使用 VLAN 结合 Bridge 实现文中第一张图上的多个 VLAN 子设备&lt;code>VLAN 100, VLAN X, VLAN Y, ...&lt;/code>以及多个网桥&lt;code>br0, brX, brY, ...&lt;/code>(X Y 都是小于 2048 的正整数)。前提是此主机上有一块网卡设备比如 em2，不管 em2 为物理网卡或虚拟网卡，em2 所连接的交换机端口必须设置为 trunk。添加 VLAN 子设备&lt;code>VLAN 100, VLAN X, VLAN Y, ...&lt;/code>&lt;/p>
&lt;pre>&lt;code>#cat /etc/sysconfig/network-scripts/ifcfg-em2
TYPE=Ethernet
BOOTPROTO=none
IPV4_FAILURE_FATAL=no
NAME=em2
UUID=4f2cfd28-ba78-4f25-afa1-xxxxxxxxxxxxx
DEVICE=em2
ONBOOT=yes
#添加vlan子设备em2.100
#cat /etc/sysconfig/network-scripts/ifcfg-em2.100
DEVICE=em2.100
BOOTPROTO=static
ONBOOT=yes
VLAN=yes
#可以继续添加多个带不同vlan tag的子设备,比如VLAN X, VLAN Y, VLAN ...
#vlan子设备em2.X
#cat /etc/sysconfig/network-scripts/ifcfg-em2.X
#DEVICE=em2.X
#BOOTPROTO=static
#ONBOOT=yes
#VLAN=yes
#vlan子设备em2.Y
#cat /etc/sysconfig/network-scripts/ifcfg-em2.Y
#DEVICE=em2.Y
#BOOTPROTO=static
#ONBOOT=yes
#VLAN=yes
#...
#重启网络
service network restart
&lt;/code>&lt;/pre>
&lt;p>查看子设备 em2.100，可以看到，其 driver 为 VALN&lt;/p>
&lt;pre>&lt;code>#ethtool -i eth1.101
driver: 802.1Q VLAN Support
version: 1.8
...
&lt;/code>&lt;/pre>
&lt;p>添加网桥&lt;code>br0, brX, brY, ...&lt;/code>&lt;/p>
&lt;pre>&lt;code>#添加br0网桥
brctl addbr br0
# br0添加em2.100子设备 凡是桥接到br0上的数据包都会带上tag 100
brctl addif br0 em2.100
#可以继续添加多个网桥brX, brY, ...
# brctl addbr brX
#凡是桥接到brX上的数据包都会带上tag X
# brctl addif brX em2.X
# brctl addbr brY
# brctl addif brY em2.Y
#...
# brctl show
bridge name bridge id STP enabled interfaces
br0 8000.525400315e23 no em2.100
tap0
&lt;/code>&lt;/pre>
&lt;p>VLAN 设备的作用是建立一个个带不同 vlan tag 的子设备，它并不能建立多个可以交换转发数据的接口，因此需要借助于 Bridge，把 VLAN 建立的子设备例如 em2.100 桥接到网桥例如 br0 上，这样凡是桥接到 br0 上的设备就自动加入了 vlan 100 子网。对比一台带有两个 vlan 100，X 的物理交换机，这里 br0 网桥上所连接的接口相当于物理交换机上那些划分到 vlan 100 的端口，而 brX 所连接的接口相当于物理交换机上那些划分到 vlan X 的端口。因此 Bridge 加 VLAN 能在功能层面完整模拟现实世界里的 802.1.q 交换机。&lt;/p>
&lt;p>参考文中第一张图，我们从网桥 br0 上 tap0 接口(E)角度，来看下具体的数据收发流程：&lt;/p>
&lt;ul>
&lt;li>数据从 tap0 接口进入，发往外部网络 A&lt;/li>
&lt;/ul>
&lt;p>&lt;code>tap0收到的数据被发送给br0(E) --&amp;gt; D --&amp;gt; br0把数据从em2.100接口发出(C) --&amp;gt; 母设备em2收到em2.100发来的数据(B) --&amp;gt; 母设备em2给数据打上100的vlan tag(因为来自em2.100) --&amp;gt; em2将带有100 tag的数据发出到外部网络(A) --&amp;gt; em2所连接的交换机收到数据(trunk口)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>数据从 em2 网卡进入，发往 tap0&lt;/li>
&lt;/ul>
&lt;p>&lt;code>em2从所连接的交换机收到tag 100的数据 --&amp;gt; em2发现此数据带有tag 100,移除数据包中tag --&amp;gt; 不带tag的数据发给em2.100子设备 --&amp;gt; br0收到从em2.100进入的数据包 --&amp;gt; D --&amp;gt; br0转发数据到tap0&lt;/code>&lt;/p>
&lt;p>上面忽略了对于数据包是否带 tag，以及数据包所带 tag 的子设备是否存在的检查。这些属于 vlan 基础知识&lt;/p>
&lt;h3 id="vlan-在-openstack-中的应用">VLAN 在 openstack 中的应用&lt;/h3>
&lt;p>openstack 中虚拟机网络使用 VLAN 模式的话，就会用到 VLAN 设备。openstack 中配置 vlan+bridge 模式如下&lt;/p>
&lt;pre>&lt;code>#neutron-server节点(网络节点)配置
#cat /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
#neutron-server启动时，加载flat，vlan两种网络类型驱动
type_drivers = flat,vlan
#vlan模式不需要tenant_network，留空
tenant_network_types =
#neutron-server启动时加载linuxbridge和openvswitch网桥驱动
mechanism_drivers = linuxbridge,openvswitch
[ml2_type_flat]
# 在命令行或控制台新建flat类型网络时需要指定的名称，此名称会配置映射到计算节点上某块网卡，下面会设置
flat_networks = proext
[ml2_type_vlan]
# 在命令行或控制台新建vlan类型网络时需要指定的名称，此名称会配置映射到计算节点上某块网卡，下面会设置
network_vlan_ranges = provlan
#重启neutron-server服务
# 使用Bridge+vlan网络模式的nova-compute节点(计算节点)配置
#cat /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[linux_bridge]
#provlan名称映射到此计算节点eth2网卡，因为使用vlan模式，eth2需要设置为trunk
#proext名称映射到此计算节点eth3网卡，我这个环境下eth3网卡为虚拟机连接外网接口
physical_interface_mappings = provlan:eth2,proext:eth3
#重启此计算节点nova-compute服务
#配置中只需要指定vlan要用的母设备eth2，后续控制台新建带tag的网络时，neutron会自动建立eth2.{TAG}子设备并加入到网桥
&lt;/code>&lt;/pre>
&lt;p>在控制台新建一个 vlan tag 为 1023 的 Provider network: subvlan-1023，使用此 subvlan-1023 网络新建几台虚拟机，看下计算节点上网桥配置&lt;/p>
&lt;pre>&lt;code>[root@compute03 neutron]# brctl show
bridge name bridge id STP enabled interfaces
brq82405415-7a 8000.52540048b1a9 no eth2.1023
tap10f15e45-aa
tapa659a214-b1
brqf5808b72-44 8000.5254001ac83d no eth3
tapd3388a60-ae
[root@compute03 neutron]# virsh domiflist instance-00000145
Interface Type Source Model MAC
-------------------------------------------------------
tapa659a214-b1 bridge brq82405415-7a virtio fa:16:3e:bc:c9:e0
&lt;/code>&lt;/pre>
&lt;p>虚拟机&lt;code>instance-00000145&lt;/code>的网卡&lt;code>tapa659a214-b1&lt;/code>桥接到&lt;code>brq82405415-7a&lt;/code>。跟上面介绍的类似，桥接到&lt;code>brq82405415-7a&lt;/code>上的接口设备就自动加入了 vlan 1023 子网，因此从&lt;code>instance-00000145&lt;/code>发出的数据包会带有 tag 1023(eth2.1023 的母设备 eth2 负责添加或移除 tag)&lt;/p>
&lt;p>假如在控制台或命令行再新建一个 tag 为 1024 的子网，则网桥配置如下&lt;/p>
&lt;pre>&lt;code>[root@compute03 neutron]# brctl show
bridge name bridge id STP enabled interfaces
brq82405415-7a 8000.52540048b1a9 no eth2.1023
tap10f15e45-aa
tapa659a214-b1
brq7d59440b-cc 8000.525400aabbcc no eth2.1024
tap20ffafb2-1b
brqf5808b72-44 8000.5254001ac83d no eth3
tapd3388a60-ae
[root@compute03 neutron]# virsh domiflist instance-00000147
Interface Type Source Model MAC
-------------------------------------------------------
tap20ffafb2-1b bridge brq7d59440b-cc virtio fa:16:3e:bd:12:40
&lt;/code>&lt;/pre>
&lt;p>虚拟机&lt;code>instance-00000147&lt;/code>的网卡&lt;code>tap20ffafb2-1b&lt;/code>桥接到&lt;code>brq7d59440b-cc&lt;/code>,&lt;code>instance-00000147&lt;/code>属于 vlan 1024 子网,这就实现了属于不同 vlan 子网的&lt;code>instance-00000145&lt;/code>与&lt;code>instance-00000147&lt;/code>的隔离性。他们虽然在同一台计算节点上，但彼此不互通，除非设置为两个 VLAN 可以互通感谢 Bridge 和 VLAN 设备，他们让 openstack 配置 vlan 网络成了可能，BUT!, Bridge+VLAN 不是唯一的选择，openstack 也支持 OVS，OVS 中是靠给不同 instance 接口打不同 tag 来实现 instance 的多 vlan 环境，OVS 模式除了配置部分跟 Bridge+VLAN 不同之外，使用上并没有什么区别，这里的设置&lt;code>mechanism_drivers = linuxbridge,openvswitch&lt;/code>加载相应驱动，屏蔽掉了底层操作的差别与 Bridge 中&lt;code>provlan，proext&lt;/code>映射到计算节点网卡的配置不同，OVS 配置文件中映射关系为 vlan 类型网络&lt;code>provlan&lt;/code>映射到网桥&lt;code>br-vlan&lt;/code>，flat 类型网络&lt;code>proext&lt;/code>映射到网桥&lt;code>br-ext&lt;/code>。至于&lt;code>br-vlan&lt;/code>桥接 eth2 网卡，&lt;code>br-ext&lt;/code>桥接 eth3 网卡则需要预先手动配置好，来看一个使用 OVS 的计算节点网桥&lt;/p>
&lt;pre>&lt;code>[root@compute01 neutron]# ovs-vsctl show
dd7ccaae-6a24-4d28-8577-9e5e6b5dfbd3
Manager &amp;quot;ptcp:6640:127.0.0.1&amp;quot;
is_connected: true
Bridge br-ext
Controller &amp;quot;tcp:127.0.0.1:6633&amp;quot;
is_connected: true
fail_mode: secure
Port phy-br-ext
Interface phy-br-ext
type: patch
options: {peer=int-br-ext}
Port br-ext
Interface br-ext
type: internal
Port &amp;quot;eth3&amp;quot;
Interface &amp;quot;eth3&amp;quot;
Bridge br-vlan
Controller &amp;quot;tcp:127.0.0.1:6633&amp;quot;
is_connected: true
fail_mode: secure
Port br-vlan
Interface br-vlan
type: internal
Port phy-br-vlan
Interface phy-br-vlan
type: patch
options: {peer=int-br-vlan}
Port &amp;quot;eth2&amp;quot;
Interface &amp;quot;eth2&amp;quot;
Bridge br-int
Controller &amp;quot;tcp:127.0.0.1:6633&amp;quot;
is_connected: true
fail_mode: secure
Port int-br-ext
Interface int-br-ext
type: patch
options: {peer=phy-br-ext}
Port br-int
Interface br-int
type: internal
Port int-br-vlan
Interface int-br-vlan
type: patch
options: {peer=phy-br-vlan}
Port &amp;quot;qvo7d59440b-cc&amp;quot;
tag: 1
Interface &amp;quot;qvo7d59440b-cc&amp;quot;
ovs_version: &amp;quot;2.5.0&amp;quot;
[root@compute01 neutron]# brctl show
bridge name bridge id STP enabled interfaces
qbr7d59440b-cc 8000.26d03016fcf6 no qvb7d59440b-cc
tap7d59440b-cc
#查看虚拟机网卡
[root@compute01 neutron]# virsh domiflist instance-00000149
Interface Type Source Model MAC
-------------------------------------------------------
tap7d59440b-cc bridge qbr7d59440b-cc virtio fa:16:3e:12:ba:e6
&lt;/code>&lt;/pre>
&lt;p>&lt;code>instance-00000149&lt;/code>出数据流向为&lt;code>tap7d59440b-cc --&amp;gt; qbr7d59440b-cc --&amp;gt; qvo7d59440b-cc(tag 1) --&amp;gt; br-int --&amp;gt; br-vlan --&amp;gt; eth2&lt;/code>。qvb7d59440b-cc 与 qvo7d59440b-cc 为一对 veth 设备这其中牵涉 OVS 流表和 OVS 内外部 tag 转换问题，又足够写一篇文章来介绍了，本文暂不继续介绍。还有一点，在使用 OVS 做网桥的同时又开启安全组功能时，会多出一个 Bridge 网桥用于设置安全组，如上面的&lt;code>qbr7d59440b-cc&lt;/code>, 因为目前 iptables 不支持 OVS，只能在虚拟机与 OVS 网桥之间加进一个 Bridge 网桥用于设置 iptables 规则&lt;/p>
&lt;p>其它网络设备会在另一篇文章介绍，本文完&lt;/p></description></item><item><title>Docs: 云计算底层技术-虚拟网络设备(tun tap,veth) opengers</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%BA%95%E5%B1%82%E6%8A%80%E6%9C%AF-%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87tun-tapveth-opengers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/1.1.%E8%99%9A%E6%8B%9F%E5%8C%96/Network-Virtual%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%BA%95%E5%B1%82%E6%8A%80%E6%9C%AF-%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87tun-tapveth-opengers/</guid><description>
&lt;h1 id="虚拟网络设备tuntapveth介绍">虚拟网络设备(tun/tap,veth)介绍&lt;/h1>
&lt;p>Posted on September 30, 2017 by opengers in openstack
&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-bridge-and-vlan/">openstack 底层技术-各种虚拟网络设备一(Bridge,VLAN)&lt;/a>
&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/">openstack 底层技术-各种虚拟网络设备二(tun/tap,veth)&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>第一篇文章介绍了 Bridge 和 VLAN，本文继续介绍 tun/tap，veth 等虚拟设备，除了 tun，其它设备都能在 openstack 中找到应用，这些各种各样的虚拟网络设备使网络虚拟化成为了可能&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/#tuntap">tun/tap&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/#tap%E8%AE%BE%E5%A4%87%E4%BD%9C%E4%B8%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E5%8D%A1">tap 设备作为虚拟机网卡&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/#openvpn%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9A%84tun%E8%AE%BE%E5%A4%87">openvpn 中使用的 tun 设备&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/#veth%E8%AE%BE%E5%A4%87">veth 设备&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/#veth%E8%AE%BE%E5%A4%87%E5%9C%A8openstack%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8">veth 设备在 openstack 中的应用&lt;/a>
tun/tap&lt;/li>
&lt;/ul>
&lt;p>我们知道 KVM 虚拟化中单个虚拟机是主机上的一个普通&lt;code>qemu-kvm&lt;/code>进程，虚拟机当然也需要网卡，最常见的虚拟网卡就是使用主机上的 tap 设备。那从主机的角度看，这个&lt;code>qemu-kvm&lt;/code>进程是如何使用 tap 设备呢，下面先介绍下&lt;code>tun/tap&lt;/code>设备概念，然后分别用一个实例来解释&lt;code>tun/tap&lt;/code>的具体用途&lt;code>tun/tap&lt;/code>是操作系统内核中的虚拟网络设备，他们为用户层程序提供数据的接收与传输。实现&lt;code>tun/tap&lt;/code>设备的内核模块为&lt;code>tun&lt;/code>，其模块介绍为&lt;code>Universal TUN/TAP device driver&lt;/code>，该模块提供了一个设备接口&lt;code>/dev/net/tun&lt;/code>供用户层程序读写，用户层程序通过读写&lt;code>/dev/net/tun&lt;/code>来向主机内核协议栈注入数据或接收来自主机内核协议栈的数据，&lt;strong>可以把 tun/tap 看成数据管道，它一端连接主机协议栈，另一端连接用户程序&lt;/strong>&lt;/p>
&lt;pre>&lt;code>[root@compute01 ~]# modinfo tun
filename: /lib/modules/3.10.0-514.16.1.el7.x86_64/kernel/drivers/net/tun.ko
alias: devname:net/tun
...
description: Universal TUN/TAP device driver
...
[root@compute01 ~]# ls /dev/net/tun
/dev/net/tun
&lt;/code>&lt;/pre>
&lt;p>为了使用&lt;code>tun/tap&lt;/code>设备，用户层程序需要通过系统调用打开&lt;code>/dev/net/tun&lt;/code>获得一个读写该设备的文件描述符(FD)，并且调用 ioctl()向内核注册一个 TUN 或 TAP 类型的虚拟网卡(实例化一个 tun/tap 设备)，其名称可能是&lt;code>tap7b7ee9a9-c1/vnetXX/tunXX/tap0&lt;/code>等。此后，用户程序可以通过该虚拟网卡与主机内核协议栈交互。当用户层程序关闭后，其注册的 TUN 或 TAP 虚拟网卡以及路由表相关条目(使用 tun 可能会产生路由表条目，比如 openvpn)都会被内核释放。可以把用户层程序看做是网络上另一台主机，他们通过 tap 虚拟网卡相连 TUN 和 TAP 设备区别在于他们工作的协议栈层次不同，TAP 等同于一个以太网设备，用户层程序向 tap 设备读写的是二层数据包如以太网数据帧，tap 设备最常用的就是作为虚拟机网卡。TUN 则模拟了网络层设备，操作第三层数据包比如 IP 数据包，&lt;code>openvpn&lt;/code>使用 TUN 设备在 C/S 间建立 VPN 隧道&lt;/p>
&lt;h1 id="tap-设备关联虚拟机网卡">tap 设备关联虚拟机网卡&lt;/h1>
&lt;p>tap 设备最常见的用途就是关联虚拟机网卡，这里以一个具体的虚拟机 P2 为例，来看看它与其所在主机，及网络上其它主机通信的数据流向&lt;/p>
&lt;p>本文依然使用下面这张图作为参考&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/casm7q/1616124199627-2d2cd67b-8e31-44f4-a932-6e1925d09376.png" alt="">&lt;/p>
&lt;p>虚拟机 P2 使用桥接模式，网桥为 br0，先启动虚拟机&lt;/p>
&lt;pre>&lt;code>#virsh domiflist P2
Interface Type Source Model MAC
-------------------------------------------------------
tap0 bridge br0 virtio fa:16:3e:b1:70:52
...
&lt;/code>&lt;/pre>
&lt;p>虚拟机启动后，主机上多了一张虚拟网卡&lt;code>tap0&lt;/code>，&lt;code>tap0&lt;/code>桥接在 br0 网桥上，查看此虚拟机进程&lt;/p>
&lt;pre>&lt;code>#ps -ef |grep P2
qemu 7748 1 0 Nov07 ? 00:22:09 /usr/libexec/qemu-kvm -name guest=P2 ... -netdev tap,fd=26,id=hostnet0,vhost=on,vhostfd=28 ...
&lt;/code>&lt;/pre>
&lt;p>进程 PID 为 7748，其网络部分参数中，&lt;code>-netdev tap,fd=26&lt;/code> 表示连接主机上的 tap 设备，&lt;code>fd=26&lt;/code>为读写&lt;code>/dev/net/tun&lt;/code>的文件描述符。使用&lt;code>lsof -p 7748&lt;/code>也可以验证，如下，PID 为 7748 的进程打开了文件/dev/net/tun，分配的文件描述符为 26，打开设备文件类型为 CHR&lt;/p>
&lt;pre>&lt;code># lsof -p 7748
COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME
...
qemu-kvm 7748 qemu 26u CHR 10,200 0t0 17439 /dev/net/tun
...
&lt;/code>&lt;/pre>
&lt;p>因此，在虚拟机 P2 启动时，其打开了设备文件&lt;code>/dev/net/tun&lt;/code>并获得了读写该文件的文件描述符(FD)26，同时向内核注册了一个 tap 类型虚拟网卡&lt;code>tap0&lt;/code>，&lt;code>tap0&lt;/code>与 FD 26 关联，虚拟机关闭时&lt;code>tap0&lt;/code>设备会被内核释放。此虚拟网卡&lt;code>tap0&lt;/code>一端连接用户空间程序&lt;code>qemu-kvm&lt;/code>，另一端连接主机链路层&lt;/p>
&lt;p>&lt;strong>从虚拟机 P2 发送数据到外部网络&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>虚拟机通过其网卡 eth0 向外发送数据，从主机角度看，就是用户层程序&lt;code>qemu-kvm&lt;/code>进程使用文件描述符(FD)26 向字符设备&lt;code>/dev/net/tun&lt;/code>写入数据 &lt;code>P2 --&amp;gt; write(fd,...) --&amp;gt; N&lt;/code>&lt;/li>
&lt;li>文件描述符 26 与虚拟网卡&lt;code>tap0&lt;/code>关联，也就是说主机从&lt;code>tap0&lt;/code>网卡收到数据 &lt;code>N --&amp;gt; E&lt;/code>&lt;/li>
&lt;li>&lt;code>tap0&lt;/code>为网桥&lt;code>br0&lt;/code>上一个接口，需要进行&lt;code>Bridging decision&lt;/code>以决定数据包如何转发 &lt;code>E --&amp;gt; D&lt;/code>&lt;/li>
&lt;li>P2 是与外部网络其它主机通信，因此&lt;code>br0&lt;/code>转发该数据到&lt;code>em2.100&lt;/code>，最后从物理网卡 em2 发出 &lt;code>D --&amp;gt; C --&amp;gt; B -- &amp;gt; A&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>这个过程中，虚拟机发出的数据通过&lt;code>tap0&lt;/code>虚拟网卡直接注入主机链路层网桥处理逻辑中，然后被转发到外部网络。可以看出数据包没有穿过主机协议栈上层，主机仅仅起了类似物理二层交换机的数据转发功能，这是当然的也是必须的，因为虚拟机通信对象不是此主机，明白这点之后，可以对 tap 设备有更深的认识如下图，Linux 上的各种网络应用程序基本上都是通过 Linux Socket 编程接口来和内核空间的网络协议栈通信的。那么问题来了，为何虚拟机进程&lt;code>qemu-kvm&lt;/code>不能使用 Linux Socket 与外部网络通信呢？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/casm7q/1616124199610-b6861cf1-bdbb-4002-8507-8de6b84cf44d.png" alt="">&lt;/p>
&lt;p>需要注意的是，虽然&lt;code>qemu-kvm&lt;/code>只是主机上的一个进程，但它实现的是一台虚拟机，虚拟机和主机是两台机器，他们都桥接在主机上的软件交换机上。明白了这点，我们就知道主机上普通的网络程序发出的网络通信是属于”主机自身发出的数据包”，而虚拟机发出的数据包当然不可能使用另一台机器(主机)的 Linux Socket 来通信了。&lt;/p>
&lt;p>主机上能看到的是虚拟机发到 tap 设备上的二层以太网帧，因此主机上工作在内核协议栈 IP 层的 iptables 是无法过滤虚拟机数据包的，当然这也有解决方法，本系列第一篇文章有详细说明&lt;/p>
&lt;h1 id="openvpn-中使用的-tun-设备">openvpn 中使用的 tun 设备&lt;/h1>
&lt;p>TUN 设备与上面介绍的 TAP 设备很类似，只是 TUN 设备连接的是主机内核协议栈 IP 层&lt;/p>
&lt;p>openvpn 是使用 TUN 设备的一个常见例子，搭建好 openvpn server 后启动 openvpn，主机中多了一个虚拟网卡&lt;code>tun0&lt;/code>&lt;/p>
&lt;pre>&lt;code>#ifconfig tun0
tun0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00
inet addr:10.5.0.1 P-t-P:10.5.0.2 Mask:255.255.255.255
...
&lt;/code>&lt;/pre>
&lt;p>查看该虚拟网卡驱动信息，可以看到&lt;code>tun0&lt;/code>网卡使用内核模块&lt;code>tun&lt;/code>，类型为&lt;code>tun&lt;/code>设备(区别于 tap 设备的&lt;code>bus-info: tap&lt;/code>)&lt;/p>
&lt;p>#ethtool -i tun0 driver: tun … bus-info: tun …&lt;/p>
&lt;pre>&lt;code>使用`lsof`查看openvpn进程打开的所有文件
``` shell
# ps -ef |grep openvpn
root 3056 1 4 Nov14 ? 03:18:28 /usr/sbin/openvpn --daemon --config /etc/openvpn/config/server.conf
#lsof -p 3056
COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME
...
openvpn 3056 root 6u CHR 10,200 0t0 8590 /dev/net/tun
openvpn 3056 root 7u IPv4 26744 0t0 UDP *:9112
...
&lt;/code>&lt;/pre>
&lt;p>因此，openvpn 进程在启动时，打开了字符设备&lt;code>/dev/net/tun&lt;/code>并获得了文件描述符 6，同时向内核注册了一个虚拟网卡&lt;code>tun0&lt;/code>。openvpn 进程还打开了一个 udp 的 socket，监听在&lt;code>9112&lt;/code>端口。 下面我们来看下用户通过 vpn 访问 web 服务的数据包流向&lt;/p>
&lt;p>&lt;strong>客户端使用 openvpn 访问 web 服务&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>客户端启动 openvpn client 进程连接 openvpn server&lt;/li>
&lt;li>server 下发路由条目到客户端机器路由表中，同时生成虚拟网卡&lt;code>tun1&lt;/code>(tun 设备，openvpn client 进程与 openvpn server 一样会注册 tun 虚拟网卡)&lt;/li>
&lt;li>客户端通过浏览器访问 web 服务&lt;/li>
&lt;li>浏览器生成的数据包在协议栈 IP 层进行路由选择，决定通过虚拟网卡&lt;code>tun1&lt;/code>发出&lt;/li>
&lt;li>虚拟网卡&lt;code>tun1&lt;/code>另一端连接用户层 openvpn client 进程&lt;/li>
&lt;li>openvpn client 进程收到原始请求数据包&lt;/li>
&lt;li>openvpn client 封装原始请求数据包，通过 udp 协议发送 vpn 封包到 openvpn server 上的 9112 端口 &lt;code>A -- &amp;gt; T --&amp;gt; K --&amp;gt; R --&amp;gt; P1&lt;/code>&lt;/li>
&lt;li>openvpn server 上的 openvpn 进程收到 vpn 封包，解包，使用文件描述符 6 写数据到&lt;code>/dev/net/tun&lt;/code> &lt;code>P1 --&amp;gt; write(fd,...) --&amp;gt; N&lt;/code>&lt;/li>
&lt;li>文件描述符 6 与虚拟网卡&lt;code>tun0&lt;/code>关联，主机从&lt;code>tun0&lt;/code>网卡收到数据包 &lt;code>N --&amp;gt; H ---&amp;gt; I&lt;/code>&lt;/li>
&lt;li>主机进行&lt;code>Routing decision&lt;/code>，根据数据包目的 IP(用户访问 web 网站 IP 地址)从相应网卡发出 &lt;code>I --&amp;gt; K --&amp;gt; T --&amp;gt; M --&amp;gt; A&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>如何创建 tun 设备&lt;/strong>&lt;/p>
&lt;pre>&lt;code>#添加一个tun设备，并配置ip
ip tuntap add mode tun2
ip link set tun2 up
ip addr add 172.16.1.2/24 dev tun2
#删除tun2
ip link del dev tun2
&lt;/code>&lt;/pre>
&lt;h1 id="veth-设备">veth 设备&lt;/h1>
&lt;p>veth 也是 Linux 实现的虚拟网络设备，veth 设备总是成对出现，其作用是反转数据流方向。例如如果 veth-a 和 veth-b 是一对 veth 设备，veth-a 收到的数据会从 veth-b 发出。相反，veth-b 收到的数据会从 veth-a 发出。一个常见用途是连接两个 netwok namespace。openstack Neutron 网络中，dhcp agent 和 l3 agent 都用到了 veth 设备对。拿 dhcp agent 来说，openstack 中每个网络都需要起一个 dhcp 服务器用于给此网络虚拟机分配 ip 地址，每个 openstack 网络都使用一个单独的 network namespace，每个 network namespace 和网络节点上 Bridge 通过 veth 设备对连接，这样多个 openstack 网络才不会引起冲突。理解 veth 设备对和 network namespace 是解决 openstack 中虚拟机网络故障问题的关键&lt;/p>
&lt;p>使用 ip 命令新建一对 veth 设备 veth-a veth-b&lt;/p>
&lt;pre>&lt;code>ip link add veth-a type veth peer name veth-b
&lt;/code>&lt;/pre>
&lt;p>如何确定一个网络设备是 veth 设备，如下&lt;code>driver: veth&lt;/code>&lt;/p>
&lt;pre>&lt;code>ethtool -i veth-a
driver: veth
...
&lt;/code>&lt;/pre>
&lt;p>如何查找&lt;code>veth-a&lt;/code>的对端设备呢&lt;/p>
&lt;pre>&lt;code>ip -d link show
7: veth-b@veth-a: &amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000
link/ether 82:e6:4f:09:b2:df brd ff:ff:ff:ff:ff:ff promiscuity 0
veth addrgenmode eui64
8: veth-a@veth-b: &amp;lt;BROADCAST,MULTICAST,M-DOWN&amp;gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000
link/ether d6:4b:5e:69:ff:2d brd ff:ff:ff:ff:ff:ff promiscuity 0
veth addrgenmode eui64
&lt;/code>&lt;/pre>
&lt;p>若 veth 设备对其中一个设备位于网络命令空间中，可以这样查找&lt;/p>
&lt;pre>&lt;code>#查看网络命名空间qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a中设备ns-45400fa0-9c的对端设备序号
ip netns exec qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a ethtool -S ns-45400fa0-9c
NIC statistics:
peer_ifindex: 6
#查看序号为6的网络设备
ip a |grep '^6:'
6: tap45400fa0-9c@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master brqba48a3fc-e9 state UP qlen 1000
&lt;/code>&lt;/pre>
&lt;p>因此网络命名空间中的&lt;code>ns-45400fa0-9c&lt;/code>与主机上的设备&lt;code>tap45400fa0-9c&lt;/code>是一对 veth 设备&lt;/p>
&lt;h1 id="veth-设备在-openstack-中的应用">veth 设备在 openstack 中的应用&lt;/h1>
&lt;p>如下，我们使用 veth 设备对连接 Bridge 和 network namespace，凡是桥接到 Bridge&lt;code>brqba48a3fc-e9&lt;/code>上的虚拟机，其发出的 dhcp 请求都会进入网络命名空间&lt;code>qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a&lt;/code>中的 dhcp 服务器。&lt;/p>
&lt;pre>&lt;code>#新建veth设备veth-a,veth-b
ip link add veth-a type veth peer name veth-b
#新建一个network namespace
ip netns add qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a
#将 veth-b 添加到 network namespace
ip link set veth-b netns qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a
#给命名空间中的veth设备 veth-b 设置ip地址， 此IP地址做为同网段虚拟机的dhcp服务器
ip netns exec qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a ip addr add 10.0.0.2/32 dev veth-b
ip netns exec qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a ip link set dev veth-b up
# 在nstest命名空间中起一个dnsmasq服务器
ip netns exec qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a /usr/sbin/dnsmasq --no-hosts --no-resolv --except-interface=lo --bind-interfaces --interface=veth-b ...
#查看此命令空间中服务监听端口
ip netns exec qdhcp-ba48a3fc-e9e8-4ce0-8691-3d35b6cca80a netstat -tnpl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name
tcp 0 0 10.0.0.2:53 0.0.0.0:* LISTEN 28795/dnsmasq
#新建网桥brqba48a3fc-e9
brctl add brqba48a3fc-e9
#将veth-a添加到网桥 brqba48a3fc-e9
brctl addif brqba48a3fc-e9 veth-a
#新建kvm虚拟机过程忽略，查看此虚拟机网卡设备信息
virsh domiflist testvm
Interface Type Source Model MAC
-------------------------------------------------------
tap796091c0-07 bridge brqba48a3fc-e9 virtio fa:16:3e:85:f8:c
&lt;/code>&lt;/pre>
&lt;p>如上步骤，这样虚拟机发出的 dhcp 请求包会被网桥&lt;code>brqba48a3fc-e9&lt;/code>转发到接口&lt;code>veth-a&lt;/code>，由于 veth-a 与 veth-b 是一对 veth 设备，因此数据包会到达网络命名空间中的 veth-b，也即虚拟机 dhcp 请求能够成功我们上面是直接在网桥&lt;code>brqba48a3fc-e9&lt;/code>上建立虚拟机测试 dhcp 请求，也可以把主机上一块物理网卡添加到此网桥，这样此物理网卡所在网络上的其它所有主机都能使用此 dhcp 服务器。并且，仿照上面步骤，我们可以建多个 Bridge，每个 Bridge 都建立有自己的 network namespace 和 dhcp 服务器。这其实就是 openstack Bridge+Vlan(无 l3 agent)网络模式下，虚拟机自动获取 IP 的机制，如下图&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/casm7q/1616124199638-9318f4cd-d1a2-4ff2-ae9b-4fc0cbdfb562.png" alt="">&lt;/p>
&lt;p>(本文完)&lt;/p></description></item></channel></rss>