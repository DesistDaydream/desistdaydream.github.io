<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – 5.Scheduling(调度)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/</link><description>Recent content in 5.Scheduling(调度) on 断念梦</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 5.Scheduling(调度)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/5.Scheduling%E8%B0%83%E5%BA%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/5.Scheduling%E8%B0%83%E5%BA%A6/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/">官方文档,概念-调度、抢占与驱逐&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Scheduling(调度)&lt;/strong> 是一个行为，用来让 Pod 匹配到 Node，以便 Node 上的 Kubelet 可以运行这些 Pod。如果没有调度系统，Kubernetes 集群就不知道 Pod 应该运行在哪里。这种调度的概念，与 Linux 中调度任务来使用 CPU 是一个意思。可以看看 &lt;a href="blog/copy/%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%B2%BE%E8%A6%81.md">调度系统设计精要&lt;/a> 文章，调度是在 IT 行业中，很多程序都很重要的概念。&lt;/p>
&lt;p>与 Scheduling(调度) 伴生的，还有 &lt;strong>Preemption(抢占)&lt;/strong> 与 &lt;strong>Eviction(驱逐)&lt;/strong> 两个概念。顾名思义：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Preemption(抢占)&lt;/strong> 是指终止优先级较低的 Pod 的行为，以便优先级较高的 Pod 可以在节点上调度。
&lt;ul>
&lt;li>抢占行为通常发生在资源不足时，当一个新 Pod 需要调度，但是资源不足，那么就可能需要抢占优先级低的 Pod，这个低优先级的 Pod 将会被驱逐，以便让优先级高的 Pod 运行在节点上。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eviction(驱逐)&lt;/strong> 是指终止节点上一个或多个 Pod 的行为。&lt;/li>
&lt;/ul>
&lt;p>由 抢占 与 驱逐 两个行为，还引申出了 &lt;strong>Pod Disruption(中断)&lt;/strong> 的概念。&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod Disruption(中断)&lt;/a> 是指节点上的 Pod 自愿或者非资源终止运行的行为。&lt;/p>
&lt;ul>
&lt;li>自愿中断是由应用程序所有者或者集群管理故意启动的(比如.维护节点前手动驱逐 Pod)&lt;/li>
&lt;li>非自愿中断是无意的，可能由不可避免的问题触发(比如.节点资源耗尽或意外删除)&lt;/li>
&lt;/ul></description></item><item><title>Docs: kube-scheduler 实现调度器的程序</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/kube-scheduler-%E5%AE%9E%E7%8E%B0%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E7%A8%8B%E5%BA%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/kube-scheduler-%E5%AE%9E%E7%8E%B0%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E7%A8%8B%E5%BA%8F/</guid><description>
&lt;h1 id="kube-scheduler-是实现-kuberntes-scheduler-的应用程序">kube-scheduler 是实现 kuberntes Scheduler 的应用程序&lt;/h1>
&lt;p>kube-scheduler 启动后监听两个端口：&lt;/p>
&lt;ol>
&lt;li>10251 端口为无需身份验证和授权即可不安全地为 HTTP 服务的端口。(1.18 版本后将要弃用)&lt;/li>
&lt;li>10259 端口为需要身份验证和授权为 HTTPS 服务的端口。&lt;/li>
&lt;/ol>
&lt;h2 id="kube-scheduler-高科用">kube-scheduler 高科用&lt;/h2>
&lt;p>与 [kube-controller-manager 高可用](/docs/10.云原生/2.3.Kubernetes%20 容器编排系统/4.Controller(控制器)/kube-controller-manager%20 实现控制器的程序.md 实现控制器的程序.md) 原理相同。&lt;/p>
&lt;h2 id="kube-scheduler-监控指标">kube-scheduler 监控指标&lt;/h2>
&lt;p>详见：[kubernetes 监控](/docs/10.云原生/2.3.Kubernetes%20 容器编排系统/Kubernetes%20 管理/Kubernetes%20 监控/Kubernetes%20 系统组件指标.md 管理/Kubernetes 监控/Kubernetes 系统组件指标.md)&lt;/p>
&lt;h1 id="kube-scheduler-参数详解">Kube-scheduler 参数详解&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">官方文档，参考-组件工具-kube-scheduler&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="默认的-manifest-示例">默认的 manifest 示例&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">creationTimestamp&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">component&lt;/span>: &lt;span style="color:#ae81ff">kube-scheduler&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tier&lt;/span>: &lt;span style="color:#ae81ff">control-plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">kube-scheduler&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">kube-system&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">kube-scheduler&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">authentication-kubeconfig=/etc/kubernetes/scheduler.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">authorization-kubeconfig=/etc/kubernetes/scheduler.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">bind-address=0.0.0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">kubeconfig=/etc/kubernetes/scheduler.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --&lt;span style="color:#ae81ff">leader-elect=true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">k8s.gcr.io/kube-scheduler:v1.16.3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">IfNotPresent&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">livenessProbe&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">failureThreshold&lt;/span>: &lt;span style="color:#ae81ff">8&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">httpGet&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">host&lt;/span>: &lt;span style="color:#ae81ff">127.0.0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/healthz&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">10251&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">scheme&lt;/span>: &lt;span style="color:#ae81ff">HTTP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">initialDelaySeconds&lt;/span>: &lt;span style="color:#ae81ff">15&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeoutSeconds&lt;/span>: &lt;span style="color:#ae81ff">15&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">kube-scheduler&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#ae81ff">100m&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/etc/kubernetes/scheduler.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">kubeconfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">readOnly&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hostNetwork&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">priorityClassName&lt;/span>: &lt;span style="color:#ae81ff">system-cluster-critical&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">hostPath&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#ae81ff">/etc/kubernetes/scheduler.conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">FileOrCreate&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">kubeconfig&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Scheduler(调度器)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/Scheduler%E8%B0%83%E5%BA%A6%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/Scheduler%E8%B0%83%E5%BA%A6%E5%99%A8/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/">官方文档,概念-调度与驱逐&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>Scheduler(调度器)&lt;/strong> 负责决定 Pod 与 Node 的匹配关系，并将 Pod 调度到匹配到的 Node 上，以便 Kubelet 可以运行这些 Pod。Scheduler 在调度时会充分考虑 Cluster 的拓扑结构，当前各个节点的负载，以及应用对高可用、性能、数据亲和性的需求。&lt;/p>
&lt;p>Scheduler 通过 Kubernetes 的 watch 机制来发现集群中新创建且尚未被调度到 Node 的 Pod。Scheduler 会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上去运行。&lt;/p>
&lt;h1 id="调度器的实现">调度器的实现&lt;/h1>
&lt;p>&lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes%20%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling(%E8%B0%83%E5%BA%A6)/kube-scheduler%20%E5%AE%9E%E7%8E%B0%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E7%A8%8B%E5%BA%8F.md">kube-scheduler&lt;/a> 是实现 Kubernetes 调度器功能的程序。&lt;/p>
&lt;h1 id="scheduler-调度策略">Scheduler 调度策略&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/scheduling/policies/">官方文档，参考-调度-调度策略&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Scheduler 调度的时候，通过以下步骤来完成调度&lt;/p>
&lt;ol>
&lt;li>Predicate(预选) # 排除那些不满足 Pod 运行环境的 Node&lt;/li>
&lt;li>Priorities(优选) # 通过算法，剩余可运行 Pod 的 Node 进行计算后排序，选择结果最高的 Node&lt;/li>
&lt;li>Select(选定) # 若优选后有多个 Node 得分相同，则随机挑选，将选择的结果告诉 APIServer 用哪个 Node 部署 Pod&lt;/li>
&lt;/ol>
&lt;p>调度倾向性：亲合性 Affinity，反亲合性 AntiAffinity，污点 Taints，容忍度 Tolerations&lt;/p>
&lt;p>预选策略部分说明：预选策略中需要检查所有项，其中任意一项不通过，则不满足部署条件&lt;/p>
&lt;ol>
&lt;li>CheckNodeCondition&lt;/li>
&lt;li>GeneralPrediactes
&lt;ol>
&lt;li>HostName&lt;/li>
&lt;li>PodFitsHostPorts&lt;/li>
&lt;li>MatchNodeSelector&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>NoDiskConflict&lt;/li>
&lt;li>PodToleratesNodeTaints：检查 Pod 上的 spec.toleratons 可容忍的污点是否完全包含 Node 上的污点，Node 上后加的污点就算不在可容忍范围也可接受&lt;/li>
&lt;li>PodToleratesNodeNoExecuteTaints：检查 Pod 上的 spec.toleratons 可容忍的污点是否完全包含 Node 上的污点，Node 上后加的污点如果不在可容忍范围，则会被 Node 驱除&lt;/li>
&lt;li>CheckNodeLabelPresence：检查 Node 标签存在性&lt;/li>
&lt;li>CheckServiceAffinity：检查 Service 亲合性&lt;/li>
&lt;li>CheckVolumeBinding：检查 Node 已绑定和未绑定的 PVC&lt;/li>
&lt;li>NoVolumeZoneConflict：&lt;/li>
&lt;li>CheckNodeMemoryPressure：检查节点的内存资源压力是否处于过大状态&lt;/li>
&lt;li>CheckNoePIDPressure：检查节点的 PID 压力是否处于过大状态&lt;/li>
&lt;li>CheckNodeDiskPressure：检查节点的磁盘压力是否处于过大状态&lt;/li>
&lt;li>MatchInterPodAffinity：匹配 Pod 内部的亲合性(哪些 Pod 更倾向运行在同一节点等)&lt;/li>
&lt;/ol>
&lt;p>优选函数部分说明：&lt;/p>
&lt;ol>
&lt;li>LeastRequested：最少被请求的，得分越低，优先级越高
&lt;ol>
&lt;li>（cpu((capacity-sum(requested))*10/capacity)+（memory((capacity-sum(requested))*10/capacity))/2 # CPU 与内存分别计算（总容量-已被请求的）总容量*10，并求和，再除以 2,即为该 Node 资源得分&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>MostRequested：得分越多，优先级越高，&lt;/li>
&lt;li>BalancedResourceAllocation：均衡资源分配
&lt;ol>
&lt;li>CPU 和内存资源被占用率相近的胜出&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>NodePreferAvoidPods：
&lt;ol>
&lt;li>节点注解信息“scheduler.alpha.kubernetes.io/preferAvoidPods”&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>TaintToleration：将 Pod 对象的 spec.tolerations 列表项与 Node 的 taints 列表项进行匹配度检查，匹配的条目越多得分越低&lt;/li>
&lt;li>SelectorSpreading：与当前 Pod 对象同属相同选择器的 Pod 越多，得分越低&lt;/li>
&lt;li>InterPodAffinity：&lt;/li>
&lt;/ol></description></item><item><title>Docs: 让 Pod 运行在指定 Node 上</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/%E8%AE%A9-Pod-%E8%BF%90%E8%A1%8C%E5%9C%A8%E6%8C%87%E5%AE%9A-Node-%E4%B8%8A/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/5.Scheduling%E8%B0%83%E5%BA%A6/%E8%AE%A9-Pod-%E8%BF%90%E8%A1%8C%E5%9C%A8%E6%8C%87%E5%AE%9A-Node-%E4%B8%8A/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">官方文档,概念-调度、抢占、驱逐-让 Pod 运行在指定节点上&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">官方文档,概念-调度、抢占、驱逐-污点与容忍度&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/">Kubernetes 内置的 标签、注释、污点&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.csdn.net/asdfsadfasdfsa/article/details/106027367">CSDN,k8s 之 pod 亲和与反亲和的 topologyKey&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>通常情况下 Scheduler(调度器) 将自动进行合理的分配(例如，将 Pods 分散到所有节点上，以防止单独节点上资源使用率远高于其他节点)。但是在某些情况下我们需要更多控制 Pods 落在某个指定的节点上，例如确保一个 Pod 部署在装有 SSD 的机器上，或者将两个不同服务中的 Pods 共同定位到同一可用区域。&lt;/p>
&lt;p>所以我们需要 constrain(约束) Pod 运行在指定的节点。可以实现该效果的方式有以下几种：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>nodeName(节点名称)&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>nodeSelector(节点选择器)&lt;/strong> # 根据节点的标签，选择 pod 要运行在哪个节点上
&lt;ul>
&lt;li>这种行为定义 Pod 必须在特定节点上运行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Affinity(亲和) 与 Anti-Affinity(反亲和)&lt;/strong> # 根据亲和原则，让 pod 更趋向于与哪些 XXX 运行在同一个节点
&lt;ul>
&lt;li>这种行为定义 Pod 更倾向于在特定节点上运行。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Taint(污点) 与 Toleration(容忍度)&lt;/strong> # 根据节点上的污点，以及 pod 是否可以容忍该污点来决定 pod 是否可以运行在哪些节点上&lt;/li>
&lt;/ul>
&lt;p>其中 nodeSelector 和 Affinity 与 Anti-Affinity 是通过 &lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes%20%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/1.API%20Resource%20%E4%B8%8E%20Object/Object%20%E7%AE%A1%E7%90%86/Label%20and%20Selector(%E6%A0%87%E7%AD%BE%E5%92%8C%E9%80%89%E6%8B%A9%E5%99%A8)/Label%20and%20Selector(%E6%A0%87%E7%AD%BE%E5%92%8C%E9%80%89%E6%8B%A9%E5%99%A8).md">Label Selectors(标签选择器)&lt;/a> 来实现的。而 Taint 与 Toleration 是另一套类似于标签选择器的机制。&lt;/p>
&lt;h1 id="nodename节点名称">nodeName(节点名称)&lt;/h1>
&lt;p>&lt;code>nodeName&lt;/code> 是节点选择约束的最简单方法，但是由于其自身限制，通常不使用它。 &lt;code>nodeName&lt;/code> 是 PodSpec 的一个字段。 如果它不为空，调度器将忽略 Pod，并且给定节点上运行的 kubelet 进程尝试执行该 Pod。 因此，如果 &lt;code>nodeName&lt;/code> 在 PodSpec 中指定了，则它优先于上面的节点选择方法。
使用 &lt;code>nodeName&lt;/code> 来选择节点的一些限制：&lt;/p>
&lt;ul>
&lt;li>如果指定的节点不存在，&lt;/li>
&lt;li>如果指定的节点没有资源来容纳 Pod，Pod 将会调度失败并且其原因将显示为， 比如 OutOfmemory 或 OutOfcpu。&lt;/li>
&lt;li>云环境中的节点名称并非总是可预测或稳定的。&lt;/li>
&lt;/ul>
&lt;p>下面的是使用 &lt;code>nodeName&lt;/code> 字段的 Pod 配置文件的例子：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeName&lt;/span>: &lt;span style="color:#ae81ff">kube-01&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的 pod 将运行在 kube-01 节点上。&lt;/p>
&lt;h1 id="nodeselector节点选择器">NodeSelector(节点选择器)&lt;/h1>
&lt;p>nodeSelector(节点选择器) 是最简单的约束行为。在 Pod 的 manifest 中使用 &lt;code>nodeSelector&lt;/code> 字段进行配置。&lt;code>nodeSelector&lt;/code> 字段以键值对的方式描述该 Pod 应该运行在哪些具有指定标签的节点上。为了使 Pod 可以运行在指定的节点上，那么该节点的所有标签中，必须包含 Pod 中 .spec.nodeSelector 字段下定义的每个键值对.&lt;/p>
&lt;h2 id="使用示例">使用示例&lt;/h2>
&lt;p>首先为一个节点设置标签，标签设置方法详见[本章节](Label%20and%20Selector(标签和选择器).md and Selector(标签和选择器).md)。假如现在为节点设置了 disktype: ssd 标签&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 设置标签&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@lichenhao:~# kubectl label nodes master-3.bj-test disktype&lt;span style="color:#f92672">=&lt;/span>ssd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>node/master-3.bj-test labeled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 查看该节点标签&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@lichenhao:~# kubectl get nodes master-3.bj-test --show-labels
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS ROLES AGE VERSION LABELS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>master-3.bj-test Ready master,work 183d v1.19.2 ....,disktype&lt;span style="color:#f92672">=&lt;/span>ssd...这里省略了很多其他标签
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 Pod 的 Manifest 中添加 &lt;code>nodeSelector&lt;/code> 字段，示例如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>: &lt;span style="color:#ae81ff">test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">IfNotPresent&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">disktype&lt;/span>: &lt;span style="color:#ae81ff">ssd&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>该示例 Pod 启动后，会调度到具有 &lt;code>disktype=ssd&lt;/code> 标签的节点上。&lt;/p>
&lt;h1 id="affinity亲和性">Affinity(亲和性)&lt;/h1>
&lt;p>&lt;strong>Affinity(亲和性)&lt;/strong> 可以提供比 NodeSelector 更丰富的调度规则。亲和性主要就是只两个事物之间的 &lt;strong>Affinity(亲和)&lt;/strong> 和 **Anti-Affinity(反亲和)。**当两个事物比较亲和，则更愿意在一起，比如 Pod A 和 Node A 亲和，则 Pod 会运行在 Node A 上；而当两个事物反亲和时，则不会在一起，比如 Pod A 和 Pod B 反亲和，则 Pod A 和 Pod B 不会在同一个 Node 上运行。&lt;/p>
&lt;p>亲和性调度规则分为 Pod 和 Node 两类：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Node Affinity(节点亲和性)&lt;/strong> # 与 NodeSelector 类似，但是提供了更灵活的规则&lt;/li>
&lt;li>&lt;strong>Pod Affinity(Pod 亲和性)&lt;/strong># 可以根据某个 Pod 与某个 Pod 的亲和关系来决定该 Pod 要运行在具有(或不具有)某个 Pod 的 Node 上。
&lt;ul>
&lt;li>Pod 亲和可以分为两部分：
&lt;ul>
&lt;li>&lt;strong>Inter-pod Affinity(Pod 之间亲和)&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Inter-pod Anti-affinity(Pod 之间反亲和)&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>每种调度规则有 hard 和 soft 两种强度。通过如下两个字段&lt;/p>
&lt;ul>
&lt;li>**requiredDuringSchedulingIgnoredDuringExecution # 硬。**表示必须遵守调度规则，如果节点不存在或者其他原因导致调度失败，则 Pod 不会创建&lt;/li>
&lt;li>**preferredDuringSchedulingIgnoredDuringExecution # 软。**表示倾向于遵守调度规则，如果节点不存在或其他原因导致调度失败，则 Pod 会在其他不满足调度规则的节点上创建。&lt;/li>
&lt;/ul>
&lt;p>Pod Manifest 的 &lt;code>spec.affinity&lt;/code> 字段可以配置 亲和/反亲和 的调度规则。效果如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#ae81ff">......&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 节点亲和&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution&lt;/span>: &lt;span style="color:#ae81ff">......具体的调度规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>: &lt;span style="color:#ae81ff">......具体的调度规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Pod 亲和&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution&lt;/span>: &lt;span style="color:#ae81ff">......具体的调度规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>: &lt;span style="color:#ae81ff">......具体的调度规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Pod 反亲和&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">PodAntiAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution&lt;/span>: &lt;span style="color:#ae81ff">......具体的调度规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>: &lt;span style="color:#ae81ff">......具体的调度规则&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">XXX&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>注意：&lt;/p>
&lt;ul>
&lt;li>亲和性只在 Pod 调度期间有效。比如修改或删除了 pod 所在节点的标签，Pod 不会被删除。 也就是说，当 Pod 已经运行时，亲和性是不起作用的。&lt;/li>
&lt;li>Affinity 的 Manifests 各字段详见 [Pod Manifest 详解](/docs/10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/工作负载资源/Pod%20Manifest%20 详解.md 参考/工作负载资源/Pod Manifest 详解.md)&lt;/li>
&lt;/ul>
&lt;h2 id="node-affinity">Node Affinity&lt;/h2>
&lt;p>Node Affinity 概念上类似于 NodeSelector，属于 Pod 与 Node 之前的亲和性关系。可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。&lt;/p>
&lt;p>Affinity 的 Manifests 各字段详见 [Pod Manifest 详解](/docs/10.云原生/2.3.Kubernetes%20 容器编排系统/1.API、Resource(资源)、Object(对象)/API%20 参考/工作负载资源/Pod%20Manifest%20 详解.md 参考/工作负载资源/Pod Manifest 详解.md)&lt;/p>
&lt;h3 id="应用示例">应用示例&lt;/h3>
&lt;p>让 Pod 调度到具有标签 &lt;code>kubernetes.io/hostname&lt;/code> 的值为 &lt;code>node-01.test.tjiptv.net&lt;/code> 或者 &lt;code>node-02.test.tjiptv.net&lt;/code> 的 Node 上&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">with-node-affinity&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeSelectorTerms&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">kubernetes.io/e2e-az-name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">e2e-az1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">e2e-az2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">weight&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preference&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">another-node-label-key&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">another-node-label-value&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">with-node-affinity&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">k8s.gcr.io/pause:2.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面这个示例中的亲和性调度规则分为两部分：&lt;/p>
&lt;ul>
&lt;li>requiredDuringSchedulingIgnoredDuringExecution 字段下的规则为：Pod 只能被调度到具有 &lt;code>kubernetes.io/e2e-az-name&lt;/code> 标签，且标签值为 &lt;code>e2e-az1&lt;/code> 或 &lt;code>e2e-az2&lt;/code> 的节点上。
&lt;ul>
&lt;li>除了满足 requiredDuringSchedulingIgnoredDuringExecution 字段的规则外，也就是说，匹配到的这些节点中，还会根据下面的规则，进一步约束 Pod 被调度的节点&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>preferredDuringSchedulingIgnoredDuringExecution 字段下的规则为：具有 &lt;code>another-node-label-key&lt;/code> 标签，且标签值为 &lt;code>another-node-label-value&lt;/code> 的节点应该优先调度。除非这些节点不可用或由于其他原因不可调度，那么才会选择其他节点。&lt;/li>
&lt;/ul>
&lt;p>从上面的例子可以看到，想要匹配到想要的 Node，还是基于 [Selector(选择器)](Label%20and%20Selector(标签和选择器).md and Selector(标签和选择器).md) 来实现的，其中 &lt;code>operator&lt;/code> 字段中，可以通过设定 NotIn 或者 DoesNotExist 来实现&lt;strong>节点反亲和&lt;/strong>的效果。&lt;/p>
&lt;p>选择器规则：&lt;/p>
&lt;ul>
&lt;li>如果你同时指定了 &lt;code>nodeSelector&lt;/code> 和 &lt;code>nodeAffinity&lt;/code>，_两者_必须都要满足， 才能将 Pod 调度到候选节点上。&lt;/li>
&lt;li>如果你指定了多个与 &lt;code>nodeAffinity&lt;/code> 类型关联的 &lt;code>nodeSelectorTerms&lt;/code>，则 &lt;strong>如果其中一个&lt;/strong> &lt;code>nodeSelectorTerms&lt;/code> 满足的话，pod 将可以调度到节点上。&lt;/li>
&lt;li>如果你指定了多个与 &lt;code>nodeSelectorTerms&lt;/code> 关联的 &lt;code>matchExpressions&lt;/code>，则 &lt;strong>只有当所有&lt;/strong> &lt;code>matchExpressions&lt;/code> 满足的话，Pod 才会可以调度到节点上。&lt;/li>
&lt;/ul>
&lt;p>&lt;code>weight&lt;/code> 字段的值的范围是 1-100。对于每个符合所有调度要求（资源请求、RequiredDuringScheduling 亲和性表达式等） 的节点，调度器将遍历该字段的元素来计算总和，并且如果节点匹配对应的 MatchExpressions，则添加“权重”到总和。 然后将这个评分与该节点的其他优先级函数的评分进行组合。 总分最高的节点是最优选的&lt;/p>
&lt;h2 id="pod-affinity">Pod Affinity&lt;/h2>
&lt;p>Pod Affinity 表示 Pod 间亲和性与反亲和性，使我们可以基于已经在节点上运行的 Pod 的标签来约束待调度的 Pod 可以调度到的节点，而不是基于节点上的标签。&lt;/p>
&lt;p>这种规则可以这么描述：&lt;strong>“如果 X 节点上已经运行了一个或多个满足规则 Y 的 Pod， 则这个 Pod 应该(在反亲和情况下不应该)运行在 X 节点”&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Y&lt;/strong> 表示一个具有可选的关联命令空间列表的 [&lt;strong>LabelSelector(标签选择器)&lt;/strong>](Label%20and%20Selector(标签和选择器).md and Selector(标签和选择器).md)； 与节点不同，因为 Pod 是命名空间限定的(因此 Pod 上的标签也是命名空间限定的)， 因此作用于 Pod 标签的标签选择算符必须指定选择算符应用在哪个命名空间。&lt;/li>
&lt;li>&lt;strong>X&lt;/strong> 我们使用 &lt;code>topologyKey&lt;/code> 字段来表示 X，&lt;code>topologyKey&lt;/code> 用来指定节点标签的键，用来表示 &lt;strong>Topology Domain(拓扑域)&lt;/strong>。 从概念上讲，k8s 的节点、机架、云供应商可用区、云供应商地理区域等，都有区域的概念，都可以称为拓扑域。说白了，&lt;code>topologyKey&lt;/code> 字段还是用来筛选 Node 的。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>说明：&lt;/p>
&lt;ul>
&lt;li>Pod 间亲和性与反亲和性需要大量的处理，这可能会显著减慢大规模集群中的调度。 我们不建议在超过数百个节点的集群中使用它们。&lt;/li>
&lt;li>Pod 反亲和性需要对节点进行一致的标记，即集群中的每个节点必须具有适当的标签能够匹配 &lt;code>topologyKey&lt;/code>。如果某些或所有节点缺少指定的 &lt;code>topologyKey&lt;/code> 标签，可能会导致意外行为。&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Pod 间亲和性通过 PodSpec 中 affinity 字段下的 podAffinity 字段进行指定。 而 Pod 间反亲和性通过 PodSpec 中 affinity 字段下的 podAntiAffinity 字段进行指定。&lt;/p>
&lt;h3 id="应用示例-1">应用示例&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">with-pod-affinity&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">labelSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">security&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">S1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">topologyKey&lt;/span>: &lt;span style="color:#ae81ff">topology.kubernetes.io/zone&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAntiAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">weight&lt;/span>: &lt;span style="color:#ae81ff">100&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAffinityTerm&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labelSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">security&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">S2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">topologyKey&lt;/span>: &lt;span style="color:#ae81ff">topology.kubernetes.io/zone&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">with-pod-affinity&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">k8s.gcr.io/pause:2.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在此示例中，在这个 Pod 的亲和性配置定义了两条规则：&lt;/p>
&lt;ul>
&lt;li>Pod 亲和性规则
&lt;ul>
&lt;li>仅当节点和至少一个已运行且有键为“security”且值为“S1”的标签 的 Pod 处于同一区域时，才可以将该 Pod 调度到节点上。(更确切的说，如果节点 N 具有 &lt;code>topology.kubernetes.io/zone&lt;/code> 标签， 则 Pod 有资格在节点 N 上运行，以便集群中至少有一个具有 &lt;code>topology.kubernetes.io/zone&lt;/code> 标签的节点正在运行具有键“security”和值 “S1”的标签的 pod。)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Pod 反亲和性规则
&lt;ul>
&lt;li>如果在具有 &lt;code>topology.kubernetes.io/zone&lt;/code> 标签的节点上已经运行了具有键为 &lt;code>security&lt;/code> 和值 &lt;code>S2&lt;/code> 标签的 Pod，则同样具有相同标签的 Pod 不能被调度到该节点。
&lt;ul>
&lt;li>也就是说，具有 security=S2 标签的 Pod 在具有 topology.kubernetes.io/zone 标签的节点上，只能运行一个。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Pod 亲和性与反亲和性的合法 &lt;code>operator&lt;/code> 字段的值有 In，NotIn，Exists，DoesNotExist。&lt;/p>
&lt;p>既然 &lt;code>topologyKey&lt;/code> 是拓扑域，那 Pod 之间怎样才是属于同一个拓扑域？&lt;/p>
&lt;ul>
&lt;li>如果使用  kubernetes.io/hostname 标签，则表示拓扑域为 Node 范围，那么  kubernetes.io/hostname  对应的值不一样就是不同的拓扑域。比如 Pod1 在  kubernetes.io/hostname=node1  的 Node 上，Pod2 在  kubernetes.io/hostname=node2  的 Node 上，Pod3 在  kubernetes.io/hostname=node1  的 Node 上，则 Pod1 和 Pod3 在同一个拓扑域，而 Pod2 则与 Pod1 和 Pod3 不在同一个拓扑域。&lt;/li>
&lt;li>如果使用  failure-domain.kubernetes.io/zone ，则表示拓扑域为一个区域。同样，Node 的标签  failure-domain.kubernetes.io/zone  对应的值不一样也不是同一个拓扑域，比如 Pod1 在  failure-domain.kubernetes.io/zone=beijing  的 Node 上，Pod2 在  failure-domain.kubernetes.io/zone=hangzhou  的 Node 上，则 Pod1 和 Pod2 不属于同一个拓扑域。&lt;/li>
&lt;li>当然，topologyKey 也可以使用自定义标签。比如可以给一组 Node 打上标签  custom_topology，那么拓扑域就是针对这个标签了，则该标签相同的 Node 上的 Pod 属于同一个拓扑域。&lt;/li>
&lt;/ul>
&lt;p>原则上，topologyKey 可以是任何合法的标签键。 然而，出于性能和安全原因，topologyKey 受到一些限制：&lt;/p>
&lt;ul>
&lt;li>在 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 中，topologyKey 不允许为空。&lt;/li>
&lt;li>对于 &lt;code>podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution&lt;/code>， 准入控制器 &lt;code>LimitPodHardAntiAffinityTopology&lt;/code> 被引入以确保 &lt;code>topologyKey&lt;/code> 只能是 &lt;code>kubernetes.io/hostname&lt;/code>。如果你希望 topologyKey 也可用于其他定制拓扑逻辑，你可以更改准入控制器或者禁用之。&lt;/li>
&lt;li>除上述情况外，topologyKey 可以是任何合法的标签键。&lt;/li>
&lt;/ul>
&lt;p>除了 labelSelector 和 topologyKey，你也可以指定表示命名空间的 namespaces 队列，labelSelector 也应该匹配它 （这个与 labelSelector 和 topologyKey 的定义位于相同的级别）。 如果忽略或者为空，则默认为 Pod 亲和性/反亲和性的定义所在的命名空间。&lt;/p>
&lt;p>所有与 requiredDuringSchedulingIgnoredDuringExecution 亲和性与反亲和性 关联的 matchExpressions 必须满足，才能将 pod 调度到节点上。&lt;/p>
&lt;h3 id="更实际的用例">更实际的用例&lt;/h3>
&lt;p>Pod 间亲和性与反亲和性在与更高级别的集合(例如 ReplicaSets、StatefulSets、 Deployments 等)一起使用时，它们更加有用。 可以轻松配置一组应位于相同定义拓扑（例如，节点）中的工作负载。&lt;/p>
&lt;p>在三节点集群中，一个 web 应用程序具有内存缓存(例如 redis)。 我们希望 web 服务器尽可能与缓存放置在同一位置，并且多个 Redis 不在同一个节点上。&lt;/p>
&lt;h4 id="永远放置在不同节点上">永远放置在不同节点上&lt;/h4>
&lt;p>下面的例子使用 PodAntiAffinity 规则和 topologyKey: &amp;ldquo;kubernetes.io/hostname&amp;rdquo; 来部署 redis 集群以便在同一主机上没有两个实例。 它有三个副本和选择器标签 app=store。 Deployment 配置了 PodAntiAffinity，用来确保调度器不会将副本调度到单个节点上。&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://kubernetes.io/zh/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure">ZooKeeper 教程&lt;/a> 中使用了相同的反亲和性配置方式， 来达到高可用性的 StatefulSet 的样例。&lt;/p>
&lt;/blockquote>
&lt;p>这一段反亲和性可以描述为：具有 &lt;code>kubernetes.io/hostname&lt;/code> 标签的节点上只能运行一个具有 &lt;code>app=store&lt;/code> 标签的 Pod。这就保证了，3 个 Redis 将会分散到不同的节点。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">redis-cache&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAntiAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">labelSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">app&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">topologyKey&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">redis-server&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">redis:3.2-alpine&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="始终放置在相同节点上">始终放置在相同节点上&lt;/h4>
&lt;p>下面 webserver Deployment 的 YAML 代码段中配置了 podAntiAffinity 和 podAffinity。 这将通知调度器将它的所有副本与具有 app=store 选择器标签的 Pod 放置在一起。 这还确保每个 web 服务器副本不会调度到单个节点上。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">web-server&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">web-store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">template&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">web-store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAntiAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">labelSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">app&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">web-store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">topologyKey&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">podAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">labelSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">app&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">topologyKey&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">web-app&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx:1.16-alpine&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果我们创建了上面的两个 Deployment，我们的三节点集群将如下表所示。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>node-1&lt;/th>
&lt;th>node-2&lt;/th>
&lt;th>node-3&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>webserver-1&lt;/em>&lt;/td>
&lt;td>&lt;em>webserver-2&lt;/em>&lt;/td>
&lt;td>&lt;em>webserver-3&lt;/em>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>cache-1&lt;/em>&lt;/td>
&lt;td>&lt;em>cache-2&lt;/em>&lt;/td>
&lt;td>&lt;em>cache-3&lt;/em>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，Web 服务器的 3 个副本，逐一与缓存共存，并且每个节点只有一个 Web 服务器和一个缓存。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get pods -o wide
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>输出类似于如下内容：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE IP NODE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>redis-cache-1450370735-6dzlj 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 8m 10.192.4.2 kube-node-3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>redis-cache-1450370735-j2j96 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 8m 10.192.2.2 kube-node-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>redis-cache-1450370735-z73mh 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 8m 10.192.3.1 kube-node-2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>web-server-1287567482-5d4dz 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 7m 10.192.2.3 kube-node-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>web-server-1287567482-6f7v5 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 7m 10.192.4.3 kube-node-3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>web-server-1287567482-s330j 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 7m 10.192.3.2 kube-node-2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="taint污点-与-toleration容忍度">Taint(污点) 与 Toleration(容忍度)&lt;/h1>
&lt;p>该方式可以由 Node 来决定是否拒绝运行 Pod。把 Node 当作第一人称“我”来描述，给 Node 打上一个 Taint，我会拒绝不容忍我的 Taint 的 Pod 运行在我身上；还有一种情况是如果 Pod 已经运行在我身上了，那么这时我多了一个 Pod 不容忍的 Taint，那么如果我身上的 Pod 不容忍，我就要驱逐这些 Pod。&lt;/p>
&lt;p>通常情况下，Taint 定义在 Node 上，Toleration 定义在 Pod 上。&lt;/p>
&lt;h2 id="taint">Taint&lt;/h2>
&lt;p>&lt;strong>taints([]Object)&lt;/strong> # 定义 Node 污点。凡是具有污点的节点，都对不容忍污点的 Pod 具有某些 effect(效果)。&lt;/p>
&lt;ul>
&lt;li>**effect &lt;!-- raw HTML omitted --> # 必须的。**定义当 Pod 不能容忍 Node 的污点的时候，Node 对 Pod 的排斥效果（效果即使要采取的行为是什么)。
&lt;ul>
&lt;li>NoSchedule # 仅影响调度过程(仅影响调度过程，对于已经调度到该 Node 上的 Pod 则没效果，即调度完成后再给 Node 上加的污点就算 Pod 不容忍也没影响)；&lt;/li>
&lt;li>NoExecut # 即影响调度过程，也影响现存的 Pod(即不容忍 taint 的 Pod 不但不会调度到 Node 上，如果不容忍该 Taint 的 Pod 已经在该 Node 上，会被 Node 驱逐)；
&lt;ul>
&lt;li>在什么情况下会触发驱逐 Pod 的效果呢？e.g.在 Pod 已经调度到 Node 上之后，再给 Node 添加 Taint，这时候该 Pod 不容忍新增加的污点，那么 Node 就会驱逐该 Pod&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>PreferNoschedule # 最好不调度，实在不行了还可以调度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>key: &lt;!-- raw HTML omitted --> # 必须的。&lt;/strong>&lt;/li>
&lt;li>&lt;strong>value: &lt;!-- raw HTML omitted -->&lt;/strong> #&lt;/li>
&lt;li>&lt;strong>timeAdded: &lt;!-- raw HTML omitted -->&lt;/strong>#&lt;/li>
&lt;/ul>
&lt;p>可以直接使用 kubectl 命令为节点添加一个污点，比如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl taint nodes NODE KEY&lt;span style="color:#f92672">[=&lt;/span>VALUE&lt;span style="color:#f92672">]&lt;/span>:EFFECT
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="应用示例-2">应用示例&lt;/h3>
&lt;ul>
&lt;li>为 node-t.tj-test 这个节点添加污点，污点的 key 为 node-role.kubernetes.io/master，没有 value，污点效果为 NodSchedule
&lt;ul>
&lt;li>kubectl taint node node-4.tj-test node-role.kubernetes.io/master:NoSchedule&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>删除所有节点上 node-role.kubernetes.io/master 这个污点。也就是让 master 可以当作 node 使用
&lt;ul>
&lt;li>kubectl taint nodes &amp;ndash;all node-role.kubernetes.io/master-&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>该命令可以获取所有节点上的污点
&lt;ul>
&lt;li>kubectl get nodes &amp;ndash;template=&amp;rsquo;{{ range .items }}{{ .metadata.name }}{{&amp;quot;\t&amp;quot;}}{{ .spec.taints }}{{&amp;quot;\n&amp;quot;}}{{end}}'&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="toleration">Toleration&lt;/h2>
&lt;p>&lt;strong>tolerations([]Object)&lt;/strong> # 为 Pod 添加容忍规则&lt;/p>
&lt;ul>
&lt;li>&lt;strong>effect: &lt;!-- raw HTML omitted -->&lt;/strong># 表明匹配的 taint 的 effect 字段，包括三个效果 NoSchedule, PreferNoSchedule and NoExecute，如果不指定该字段则匹配所有污点效果&lt;/li>
&lt;li>&lt;strong>key: &lt;!-- raw HTML omitted -->&lt;/strong> # 指明要容忍的 taint 中的 key&lt;/li>
&lt;li>&lt;strong>operator: &lt;!-- raw HTML omitted -->&lt;/strong> # 定义容忍要满足的条件
&lt;ul>
&lt;li>Exists # 只要 Key 一样，则容忍&lt;/li>
&lt;li>Equal # 需要 Key 和 value 都一样才容忍&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>tolerationSeconds: &lt;!-- raw HTML omitted -->&lt;/strong> # 定义容忍时间&lt;/li>
&lt;li>&lt;strong>value: &lt;!-- raw HTML omitted -->&lt;/strong> # 指明要容忍的 taint 中的 value&lt;/li>
&lt;/ul>
&lt;h3 id="应用示例-3">应用示例&lt;/h3>
&lt;p>仅需下面两行，就表示容忍所有污点&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">tolerations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">Exists&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="taint-based-evictions基于污点的驱逐">Taint based Evictions(基于污点的驱逐)&lt;/h2>
&lt;p>前文提到过污点的 effect 值 &lt;code>NoExecute&lt;/code>会影响已经在节点上运行的 Pod&lt;/p>
&lt;ul>
&lt;li>如果 Pod 不能忍受 effect 值为 &lt;code>NoExecute&lt;/code> 的污点，那么 Pod 将马上被驱逐&lt;/li>
&lt;li>如果 Pod 能够忍受 effect 值为 &lt;code>NoExecute&lt;/code> 的污点，但是在容忍度定义中没有指定 &lt;code>tolerationSeconds&lt;/code>，则 Pod 还会一直在这个节点上运行。&lt;/li>
&lt;li>如果 Pod 能够忍受 effect 值为 &lt;code>NoExecute&lt;/code> 的污点，而且指定了 &lt;code>tolerationSeconds&lt;/code>， 则 Pod 还能在这个节点上继续运行这个指定的时间长度。&lt;/li>
&lt;/ul>
&lt;p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>node.kubernetes.io/not-ready&lt;/code>：节点未准备好。这相当于节点状态 &lt;code>Ready&lt;/code> 的值为 &amp;ldquo;&lt;code>False&lt;/code>&amp;quot;。&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/unreachable&lt;/code>：节点控制器访问不到节点. 这相当于节点状态 &lt;code>Ready&lt;/code> 的值为 &amp;ldquo;&lt;code>Unknown&lt;/code>&amp;quot;。&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/out-of-disk&lt;/code>：节点磁盘耗尽。&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/memory-pressure&lt;/code>：节点存在内存压力。&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/disk-pressure&lt;/code>：节点存在磁盘压力。&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/network-unavailable&lt;/code>：节点网络不可用。&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/unschedulable&lt;/code>: 节点不可调度。&lt;/li>
&lt;li>&lt;code>node.cloudprovider.kubernetes.io/uninitialized&lt;/code>：如果 kubelet 启动时指定了一个 &amp;ldquo;外部&amp;rdquo; 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。&lt;/li>
&lt;/ul>
&lt;p>在节点被驱逐时，节点控制器或者 kubelet 会添加带有 &lt;code>NoExecute&lt;/code> 效应的相关污点。 如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>说明：&lt;/strong> 为了保证由于节点问题引起的 Pod 驱逐 &lt;a href="https://kubernetes.io/zh/docs/concepts/architecture/nodes/">速率限制&lt;/a>行为正常， 系统实际上会以限定速率的方式添加污点。在像主控节点与工作节点间通信中断等场景下， 这样做可以避免 Pod 被大量驱逐。&lt;/p>
&lt;/blockquote>
&lt;p>使用这个功能特性，结合 &lt;code>tolerationSeconds&lt;/code>，Pod 就可以指定当节点出现一个 或全部上述问题时还将在这个节点上运行多长的时间。
比如，一个使用了很多本地状态的应用程序在网络断开时，仍然希望停留在当前节点上运行一段较长的时间， 愿意等待网络恢复以避免被驱逐。在这种情况下，Pod 的容忍度可能是下面这样的：&lt;/p>
&lt;pre>&lt;code>tolerations:
- key: &amp;quot;node.kubernetes.io/unreachable&amp;quot;
operator: &amp;quot;Exists&amp;quot;
effect: &amp;quot;NoExecute&amp;quot;
tolerationSeconds: 6000
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>&lt;strong>说明：&lt;/strong>
Kubernetes 会自动给 Pod 添加一个 key 为 &lt;code>node.kubernetes.io/not-ready&lt;/code> 的容忍度 并配置 &lt;code>tolerationSeconds=300&lt;/code>，除非用户提供的 Pod 配置中已经已存在了 key 为 &lt;code>node.kubernetes.io/not-ready&lt;/code> 的容忍度。
同样，Kubernetes 会给 Pod 添加一个 key 为 &lt;code>node.kubernetes.io/unreachable&lt;/code> 的容忍度 并配置 &lt;code>tolerationSeconds=300&lt;/code>，除非用户提供的 Pod 配置中已经已存在了 key 为 &lt;code>node.kubernetes.io/unreachable&lt;/code> 的容忍度。&lt;/p>
&lt;/blockquote>
&lt;p>这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod 默认能够继续停留在当前节点运行 5 分钟。
&lt;a href="https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet&lt;/a> 中的 Pod 被创建时， 针对以下污点自动添加的 &lt;code>NoExecute&lt;/code> 的容忍度将不会指定 &lt;code>tolerationSeconds&lt;/code>：&lt;/p>
&lt;ul>
&lt;li>&lt;code>node.kubernetes.io/unreachable&lt;/code>&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/not-ready&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。&lt;/p>
&lt;h2 id="基于节点状态添加污点">基于节点状态添加污点&lt;/h2>
&lt;p>Node 生命周期控制器会自动创建与 Node 条件相对应的带有 &lt;code>NoSchedule&lt;/code> 效应的污点。 同样，调度器不检查节点条件，而是检查节点污点。这确保了节点条件不会影响调度到节点上的内容。 用户可以通过添加适当的 Pod 容忍度来选择忽略某些 Node 的问题(表示为 Node 的调度条件)。
DaemonSet 控制器自动为所有守护进程添加如下 &lt;code>NoSchedule&lt;/code> 容忍度以防 DaemonSet 崩溃：&lt;/p>
&lt;ul>
&lt;li>&lt;code>node.kubernetes.io/memory-pressure&lt;/code>&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/disk-pressure&lt;/code>&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/out-of-disk&lt;/code> (&lt;em>只适合关键 Pod&lt;/em>)&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/unschedulable&lt;/code> (1.10 或更高版本)&lt;/li>
&lt;li>&lt;code>node.kubernetes.io/network-unavailable&lt;/code> (&lt;em>只适合主机网络配置&lt;/em>)&lt;/li>
&lt;/ul>
&lt;p>添加上述容忍度确保了向后兼容，您也可以选择自由向 DaemonSet 添加容忍度。&lt;/p></description></item></channel></rss>