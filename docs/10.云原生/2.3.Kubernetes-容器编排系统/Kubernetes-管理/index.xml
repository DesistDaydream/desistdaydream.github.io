<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – Kubernetes 管理</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/</link><description>Recent content in Kubernetes 管理 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: HPA(Horizontal Pod Autoscaler)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/HPAHorizontal-Pod-Autoscaler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/HPAHorizontal-Pod-Autoscaler/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>官方文档：&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.qikqiak.com/post/k8s-hpa-usage/">https://www.qikqiak.com/post/k8s-hpa-usage/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Pod 水平自动扩缩（Horizontal Pod Autoscaler） 可以基于 CPU 利用率自动扩缩 ReplicationController、Deployment 和 ReplicaSet 中的 Pod 数量。 除了 CPU 利用率，也可以基于其他应程序提供的&lt;a href="https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md">自定义度量指标&lt;/a> 来执行自动扩缩。 Pod 自动扩缩不适用于无法扩缩的对象，比如 DaemonSet。&lt;/p>
&lt;p>Pod 水平自动扩缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的调整副本控制器或 Deployment 中的副本数量，以使得 Pod 的平均 CPU 利用率与用户所设定的目标值匹配。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kpk0nr/1616116805897-3bfc7a8f-d1bb-4268-a2a3-eebfb62a1e45.png" alt="">&lt;/p>
&lt;p>我们可以简单的通过 kubectl autoscale 命令来创建一个 HPA 资源对象， HPAController 默认 30s 轮询一次（可通过 kube-controller-manager 的 &amp;ndash;horizontal-pod-autoscaler-sync-period 参数进行设置），查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。&lt;/p>
&lt;h2 id="metrics-server">Metrics Server&lt;/h2>
&lt;p>在 HPA 的第一个版本中，我们需要 &lt;code>Heapster&lt;/code> 提供 CPU 和内存指标，在 HPA v2 过后就需要安装 Metrcis Server 了，&lt;code>Metrics Server&lt;/code> 可以通过标准的 Kubernetes API 把监控数据暴露出来，有了 &lt;code>Metrics Server&lt;/code> 之后，我们就完全可以通过标准的 Kubernetes API 来访问我们想要获取的监控数据了：&lt;/p>
&lt;pre>&lt;code>https://10.96.0.1/apis/metrics.k8s.io/v1beta1/namespaces/&amp;lt;namespace-name&amp;gt;/pods/&amp;lt;pod-name&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>比如当我们访问上面的 API 的时候，我们就可以获取到该 Pod 的资源数据，这些数据其实是来自于 kubelet 的 &lt;code>Summary API&lt;/code> 采集而来的。不过需要说明的是我们这里可以通过标准的 API 来获取资源监控数据，并不是因为 &lt;code>Metrics Server&lt;/code> 就是 APIServer 的一部分，而是通过 Kubernetes 提供的 &lt;code>Aggregator&lt;/code> 汇聚插件来实现的，是独立于 APIServer 之外运行的。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kpk0nr/1616116806050-9e81e7fd-4f38-4745-bcb0-7774067f17f7.png" alt="">&lt;/p>
&lt;p>HAP Metrics Server&lt;/p>
&lt;h3 id="聚合-api">聚合 API&lt;/h3>
&lt;p>&lt;code>Aggregator&lt;/code> 允许开发人员编写一个自己的服务，把这个服务注册到 Kubernetes 的 APIServer 里面去，这样我们就可以像原生的 APIServer 提供的 API 使用自己的 API 了，我们把自己的服务运行在 Kubernetes 集群里面，然后 Kubernetes 的 &lt;code>Aggregator&lt;/code> 通过 Service 名称就可以转发到我们自己写的 Service 里面去了。这样这个聚合层就带来了很多好处：&lt;/p>
&lt;ul>
&lt;li>增加了 API 的扩展性，开发人员可以编写自己的 API 服务来暴露他们想要的 API。&lt;/li>
&lt;li>丰富了 API，核心 kubernetes 团队阻止了很多新的 API 提案，通过允许开发人员将他们的 API 作为单独的服务公开，这样就无须社区繁杂的审查了。&lt;/li>
&lt;li>开发分阶段实验性 API，新的 API 可以在单独的聚合服务中开发，当它稳定之后，在合并会 APIServer 就很容易了。&lt;/li>
&lt;li>确保新 API 遵循 Kubernetes 约定，如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这样很可能造成社区成员和社区约定不一致。&lt;/li>
&lt;/ul>
&lt;h3 id="安装">安装&lt;/h3>
&lt;p>所以现在我们要使用 HPA，就需要在集群中安装 &lt;code>Metrics Server&lt;/code> 服务，要安装 &lt;code>Metrics Server&lt;/code> 就需要开启 &lt;code>Aggregator&lt;/code>，因为 &lt;code>Metrics Server&lt;/code> 就是通过该代理进行扩展的，不过我们集群是通过 Kubeadm 搭建的，默认已经开启了，如果是二进制方式安装的集群，需要单独配置 kube-apsierver 添加如下所示的参数：&lt;/p>
&lt;pre>&lt;code>--requestheader-client-ca-file=&amp;lt;path to aggregator CA cert&amp;gt;
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=&amp;lt;path to aggregator proxy cert&amp;gt;
--proxy-client-key-file=&amp;lt;path to aggregator proxy key&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>如果 &lt;code>kube-proxy&lt;/code> 没有和 APIServer 运行在同一台主机上，那么需要确保启用了如下 kube-apsierver 的参数：&lt;/p>
&lt;pre>&lt;code>--enable-aggregator-routing=true
&lt;/code>&lt;/pre>
&lt;p>对于这些证书的生成方式，我们可以查看官方文档：&lt;a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md">https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md&lt;/a>。&lt;/p>
&lt;p>&lt;code>Aggregator&lt;/code> 聚合层启动完成后，就可以来安装 &lt;code>Metrics Server&lt;/code> 了，我们可以获取该仓库的官方安装资源清单：&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/kubernetes-incubator/metrics-server
$ cd metrics-server
$ kubectl apply -f deploy/1.8+/
&lt;/code>&lt;/pre>
&lt;p>在部署之前，修改 &lt;code>metrcis-server/deploy/1.8+/metrics-server-deployment.yaml&lt;/code> 的镜像地址为：&lt;/p>
&lt;pre>&lt;code>containers:
- name: metrics-server
image: gcr.azk8s.cn/google_containers/metrics-server-amd64:v0.3.6
&lt;/code>&lt;/pre>
&lt;p>等部署完成后，可以查看 Pod 日志是否正常：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n kube-system -l k8s-app=metrics-server
NAME READY STATUS RESTARTS AGE
metrics-server-6886856d7c-g5k6q 1/1 Running 0 2m39s
$ kubectl logs -f metrics-server-6886856d7c-g5k6q -n kube-system
......
E1119 09:05:57.234312 1 manager.go:111] unable to fully collect metrics: [unable to fully scrape metrics from source kubelet_summary:ydzs-node1: unable to fetch metrics from Kubelet ydzs-node1 (ydzs-node1): Get https://ydzs-node1:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup ydzs-node1 on 10.96.0.10:53: no such host, unable to fully scrape metrics from source kubelet_summary:ydzs-node4: unable to fetch metrics from Kubelet ydzs-node4 (ydzs-node4): Get https://ydzs-node4:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup ydzs-node4 on 10.96.0.10:53: no such host, unable to fully scrape metrics from source kubelet_summary:ydzs-node3: unable to fetch metrics from Kubelet ydzs-node3 (ydzs-node3): Get https://ydzs-node3:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup ydzs-node3 on 10.96.0.10:53: no such host, unable to fully scrape metrics from source kubelet_summary:ydzs-master: unable to fetch metrics from Kubelet ydzs-master (ydzs-master): Get https://ydzs-master:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup ydzs-master on 10.96.0.10:53: no such host, unable to fully scrape metrics from source kubelet_summary:ydzs-node2: unable to fetch metrics from Kubelet ydzs-node2 (ydzs-node2): Get https://ydzs-node2:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup ydzs-node2 on 10.96.0.10:53: no such host]
&lt;/code>&lt;/pre>
&lt;p>我们可以发现 Pod 中出现了一些错误信息：&lt;code>xxx: no such host&lt;/code>，我们看到这个错误信息一般就可以确定是 DNS 解析不了造成的，我们可以看到 Metrics Server 会通过 kubelet 的 10250 端口获取信息，使用的是 hostname，我们部署集群的时候在节点的 &lt;code>/etc/hosts&lt;/code> 里面添加了节点的 hostname 和 ip 的映射，但是是我们的 Metrics Server 的 Pod 内部并没有这个 hosts 信息，当然也就不识别 hostname 了，要解决这个问题，有两种方法：第一种方法就是在集群内部的 DNS 服务里面添加上 hostname 的解析，比如我们这里集群中使用的是 &lt;code>CoreDNS&lt;/code>，我们就可以去修改下 CoreDNS 的 Configmap 信息，添加上 hosts 信息：&lt;/p>
&lt;pre>&lt;code>$ kubectl edit configmap coredns -n kube-system
apiVersion: v1
data:
Corefile: |
.:53 {
errors
health
hosts { # 添加集群节点hosts隐射信息
10.151.30.11 ydzs-master
10.151.30.57 ydzs-node3
10.151.30.59 ydzs-node4
10.151.30.22 ydzs-node1
10.151.30.23 ydzs-node2
fallthrough
}
kubernetes cluster.local in-addr.arpa ip6.arpa {
pods insecure
upstream
fallthrough in-addr.arpa ip6.arpa
}
prometheus :9153
proxy . /etc/resolv.conf
cache 30
reload
}
kind: ConfigMap
metadata:
creationTimestamp: 2019-05-18T11:07:46Z
name: coredns
namespace: kube-system
&lt;/code>&lt;/pre>
&lt;p>这样当在集群内部访问集群的 hostname 的时候就可以解析到对应的 ip 了，另外一种方法就是在 metrics-server 的启动参数中修改 &lt;code>kubelet-preferred-address-types&lt;/code> 参数，如下：&lt;/p>
&lt;pre>&lt;code>args:
- --cert-dir=/tmp
- --secure-port=4443
- --kubelet-preferred-address-types=InternalIP
&lt;/code>&lt;/pre>
&lt;p>我们这里使用第二种方式，然后重新安装：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n kube-system -l k8s-app=metrics-server
NAME READY STATUS RESTARTS AGE
metrics-server-6dcfdf89b5-tvdcp 1/1 Running 0 33s
$ kubectl logs -f metric-metrics-server-58fc94d9f-jlxcb -n kube-system
......
E1119 09:08:49.805959 1 manager.go:111] unable to fully collect metrics: [unable to fully scrape metrics from source kubelet_summary:ydzs-node3: unable to fetch metrics from Kubelet ydzs-node3 (10.151.30.57): Get https://10.151.30.57:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.57 because it doesn't contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-node4: unable to fetch metrics from Kubelet ydzs-node4 (10.151.30.59): Get https://10.151.30.59:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.59 because it doesn't contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-node2: unable to fetch metrics from Kubelet ydzs-node2 (10.151.30.23): Get https://10.151.30.23:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.23 because it doesn't contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-master: unable to fetch metrics from Kubelet ydzs-master (10.151.30.11): Get https://10.151.30.11:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.11 because it doesn't contain any IP SANs, unable to fully scrape metrics from source kubelet_summary:ydzs-node1: unable to fetch metrics from Kubelet ydzs-node1 (10.151.30.22): Get https://10.151.30.22:10250/stats/summary?only_cpu_and_memory=true: x509: cannot validate certificate for 10.151.30.22 because it doesn't contain any IP SANs]
&lt;/code>&lt;/pre>
&lt;p>因为部署集群的时候，CA 证书并没有把各个节点的 IP 签上去，所以这里 &lt;code>Metrics Server&lt;/code> 通过 IP 去请求时，提示签的证书没有对应的 IP（错误：&lt;code>x509: cannot validate certificate for 10.151.30.22 because it doesn’t contain any IP SANs&lt;/code>），我们可以添加一个&lt;code>--kubelet-insecure-tls&lt;/code>参数跳过证书校验：&lt;/p>
&lt;pre>&lt;code>args:
- --cert-dir=/tmp
- --secure-port=4443
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP
&lt;/code>&lt;/pre>
&lt;p>然后再重新安装即可成功！可以通过如下命令来验证：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f deploy/1.8+/
$ kubectl get pods -n kube-system -l k8s-app=metrics-server
NAME READY STATUS RESTARTS AGE
metrics-server-5d4dbb78bb-6klw6 1/1 Running 0 14s
$ kubectl logs -f metrics-server-5d4dbb78bb-6klw6 -n kube-system
I1119 09:10:44.249092 1 serving.go:312] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)
I1119 09:10:45.264076 1 secure_serving.go:116] Serving securely on [::]:4443
$ kubectl get apiservice | grep metrics
v1beta1.metrics.k8s.io kube-system/metrics-server True 9m
$ kubectl get --raw &amp;quot;/apis/metrics.k8s.io/v1beta1/nodes&amp;quot;
{&amp;quot;kind&amp;quot;:&amp;quot;NodeMetricsList&amp;quot;,&amp;quot;apiVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes&amp;quot;},&amp;quot;items&amp;quot;:[{&amp;quot;metadata&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;ydzs-node3&amp;quot;,&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node3&amp;quot;,&amp;quot;creationTimestamp&amp;quot;:&amp;quot;2019-11-19T09:11:53Z&amp;quot;},&amp;quot;timestamp&amp;quot;:&amp;quot;2019-11-19T09:11:38Z&amp;quot;,&amp;quot;window&amp;quot;:&amp;quot;30s&amp;quot;,&amp;quot;usage&amp;quot;:{&amp;quot;cpu&amp;quot;:&amp;quot;240965441n&amp;quot;,&amp;quot;memory&amp;quot;:&amp;quot;3004360Ki&amp;quot;}},{&amp;quot;metadata&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;ydzs-node4&amp;quot;,&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node4&amp;quot;,&amp;quot;creationTimestamp&amp;quot;:&amp;quot;2019-11-19T09:11:53Z&amp;quot;},&amp;quot;timestamp&amp;quot;:&amp;quot;2019-11-19T09:11:37Z&amp;quot;,&amp;quot;window&amp;quot;:&amp;quot;30s&amp;quot;,&amp;quot;usage&amp;quot;:{&amp;quot;cpu&amp;quot;:&amp;quot;167036681n&amp;quot;,&amp;quot;memory&amp;quot;:&amp;quot;2574664Ki&amp;quot;}},{&amp;quot;metadata&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;ydzs-master&amp;quot;,&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-master&amp;quot;,&amp;quot;creationTimestamp&amp;quot;:&amp;quot;2019-11-19T09:11:53Z&amp;quot;},&amp;quot;timestamp&amp;quot;:&amp;quot;2019-11-19T09:11:38Z&amp;quot;,&amp;quot;window&amp;quot;:&amp;quot;30s&amp;quot;,&amp;quot;usage&amp;quot;:{&amp;quot;cpu&amp;quot;:&amp;quot;350907350n&amp;quot;,&amp;quot;memory&amp;quot;:&amp;quot;2986716Ki&amp;quot;}},{&amp;quot;metadata&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;ydzs-node1&amp;quot;,&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node1&amp;quot;,&amp;quot;creationTimestamp&amp;quot;:&amp;quot;2019-11-19T09:11:53Z&amp;quot;},&amp;quot;timestamp&amp;quot;:&amp;quot;2019-11-19T09:11:39Z&amp;quot;,&amp;quot;window&amp;quot;:&amp;quot;30s&amp;quot;,&amp;quot;usage&amp;quot;:{&amp;quot;cpu&amp;quot;:&amp;quot;1319638039n&amp;quot;,&amp;quot;memory&amp;quot;:&amp;quot;2094376Ki&amp;quot;}},{&amp;quot;metadata&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;ydzs-node2&amp;quot;,&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes/ydzs-node2&amp;quot;,&amp;quot;creationTimestamp&amp;quot;:&amp;quot;2019-11-19T09:11:53Z&amp;quot;},&amp;quot;timestamp&amp;quot;:&amp;quot;2019-11-19T09:11:36Z&amp;quot;,&amp;quot;window&amp;quot;:&amp;quot;30s&amp;quot;,&amp;quot;usage&amp;quot;:{&amp;quot;cpu&amp;quot;:&amp;quot;320381888n&amp;quot;,&amp;quot;memory&amp;quot;:&amp;quot;3270368Ki&amp;quot;}}]}
$ kubectl top nodes
NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%
ydzs-master 351m 17% 2916Mi 79%
ydzs-node1 1320m 33% 2045Mi 26%
ydzs-node2 321m 8% 3193Mi 41%
ydzs-node3 241m 6% 2933Mi 37%
ydzs-node4 168m 4% 2514Mi 32%
&lt;/code>&lt;/pre>
&lt;p>现在我们可以通过 &lt;code>kubectl top&lt;/code> 命令来获取到资源数据了，证明 &lt;code>Metrics Server&lt;/code> 已经安装成功了。&lt;/p>
&lt;h2 id="基于-cpu">基于 CPU&lt;/h2>
&lt;p>现在我们用 Deployment 来创建一个 Nginx Pod，然后利用 &lt;code>HPA&lt;/code> 来进行自动扩缩容。资源清单如下所示：（hpa-demo.yaml）&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: hpa-demo
spec:
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
containers:
- name: nginx
image: nginx
ports:
- containerPort: 80
&lt;/code>&lt;/pre>
&lt;p>然后直接创建 Deployment：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f hpa-demo.yaml
deployment.apps/hpa-demo created
$ kubectl get pods -l app=nginx
NAME READY STATUS RESTARTS AGE
hpa-demo-85ff79dd56-pz8th 1/1 Running 0 21s
&lt;/code>&lt;/pre>
&lt;p>现在我们来创建一个 &lt;code>HPA&lt;/code> 资源对象，可以使用&lt;code>kubectl autoscale&lt;/code>命令来创建：&lt;/p>
&lt;pre>&lt;code>$ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
hpa-demo Deployment/hpa-demo &amp;lt;unknown&amp;gt;/10% 1 10 1 16s
&lt;/code>&lt;/pre>
&lt;p>此命令创建了一个关联资源 hpa-demo 的 HPA，最小的 Pod 副本数为 1，最大为 10。HPA 会根据设定的 cpu 使用率（10%）动态的增加或者减少 Pod 数量。&lt;/p>
&lt;p>当然我们依然还是可以通过创建 YAML 文件的形式来创建 HPA 资源对象。如果我们不知道怎么编写的话，可以查看上面命令行创建的 HPA 的 YAML 文件：&lt;/p>
&lt;pre>&lt;code>$ kubectl get hpa hpa-demo -o yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
annotations:
autoscaling.alpha.kubernetes.io/conditions: '[{&amp;quot;type&amp;quot;:&amp;quot;AbleToScale&amp;quot;,&amp;quot;status&amp;quot;:&amp;quot;True&amp;quot;,&amp;quot;lastTransitionTime&amp;quot;:&amp;quot;2019-11-19T09:15:12Z&amp;quot;,&amp;quot;reason&amp;quot;:&amp;quot;SucceededGetScale&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;the
HPA controller was able to get the target''s current scale&amp;quot;},{&amp;quot;type&amp;quot;:&amp;quot;ScalingActive&amp;quot;,&amp;quot;status&amp;quot;:&amp;quot;False&amp;quot;,&amp;quot;lastTransitionTime&amp;quot;:&amp;quot;2019-11-19T09:15:12Z&amp;quot;,&amp;quot;reason&amp;quot;:&amp;quot;FailedGetResourceMetric&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;the
HPA was unable to compute the replica count: missing request for cpu&amp;quot;}]'
creationTimestamp: &amp;quot;2019-11-19T09:14:56Z&amp;quot;
name: hpa-demo
namespace: default
resourceVersion: &amp;quot;3094084&amp;quot;
selfLink: /apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers/hpa-demo
uid: b84d79f1-75b0-46e0-95b5-4cbe3509233b
spec:
maxReplicas: 10
minReplicas: 1
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: hpa-demo
targetCPUUtilizationPercentage: 10
status:
currentReplicas: 1
desiredReplicas: 0
&lt;/code>&lt;/pre>
&lt;p>然后我们可以根据上面的 YAML 文件就可以自己来创建一个基于 YAML 的 HPA 描述文件了。但是我们发现上面信息里面出现了一些 Fail 信息，我们来查看下这个 HPA 对象的信息：&lt;/p>
&lt;pre>&lt;code>$ kubectl describe hpa hpa-demo
Name: hpa-demo
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
CreationTimestamp: Tue, 19 Nov 2019 17:14:56 +0800
Reference: Deployment/hpa-demo
Metrics: ( current / target )
resource cpu on pods (as a percentage of request): &amp;lt;unknown&amp;gt; / 10%
Min replicas: 1
Max replicas: 10
Deployment pods: 1 current / 0 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True SucceededGetScale the HPA controller was able to get the target's current scale
ScalingActive False FailedGetResourceMetric the HPA was unable to compute the replica count: missing request for cpu
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedGetResourceMetric 14s (x4 over 60s) horizontal-pod-autoscaler missing request for cpu
Warning FailedComputeMetricsReplicas 14s (x4 over 60s) horizontal-pod-autoscaler invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: missing request for cpu
&lt;/code>&lt;/pre>
&lt;p>我们可以看到上面的事件信息里面出现了 &lt;code>failed to get cpu utilization: missing request for cpu&lt;/code> 这样的错误信息。这是因为我们上面创建的 Pod 对象没有添加 request 资源声明，这样导致 HPA 读取不到 CPU 指标信息，所以如果要想让 HPA 生效，对应的 Pod 资源必须添加 requests 资源声明，更新我们的资源清单文件：&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: hpa-demo
spec:
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
containers:
- name: nginx
image: nginx
ports:
- containerPort: 80
resources:
requests:
memory: 50Mi
cpu: 50m
&lt;/code>&lt;/pre>
&lt;p>然后重新更新 Deployment，重新创建 HPA 对象：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f hpa.yaml
deployment.apps/hpa-demo configured
$ kubectl get pods -o wide -l app=nginx
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
hpa-demo-69968bb59f-twtdp 1/1 Running 0 4m11s 10.244.4.97 ydzs-node4 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
$ kubectl delete hpa hpa-demo
horizontalpodautoscaler.autoscaling &amp;quot;hpa-demo&amp;quot; deleted
$ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
$ kubectl describe hpa hpa-demo
Name: hpa-demo
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
CreationTimestamp: Tue, 19 Nov 2019 17:23:49 +0800
Reference: Deployment/hpa-demo
Metrics: ( current / target )
resource cpu on pods (as a percentage of request): 0% (0) / 10%
Min replicas: 1
Max replicas: 10
Deployment pods: 1 current / 1 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True ScaleDownStabilized recent recommendations were higher than current one, applying the highest recent recommendation
ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
ScalingLimited False DesiredWithinRange the desired count is within the acceptable range
Events: &amp;lt;none&amp;gt;
$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
hpa-demo Deployment/hpa-demo 0%/10% 1 10 1 52s
&lt;/code>&lt;/pre>
&lt;p>现在可以看到 HPA 资源对象已经正常了，现在我们来增大负载进行测试，我们来创建一个 busybox 的 Pod，并且循环访问上面创建的 Pod：&lt;/p>
&lt;pre>&lt;code>$ kubectl run -it --image busybox test-hpa --restart=Never --rm /bin/sh
If you don't see a command prompt, try pressing enter.
/ # while true; do wget -q -O- http://10.244.4.97; done
&lt;/code>&lt;/pre>
&lt;p>下图可以看到，HPA 已经开始工作：&lt;/p>
&lt;pre>&lt;code>$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
hpa-demo Deployment/hpa-demo 338%/10% 1 10 1 5m15s
$ kubectl get pods -l app=nginx --watch
NAME READY STATUS RESTARTS AGE
hpa-demo-69968bb59f-8hjnn 1/1 Running 0 22s
hpa-demo-69968bb59f-9ss9f 1/1 Running 0 22s
hpa-demo-69968bb59f-bllsd 1/1 Running 0 22s
hpa-demo-69968bb59f-lnh8k 1/1 Running 0 37s
hpa-demo-69968bb59f-r8zfh 1/1 Running 0 22s
hpa-demo-69968bb59f-twtdp 1/1 Running 0 6m43s
hpa-demo-69968bb59f-w792g 1/1 Running 0 37s
hpa-demo-69968bb59f-zlxkp 1/1 Running 0 37s
hpa-demo-69968bb59f-znp6q 0/1 ContainerCreating 0 6s
hpa-demo-69968bb59f-ztnvx 1/1 Running 0 6s
&lt;/code>&lt;/pre>
&lt;p>我们可以看到已经自动拉起了很多新的 Pod，最后定格在了我们上面设置的 10 个 Pod，同时查看资源 hpa-demo 的副本数量，副本数量已经从原来的 1 变成了 10 个：&lt;/p>
&lt;pre>&lt;code>$ kubectl get deployment hpa-demo
NAME READY UP-TO-DATE AVAILABLE AGE
hpa-demo 10/10 10 10 17m
&lt;/code>&lt;/pre>
&lt;p>查看 HPA 资源的对象了解工作过程：&lt;/p>
&lt;pre>&lt;code>$ kubectl describe hpa hpa-demo
Name: hpa-demo
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
CreationTimestamp: Tue, 19 Nov 2019 17:23:49 +0800
Reference: Deployment/hpa-demo
Metrics: ( current / target )
resource cpu on pods (as a percentage of request): 0% (0) / 10%
Min replicas: 1
Max replicas: 10
Deployment pods: 10 current / 10 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True ScaleDownStabilized recent recommendations were higher than current one, applying the highest recent recommendation
ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
ScalingLimited True TooManyReplicas the desired replica count is more than the maximum replica count
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal SuccessfulRescale 5m45s horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target
Normal SuccessfulRescale 5m30s horizontal-pod-autoscaler New size: 8; reason: cpu resource utilization (percentage of request) above target
Normal SuccessfulRescale 5m14s horizontal-pod-autoscaler New size: 10; reason: cpu resource utilization (percentage of request) above target
&lt;/code>&lt;/pre>
&lt;p>同样的这个时候我们来关掉 busybox 来减少负载，然后等待一段时间观察下 HPA 和 Deployment 对象：&lt;/p>
&lt;pre>&lt;code>$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
hpa-demo Deployment/hpa-demo 0%/10% 1 10 1 14m
$ kubectl get deployment hpa-demo
NAME READY UP-TO-DATE AVAILABLE AGE
hpa-demo 1/1 1 1 24m
&lt;/code>&lt;/pre>
&lt;p>可以看到副本数量已经由 10 变为 1，当前我们只是演示了 CPU 使用率这一个指标，在后面的课程中我们还会学习到根据自定义的监控指标来自动对 Pod 进行扩缩容。&lt;/p>
&lt;h2 id="基于内存">基于内存&lt;/h2>
&lt;p>&lt;code>HorizontalPodAutoscaler&lt;/code> 是 Kubernetes autoscaling API 组的资源，在当前稳定版本 &lt;code>autoscaling/v1&lt;/code> 中只支持基于 CPU 指标的缩放。在 Beta 版本 &lt;code>autoscaling/v2beta2&lt;/code>，引入了基于内存和自定义指标的缩放。所以我们这里需要使用 Beta 版本的 API。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kpk0nr/1616116805951-274bb778-81fd-4004-a69d-c928b5d90fa7.png" alt="">&lt;/p>
&lt;p>hpa api version&lt;/p>
&lt;p>现在我们用 Deployment 来创建一个 Nginx Pod，然后利用 HPA 来进行自动扩缩容。资源清单如下所示：（hpa-mem-demo.yaml）&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: hpa-mem-demo
spec:
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
volumes:
- name: increase-mem-script
configMap:
name: increase-mem-config
containers:
- name: nginx
image: nginx
ports:
- containerPort: 80
volumeMounts:
- name: increase-mem-script
mountPath: /etc/script
resources:
requests:
memory: 50Mi
cpu: 50m
securityContext:
privileged: true
&lt;/code>&lt;/pre>
&lt;p>这里和前面普通的应用有一些区别，我们将一个名为 &lt;code>increase-mem-config&lt;/code> 的 ConfigMap 资源对象挂载到了容器中，该配置文件是用于后面增加容器内存占用的脚本，配置文件如下所示：（increase-mem-cm.yaml）&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: ConfigMap
metadata:
name: increase-mem-config
data:
increase-mem.sh: |
#!/bin/bash
mkdir /tmp/memory
mount -t tmpfs -o size=40M tmpfs /tmp/memory
dd if=/dev/zero of=/tmp/memory/block
sleep 60
rm /tmp/memory/block
umount /tmp/memory
rmdir /tmp/memory
&lt;/code>&lt;/pre>
&lt;p>由于这里增加内存的脚本需要使用到 &lt;code>mount&lt;/code> 命令，这需要声明为特权模式，所以我们添加了 &lt;code>securityContext.privileged=true&lt;/code> 这个配置。现在我们直接创建上面的资源对象即可：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f increase-mem-cm.yaml
$ kubectl apply -f hpa-mem-demo.yaml
$ kubectl get pods -l app=nginx
NAME READY STATUS RESTARTS AGE
hpa-mem-demo-66944b79bf-tqrn9 1/1 Running 0 35s
&lt;/code>&lt;/pre>
&lt;p>然后需要创建一个基于内存的 HPA 资源对象：（hpa-mem.yaml）&lt;/p>
&lt;pre>&lt;code>apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
name: nginx-hpa
spec:
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: hpa-mem-demo
minReplicas: 1
maxReplicas: 5
metrics:
- type: Resource
resource:
name: memory
targetAverageUtilization: 60
&lt;/code>&lt;/pre>
&lt;p>要注意这里使用的 &lt;code>apiVersion&lt;/code> 是 &lt;code>autoscaling/v2beta1&lt;/code>，然后 &lt;code>metrics&lt;/code> 属性里面指定的是内存的配置，直接创建上面的资源对象即可：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f hpa-mem.yaml
horizontalpodautoscaler.autoscaling/nginx-hpa created
$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
nginx-hpa Deployment/hpa-mem-demo 2%/60% 1 5 1 12s
&lt;/code>&lt;/pre>
&lt;p>到这里证明 HPA 资源对象已经部署成功了，接下来我们对应用进行压测，将内存压上去，直接执行上面我们挂载到容器中的 &lt;code>increase-mem.sh&lt;/code> 脚本即可：&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it hpa-mem-demo-66944b79bf-tqrn9 /bin/bash
root@hpa-mem-demo-66944b79bf-tqrn9:/# ls /etc/script/
increase-mem.sh
root@hpa-mem-demo-66944b79bf-tqrn9:/# source /etc/script/increase-mem.sh
dd: writing to '/tmp/memory/block': No space left on device
81921+0 records in
81920+0 records out
41943040 bytes (42 MB, 40 MiB) copied, 0.584029 s, 71.8 MB/s
&lt;/code>&lt;/pre>
&lt;p>然后打开另外一个终端观察 HPA 资源对象的变化情况：&lt;/p>
&lt;pre>&lt;code>$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
nginx-hpa Deployment/hpa-mem-demo 83%/60% 1 5 1 5m3s
$ kubectl describe hpa nginx-hpa
Name: nginx-hpa
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: kubectl.kubernetes.io/last-applied-configuration:
{&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling/v2beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;HorizontalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-hpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;...
CreationTimestamp: Tue, 07 Apr 2020 13:13:59 +0800
Reference: Deployment/hpa-mem-demo
Metrics: ( current / target )
resource memory on pods (as a percentage of request): 3% (1740800) / 60%
Min replicas: 1
Max replicas: 5
Deployment pods: 2 current / 2 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True ScaleDownStabilized recent recommendations were higher than current one, applying the highest recent recommendation
ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from memory resource utilization (percentage of request)
ScalingLimited False DesiredWithinRange the desired count is within the acceptable range
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedGetResourceMetric 5m26s (x3 over 5m58s) horizontal-pod-autoscaler unable to get metrics for resource memory: no metrics returned from resource metrics API
Warning FailedComputeMetricsReplicas 5m26s (x3 over 5m58s) horizontal-pod-autoscaler invalid metrics (1 invalid out of 1), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
Normal SuccessfulRescale 77s horizontal-pod-autoscaler New size: 2; reason: memory resource utilization (percentage of request) above target
$ kubectl top pod hpa-mem-demo-66944b79bf-tqrn9
NAME CPU(cores) MEMORY(bytes)
hpa-mem-demo-66944b79bf-tqrn9 0m 41Mi
&lt;/code>&lt;/pre>
&lt;p>可以看到内存使用已经超过了我们设定的 60% 这个阈值了，HPA 资源对象也已经触发了自动扩容，变成了两个副本了：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -l app=nginx
NAME READY STATUS RESTARTS AGE
hpa-mem-demo-66944b79bf-8m4d9 1/1 Running 0 2m51s
hpa-mem-demo-66944b79bf-tqrn9 1/1 Running 0 8m11s
&lt;/code>&lt;/pre>
&lt;p>当内存释放掉后，controller-manager 默认 5 分钟过后会进行缩放，到这里就完成了基于内存的 HPA 操作。&lt;/p>
&lt;h2 id="基于自定义指标">基于自定义指标&lt;/h2>
&lt;p>除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 &lt;code>Prometheus Adapter&lt;/code>，Prometheus 用于监控应用的负载和集群本身的各种指标，&lt;code>Prometheus Adapter&lt;/code> 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kpk0nr/1616116805981-8c3dbfc1-65c1-48db-8e3b-710da1c58765.png" alt="">&lt;/p>
&lt;p>custom metrics by prometheus&lt;/p>
&lt;p>首先，我们部署一个示例应用，在该应用程序上测试 Prometheus 指标自动缩放，资源清单文件如下所示：（hpa-prome-demo.yaml）&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: hpa-prom-demo
spec:
selector:
matchLabels:
app: nginx-server
template:
metadata:
labels:
app: nginx-server
spec:
containers:
- name: nginx-demo
image: cnych/nginx-vts:v1.0
resources:
limits:
cpu: 50m
requests:
cpu: 50m
ports:
- containerPort: 80
name: http
---
apiVersion: v1
kind: Service
metadata:
name: hpa-prom-demo
annotations:
prometheus.io/scrape: &amp;quot;true&amp;quot;
prometheus.io/port: &amp;quot;80&amp;quot;
prometheus.io/path: &amp;quot;/status/format/prometheus&amp;quot;
spec:
ports:
- port: 80
targetPort: 80
name: http
selector:
app: nginx-server
type: NodePort
&lt;/code>&lt;/pre>
&lt;p>这里我们部署的应用是在 80 端口的 &lt;code>/status/format/prometheus&lt;/code> 这个端点暴露 nginx-vts 指标的，前面我们已经在 Prometheus 中配置了 Endpoints 的自动发现，所以我们直接在 Service 对象的 &lt;code>annotations&lt;/code> 中进行配置，这样我们就可以在 Prometheus 中采集该指标数据了。为了测试方便，我们这里使用 NodePort 类型的 Service，现在直接创建上面的资源对象即可：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f hpa-prome-demo.yaml
deployment.apps/hpa-prom-demo created
service/hpa-prom-demo created
$ kubectl get pods -l app=nginx-server
NAME READY STATUS RESTARTS AGE
hpa-prom-demo-755bb56f85-lvksr 1/1 Running 0 4m52s
$ kubectl get svc
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
hpa-prom-demo NodePort 10.101.210.158 &amp;lt;none&amp;gt; 80:32408/TCP 5m44s
......
&lt;/code>&lt;/pre>
&lt;p>部署完成后我们可以使用如下命令测试应用是否正常，以及指标数据接口能够正常获取：&lt;/p>
&lt;pre>&lt;code>$ curl http://k8s.qikqiak.com:32408
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
body {
width: 35em;
margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif;
}
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
$ curl http://k8s.qikqiak.com:32408/status/format/prometheus
# HELP nginx_vts_info Nginx info
# TYPE nginx_vts_info gauge
nginx_vts_info{hostname=&amp;quot;hpa-prom-demo-755bb56f85-lvksr&amp;quot;,version=&amp;quot;1.13.12&amp;quot;} 1
# HELP nginx_vts_start_time_seconds Nginx start time
# TYPE nginx_vts_start_time_seconds gauge
nginx_vts_start_time_seconds 1586240091.623
# HELP nginx_vts_main_connections Nginx connections
# TYPE nginx_vts_main_connections gauge
......
&lt;/code>&lt;/pre>
&lt;p>上面的指标数据中，我们比较关心的是 &lt;code>nginx_vts_server_requests_total&lt;/code> 这个指标，表示请求总数，是一个 &lt;code>Counter&lt;/code> 类型的指标，我们将使用该指标的值来确定是否需要对我们的应用进行自动扩缩容。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kpk0nr/1616116805999-cf50cbd9-8273-472c-bb6f-dac9055780b5.png" alt="">&lt;/p>
&lt;p>nginx_vts_server_requests_total&lt;/p>
&lt;p>接下来我们将 Prometheus-Adapter 安装到集群中，并添加一个规则来跟踪 Pod 的请求，我们可以将 Prometheus 中的任何一个指标都用于 HPA，但是前提是你得通过查询语句将它拿到（包括指标名称和其对应的值）。&lt;/p>
&lt;p>这里我们定义一个如下所示的规则：&lt;/p>
&lt;pre>&lt;code>rules:
- seriesQuery: 'nginx_vts_server_requests_total'
seriesFilters: []
resources:
overrides:
kubernetes_namespace:
resource: namespace
kubernetes_pod_name:
resource: pod
name:
matches: &amp;quot;^(.*)_total&amp;quot;
as: &amp;quot;${1}_per_second&amp;quot;
metricsQuery: (sum(rate(&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;.GroupBy&amp;gt;))
&lt;/code>&lt;/pre>
&lt;p>这是一个带参数的 Prometheus 查询，其中：&lt;/p>
&lt;ul>
&lt;li>&lt;code>seriesQuery&lt;/code>：查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA&lt;/li>
&lt;li>&lt;code>seriesFilters&lt;/code>：查询到的指标可能会存在不需要的，可以通过它过滤掉。&lt;/li>
&lt;li>&lt;code>resources&lt;/code>：通过 &lt;code>seriesQuery&lt;/code> 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询，&lt;code>resources&lt;/code> 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 &lt;code>overrides&lt;/code>，另一种是 &lt;code>template&lt;/code>。
&lt;ul>
&lt;li>&lt;code>overrides&lt;/code>：它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 &lt;code>nginx: {group: &amp;quot;apps&amp;quot;, resource: &amp;quot;deployment&amp;quot;}&lt;/code> 这么写表示的就是将指标中 nginx 这个标签和 apps 这个 api 组中的 &lt;code>deployment&lt;/code> 资源关联起来；&lt;/li>
&lt;li>template：通过 go 模板的形式。比如&lt;code>template: &amp;quot;kube_&amp;lt;&amp;lt;.Group&amp;gt;&amp;gt;_&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt;&amp;quot;&lt;/code> 这么写表示，假如 &lt;code>&amp;lt;&amp;lt;.Group&amp;gt;&amp;gt;&lt;/code> 为 apps，&lt;code>&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt;&lt;/code> 为 deployment，那么它就是将指标中 &lt;code>kube_apps_deployment&lt;/code> 标签和 deployment 资源关联起来。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>name&lt;/code>：用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。
&lt;ul>
&lt;li>&lt;code>matches&lt;/code>：通过正则表达式来匹配指标名，可以进行分组&lt;/li>
&lt;li>&lt;code>as&lt;/code>：默认值为 &lt;code>$1&lt;/code>，也就是第一个分组。&lt;code>as&lt;/code> 为空就是使用默认值的意思。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>metricsQuery&lt;/code>：这就是 Prometheus 的查询语句了，前面的 &lt;code>seriesQuery&lt;/code> 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。
&lt;ul>
&lt;li>&lt;code>Series&lt;/code>：表示指标名称&lt;/li>
&lt;li>&lt;code>LabelMatchers&lt;/code>：附加的标签，目前只有 &lt;code>pod&lt;/code> 和 &lt;code>namespace&lt;/code> 两种，因此我们要在之前使用 &lt;code>resources&lt;/code> 进行关联&lt;/li>
&lt;li>&lt;code>GroupBy&lt;/code>：就是 pod 名称，同样需要使用 &lt;code>resources&lt;/code> 进行关联。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>接下来我们通过 Helm Chart 来部署 Prometheus Adapter，新建 &lt;code>hpa-prome-adapter-values.yaml&lt;/code> 文件覆盖默认的 Values 值，内容如下所示：&lt;/p>
&lt;pre>&lt;code>rules:
default: false
custom:
- seriesQuery: 'nginx_vts_server_requests_total'
resources:
overrides:
kubernetes_namespace:
resource: namespace
kubernetes_pod_name:
resource: pod
name:
matches: &amp;quot;^(.*)_total&amp;quot;
as: &amp;quot;${1}_per_second&amp;quot;
metricsQuery: (sum(rate(&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;.GroupBy&amp;gt;))
prometheus:
url: http://thanos-querier.kube-mon.svc.cluster.local
&lt;/code>&lt;/pre>
&lt;p>这里我们添加了一条 rules 规则，然后指定了 Prometheus 的地址，我们这里是使用了 Thanos 部署的 Promethues 集群，所以用 Querier 的地址。使用下面的命令一键安装：&lt;/p>
&lt;pre>&lt;code>$ helm install prometheus-adapter stable/prometheus-adapter -n kube-mon -f hpa-prome-adapter-values.yaml
NAME: prometheus-adapter
LAST DEPLOYED: Tue Apr 7 15:26:36 2020
NAMESPACE: kube-mon
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
prometheus-adapter has been deployed.
In a few minutes you should be able to list metrics using the following command(s):
kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
&lt;/code>&lt;/pre>
&lt;p>等一小会儿，安装完成后，可以使用下面的命令来检测是否生效了：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n kube-mon -l app=prometheus-adapter
NAME READY STATUS RESTARTS AGE
prometheus-adapter-58b559fc7d-l2j6t 1/1 Running 0 3m21s
$ kubectl get --raw=&amp;quot;/apis/custom.metrics.k8s.io/v1beta1&amp;quot; | jq
{
&amp;quot;kind&amp;quot;: &amp;quot;APIResourceList&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
&amp;quot;groupVersion&amp;quot;: &amp;quot;custom.metrics.k8s.io/v1beta1&amp;quot;,
&amp;quot;resources&amp;quot;: [
{
&amp;quot;name&amp;quot;: &amp;quot;namespaces/nginx_vts_server_requests_per_second&amp;quot;,
&amp;quot;singularName&amp;quot;: &amp;quot;&amp;quot;,
&amp;quot;namespaced&amp;quot;: false,
&amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
&amp;quot;verbs&amp;quot;: [
&amp;quot;get&amp;quot;
]
},
{
&amp;quot;name&amp;quot;: &amp;quot;pods/nginx_vts_server_requests_per_second&amp;quot;,
&amp;quot;singularName&amp;quot;: &amp;quot;&amp;quot;,
&amp;quot;namespaced&amp;quot;: true,
&amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
&amp;quot;verbs&amp;quot;: [
&amp;quot;get&amp;quot;
]
}
]
}
&lt;/code>&lt;/pre>
&lt;p>我们可以看到 &lt;code>nginx_vts_server_requests_per_second&lt;/code> 指标可用。 现在，让我们检查该指标的当前值：&lt;/p>
&lt;pre>&lt;code>$ kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second&amp;quot; | jq .
{
&amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;custom.metrics.k8s.io/v1beta1&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;selfLink&amp;quot;: &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second&amp;quot;
},
&amp;quot;items&amp;quot;: [
{
&amp;quot;describedObject&amp;quot;: {
&amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
&amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;hpa-prom-demo-755bb56f85-lvksr&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;/v1&amp;quot;
},
&amp;quot;metricName&amp;quot;: &amp;quot;nginx_vts_server_requests_per_second&amp;quot;,
&amp;quot;timestamp&amp;quot;: &amp;quot;2020-04-07T09:45:45Z&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;527m&amp;quot;,
&amp;quot;selector&amp;quot;: null
}
]
}
&lt;/code>&lt;/pre>
&lt;p>出现类似上面的信息就表明已经配置成功了，接下来我们部署一个针对上面的自定义指标的 HAP 资源对象，如下所示：(hpa-prome.yaml)&lt;/p>
&lt;pre>&lt;code>apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
name: nginx-custom-hpa
spec:
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: hpa-prom-demo
minReplicas: 2
maxReplicas: 5
metrics:
- type: Pods
pods:
metricName: nginx_vts_server_requests_per_second
targetAverageValue: 10
&lt;/code>&lt;/pre>
&lt;p>如果请求数超过每秒 10 个，则将对应用进行扩容。直接创建上面的资源对象：&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f hpa-prome.yaml
horizontalpodautoscaler.autoscaling/nginx-custom-hpa created
$ kubectl describe hpa nginx-custom-hpa
Name: nginx-custom-hpa
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: kubectl.kubernetes.io/last-applied-configuration:
{&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling/v2beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;HorizontalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-custom-hpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;d...
CreationTimestamp: Tue, 07 Apr 2020 17:54:55 +0800
Reference: Deployment/hpa-prom-demo
Metrics: ( current / target )
&amp;quot;nginx_vts_server_requests_per_second&amp;quot; on pods: &amp;lt;unknown&amp;gt; / 10
Min replicas: 2
Max replicas: 5
Deployment pods: 1 current / 2 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True SucceededRescale the HPA controller was able to update the target scale to 2
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal SuccessfulRescale 7s horizontal-pod-autoscaler New size: 2; reason: Current number of replicas below Spec.MinReplicas
&lt;/code>&lt;/pre>
&lt;p>可以看到 HPA 对象已经生效了，会应用最小的副本数 2，所以会新增一个 Pod 副本：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -l app=nginx-server
NAME READY STATUS RESTARTS AGE
hpa-prom-demo-755bb56f85-s5dzf 1/1 Running 0 67s
hpa-prom-demo-755bb56f85-wbpfr 1/1 Running 0 3m30s
&lt;/code>&lt;/pre>
&lt;p>接下来我们同样对应用进行压测：&lt;/p>
&lt;pre>&lt;code>$ while true; do wget -q -O- http://k8s.qikqiak.com:32408; done
&lt;/code>&lt;/pre>
&lt;p>打开另外一个终端观察 HPA 对象的变化：&lt;/p>
&lt;pre>&lt;code>$ kubectl get hpa
NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE
nginx-custom-hpa Deployment/hpa-prom-demo 14239m/10 2 5 2 4m27s
$ kubectl describe hpa nginx-custom-hpa
Name: nginx-custom-hpa
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: kubectl.kubernetes.io/last-applied-configuration:
{&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling/v2beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;HorizontalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-custom-hpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;d...
CreationTimestamp: Tue, 07 Apr 2020 17:54:55 +0800
Reference: Deployment/hpa-prom-demo
Metrics: ( current / target )
&amp;quot;nginx_vts_server_requests_per_second&amp;quot; on pods: 14308m / 10
Min replicas: 2
Max replicas: 5
Deployment pods: 3 current / 3 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True ReadyForNewScale recommended size matches current size
ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric nginx_vts_server_requests_per_second
ScalingLimited False DesiredWithinRange the desired count is within the acceptable range
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal SuccessfulRescale 5m2s horizontal-pod-autoscaler New size: 2; reason: Current number of replicas below Spec.MinReplicas
Normal SuccessfulRescale 61s horizontal-pod-autoscaler New size: 3; reason: pods metric nginx_vts_server_requests_per_second above target
&lt;/code>&lt;/pre>
&lt;p>可以看到指标 &lt;code>nginx_vts_server_requests_per_second&lt;/code> 的数据已经超过阈值了，触发扩容动作了，副本数变成了 3，但是后续很难继续扩容了，这是因为上面我们的 &lt;code>while&lt;/code> 命令并不够快，3 个副本完全可以满足每秒不超过 10 个请求的阈值。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kpk0nr/1616116806033-505c410e-406f-4fda-bc27-030171a8a558.png" alt="">&lt;/p>
&lt;p>nginx_vts_server_requests_per_second&lt;/p>
&lt;p>如果需要更好的进行测试，我们可以使用一些压测工具，比如 ab、fortio 等工具。当我们中断测试后，默认 5 分钟过后就会自动缩容：&lt;/p>
&lt;pre>&lt;code>$ kubectl describe hpa nginx-custom-hpa
Name: nginx-custom-hpa
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: kubectl.kubernetes.io/last-applied-configuration:
{&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling/v2beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;HorizontalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-custom-hpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;d...
CreationTimestamp: Tue, 07 Apr 2020 17:54:55 +0800
Reference: Deployment/hpa-prom-demo
Metrics: ( current / target )
&amp;quot;nginx_vts_server_requests_per_second&amp;quot; on pods: 533m / 10
Min replicas: 2
Max replicas: 5
Deployment pods: 2 current / 2 desired
Conditions:
Type Status Reason Message
---- ------ ------ -------
AbleToScale True ReadyForNewScale recommended size matches current size
ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric nginx_vts_server_requests_per_second
ScalingLimited True TooFewReplicas the desired replica count is less than the minimum replica count
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal SuccessfulRescale 23m horizontal-pod-autoscaler New size: 2; reason: Current number of replicas below Spec.MinReplicas
Normal SuccessfulRescale 19m horizontal-pod-autoscaler New size: 3; reason: pods metric nginx_vts_server_requests_per_second above target
Normal SuccessfulRescale 4m2s horizontal-pod-autoscaler New size: 2; reason: All metrics below target
&lt;/code>&lt;/pre>
&lt;p>到这里我们就完成了使用自定义的指标对应用进行自动扩缩容的操作。如果 Prometheus 安装在我们的 Kubernetes 集群之外，则只需要确保可以从集群访问到查询的端点，并在 adapter 的部署清单中对其进行更新即可。在更复杂的场景中，可以获取多个指标结合使用来制定扩展策略。&lt;/p></description></item><item><title>Docs: kubeadm 命令行工具</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/kubeadm-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/kubeadm-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description/></item><item><title>Docs: kubectl 命令行工具</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/kubectl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/kubectl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description/></item><item><title>Docs: Kubernetes 管理</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%AE%A1%E7%90%86/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;h1 id="telepresence">Telepresence&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/telepresenceio/telepresence">GitHub 项目，telepresenceio/telepresence&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/FhpgIqqbJeeGNjSqzMdP8Q">公众号-马哥 Linux 运维，K8S 运维开发调试神器 Telepresence 实践及踩坑记&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote></description></item><item><title>Docs: Kubernetes 管理案例</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%AE%A1%E7%90%86%E6%A1%88%E4%BE%8B/</guid><description/></item><item><title>Docs: Kubernetes 监控</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%9B%91%E6%8E%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%9B%91%E6%8E%A7/</guid><description/></item><item><title>Docs: Kubernetes 日志</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/</guid><description/></item><item><title>Docs: Quota(配额)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Quota%E9%85%8D%E9%A2%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/Quota%E9%85%8D%E9%A2%9D/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/limit-range/">官方文档,概念-策略-LimitRange&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">官方文档,概念-策略-ResourceQuotas&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h1 id="namespace-中的资源配额">namespace 中的资源配额&lt;/h1>
&lt;p>官方文档：&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/&lt;/a>&lt;/p>
&lt;p>当多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。比如，不同团队使用不同的 namespace，然后给该 namespace 进行资源限制即可&lt;/p>
&lt;p>目前有两种 k8s 对象分配管理相关的控制策略&lt;/p>
&lt;h2 id="limitrange限制范围">LimitRange(限制范围)&lt;/h2>
&lt;p>设定 pod 等对象的默认资源消耗以及可以消耗的资源范围&lt;/p>
&lt;p>官方文档：&lt;/p>
&lt;ul>
&lt;li>概念：&lt;a href="https://kubernetes.io/docs/concepts/policy/limit-range/">https://kubernetes.io/docs/concepts/policy/limit-range/&lt;/a>&lt;/li>
&lt;li>用法：
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/&lt;/a>&lt;/li>
&lt;li>&amp;hellip;..等等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="resourcequota资源配额">ResourceQuota(资源配额)&lt;/h2>
&lt;p>基于 namespace，限制该 namesapce 下的总体资源的创建和消耗&lt;/p>
&lt;p>官方文档：&lt;/p>
&lt;ul>
&lt;li>概念：&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">https://kubernetes.io/docs/concepts/policy/resource-quotas/&lt;/a>&lt;/li>
&lt;li>用法：
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/&lt;/a> # 为指定的 API 对象设置 resourceQuota&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>资源配额分为三种类型：&lt;/p>
&lt;ul>
&lt;li>计算资源配额&lt;/li>
&lt;li>存储资源配额&lt;/li>
&lt;li>对象数量配额&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结&lt;/h2>
&lt;ul>
&lt;li>仅设置 ResourceQuota 时，如果不再 pod 上设置资源的需求和限制，则无法成功创建 pod，需要配合 LimitRange 设置 pod 的默认需求和限制，才可成功创建 pod&lt;/li>
&lt;li>两种控制策略的作用范围都是对于某一 namespace
&lt;ul>
&lt;li>ResourceQuota 用来限制 namespace 中所有的 Pod 占用的总的资源 request 和 limit&lt;/li>
&lt;li>LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值，还有，Pod 的可用资源的 request 和 limit 值的最大与最小值。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="简单的应用示例">简单的应用示例&lt;/h1>
&lt;p>Note：polinux/stress 这是一个非常好用的压测容器，可以对容器指定其所使用的内存和 cpu 等资源的大小。当创建完资源配合等资源限制的对象后，可以通过该容器来测试资源限制是否生效。&lt;/p>
&lt;h2 id="配置计算资源配额">配置计算资源配额&lt;/h2>
&lt;p>为 test 名称空间分配了如下配合，最多能建立 2 个 pod，最多 request 的 cpu 数量为 2 个，内存为 10G，最多 limit 的 cpu 数量为 4 个，内存为 20G&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ResourceQuota&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">compute-resources&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hard&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">pods&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests.cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requests.memory&lt;/span>: &lt;span style="color:#ae81ff">10Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits.cpu&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;4&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits.memory&lt;/span>: &lt;span style="color:#ae81ff">20Gi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="配置-api-对象数量限制">配置 API 对象数量限制&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ResourceQuota&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">object-counts&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hard&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">configmaps&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">persistentvolumeclaims&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;4&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">replicationcontrollers&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">secrets&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">services&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;10&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">services.loadbalancers&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="配置-cpu-和内存-limitrange">配置 CPU 和内存 LimitRange&lt;/h2>
&lt;p>test 名称空间下的 pod 启动后，默认 request 的 cpu 为 0.5，内存为 256M，默认 limit 的 cpu 为 1，内存为 512M&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">LimitRange&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">limit-range&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">test&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">limits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">default&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#ae81ff">512Mi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">defaultRequest&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">memory&lt;/span>: &lt;span style="color:#ae81ff">256Mi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cpu&lt;/span>: &lt;span style="color:#ae81ff">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">Container&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note:&lt;/p>
&lt;ul>
&lt;li>default 即 limit 的值&lt;/li>
&lt;li>defaultRequest 即 request 的值&lt;/li>
&lt;/ul>
&lt;p>在 limits 字段下还有其他的可用字段如下：&lt;/p>
&lt;ul>
&lt;li>max 代表 limit 的最大值&lt;/li>
&lt;li>min 代表 request 的最小值&lt;/li>
&lt;li>maxLimitRequestRatio 代表 limit / request 的最大值。由于节点是根据 pod request 调度资源，可以做到节点超卖，maxLimitRequestRatio 代表 pod 最大超卖比例。&lt;/li>
&lt;/ul></description></item><item><title>Docs: 好用的镜像-有特殊功能</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E5%A5%BD%E7%94%A8%E7%9A%84%E9%95%9C%E5%83%8F-%E6%9C%89%E7%89%B9%E6%AE%8A%E5%8A%9F%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E5%A5%BD%E7%94%A8%E7%9A%84%E9%95%9C%E5%83%8F-%E6%9C%89%E7%89%B9%E6%AE%8A%E5%8A%9F%E8%83%BD/</guid><description>
&lt;p>polinux/stress&lt;/p>
&lt;p>一个非常好用的压测容器，可以对容器指定其所使用的内存和 cpu 等资源的大小。当创建完资源配合等资源限制的对象后，可以通过该容器来测试资源限制是否生效。&lt;/p>
&lt;p>containous/whoami&lt;/p>
&lt;p>一个 go 语言编写的 web 服务器，当请求该容器时，可以输出操作系统信息和 HTTP 请求等，信息如下所示：包括当前容器的 ip 地址，容器的主机名等等&lt;/p>
&lt;pre>&lt;code>Hostname: whoami-bd6b677dc-7tq7h
IP: 127.0.0.1
IP: 10.252.131.122
RemoteAddr: 127.0.0.1:35358
GET /notls HTTP/1.1
Host: 10.10.9.51:30272
User-Agent: curl/7.29.0
Accept: */*
Accept-Encoding: gzip
X-Forwarded-For: 10.10.9.51
X-Forwarded-Host: 10.10.9.51:30272
X-Forwarded-Port: 30272
X-Forwarded-Proto: http
X-Forwarded-Server: traefik-6fbbb464b5-mcq99
X-Real-Ip: 10.10.9.51
&lt;/code>&lt;/pre>
&lt;h1 id="kiwigridk8s-sidecar">kiwigrid/k8s-sidecar&lt;/h1>
&lt;p>参考：&lt;a href="https://github.com/kiwigrid/k8s-sidecar">GitHub 项目&lt;/a>&lt;/p>
&lt;p>该容器会持续监听指定的 configmap 和 secret 资源，当 configmap 或 secret 对象被创建或更新时，会将该对象内的数据，转换成文件，并保存在容器内指定的路径中。&lt;/p>
&lt;p>这个&lt;strong>镜像常常作为 sidecar 容器使用&lt;/strong>，与主容器共享相同目录，这样，主程序就可以实时读取到新创建的 configmap 或 secret&lt;/p>
&lt;p>比如，该容器可以与 Grafana 一起使用，用来为 Grafana 实时提供 provisioning 功能的 dashboard。kiwigrid/k8s-sidecar 容器与 Grafana 容器 首先挂载相同的目录。此时，我们可以为每个 dashboard 都创建一个 configmap，然后带上 kiwigrid/k8s-sidecar 容器所需的标签。这样每当创建或修改一个仪表盘时， kiwigrid/k8s-sidecar 容器就会将 configmap 变为文件，并保存到与 Grafana 相同挂载的目录，此时，Grafana 的 provisioning 功能定时扫描该目录时，就会加载到相关的仪表盘&lt;/p></description></item><item><title>Docs: 性能优化与故障处理</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</guid><description/></item><item><title>Docs: 重大变化</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E9%87%8D%E5%A4%A7%E5%8F%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E9%87%8D%E5%A4%A7%E5%8F%98%E5%8C%96/</guid><description/></item></channel></rss>