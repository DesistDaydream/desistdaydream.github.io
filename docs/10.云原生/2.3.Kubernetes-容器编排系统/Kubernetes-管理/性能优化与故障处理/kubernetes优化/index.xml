<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦 – kubernetes优化</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/</link><description>Recent content in kubernetes优化 on 断念梦</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: K8s 集群稳定性：LIST 请求源码分析、性能评估与大规模基础服务部署调优</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/K8s-%E9%9B%86%E7%BE%A4%E7%A8%B3%E5%AE%9A%E6%80%A7LIST-%E8%AF%B7%E6%B1%82%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E4%B8%8E%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E8%B0%83%E4%BC%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/K8s-%E9%9B%86%E7%BE%A4%E7%A8%B3%E5%AE%9A%E6%80%A7LIST-%E8%AF%B7%E6%B1%82%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E4%B8%8E%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E8%B0%83%E4%BC%98/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://mp.weixin.qq.com/s/fcytXp2skFIjbYBLs5VzSQ">https://mp.weixin.qq.com/s/fcytXp2skFIjbYBLs5VzSQ&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/">https://arthurchiao.art/blog/k8s-reliability-list-data-zh/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Published at 2022-05-19 | Last Update 2022-05-19&lt;/p>
&lt;p>对于非结构化的数据存储系统来说，LIST 操作通常都是非常重量级的，不仅占用大量的 磁盘 IO、网络带宽和 CPU，而且会影响同时间段的其他请求（尤其是响应延迟要求极高的 选主请求），是集群稳定性的一大杀手。&lt;/p>
&lt;p>例如，对于 Ceph 对象存储来说，每个 LIST bucket 请求都需要去多个磁盘中捞出这个 bucket 的全部数据；不仅自身很慢，还影响了同一时间段内的其他普通读写请求，因为 IO 是共享的，导致响应延迟上升乃至超时。如果 bucket 内的对象非常多（例如用作 harbor/docker-registry 的存储后端），LIST 操作甚至都无法在常规时间内完成（ 因而依赖 LIST bucket 操作的 registry GC 也就跑不起来）。&lt;/p>
&lt;p>又如 KV 存储 etcd。相比于 Ceph，一个实际 etcd 集群存储的数据量可能很小（几个 ~ 几十个 GB），甚至足够缓存到内存中。但与 Ceph 不同的是，它的并发请求数量可能会高 几个量级，比如它是一个 ~4000 nodes 的 k8s 集群的 etcd。单个 LIST 请求可能只需要 返回几十 MB 到上 GB 的流量，但并发请求一多，etcd 显然也扛不住，所以最好在前面有 一层缓存，这就是 apiserver 的功能（之一）。K8s 的 LIST 请求大部分都应该被 apiserver 挡住，从它的本地缓存提供服务，但如果使用不当，就会跳过缓存直接到达 etcd，有很大的稳定性风险。&lt;/p>
&lt;p>本文深入研究 k8s apiserver/etcd 的 LIST 操作处理逻辑和性能瓶颈，并提供一些基础服务的 LIST 压力测试、 部署和调优建议，提升大规模 K8s 集群的稳定性。&lt;/p>
&lt;p>&lt;code>kube-apiserver&lt;/code> &lt;code>LIST&lt;/code> 请求处理逻辑：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/91668091-37cf-4cda-ad10-f9ffd85129af/apiserver-processing-list-request.png" alt="">&lt;/p>
&lt;p>代码基于 v1.24.0，不过 1.19~1.24 的基本逻辑和代码路径是一样的，有需要可对照参考。&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;a href="#1-%E5%BC%95%E8%A8%80">1 引言&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#11-k8s-%E6%9E%B6%E6%9E%84%E7%8E%AF%E5%BD%A2%E5%B1%82%E6%AC%A1%E8%A7%86%E5%9B%BE">1.1 K8s 架构：环形层次视图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-apiserveretcd-%E8%A7%92%E8%89%B2">1.2 &lt;/a>&lt;code>[apiserver/etcd](#12-apiserveretcd-%E8%A7%92%E8%89%B2)&lt;/code>&lt;a href="#12-apiserveretcd-%E8%A7%92%E8%89%B2"> 角色&lt;/a>&lt;/li>
&lt;li>&lt;a href="#13-apiserveretcd-list-%E5%BC%80%E9%94%80">1.3 &lt;/a>&lt;code>[apiserver/etcd](#13-apiserveretcd-list-%E5%BC%80%E9%94%80)&lt;/code>&lt;a href="#13-apiserveretcd-list-%E5%BC%80%E9%94%80"> List 开销&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#131-%E8%AF%B7%E6%B1%82%E4%B8%BE%E4%BE%8B">1.3.1 请求举例&lt;/a>&lt;/li>
&lt;li>&lt;a href="#132-%E5%A4%84%E7%90%86%E5%BC%80%E9%94%80">1.3.2 处理开销&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#14-%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%83%A8%E7%BD%B2%E6%97%B6%E6%BD%9C%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98">1.4 大规模部署时潜在的问题&lt;/a>&lt;/li>
&lt;li>&lt;a href="#15-%E6%9C%AC%E6%96%87%E7%9B%AE%E7%9A%84">1.5 本文目的&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-apiserver-list-%E6%93%8D%E4%BD%9C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90">2 apiserver &lt;/a>&lt;code>[List()](#2-apiserver-list-%E6%93%8D%E4%BD%9C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90)&lt;/code>&lt;a href="#2-apiserver-list-%E6%93%8D%E4%BD%9C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"> 操作源码分析&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-%E8%B0%83%E7%94%A8%E6%A0%88%E5%92%8C%E6%B5%81%E7%A8%8B%E5%9B%BE">2.1 调用栈和流程图&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E5%85%A5%E5%8F%A3list">2.2 请求处理入口：&lt;/a>&lt;code>[List()](#22-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E5%85%A5%E5%8F%A3list)&lt;/code>&lt;/li>
&lt;li>&lt;a href="#23-listpredicate">2.3 &lt;/a>&lt;code>[ListPredicate()](#23-listpredicate)&lt;/code>&lt;/li>
&lt;li>&lt;a href="#24-%E8%AF%B7%E6%B1%82%E6%8C%87%E5%AE%9A%E4%BA%86%E8%B5%84%E6%BA%90%E5%90%8Dresource-name%E8%8E%B7%E5%8F%96%E5%8D%95%E4%B8%AA%E5%AF%B9%E8%B1%A1">2.4 请求指定了资源名（resource name）：获取单个对象&lt;/a>&lt;/li>
&lt;li>&lt;a href="#25-%E8%AF%B7%E6%B1%82%E6%9C%AA%E6%8C%87%E5%AE%9A%E8%B5%84%E6%BA%90%E5%90%8D%E8%8E%B7%E5%8F%96%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE%E5%81%9A%E8%BF%87%E6%BB%A4">2.5 请求未指定资源名，获取全量数据做过滤&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#251-apiserver-%E7%BC%93%E5%AD%98%E5%B1%82getlist-%E5%A4%84%E7%90%86%E9%80%BB%E8%BE%91">2.5.1 apiserver 缓存层：&lt;/a>&lt;code>[GetList()](#251-apiserver-%E7%BC%93%E5%AD%98%E5%B1%82getlist-%E5%A4%84%E7%90%86%E9%80%BB%E8%BE%91)&lt;/code>&lt;a href="#251-apiserver-%E7%BC%93%E5%AD%98%E5%B1%82getlist-%E5%A4%84%E7%90%86%E9%80%BB%E8%BE%91"> 处理逻辑&lt;/a>&lt;/li>
&lt;li>&lt;a href="#252-%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E5%BF%85%E9%A1%BB%E4%BB%8E-etcd-%E8%AF%BB%E6%95%B0%E6%8D%AEshoulddelegatelist">2.5.2 判断是否必须从 etcd 读数据：&lt;/a>&lt;code>[shouldDelegateList()](#252-%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E5%BF%85%E9%A1%BB%E4%BB%8E-etcd-%E8%AF%BB%E6%95%B0%E6%8D%AEshoulddelegatelist)&lt;/code>&lt;/li>
&lt;li>&lt;a href="#253-%E6%83%85%E5%86%B5%E4%B8%80listoption-%E8%A6%81%E6%B1%82%E4%BB%8E-etcd-%E8%AF%BB%E6%95%B0%E6%8D%AE">2.5.3 情况一：ListOption 要求从 etcd 读数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="#254-%E6%83%85%E5%86%B5%E4%BA%8C%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%E8%BF%98%E6%B2%A1%E5%BB%BA%E5%A5%BD%E5%8F%AA%E8%83%BD%E4%BB%8E-etcd-%E8%AF%BB%E6%95%B0%E6%8D%AE">2.5.4 情况二：本地缓存还没建好，只能从 etcd 读数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="#255-%E6%83%85%E5%86%B5%E4%B8%89%E4%BD%BF%E7%94%A8%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98">2.5.5 情况三：使用本地缓存&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-list-%E6%B5%8B%E8%AF%95">3 LIST 测试&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-%E6%8C%87%E5%AE%9A-limit2response-%E5%B0%86%E8%BF%94%E5%9B%9E%E5%88%86%E9%A1%B5%E4%BF%A1%E6%81%AFcontinue">3.1 指定 &lt;/a>&lt;code>[limit=2](#31-%E6%8C%87%E5%AE%9A-limit2response-%E5%B0%86%E8%BF%94%E5%9B%9E%E5%88%86%E9%A1%B5%E4%BF%A1%E6%81%AFcontinue)&lt;/code>&lt;a href="#31-%E6%8C%87%E5%AE%9A-limit2response-%E5%B0%86%E8%BF%94%E5%9B%9E%E5%88%86%E9%A1%B5%E4%BF%A1%E6%81%AFcontinue">：response 将返回分页信息（&lt;/a>&lt;code>[continue](#31-%E6%8C%87%E5%AE%9A-limit2response-%E5%B0%86%E8%BF%94%E5%9B%9E%E5%88%86%E9%A1%B5%E4%BF%A1%E6%81%AFcontinue)&lt;/code>&lt;a href="#31-%E6%8C%87%E5%AE%9A-limit2response-%E5%B0%86%E8%BF%94%E5%9B%9E%E5%88%86%E9%A1%B5%E4%BF%A1%E6%81%AFcontinue">）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#311-curl-%E6%B5%8B%E8%AF%95">3.1.1 &lt;/a>&lt;code>[curl](#311-curl-%E6%B5%8B%E8%AF%95)&lt;/code>&lt;a href="#311-curl-%E6%B5%8B%E8%AF%95"> 测试&lt;/a>&lt;/li>
&lt;li>&lt;a href="#312-kubectl-%E6%B5%8B%E8%AF%95">3.1.2 &lt;/a>&lt;code>[kubectl](#312-kubectl-%E6%B5%8B%E8%AF%95)&lt;/code>&lt;a href="#312-kubectl-%E6%B5%8B%E8%AF%95"> 测试&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#32-%E6%8C%87%E5%AE%9A-limit2resourceversion0limit2-%E5%B0%86%E8%A2%AB%E5%BF%BD%E7%95%A5%E8%BF%94%E5%9B%9E%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE">3.2 指定 &lt;/a>&lt;code>[limit=2&amp;amp;resourceVersion=0](#32-%E6%8C%87%E5%AE%9A-limit2resourceversion0limit2-%E5%B0%86%E8%A2%AB%E5%BF%BD%E7%95%A5%E8%BF%94%E5%9B%9E%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE)&lt;/code>&lt;a href="#32-%E6%8C%87%E5%AE%9A-limit2resourceversion0limit2-%E5%B0%86%E8%A2%AB%E5%BF%BD%E7%95%A5%E8%BF%94%E5%9B%9E%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE">：&lt;/a>&lt;code>[limit=2](#32-%E6%8C%87%E5%AE%9A-limit2resourceversion0limit2-%E5%B0%86%E8%A2%AB%E5%BF%BD%E7%95%A5%E8%BF%94%E5%9B%9E%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE)&lt;/code>&lt;a href="#32-%E6%8C%87%E5%AE%9A-limit2resourceversion0limit2-%E5%B0%86%E8%A2%AB%E5%BF%BD%E7%95%A5%E8%BF%94%E5%9B%9E%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE"> 将被忽略，返回全量数据&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-%E6%8C%87%E5%AE%9A-specnodenamenode1resourceversion0-vs-specnodenamenode1">3.3 指定 &lt;/a>&lt;code>[spec.nodeName=node1&amp;amp;resourceVersion=0](#33-%E6%8C%87%E5%AE%9A-specnodenamenode1resourceversion0-vs-specnodenamenode1)&lt;/code>&lt;a href="#33-%E6%8C%87%E5%AE%9A-specnodenamenode1resourceversion0-vs-specnodenamenode1"> vs. &lt;/a>&lt;code>[spec.nodeName=node1&amp;quot;](#33-%E6%8C%87%E5%AE%9A-specnodenamenode1resourceversion0-vs-specnodenamenode1)&lt;/code>
&lt;ul>
&lt;li>&lt;a href="#%E7%BB%93%E6%9E%9C%E7%9B%B8%E5%90%8C">结果相同&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E9%80%9F%E5%BA%A6%E5%B7%AE%E5%BC%82%E5%BE%88%E5%A4%A7">速度差异很大&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-list-%E8%AF%B7%E6%B1%82%E5%AF%B9%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E5%8E%8B%E5%8A%9B%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90">4 LIST 请求对控制平面压力：量化分析&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-%E6%94%B6%E9%9B%86-list-%E8%AF%B7%E6%B1%82">4.1 收集 LIST 请求&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-%E6%B5%8B%E8%AF%95-list-%E8%AF%B7%E6%B1%82%E6%95%B0%E6%8D%AE%E9%87%8F%E5%92%8C%E8%80%97%E6%97%B6">2.2 测试 LIST 请求数据量和耗时&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">4.3 测试结果分析&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E5%92%8C%E8%B0%83%E4%BC%98%E5%BB%BA%E8%AE%AE">5 大规模基础服务：部署和调优建议&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-list-%E8%AF%B7%E6%B1%82%E9%BB%98%E8%AE%A4%E8%AE%BE%E7%BD%AE-resourceversion0">5.1 List 请求默认设置 &lt;/a>&lt;code>[ResourceVersion=0](#51-list-%E8%AF%B7%E6%B1%82%E9%BB%98%E8%AE%A4%E8%AE%BE%E7%BD%AE-resourceversion0)&lt;/code>&lt;/li>
&lt;li>&lt;a href="#52-%E4%BC%98%E5%85%88%E4%BD%BF%E7%94%A8-namespaced-api">5.2 优先使用 namespaced API&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-restart-backoff">5.3 Restart backoff&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-%E4%BC%98%E5%85%88%E9%80%9A%E8%BF%87-labelfield-selector-%E5%9C%A8%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%81%9A%E8%BF%87%E6%BB%A4">5.4 优先通过 label/field selector 在服务端做过滤&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#541-label-selector">5.4.1 Label selector&lt;/a>&lt;/li>
&lt;li>&lt;a href="#542-field-selector">5.4.2 Field selector&lt;/a>&lt;/li>
&lt;li>&lt;a href="#543-namespace-selector">5.4.3 Namespace selector&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#55-%E9%85%8D%E5%A5%97%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6%E7%AD%89">5.5 配套基础设施（监控、告警等）&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#551-%E4%BD%BF%E7%94%A8%E7%8B%AC%E7%AB%8B-serviceaccount">5.5.1 使用独立 ServiceAccount&lt;/a>&lt;/li>
&lt;li>&lt;a href="#552-liveness-%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6">5.5.2 Liveness 监控告警&lt;/a>&lt;/li>
&lt;li>&lt;a href="#553-%E7%9B%91%E6%8E%A7%E5%92%8C%E8%B0%83%E4%BC%98-etcd">5.5.3 监控和调优 etcd&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-%E5%85%B6%E4%BB%96">6 其他&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-get-%E8%AF%B7%E6%B1%82getoptions">6.1 Get 请求：&lt;/a>&lt;code>[GetOptions{}](#61-get-%E8%AF%B7%E6%B1%82getoptions)&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="11-k8s-架构环形层次视图">1.1 K8s 架构：环形层次视图&lt;/h2>
&lt;p>从架构层次和组件依赖角度，可以将一个 K8s 集群和一台 Linux 主机做如下类比：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/91668091-37cf-4cda-ad10-f9ffd85129af/linux-node-vs-k8s-cluster.png" alt="">&lt;/p>
&lt;p>Fig 1. Anology: a Linux host and a Kubernetes cluster&lt;/p>
&lt;p>对于 K8s 集群，从内到外的几个组件和功能：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>etcd&lt;/strong>：持久化 KV 存储，集群资源（pods/services/networkpolicies/…）的唯一的权威数据（状态）源；&lt;/li>
&lt;li>&lt;strong>apiserver&lt;/strong>：从 etcd 读取（&lt;code>**ListWatch**&lt;/code>）全量数据，并缓存在内存中；&lt;strong>无状态服务&lt;/strong>，可水平扩展；&lt;/li>
&lt;li>各种&lt;strong>基础服务&lt;/strong>（e.g. &lt;code>kubelet&lt;/code>、&lt;code>*-agent&lt;/code>、&lt;code>*-operator&lt;/code>）：连接 apiserver，获取（&lt;code>**List/ListWatch**&lt;/code>）各自需要的数据；&lt;/li>
&lt;li>集群内的 &lt;strong>workloads&lt;/strong>：在 1 和 2 正常的情况下由 3 来创建、管理和 reconcile，例如 kubelet 创建 pod、cilium 配置网络和安全策略。&lt;/li>
&lt;/ol>
&lt;h2 id="12-apiserveretcd-角色">1.2 &lt;code>apiserver/etcd&lt;/code> 角色&lt;/h2>
&lt;p>以上可以看到，系统路径中存在&lt;strong>两级 List/ListWatch&lt;/strong>（但数据是同一份）：&lt;/p>
&lt;ol>
&lt;li>apiserver List/ListWatch etcd&lt;/li>
&lt;li>基础服务 List/ListWatch apiserver&lt;/li>
&lt;/ol>
&lt;p>因此，从最简形式上来说，&lt;strong>apiserver 就是挡在 etcd 前面的一个代理&lt;/strong>（proxy），&lt;/p>
&lt;pre>&lt;code> +--------+ +---------------+ +------------+
| Client | -----------&amp;gt; | Proxy (cache) | --------------&amp;gt; | Data store |
+--------+ +---------------+ +------------+
infra services apiserver etcd
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>绝大部分情况下，apiserver 直接从本地缓存提供服务（因为它缓存了集群全量数据）；&lt;/li>
&lt;li>某些特殊情况，例如，&lt;/li>
&lt;li>&lt;strong>客户端明确要求从 etcd 读数据&lt;/strong>（追求最高的数据准确性），&lt;/li>
&lt;li>&lt;strong>apiserver 本地缓存还没建好&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>apiserver 就只能将请求转发给 etcd —— &lt;strong>这里就要特别注意了&lt;/strong> —— 客户端 LIST 参数设置不当也可能会走到这个逻辑。&lt;/p>
&lt;h2 id="13-apiserveretcd-list-开销">1.3 &lt;code>apiserver/etcd&lt;/code> List 开销&lt;/h2>
&lt;h3 id="131-请求举例">1.3.1 请求举例&lt;/h3>
&lt;p>考虑下面几个 LIST 操作：&lt;/p>
&lt;ol>
&lt;li>&lt;code>**LIST apis/cilium.io/v2/ciliumendpoints?limit=500&amp;amp;resourceVersion=0**&lt;/code>
这里同时传了两个参数，但 &lt;code>resourceVersion=0&lt;/code> 会导致 apiserver 忽略 &lt;code>limit=500&lt;/code>， 所以客户端拿到的是全量 ciliumendpoints 数据。
一种资源的全量数据可能是比较大的，&lt;strong>需要考虑清楚是否真的需要全量数据&lt;/strong>。 后文会介绍&lt;strong>定量测量与分析&lt;/strong>方法。&lt;/li>
&lt;li>&lt;code>**LIST api/v1/pods?filedSelector=spec.nodeName%3Dnode1**&lt;/code>
这个请求是获取 &lt;code>node1&lt;/code> 上的所有 pods（&lt;code>%3D&lt;/code> 是 &lt;code>=&lt;/code> 的转义）。
根据 nodename 做过滤，给人的感觉可能是数据量不太大，但其实背后要比看上去复杂：&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>首先，这里没有指定 resourceVersion=0，导致 &lt;strong>apiserver 跳过缓存，直接去 etcd 读数据&lt;/strong>；&lt;/li>
&lt;li>其次，&lt;strong>etcd 只是 KV 存储，没有按 label/field 过滤功能&lt;/strong>（只处理 &lt;code>limit/continue&lt;/code>），&lt;/li>
&lt;li>所以，apiserver 是从 etcd 拉全量数据，然后在&lt;strong>内存做过滤&lt;/strong>，开销也是很大的，后文有代码分析。&lt;/li>
&lt;/ul>
&lt;p>这种行为是要避免的，除非对数据准确性有极高要求，特意要绕过 apiserver 缓存。&lt;/p>
&lt;ol start="3">
&lt;li>&lt;code>**LIST api/v1/pods?filedSelector=spec.nodeName%3Dnode1&amp;amp;resourceVersion=0**&lt;/code>
跟 2 的区别是加上了 &lt;code>resourceVersion=0&lt;/code>，因此 apiserver 会从缓存读数据，&lt;strong>性能会有量级的提升&lt;/strong>。
但要注意，虽然实际上返回给客户端的可能只有&lt;strong>几百 KB 到上百 MB&lt;/strong>（取决于 node 上 pod 的数量、pod 上 label 的多少等因素）， 但 apiserver 需要处理的数据量可能是&lt;strong>几个 GB&lt;/strong>。 后面会有定量分析。&lt;/li>
&lt;/ol>
&lt;p>以上可以看到，不同的 LIST 操作产生的影响是不一样的，而客户端看到数据还有可能只 是 apiserver/etcd 处理数据的很小一部分。如果基础服务大规模启动或重启， 就极有可能把控制平面打爆。&lt;/p>
&lt;h3 id="132-处理开销">1.3.2 处理开销&lt;/h3>
&lt;p>List 请求可以分为两种：&lt;/p>
&lt;ol>
&lt;li>List 全量数据：开销主要花在数据传输；&lt;/li>
&lt;li>指定用 label 或字段（field）过滤，只需要匹配的数据。&lt;/li>
&lt;/ol>
&lt;p>这里需要特别说明的是第二种情况，也就是 list 请求带了过滤条件。&lt;/p>
&lt;ul>
&lt;li>大部分情况下，apiserver 会用自己的缓存做过滤，这个很快，因此&lt;strong>耗时主要花在数据传输&lt;/strong>；&lt;/li>
&lt;li>需要将请求转给 etcd 的情况，
前面已经提到，etcd 只是 KV 存储，并不理解 label/field 信息，因此它无法处理过滤请求。 实际的过程是：&lt;strong>apiserver 从 etcd 拉全量数据，然后在内存做过滤&lt;/strong>，再返回给客户端。
因此除了数据传输开销（网络带宽），这种情况下还会占用大量 apiserver &lt;strong>CPU 和内存&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h2 id="14-大规模部署时潜在的问题">1.4 大规模部署时潜在的问题&lt;/h2>
&lt;p>再来看个例子，下面这行代码用 k8s client-go 根据 nodename 过滤 pod，&lt;/p>
&lt;pre>&lt;code> podList, err := Client().CoreV1().Pods(&amp;quot;&amp;quot;).List(ctx(), ListOptions{FieldSelector: &amp;quot;spec.nodeName=node1&amp;quot;})
&lt;/code>&lt;/pre>
&lt;p>看起来非常简单的操作，我们来实际看一下它背后的数据量。 以一个 4000 node，10w pod 的集群为例，&lt;strong>全量 pod 数据量&lt;/strong>：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>etcd 中&lt;/strong>：紧凑的非结构化 KV 存储，在 &lt;strong>1GB 量级&lt;/strong>；&lt;/li>
&lt;li>&lt;strong>apiserver 缓存中&lt;/strong>：已经是结构化的 golang objects，在 &lt;strong>2GB 量级&lt;/strong>（ TODO：需进一步确认）；&lt;/li>
&lt;li>&lt;strong>apiserver 返回&lt;/strong>：client 一般选择默认的 json 格式接收， 也已经是结构化数据。全量 pod 的 json 也在 &lt;strong>2GB 量级&lt;/strong>。&lt;/li>
&lt;/ol>
&lt;p>可以看到，某些请求看起来很简单，只是客户端一行代码的事情，但背后的数据量是惊人的。 指定按 nodeName 过滤 pod 可能只返回了 500KB 数据，但 apiserver 却需要过滤 2GB 数据 —— &lt;strong>最坏的情况，etcd 也要跟着处理 1GB 数据&lt;/strong> （以上参数配置确实命中了最坏情况，见下文代码分析）。&lt;/p>
&lt;p>集群规模比较小的时候，这个问题可能看不出来（etcd 在 LIST 响应延迟超过某个阈值 后才开始打印 warning 日志）；规模大了之后，如果这样的请求比较多，apiserver/etcd 肯定是扛不住的。&lt;/p>
&lt;h2 id="15-本文目的">1.5 本文目的&lt;/h2>
&lt;p>通过深入代码查看 k8s 的 List/ListWatch 实现，加深对性能问题的理解，对大规模 K8s 集群的稳定性优化提供一些参考。&lt;/p>
&lt;p>有了以上理论预热，接下来可以看代码实现了。&lt;/p>
&lt;h2 id="21-调用栈和流程图">2.1 调用栈和流程图&lt;/h2>
&lt;pre>&lt;code>store.List
|-store.ListPredicate
|-if opt == nil
| opt = ListOptions{ResourceVersion: &amp;quot;&amp;quot;}
|-Init SelectionPredicate.Limit/Continue fileld
|-list := e.NewListFunc() // objects will be stored in this list
|-storageOpts := storage.ListOptions{opt.ResourceVersion, opt.ResourceVersionMatch, Predicate: p}
|
|-if MatchesSingle ok // 1. when &amp;quot;metadata.name&amp;quot; is specified, get single obj
| // Get single obj from cache or etcd
|
|-return e.Storage.List(KeyRootFunc(ctx), storageOpts) // 2. get all objs and perform filtering
|-cacher.List()
| // case 1: list all from etcd and filter in apiserver
|-if shouldDelegateList(opts) // true if resourceVersion == &amp;quot;&amp;quot;
| return c.storage.List // list from etcd
| |- fromRV *int64 = nil
| |- if len(storageOpts.ResourceVersion) &amp;gt; 0
| | rv = ParseResourceVersion
| | fromRV = &amp;amp;rv
| |
| |- for hasMore {
| | objs := etcdclient.KV.Get()
| | filter(objs) // filter by labels or filelds
| | }
|
| // case 2: list &amp;amp; filter from apiserver local cache (memory)
|-if cache.notready()
| return c.storage.List // get from etcd
|
| // case 3: list &amp;amp; filter from apiserver local cache (memory)
|-obj := watchCache.WaitUntilFreshAndGet
|-for elem in obj.(*storeElement)
| listVal.Set() // append results to listOjb
|-return // results stored in listObj
&lt;/code>&lt;/pre>
&lt;p>对应的流程图：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/91668091-37cf-4cda-ad10-f9ffd85129af/apiserver-processing-list-request.png" alt="">&lt;/p>
&lt;p>Fig 2-1. List operation processing in apiserver&lt;/p>
&lt;h2 id="22-请求处理入口list">2.2 请求处理入口：&lt;code>List()&lt;/code>&lt;/h2>
&lt;pre>&lt;code>// https://github.com/kubernetes/kubernetes/blob/v1.24.0/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L361
// 根据 PredicateFunc 中指定的 LabelSelector 和 FieldSelector 过滤，返回一个对象列表
func (e *Store) List(ctx, options *metainternalversion.ListOptions) (runtime.Object, error) {
label := labels.Everything()
if options != nil &amp;amp;&amp;amp; options.LabelSelector != nil
label = options.LabelSelector // Label 过滤器，例如 app=nginx
field := fields.Everything()
if options != nil &amp;amp;&amp;amp; options.FieldSelector != nil
field = options.FieldSelector // 字段过滤器，例如 spec.nodeName=node1
out := e.ListPredicate(ctx, e.PredicateFunc(label, field), options) // 拉取（List）数据并过滤（Predicate）
if e.Decorator != nil
e.Decorator(out)
return out, nil
}
&lt;/code>&lt;/pre>
&lt;h2 id="23-listpredicate">2.3 &lt;code>ListPredicate()&lt;/code>&lt;/h2>
&lt;pre>&lt;code>// https://github.com/kubernetes/kubernetes/blob/v1.24.0/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L411
func (e *Store) ListPredicate(ctx , p storage.SelectionPredicate, options *metainternalversion.ListOptions) (runtime.Object, error) {
// Step 1: 初始化
if options == nil
options = &amp;amp;metainternalversion.ListOptions{ResourceVersion: &amp;quot;&amp;quot;}
p.Limit = options.Limit
p.Continue = options.Continue
list := e.NewListFunc() // 返回结果将存储在这里面
storageOpts := storage.ListOptions{ // 将 API 侧的 ListOption 转成底层存储侧的 ListOption，字段区别见下文
ResourceVersion: options.ResourceVersion,
ResourceVersionMatch: options.ResourceVersionMatch,
Predicate: p,
Recursive: true,
}
// Step 2：如果请求指定了 metadata.name，则应获取单个 object，无需对全量数据做过滤
if name, ok := p.MatchesSingle(); ok { // 检查是否设置了 metadata.name 字段
if key := e.KeyFunc(ctx, name); err == nil { // 获取这个 object 在 etcd 中的 key（唯一或不存在）
storageOpts.Recursive = false
e.Storage.GetList(ctx, key, storageOpts, list)
return list
}
// else 逻辑：如果执行到这里，说明没有从 context 中拿到过滤用的 key，则 fallback 到下面拿全量数据再过滤
}
// Step 3: 对全量数据做过滤
e.Storage.GetList(ctx, e.KeyRootFunc(), storageOpts, list) // KeyRootFunc() 用来获取这种资源在 etcd 里面的 root key（即 prefix，不带最后的 /）
return list
}
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>1.24.0 中 case 1 &amp;amp; 2 都是 调用 &lt;code>e.Storage.GetList()&lt;/code>，之前的版本有点不同：&lt;/p>
&lt;ul>
&lt;li>Case 1 中的 e.Storage.GetToList&lt;/li>
&lt;li>Case 1 中的 e.Storage.List&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;blockquote>
&lt;p>不过基本流程是一样的。&lt;/p>
&lt;/blockquote>
&lt;ol>
&lt;li>
&lt;p>如果客户端没传 &lt;code>**ListOption**&lt;/code>，则初始化一个默认值，其中的 &lt;code>ResourceVersion&lt;/code> 设置为空字符串， 这将使 apiserver &lt;strong>从 etcd 拉取数据来返回给客户端，而不使用本地缓存&lt;/strong>（除非本地缓存还没有建好）；
举例，客户端设置 &lt;code>ListOption{Limit: 5000, ResourceVersion: 0}&lt;/code> list ciliumendpoints 时，发送的请求将为 &lt;code>**/apis/cilium.io/v2/ciliumendpoints?limit=500&amp;amp;resourceVersion=0**&lt;/code>。
&lt;code>ResourceVersion&lt;/code> 为空字符串的行为，后面会看到对它的解析。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用 listoptions 中的字段分别&lt;strong>初始化过滤器&lt;/strong>（SelectionPredicate）的 limit/continue 字段；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>初始化返回结果，&lt;code>list := e.NewListFunc()&lt;/code>；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>将 API 侧的 ListOption 转成底层存储的 ListOption，字段区别见下文
&lt;code>metainternalversion.ListOptions&lt;/code> 是 &lt;strong>API 侧的结构体&lt;/strong>，包含了&lt;/p>
&lt;p>// staging/src/k8s.io/apimachinery/pkg/apis/meta/internalversion/types.go&lt;/p>
&lt;p>// ListOptions is the query options to a standard REST list call.
type ListOptions struct {
metav1.TypeMeta&lt;/p>
&lt;pre>&lt;code> LabelSelector labels.Selector // 标签过滤器，例如 app=nginx
FieldSelector fields.Selector // 字段过滤器，例如 spec.nodeName=node1
Watch bool
AllowWatchBookmarks bool
ResourceVersion string
ResourceVersionMatch metav1.ResourceVersionMatch
TimeoutSeconds *int64 // Timeout for the list/watch call.
Limit int64
Continue string // a token returned by the server. return a 410 error if the token has expired.
&lt;/code>&lt;/pre>
&lt;p>}&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;code>storage.ListOptions&lt;/code> 是传给&lt;strong>底层存储的结构体&lt;/strong>，字段有一些区别：&lt;/p>
&lt;h2 id="24-请求指定了资源名resource-name获取单个对象">2.4 请求指定了资源名（resource name）：获取单个对象&lt;/h2>
&lt;p>接下来根据请求中是否指定了 &lt;code>meta.Name&lt;/code> 分为两种情况：&lt;/p>
&lt;ol>
&lt;li>如果指定了，说明是查询单个对象，因为 &lt;code>Name&lt;/code> 是唯一的，接下来转入查询单个 object 的逻辑；&lt;/li>
&lt;li>如果未指定，则需要&lt;strong>获取全量数据&lt;/strong>，然后在 apiserver 内存中根据 SelectionPredicate 中的过滤条件进行过滤，将最终结果返回给客户端；&lt;/li>
&lt;/ol>
&lt;p>代码如下：&lt;/p>
&lt;pre>&lt;code> // case 1：根据 metadata.name 获取单个 object，无需对全量数据做过滤
if name, ok := p.MatchesSingle(); ok { // 检查是否设置了 metadata.name 字段
if key := e.KeyFunc(ctx, name); err == nil {
e.Storage.GetList(ctx, key, storageOpts, list)
return list
}
// else 逻辑：如果执行到这里，说明没有从 context 中拿到过滤用的 key，则 fallback 到下面拿全量数据再过滤
}
&lt;/code>&lt;/pre>
&lt;p>e.Storage 是一个 Interface，&lt;/p>
&lt;pre>&lt;code>// staging/src/k8s.io/apiserver/pkg/storage/interfaces.go
// Interface offers a common interface for object marshaling/unmarshaling operations and
// hides all the storage-related operations behind it.
type Interface interface {
Create(ctx , key string, obj, out runtime.Object, ttl uint64) error
Delete(ctx , key string, out runtime.Object, preconditions *Preconditions,...)
Watch(ctx , key string, opts ListOptions) (watch.Interface, error)
Get(ctx , key string, opts GetOptions, objPtr runtime.Object) error
// unmarshall objects found at key into a *List api object (an object that satisfies runtime.IsList definition).
// If 'opts.Recursive' is false, 'key' is used as an exact match; if is true, 'key' is used as a prefix.
// The returned contents may be delayed, but it is guaranteed that they will
// match 'opts.ResourceVersion' according 'opts.ResourceVersionMatch'.
GetList(ctx , key string, opts ListOptions, listObj runtime.Object) error
&lt;/code>&lt;/pre>
&lt;p>e.Storage.GetList() 会执行到 cacher 代码。&lt;/p>
&lt;p>不管是获取单个 object，还是获取全量数据，都经历类似的过程：&lt;/p>
&lt;ol>
&lt;li>优先从 apiserver 本地缓存获取（决定因素包括 ResourceVersion 等），&lt;/li>
&lt;li>不得已才到 etcd 去获取；&lt;/li>
&lt;/ol>
&lt;p>获取单个对象的逻辑相对比较简单，这里就不看了。接下来看 List 全量数据再做过滤的逻辑。&lt;/p>
&lt;h2 id="25-请求未指定资源名获取全量数据做过滤">2.5 请求未指定资源名，获取全量数据做过滤&lt;/h2>
&lt;h3 id="251-apiserver-缓存层getlist-处理逻辑">2.5.1 apiserver 缓存层：&lt;code>GetList()&lt;/code> 处理逻辑&lt;/h3>
&lt;pre>&lt;code>// https://github.com/kubernetes/kubernetes/blob/v1.24.0/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L622
// GetList implements storage.Interface
func (c *Cacher) GetList(ctx , key string, opts storage.ListOptions, listObj runtime.Object) error {
recursive := opts.Recursive
resourceVersion := opts.ResourceVersion
pred := opts.Predicate
// 情况一：ListOption 要求必须从 etcd 读
if shouldDelegateList(opts)
return c.storage.GetList(ctx, key, opts, listObj) // c.storage 指向 etcd
// If resourceVersion is specified, serve it from cache.
listRV := c.versioner.ParseResourceVersion(resourceVersion)
// 情况二：apiserver 缓存未建好，只能从 etcd 读
if listRV == 0 &amp;amp;&amp;amp; !c.ready.check()
return c.storage.GetList(ctx, key, opts, listObj)
// 情况三：apiserver 缓存正常，从缓存读：保证返回的 objects 版本不低于 `listRV`
listPtr := meta.GetItemsPtr(listObj)
listVal := conversion.EnforcePtr(listPtr)
filter := filterWithAttrsFunction(key, pred) // 最终的过滤器
objs, readResourceVersion, indexUsed := c.listItems(listRV, key, pred, ...) // 根据 index 预筛，性能优化
for _, obj := range objs {
elem := obj.(*storeElement)
if filter(elem.Key, elem.Labels, elem.Fields) // 真正的过滤
listVal.Set(reflect.Append(listVal, reflect.ValueOf(elem))
}
// 更新最后一次读到的 ResourceVersion
if c.versioner != nil
c.versioner.UpdateList(listObj, readResourceVersion, &amp;quot;&amp;quot;, nil)
return nil
}
&lt;/code>&lt;/pre>
&lt;h3 id="252-判断是否必须从-etcd-读数据shoulddelegatelist">2.5.2 判断是否必须从 etcd 读数据：&lt;code>shouldDelegateList()&lt;/code>&lt;/h3>
&lt;pre>&lt;code>// https://github.com/kubernetes/kubernetes/blob/v1.24.0/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L591
func shouldDelegateList(opts storage.ListOptions) bool {
resourceVersion := opts.ResourceVersion
pred := opts.Predicate
pagingEnabled := DefaultFeatureGate.Enabled(features.APIListChunking) // 默认是启用的
hasContinuation := pagingEnabled &amp;amp;&amp;amp; len(pred.Continue) &amp;gt; 0 // Continue 是个 token
hasLimit := pagingEnabled &amp;amp;&amp;amp; pred.Limit &amp;gt; 0 &amp;amp;&amp;amp; resourceVersion != &amp;quot;0&amp;quot; // 只有在 resourceVersion != &amp;quot;0&amp;quot; 的情况下，hasLimit 才有可能为 true
// 1. 如果未指定 resourceVersion，从底层存储（etcd）拉去数据；
// 2. 如果有 continuation，也从底层存储拉数据；
// 3. 只有 resourceVersion != &amp;quot;0&amp;quot; 时，才会将 limit 传给底层存储（etcd），因为 watch cache 不支持 continuation
return resourceVersion == &amp;quot;&amp;quot; || hasContinuation || hasLimit || opts.ResourceVersionMatch == metav1.ResourceVersionMatchExact
}
&lt;/code>&lt;/pre>
&lt;p>这里非常重要：&lt;/p>
&lt;ol>
&lt;li>问：客户端未设置 ListOption{} 中的 &lt;code>ResourceVersion&lt;/code> 字段，是否对应到这里的 &lt;code>resourceVersion == &amp;quot;&amp;quot;&lt;/code>？
答：是的，所以&lt;strong>第一节&lt;/strong>的 &lt;a href="#client_code_empty_rv">例子&lt;/a> 会导致从 etcd 拉全量数据。&lt;/li>
&lt;li>问：客户端设置了 &lt;code>limit=500&amp;amp;resourceVersion=0&lt;/code> 是否会导致下次 &lt;code>hasContinuation==true&lt;/code>？
答：不会，&lt;strong>resourceVersion=0 将导致 limit 被忽略&lt;/strong>（&lt;code>hasLimit&lt;/code> 那一行代码），也就是说， 虽然指定了 limit=500，但&lt;strong>这个请求会返回全量数据&lt;/strong>。&lt;/li>
&lt;li>问：ResourceVersionMatch 是什么用途？
答：用来告诉 apiserver，该如何解读 ResourceVersion。官方有个很复杂的 &lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter">表格&lt;/a> ，有兴趣可以看看。&lt;/li>
&lt;/ol>
&lt;p>接下来再返回到 cacher 的 &lt;code>GetList()&lt;/code> 逻辑，来看下具体有哪几种处理情况。&lt;/p>
&lt;h3 id="253-情况一listoption-要求从-etcd-读数据">2.5.3 情况一：ListOption 要求从 etcd 读数据&lt;/h3>
&lt;p>这种情况下，apiserver 会直接从 etcd 读取所有 objects 并过滤，然后返回给客户端， 适用于数据一致性要求极其高的场景。 当然，也容易&lt;strong>误入这种场景造成 etcd 压力过大&lt;/strong>，例如 &lt;strong>第一节&lt;/strong>的&lt;a href="#client_code_empty_rv">例子&lt;/a>。&lt;/p>
&lt;pre>&lt;code>// https://github.com/kubernetes/kubernetes/blob/v1.24.0/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go#L563
// GetList implements storage.Interface.
func (s *store) GetList(ctx , key string, opts storage.ListOptions, listObj runtime.Object) error {
listPtr := meta.GetItemsPtr(listObj)
v := conversion.EnforcePtr(listPtr)
key = path.Join(s.pathPrefix, key)
keyPrefix := key // append '/' if needed
newItemFunc := getNewItemFunc(listObj, v)
var fromRV *uint64
if len(resourceVersion) &amp;gt; 0 { // 如果 RV 非空（客户端不传时，默认是空字符串）
parsedRV := s.versioner.ParseResourceVersion(resourceVersion)
fromRV = &amp;amp;parsedRV
}
// ResourceVersion, ResourceVersionMatch 等处理逻辑
switch {
case recursive &amp;amp;&amp;amp; s.pagingEnabled &amp;amp;&amp;amp; len(pred.Continue) &amp;gt; 0: ...
case recursive &amp;amp;&amp;amp; s.pagingEnabled &amp;amp;&amp;amp; pred.Limit &amp;gt; 0 : ...
default : ...
}
// loop until we have filled the requested limit from etcd or there are no more results
for {
getResp = s.client.KV.Get(ctx, key, options...) // 从 etcd 拉数据
numFetched += len(getResp.Kvs)
hasMore = getResp.More
for i, kv := range getResp.Kvs {
if limitOption != nil &amp;amp;&amp;amp; int64(v.Len()) &amp;gt;= pred.Limit {
hasMore = true
break
}
lastKey = kv.Key
data := s.transformer.TransformFromStorage(ctx, kv.Value, kv.Key)
appendListItem(v, data, kv.ModRevision, pred, s.codec, s.versioner, newItemFunc) // 这里面会做过滤
numEvald++
}
key = string(lastKey) + &amp;quot;\x00&amp;quot;
}
// instruct the client to begin querying from immediately after the last key we returned
if hasMore {
// we want to start immediately after the last key
next := encodeContinue(string(lastKey)+&amp;quot;\x00&amp;quot;, keyPrefix, returnedRV)
return s.versioner.UpdateList(listObj, uint64(returnedRV), next, remainingItemCount)
}
// no continuation
return s.versioner.UpdateList(listObj, uint64(returnedRV), &amp;quot;&amp;quot;, nil)
}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>**client.KV.Get()**&lt;/code> 就进入 etcd client 库了，感兴趣可以继续往下挖。&lt;/li>
&lt;li>&lt;code>**appendListItem()**&lt;/code> 会&lt;strong>对拿到的数据进行过滤&lt;/strong>，这就是我们第一节提到的 apiserver 内存过滤操作。&lt;/li>
&lt;/ul>
&lt;h3 id="254-情况二本地缓存还没建好只能从-etcd-读数据">2.5.4 情况二：本地缓存还没建好，只能从 etcd 读数据&lt;/h3>
&lt;p>具体执行过程与情况一相同。&lt;/p>
&lt;h3 id="255-情况三使用本地缓存">2.5.5 情况三：使用本地缓存&lt;/h3>
&lt;pre>&lt;code>// https://github.com/kubernetes/kubernetes/blob/v1.24.0/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L622
// GetList implements storage.Interface
func (c *Cacher) GetList(ctx , key string, opts storage.ListOptions, listObj runtime.Object) error {
// 情况一：ListOption 要求必须从 etcd 读
...
// 情况二：apiserver 缓存未建好，只能从 etcd 读
...
// 情况三：apiserver 缓存正常，从缓存读：保证返回的 objects 版本不低于 `listRV`
listPtr := meta.GetItemsPtr(listObj) // List elements with at least 'listRV' from cache.
listVal := conversion.EnforcePtr(listPtr)
filter := filterWithAttrsFunction(key, pred) // 最终的过滤器
objs, readResourceVersion, indexUsed := c.listItems(listRV, key, pred, ...) // 根据 index 预筛，性能优化
for _, obj := range objs {
elem := obj.(*storeElement)
if filter(elem.Key, elem.Labels, elem.Fields) // 真正的过滤
listVal.Set(reflect.Append(listVal, reflect.ValueOf(elem))
}
if c.versioner != nil
c.versioner.UpdateList(listObj, readResourceVersion, &amp;quot;&amp;quot;, nil)
return nil
}
&lt;/code>&lt;/pre>
&lt;p>为了避免客户端库（例如 client-go）自动帮我们设置一些参数，我们直接用 &lt;code>curl&lt;/code> 来测试，指定证书就行了：&lt;/p>
&lt;pre>&lt;code>$ cat curl-k8s-apiserver.sh
curl -s --cert /etc/kubernetes/pki/admin.crt --key /etc/kubernetes/pki/admin.key --cacert /etc/kubernetes/pki/ca.crt $@
&lt;/code>&lt;/pre>
&lt;p>使用方式：&lt;/p>
&lt;pre>&lt;code>$ ./curl-k8s-apiserver.sh &amp;quot;https://localhost:6443/api/v1/pods?limit=2&amp;quot;
{
&amp;quot;kind&amp;quot;: &amp;quot;PodList&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;resourceVersion&amp;quot;: &amp;quot;2127852936&amp;quot;,
&amp;quot;continue&amp;quot;: &amp;quot;eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJ...&amp;quot;,
},
&amp;quot;items&amp;quot;: [ {pod1 data }, {pod2 data}]
}
&lt;/code>&lt;/pre>
&lt;h2 id="31-指定-limit2response-将返回分页信息continue">3.1 指定 &lt;code>limit=2&lt;/code>：response 将返回分页信息（&lt;code>continue&lt;/code>）&lt;/h2>
&lt;h3 id="311-curl-测试">3.1.1 &lt;code>curl&lt;/code> 测试&lt;/h3>
&lt;pre>&lt;code>$ ./curl-k8s-apiserver.sh &amp;quot;https://localhost:6443/api/v1/pods?limit=2&amp;quot;
{
&amp;quot;kind&amp;quot;: &amp;quot;PodList&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;resourceVersion&amp;quot;: &amp;quot;2127852936&amp;quot;,
&amp;quot;continue&amp;quot;: &amp;quot;eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJ...&amp;quot;,
},
&amp;quot;items&amp;quot;: [ {pod1 data }, {pod2 data}]
}
&lt;/code>&lt;/pre>
&lt;p>可以看到，&lt;/p>
&lt;ul>
&lt;li>确实返回了两个 pod 信息，在 &lt;code>items[]&lt;/code> 字段中；&lt;/li>
&lt;li>另外在 &lt;code>metadata&lt;/code> 中返回了一个 &lt;code>continue&lt;/code> 字段，客户端下次带上这个参数，apiserver 将继续返回剩下的内容，直到 apiserver 不再返回 &lt;code>continue&lt;/code>。&lt;/li>
&lt;/ul>
&lt;h3 id="312-kubectl-测试">3.1.2 &lt;code>kubectl&lt;/code> 测试&lt;/h3>
&lt;p>调大 kubectl 的日志级别，也可以看到它背后用了 continue 来获取全量 pods：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods --all-namespaces --v=10
# 以下都是 log 输出，做了适当调整
# curl -k -v -XGET -H &amp;quot;User-Agent: kubectl/v1.xx&amp;quot; -H &amp;quot;Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json&amp;quot;
# 'http://localhost:8080/api/v1/pods?limit=500'
# GET http://localhost:8080/api/v1/pods?limit=500 200 OK in 202 milliseconds
# Response Body: {&amp;quot;kind&amp;quot;:&amp;quot;Table&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;continue&amp;quot;:&amp;quot;eyJ2Ijoib...&amp;quot;,&amp;quot;remainingItemCount&amp;quot;:54},&amp;quot;columnDefinitions&amp;quot;:[...],&amp;quot;rows&amp;quot;:[...]}
#
# curl -k -v -XGET -H &amp;quot;Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json&amp;quot; -H &amp;quot;User-Agent: kubectl/v1.xx&amp;quot;
# 'http://localhost:8080/api/v1/pods?continue=eyJ2Ijoib&amp;amp;limit=500'
# GET http://localhost:8080/api/v1/pods?continue=eyJ2Ijoib&amp;amp;limit=500 200 OK in 44 milliseconds
# Response Body: {&amp;quot;kind&amp;quot;:&amp;quot;Table&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;resourceVersion&amp;quot;:&amp;quot;2122644698&amp;quot;},&amp;quot;columnDefinitions&amp;quot;:[],&amp;quot;rows&amp;quot;:[...]}
&lt;/code>&lt;/pre>
&lt;p>第一次请求拿到了 500 个 pods，第二次请求把返回的 continue 带上了： &lt;code>**GET http://localhost:8080/api/v1/pods?continue=eyJ2Ijoib&amp;amp;limit=500**&lt;/code>，continue 是个 token， 有点长，为了更好的展示这里把它截断了。&lt;/p>
&lt;h2 id="32-指定-limit2resourceversion0limit2-将被忽略返回全量数据">3.2 指定 &lt;code>limit=2&amp;amp;resourceVersion=0&lt;/code>：&lt;code>limit=2&lt;/code> 将被忽略，返回全量数据&lt;/h2>
&lt;pre>&lt;code>$ ./curl-k8s-apiserver.sh &amp;quot;https://localhost:6443/api/v1/pods?limit=2&amp;amp;resourceVersion=0&amp;quot;
{
&amp;quot;kind&amp;quot;: &amp;quot;PodList&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;resourceVersion&amp;quot;: &amp;quot;2127852936&amp;quot;,
&amp;quot;continue&amp;quot;: &amp;quot;eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJ...&amp;quot;,
},
&amp;quot;items&amp;quot;: [ {pod1 data }, {pod2 data}, ...]
}
&lt;/code>&lt;/pre>
&lt;p>&lt;code>items[]&lt;/code> 里面是全量 pod 信息。&lt;/p>
&lt;h2 id="33-指定-specnodenamenode1resourceversion0-vs-specnodenamenode1">3.3 指定 &lt;code>spec.nodeName=node1&amp;amp;resourceVersion=0&lt;/code> vs. &lt;code>spec.nodeName=node1&amp;quot;&lt;/code>&lt;/h2>
&lt;h3 id="结果相同">结果相同&lt;/h3>
&lt;pre>&lt;code>$ ./curl-k8s-apiserver.sh &amp;quot;https://localhost:6443/api/v1/namespaces/default/pods?fieldSelector=spec.nodeName%3Dnode1&amp;quot; | jq '.items[].spec.nodeName'
&amp;quot;node1&amp;quot;
&amp;quot;node1&amp;quot;
&amp;quot;node1&amp;quot;
...
$ ./curl-k8s-apiserver.sh &amp;quot;https://localhost:6443/api/v1/namespaces/default/pods?fieldSelector=spec.nodeName%3Dnode1&amp;amp;resourceVersion=0&amp;quot; | jq '.items[].spec.nodeName'
&amp;quot;node1&amp;quot;
&amp;quot;node1&amp;quot;
&amp;quot;node1&amp;quot;
...
&lt;/code>&lt;/pre>
&lt;p>结果是一样的，除非是 apiserver 缓存和 etcd 数据出现不一致，这个概率极小，我们这里不讨论。&lt;/p>
&lt;h3 id="速度差异很大">速度差异很大&lt;/h3>
&lt;p>用 time 测量以上两种情况下的耗时，会发现对于大一些的集群，这两种请求的响应时间就会有明显差异。&lt;/p>
&lt;pre>&lt;code>$ time ./curl-k8s-apiserver.sh &amp;lt;url&amp;gt; &amp;gt; result
&lt;/code>&lt;/pre>
&lt;p>对于 4K nodes, 100K pods 规模的集群，以下数据供参考：&lt;/p>
&lt;ul>
&lt;li>不带 &lt;code>resourceVersion=0&lt;/code>（读 etcd 并在 apiserver 过滤）: 耗时 &lt;code>**10s**&lt;/code>&lt;/li>
&lt;li>带 &lt;code>resourceVersion=0&lt;/code>（读 apiserver 缓存）: 耗时 &lt;code>**0.05s**&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>差了 200 倍。&lt;/p>
&lt;blockquote>
&lt;p>全量 pod 的总大小按 2GB 计算，平均每个 20KB。&lt;/p>
&lt;/blockquote>
&lt;p>本节以 cilium-agent 为例，介绍定量测量它启动时对控制平面压力。&lt;/p>
&lt;h2 id="41-收集-list-请求">4.1 收集 LIST 请求&lt;/h2>
&lt;p>首先获取 agent 启动时，都 LIST k8s 哪些资源。有几种收集方式：&lt;/p>
&lt;ol>
&lt;li>在 k8s access log，按 ServiceAccount、verb、request_uri 等过滤；&lt;/li>
&lt;li>通过 agent 日志；&lt;/li>
&lt;li>通过进一步代码分析等等。&lt;/li>
&lt;/ol>
&lt;p>假设我们收集到如下 LIST 请求：&lt;/p>
&lt;ol>
&lt;li>&lt;code>api/v1/namespaces?resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>api/v1/pods?filedSelector=spec.nodeName%3Dnode1&amp;amp;resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>api/v1/nodes?fieldSelector=metadata.name%3Dnode1&amp;amp;resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>api/v1/services?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&lt;/code>&lt;/li>
&lt;li>&lt;code>apis/discovery.k8s.io/v1beta1/endpointslices?resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>apis/networking.k8s.io/networkpolicies?resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>apis/cilium.io/v2/ciliumnodes?resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>apis/cilium.io/v2/ciliumnetworkpolicies?resourceVersion=0&lt;/code>&lt;/li>
&lt;li>&lt;code>apis/cilium.io/v2/ciliumclusterwidenetworkpolicies?resourceVersion=0&lt;/code>&lt;/li>
&lt;/ol>
&lt;h2 id="22-测试-list-请求数据量和耗时">2.2 测试 LIST 请求数据量和耗时&lt;/h2>
&lt;p>有了 LIST 请求列表，接下来就可以手动执行这些请求，拿到如下数据：&lt;/p>
&lt;ol>
&lt;li>请求耗时&lt;/li>
&lt;li>请求处理的数据量，这里分为两种：&lt;/li>
&lt;li>apiserver 处理的数据量（全量数据），评估对 apiserver/etcd 的性能影响应该以这个为主&lt;/li>
&lt;li>agent 最终拿到的数据量（按 selector 做了过滤）&lt;/li>
&lt;/ol>
&lt;p>用下面这个脚本（放到真实环境 k8s master 上）来就可以执行一遍测试，&lt;/p>
&lt;pre>&lt;code>$ cat benchmark-list-overheads.sh
apiserver_url=&amp;quot;https://localhost:6443&amp;quot;
# List k8s core resources (e.g. pods, services)
# API: GET/LIST /api/v1/&amp;lt;resources&amp;gt;?&amp;lt;fileld/label selector&amp;gt;&amp;amp;resourceVersion=0
function benchmark_list_core_resource() {
resource=$1
selectors=$2
echo &amp;quot;----------------------------------------------------&amp;quot;
echo &amp;quot;Benchmarking list $2&amp;quot;
listed_file=&amp;quot;listed-$resource&amp;quot;
url=&amp;quot;$apiserver_url/api/v1/$resource?resourceVersion=0&amp;quot;
# first perform a request without selectors, this is the size apiserver really handles
echo &amp;quot;curl $url&amp;quot;
time ./curl-k8s-apiserver.sh &amp;quot;$url&amp;quot; &amp;gt; $listed_file
# perform another request if selectors are provided, this is the size client receives
listed_file2=&amp;quot;$listed_file-filtered&amp;quot;
if [ ! -z &amp;quot;$selectors&amp;quot; ]; then url=&amp;quot;$url&amp;amp;$selectors&amp;quot;
echo &amp;quot;curl $url&amp;quot;
time ./curl-k8s-apiserver.sh &amp;quot;$url&amp;quot; &amp;gt; $listed_file2
fi ls -ahl $listed_file $listed_file2 2&amp;gt;/dev/null
echo &amp;quot;----------------------------------------------------&amp;quot;
echo &amp;quot;&amp;quot;
}
# List k8s apiextension resources (e.g. pods, services)
# API: GET/LIST /apis/&amp;lt;api group&amp;gt;/&amp;lt;resources&amp;gt;?&amp;lt;fileld/label selector&amp;gt;&amp;amp;resourceVersion=0
function benchmark_list_apiexternsion_resource() {
api_group=$1
resource=$2
selectors=$3
echo &amp;quot;----------------------------------------------------&amp;quot;
echo &amp;quot;Benchmarking list $api_group/$resource&amp;quot;
api_group_flatten_name=$(echo $api_group | sed 's/\//-/g')
listed_file=&amp;quot;listed-$api_group_flatten_name-$resource&amp;quot;
url=&amp;quot;$apiserver_url/apis/$api_group/$resource?resourceVersion=0&amp;quot;
if [ ! -z &amp;quot;$selectors&amp;quot; ]; then url=&amp;quot;$url&amp;amp;$selectors&amp;quot;
fi echo &amp;quot;curl $url&amp;quot;
time ./curl-k8s-apiserver.sh &amp;quot;$url&amp;quot; &amp;gt; $listed_file
ls -ahl $listed_file
echo &amp;quot;----------------------------------------------------&amp;quot;
echo &amp;quot;&amp;quot;
}
benchmark_list_core_resource &amp;quot;namespaces&amp;quot; &amp;quot;&amp;quot;
benchmark_list_core_resource &amp;quot;pods&amp;quot; &amp;quot;filedSelector=spec.nodeName%3Dnode1&amp;quot;
benchmark_list_core_resource &amp;quot;nodes&amp;quot; &amp;quot;fieldSelector=metadata.name%3Dnode1&amp;quot;
benchmark_list_core_resource &amp;quot;services&amp;quot; &amp;quot;labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;discovery.k8s.io/v1beta1&amp;quot; &amp;quot;endpointslices&amp;quot; &amp;quot;&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;apiextensions.k8s.io/v1&amp;quot; &amp;quot;customresourcedefinitions&amp;quot; &amp;quot;&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;networking.k8s.io&amp;quot; &amp;quot;networkpolicies&amp;quot; &amp;quot;&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;cilium.io/v2&amp;quot; &amp;quot;ciliumnodes&amp;quot; &amp;quot;&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;cilium.io/v2&amp;quot; &amp;quot;ciliumendpoints&amp;quot; &amp;quot;&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;cilium.io/v2&amp;quot; &amp;quot;ciliumnetworkpolicies&amp;quot; &amp;quot;&amp;quot;
benchmark_list_apiexternsion_resource &amp;quot;cilium.io/v2&amp;quot; &amp;quot;ciliumclusterwidenetworkpolicies&amp;quot; &amp;quot;&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>执行效果如下：&lt;/p>
&lt;pre>&lt;code>$ benchmark-list-overheads.sh
----------------------------------------------------
Benchmarking list
curl https://localhost:6443/api/v1/namespaces?resourceVersion=0
real 0m0.090s
user 0m0.038s
sys 0m0.044s
-rw-r--r-- 1 root root 69K listed-namespaces
----------------------------------------------------
Benchmarking list fieldSelector=spec.nodeName%3Dnode1
curl https://localhost:6443/api/v1/pods?resourceVersion=0
real 0m18.332s
user 0m1.355s
sys 0m1.822s
curl https://localhost:6443/api/v1/pods?resourceVersion=0&amp;amp;fieldSelector=spec.nodeName%3Dnode1
real 0m0.242s
user 0m0.044s
sys 0m0.188s
-rw-r--r-- 1 root root 2.0G listed-pods
-rw-r--r-- 1 root root 526K listed-pods-filtered
----------------------------------------------------
...
&lt;/code>&lt;/pre>
&lt;p>说明：凡是带了 selector 的 LIST，例如 &lt;code>LIST pods?spec.nodeName=node1&lt;/code>，这个脚本会先执行一遍不带 selector 的请求，目的是测量 apiserver 需要处理的数据量，例如上面的 list pods：&lt;/p>
&lt;ol>
&lt;li>agent 真正执行的是 &lt;code>pods?resourceVersion=0&amp;amp;fieldSelector=spec.nodeName%3Dnode1&lt;/code>，所以请求耗时应该以这个为准&lt;/li>
&lt;li>额外执行了 &lt;code>pods?resourceVersion=0&lt;/code>，这样是为了测试 1 的请求到底需要 apiserver 处理多少数据量&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注意： list all pods 这样的操作会产生 2GB 的文件，因此谨慎使用这个 benchmark 工具，首先理解你写的脚本在测什么，尤其不要自动化或并发跑，可能会把 apiserver/etcd 打爆。&lt;/p>
&lt;/blockquote>
&lt;h2 id="43-测试结果分析">4.3 测试结果分析&lt;/h2>
&lt;p>以上输出有如下关键信息：&lt;/p>
&lt;ol>
&lt;li>LIST 的资源类型，例如 pods/endpoints/services&lt;/li>
&lt;li>LIST 操作耗时&lt;/li>
&lt;li>LIST 操作涉及的数据量
&lt;ol>
&lt;li>apiserver 需要处理的数据量（json 格式）：以上面 list pods 为例，对应的是 &lt;code>listed-pods&lt;/code> 文件，共 2GB；&lt;/li>
&lt;li>agent 收到的数据量（因为 agent 可能指定了 label/field 过滤器）：以上面 list pods 为例，对应 &lt;code>listed-pods-filtered&lt;/code> 文件，共计 &lt;code>526K&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>按以上方式将所有 LIST 请求都收集起来并排序，就知道了 agent 一次启动操作，对 apiserver/etcd 的压力。&lt;/p>
&lt;pre>&lt;code>$ ls -ahl listed-*
-rw-r--r-- 1 root root 222 listed-apiextensions.k8s.io-v1-customeresourcedefinitions
-rw-r--r-- 1 root root 5.8M listed-apiextensions.k8s.io-v1-customresourcedefinitions
-rw-r--r-- 1 root root 2.0M listed-cilium.io-v2-ciliumclusterwidenetworkpolicies
-rw-r--r-- 1 root root 193M listed-cilium.io-v2-ciliumendpoints
-rw-r--r-- 1 root root 185 listed-cilium.io-v2-ciliumnetworkpolicies
-rw-r--r-- 1 root root 6.6M listed-cilium.io-v2-ciliumnodes
-rw-r--r-- 1 root root 42M listed-discovery.k8s.io-v1beta1-endpointslices
-rw-r--r-- 1 root root 69K listed-namespaces
-rw-r--r-- 1 root root 222 listed-networking.k8s.io-networkpolicies
-rw-r--r-- 1 root root 70M listed-nodes # 仅用于评估 apiserver 需要处理的数据量
-rw-r--r-- 1 root root 25K listed-nodes-filtered
-rw-r--r-- 1 root root 2.0G listed-pods # 仅用于评估 apiserver 需要处理的数据量
-rw-r--r-- 1 root root 526K listed-pods-filtered
-rw-r--r-- 1 root root 23M listed-services # 仅用于评估 apiserver 需要处理的数据量
-rw-r--r-- 1 root root 23M listed-services-filtered
&lt;/code>&lt;/pre>
&lt;p>还是以 cilium 为例，有大致这样一个排序（apiserver 处理的数据量，json 格式）：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>List 资源类型&lt;/th>
&lt;th>apiserver 处理的数据量（json）&lt;/th>
&lt;th>耗时&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CiliumEndpoints (全量）&lt;/td>
&lt;td>193MB&lt;/td>
&lt;td>11s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CiliumNodes (全量）&lt;/td>
&lt;td>70MB&lt;/td>
&lt;td>0.5s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>…&lt;/td>
&lt;td>…&lt;/td>
&lt;td>…&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="51-list-请求默认设置-resourceversion0">5.1 List 请求默认设置 &lt;code>ResourceVersion=0&lt;/code>&lt;/h2>
&lt;p>前面已经介绍，不设置这个参数将导致 apiserver 从 etcd 拉全量数据再过滤，导致&lt;/p>
&lt;ol>
&lt;li>很慢&lt;/li>
&lt;li>规模大了 etcd 扛不住&lt;/li>
&lt;/ol>
&lt;p>因此，除非对数据准确性要求极高，必须从 etcd 拉数据，否则应该在 LIST 请求时设置 &lt;code>ResourceVersion=0&lt;/code> 参数， 让 apiserver 用缓存提供服务。&lt;/p>
&lt;p>如果你使用的是 &lt;strong>client-go 的 ListWatch/informer 接口&lt;/strong>， 那它默认已经设置了 &lt;code>ResourceVersion=0&lt;/code>。&lt;/p>
&lt;h2 id="52-优先使用-namespaced-api">5.2 优先使用 namespaced API&lt;/h2>
&lt;p>如果要 LIST 的资源在单个或少数几个 namespace，考虑使用 namespaced API：&lt;/p>
&lt;ul>
&lt;li>Namespaced API: &lt;code>/api/v1/namespaces/&amp;lt;ns&amp;gt;/pods?query=xxx&lt;/code>&lt;/li>
&lt;li>Un-namespaced API: &lt;code>/api/v1/pods?query=xxx&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="53-restart-backoff">5.3 Restart backoff&lt;/h2>
&lt;p>对于 per-node 部署的基础服务，例如 kubelet、cilium-agent、daemonsets，需要 通过有效的 restart backoff 降低大面积重启时对控制平面的压力。&lt;/p>
&lt;p>例如，同时挂掉后，每分钟重启的 agent 数量不超过集群规模的 10%（可配置，或可自动计算）。&lt;/p>
&lt;h2 id="54-优先通过-labelfield-selector-在服务端做过滤">5.4 优先通过 label/field selector 在服务端做过滤&lt;/h2>
&lt;p>如果需要缓存某些资源并监听变动，那需要使用 ListWatch 机制，将数据拉到本地，业务逻辑根据需要自己从 local cache 过滤。 这是 client-go 的 ListWatch/informer 机制。&lt;/p>
&lt;p>但如果只是一次性的 LIST 操作，并且有筛选条件，例如前面提到的根据 nodename 过滤 pod 的例子， 那显然应该通过设置 label 或字段过滤器，让 apiserver 帮我们把数据过滤出来。 LIST 10w pods 需要几十秒（大部分时间花在数据传输上，同时也占用 apiserver 大量 CPU/BW/IO）， 而如果只需要本机上的 pod，那设置 &lt;code>nodeName=node1&lt;/code> 之后，LIST 可能只需要 &lt;code>0.05s&lt;/code> 就能返回结果。 另外非常重要的一点时，不要忘记在请求中同时带上 &lt;code>resourceVersion=0&lt;/code>。&lt;/p>
&lt;h3 id="541-label-selector">5.4.1 Label selector&lt;/h3>
&lt;p>在 apiserver 内存过滤。&lt;/p>
&lt;h3 id="542-field-selector">5.4.2 Field selector&lt;/h3>
&lt;p>在 apiserver 内存过滤。&lt;/p>
&lt;h3 id="543-namespace-selector">5.4.3 Namespace selector&lt;/h3>
&lt;p>etcd 中 namespace 是前缀的一部分，因此能指定 namespace 过滤资源，速度比不是前缀的 selector 快很多。&lt;/p>
&lt;h2 id="55-配套基础设施监控告警等">5.5 配套基础设施（监控、告警等）&lt;/h2>
&lt;p>以上分析可以看成，client 的单个请求可能只返回几百 KB 的数据，但 apiserver（更糟糕的情况，etcd）需要处理上 GB 的数据。 因此，应该极力避免基础服务的大规模重启，为此需要在监控、告警上做的尽量完善。&lt;/p>
&lt;h3 id="551-使用独立-serviceaccount">5.5.1 使用独立 ServiceAccount&lt;/h3>
&lt;p>每个基础服务（例如 kubelet、cilium-agent 等），以及对 apiserver 有大量 LIST 操作的各种 operator， 都使用各自独立的 SA， 这样便于 apiserver 区分请求来源，对监控、排障和服务端限流都非常有用。&lt;/p>
&lt;h3 id="552-liveness-监控告警">5.5.2 Liveness 监控告警&lt;/h3>
&lt;p>基础服务必须覆盖到 liveness 监控。&lt;/p>
&lt;p>必须有 P1 级别的 liveness 告警，能第一时间发现大规模挂掉的场景。然后通过 restart backoff 降低对控制平面的压力。&lt;/p>
&lt;h3 id="553-监控和调优-etcd">5.5.3 监控和调优 etcd&lt;/h3>
&lt;p>需要针对性能相关的关键指标做好监控和告警：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>内存&lt;/p>
&lt;/li>
&lt;li>
&lt;p>带宽&lt;/p>
&lt;/li>
&lt;li>
&lt;p>大 LIST 请求数量及响应耗时
比如下面这个 &lt;code>LIST all pods&lt;/code> 日志：&lt;/p>
&lt;p>{ &amp;ldquo;level&amp;rdquo;:&amp;ldquo;warn&amp;rdquo;, &amp;ldquo;msg&amp;rdquo;:&amp;ldquo;apply request took too long&amp;rdquo;, &amp;ldquo;took&amp;rdquo;:&amp;ldquo;5357.87304ms&amp;rdquo;, &amp;ldquo;expected-duration&amp;rdquo;:&amp;ldquo;100ms&amp;rdquo;, &amp;ldquo;prefix&amp;rdquo;:&amp;ldquo;read-only range &amp;ldquo;, &amp;ldquo;request&amp;rdquo;:&amp;ldquo;key:&amp;quot;/registry/pods/&amp;quot; range_end:&amp;quot;/registry/pods0&amp;quot; &amp;ldquo;, &amp;ldquo;response&amp;rdquo;:&amp;ldquo;range_response_count:60077 size:602251227&amp;rdquo; }&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>部署和配置调优：&lt;/p>
&lt;ol>
&lt;li>K8s events 拆到单独的 etcd 集群&lt;/li>
&lt;li>其他。&lt;/li>
&lt;/ol>
&lt;h2 id="61-get-请求getoptions">6.1 Get 请求：&lt;code>GetOptions{}&lt;/code>&lt;/h2>
&lt;p>基本原理与 &lt;code>ListOption{}&lt;/code> 一样，不设置 &lt;code>ResourceVersion=0&lt;/code> 会导致 apiserver 去 etcd 拿数据，应该尽量避免。&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts&lt;/a>, kubernetes doc&lt;/li>
&lt;li>&lt;a href="https://arthurchiao.art/blog/raft-paper-zh/">(译) [论文] Raft 共识算法（及 etcd/raft 源码解析）（USENIX, 2014）&lt;/a>
&lt;a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/">https://arthurchiao.art/blog/k8s-reliability-list-data-zh/&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Docs: kubernetes优化</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/kubernetes%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/2.3.Kubernetes-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/kubernetes%E4%BC%98%E5%8C%96/kubernetes%E4%BC%98%E5%8C%96/</guid><description>
&lt;h1 id="概述">概述&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;/blockquote>
&lt;p>增加可以打开的文件数与线程数,防止 pod 无故无法启动&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt;&amp;gt; /etc/security/limits.conf &lt;span style="color:#e6db74">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* soft nproc 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* hard nproc 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* soft nofile 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">* hard nofile 1000000
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="馆长推荐的优化参数">馆长推荐的优化参数&lt;/h2>
&lt;h1 id="-endif-">&lt;a href="https://github.com/moby/moby/issues/31208">https://github.com/moby/moby/issues/31208&lt;/a>
# ipvsadm -l &amp;ndash;timout
# 修复 ipvs 模式下长连接 timeout 问题 小于 900 即可
{% if proxy.mode is defined and proxy.mode == &amp;lsquo;ipvs&amp;rsquo; %}
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
{% endif %}&lt;/h1>
&lt;p>net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_max_tw_buckets = 5000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_synack_retries = 2&lt;/p>
&lt;p>net.core.somaxconn = 10000
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.netfilter.nf_conntrack_max = 2310720
fs.inotify.max_user_watches=89100
fs.may_detach_mounts = 1
fs.file-max = 52706963
fs.nr_open = 52706963
net.bridge.bridge-nf-call-arptables = 1&lt;/p>
&lt;p>{% if not kubelet.swap %}
vm.swappiness = 0
{% endif %}&lt;/p>
&lt;p>vm.overcommit_memory=1
vm.panic_on_oom=0
vm.max_map_count = 262144&lt;/p></description></item></channel></rss>