<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – Kubelet</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/</link><description>Recent content in Kubelet on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Kubelet</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Kubelet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Kubelet/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">官方文档，参考-组件工具-kubelet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Kubelet 是在每个节点上运行的主要 &lt;strong>节点代理&lt;/strong>。它可以使用以下之一向 APIServer 注册节点：用于覆盖主机名的标志；或云提供商的特定逻辑。&lt;/p>
&lt;p>kubelet 根据 PodSpec 起作用。 PodSpec 是一个描述 Pod 的 YAML 或 JSON 对象。 kubelet 接受通过各种机制（主要是通过 apiserver）提供的一组 PodSpec，并确保这些 PodSpec 中描述的容器正在运行且运行状况良好。 Kubelet 不管理不是 Kubernetes 创建的容器。一般情况， PodSpec 都是由在 k8s 对象的 yaml 文件中定义的。&lt;/p>
&lt;p>kubelet 负责维护容器(CNI)的生命周期，同时也负责 Volume（CVI）和 Network（CNI）的管理。kubernetes 集群的宿主机上，启动的每一个 pod 都有由 kubelet 这个组件管理的。&lt;/p>
&lt;p>kubelet 在每个 Node 上都会启动一个 kubelet daemon 进程，默认监听在 &lt;strong>10250&lt;/strong> 端口。该进程用于处理 Master 节点(主要是 apiserver)下发到本节点的任务，管理 Pod 以及 Pod 中的容器。每个 kubelet 进程会在 APIServer 上注册节点自身信息，定期向 Master 节点汇报节点资源的使用情况，并通过 cAdvisor(kubelet 内部功能) 监控容器和节点资源。10248 为 kubelet 健康检查的 healthz 端口&lt;/p>
&lt;blockquote>
&lt;p>Note: 如果 master 节点不运行 Pod 的话，是不用部署 kubelet 的。&lt;/p>
&lt;/blockquote>
&lt;p>kubelet 使用 PodSpec 来对其所在节点的 Pod 进行管理，PodSpec(Pod Specification) 是描述 pod 的 yaml 或者 json 对象（PodSpec 一般是 yaml 或者 json 格式的文本文件）。这些 PodSpecs 有多个个来源&lt;/p>
&lt;ul>
&lt;li>&lt;strong>apiserver&lt;/strong> # 使用最多的方式，通过 kubectl 命令向 apiserver 提交 PodSpec 文件，然后 apiserver 再下发给相应节点的 node。还有一个通过 APIServer 监听 etcd 目录，同步 PodSpec&lt;/li>
&lt;li>&lt;strong>File&lt;/strong> # kubelet 定期监控某个路径(默认路径为/etc/kubernetes/manifests)下所有文件，把这些文件当做 PodSpec，这种方式也就是所谓的 [StaticPod(静态 Pod)](Static Pod)。默认情况下每 20 秒监控一下，可以通过 flag 进行配置，配置时可以指定具体的路径以及监控周期&lt;/li>
&lt;li>&lt;strong>HTTP endpoint(URL)&lt;/strong> # 使用&amp;ndash;manifest-url 参数，让 kubelet 每 20 秒检查一次 URL 指定的 endpoint(端点)&lt;/li>
&lt;li>&lt;strong>HTTP server&lt;/strong> # kubelet 监听 HTTP 请求，并响应简单的 API 以提交新的 Pod 清单&lt;/li>
&lt;/ul>
&lt;h2 id="static-pod">Static Pod&lt;a class="td-heading-self-link" href="#static-pod" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>所有以非 API Server 方式创建的 Pod 都叫 Static Pod(静态 Pod)。&lt;/p>
&lt;p>kubelet 的工作核心，就是一个控制循环。驱动这个控制循环运行的实践，包括四种&lt;/p>
&lt;ol>
&lt;li>Pod 更新事件&lt;/li>
&lt;li>Pod 生命周期变化&lt;/li>
&lt;li>kubelet 本身设置的执行周期&lt;/li>
&lt;li>定时的清理事件&lt;/li>
&lt;/ol>
&lt;p>注意：kubelet 调用下层容器运行时的执行过程，并不会直接调用 Docker 的 API，而是通过一组叫作 CRI（Container Runtime Interface，容器运行时接口）的 gRPC 接口来间接执行的。gRPC 接口规范详见官网：&lt;a href="https://grpc.io/docs/">https://grpc.io/docs/&lt;/a>&lt;/p>
&lt;p>kubelet 是集群的基础设施，其他主要组件如果不以 daemon 形式运行，则依赖 kubelet 以 pod 方式启动，这样，才可以组成集群最基本的形态。&lt;/p>
&lt;h2 id="kubelet-metrics">Kubelet Metrics&lt;a class="td-heading-self-link" href="#kubelet-metrics" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>详见：k8s 主要组件 metrics 获取指南&lt;/p>
&lt;h1 id="kubelet-部署">Kubelet 部署&lt;a class="td-heading-self-link" href="#kubelet-%e9%83%a8%e7%bd%b2" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &lt;span style="color:#4e9a06">&amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">[kubernetes]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">name=Kubernetes
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">enabled=1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">gpgcheck=1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">repo_gpgcheck=1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>yum install -y kubelet
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="kubelet-关联文件与配置">Kubelet 关联文件与配置&lt;a class="td-heading-self-link" href="#kubelet-%e5%85%b3%e8%81%94%e6%96%87%e4%bb%b6%e4%b8%8e%e9%85%8d%e7%bd%ae" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>kubelet 可以通过多个地方读取其自身的配置并更改自己的行为方式，可以通过指定的 yaml 格式的文件读取配置信息，也可以直接指定命令行参数传递到 kubelet 程序中。&lt;/p>
&lt;p>&lt;strong>/var/lib/kubelet/&lt;/strong> # kubelet 配置文件目录、以及运行时数据目录，包含基础配置文件、证书、通过 kubelet 启动的容器信息等等&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./config.yaml&lt;/strong> # kubelet 基础配置文件。一般在 kubelet 启动时使用 &amp;ndash;cofnig 参数指定该读取该文件的路径进行加载。
&lt;ul>
&lt;li>Note：该文件内容与 kubectl get configmap -n kube-system kubelet-config-X.XX -o yaml 命令所得结果一样
&lt;ul>
&lt;li>如果想要在 kubelet 运行时动态得更改其配置，则可以修改 configmap 中的内容，详见：&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>./kubeadm-flags.env&lt;/strong> # 该文件将内容作为 kubelet 参数，在 kubelet 启动时加载，常用来在 kubeadm 初始化时使用&lt;/li>
&lt;li>&lt;strong>./pods/&lt;/strong> # kubelet 启动的 Pod 的数据保存路径，其内目录名为 Pod 的 uid 。
&lt;ul>
&lt;li>./${POD_UID}/volumes/ # 对应 pod 挂载的 volume 保存路径，其内目录为 &lt;code>kubernetes.io~TYPE/&lt;/code> ，其中 TYPE 为 volume 的类型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>./pki/&lt;/strong> # kubelet 与 apiserver 交互时所用到的证书存放目录。
&lt;ul>
&lt;li>./kubelet.crt # 在 kubelet 完成 TLS bootstrapping 后并且没有配置 &amp;ndash;feature-gates=RotateKubeletServerCertificate=true 时生成；这种情况下该文件为一个独立于 apiserver CA 的自签 CA 证书，有效期为 1 年；被用作 kubelet 10250 api 端口。当其他东西需要访问 kubelet 的 api 时，需要使用该证书作为认证。&lt;/li>
&lt;li>./kubelet-client-current.pem # 与 API server 通讯所用到的证书，与 apiserver 交互后生成。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/kubernetes/&lt;/strong> # Kubernetes 系统组件运行时目录。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>./manifests/&lt;/strong> # Kubelet 默认从该目录中读取 Pod 的 Manifests 文件，以运行静态类型 Pod。&lt;/li>
&lt;li>kubelet 在 k8s 集群交互时认证文件所在目录，kubelet 需要读取认证配置，用来与 apiserver 进行交互。
&lt;ul>
&lt;li>&lt;strong>./bootstrap-kubelet.conf&lt;/strong> # 用于 TLS 引导程序的 KubeConfig 文件。该 kubeconfig 文件的用户信息为 token。该文件用于 kubelet 所在节点不在集群中时，向集群发起注册请求所用，如果节点已在集群中，则会自动生成 kubelet.conf 文件&lt;/li>
&lt;li>&lt;strong>./kubelet.conf&lt;/strong> # 具有唯一 kubelet 标识的 KubeConfig 文件(与 kubectl 的 config 文件一样，用于 kubelet 与 apiserver 交互时提供认证信息)。该 kubeconfig 文件的用户信息为客户端证书和私钥，一般在 kubelet 启动时由 bootstrap-kubelet.conf 文件生成。
&lt;ul>
&lt;li>当该文件不存在时，会在 kubelet 启动时，由 bootstrap-kubelet.confg 生成&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/etc/sysconfig/kubelet&lt;/strong> # 与 /var/lib/kubelet/kubeadm-flags.env 文件作用一样 ，将内容作为 kubelet 参数，在 kubelet 启动时加载。一般用于让用户指定 kubelet 的运行时参数 。 KUBELET_EXTRA_ARGS 在标志链中排在最后，并且在其他设置冲突时具有最高优先级。&lt;/p>
&lt;ul>
&lt;li>Note：对于 DEB 系统，配置文件位于：/etc/default/kubelet&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.confg&lt;/strong> # 与 kubelet 守护进程运行参数&lt;/p>
&lt;h1 id="kubelet-的启动过程--kubelet-与-apiserver-的交互说明">Kubelet 的启动过程 &amp;amp;&amp;amp; Kubelet 与 APIServer 的交互说明&lt;a class="td-heading-self-link" href="#kubelet-%e7%9a%84%e5%90%af%e5%8a%a8%e8%bf%87%e7%a8%8b--kubelet-%e4%b8%8e-apiserver-%e7%9a%84%e4%ba%a4%e4%ba%92%e8%af%b4%e6%98%8e" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="kubelet-启动过程">kubelet 启动过程&lt;a class="td-heading-self-link" href="#kubelet-%e5%90%af%e5%8a%a8%e8%bf%87%e7%a8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>kubelet 启动流程源码分析
&lt;ul>
&lt;li>&lt;a href="https://xiaohanliang.gitbook.io/notes/k8s/zhi-shi-yu-ding-yi/jie-dian-zu-jian-kubelet/kubelet-qi-dong-liu-cheng">https://xiaohanliang.gitbook.io/notes/k8s/zhi-shi-yu-ding-yi/jie-dian-zu-jian-kubelet/kubelet-qi-dong-liu-cheng&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.jianshu.com/p/e07d84cce9f9">https://www.jianshu.com/p/e07d84cce9f9&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ol>
&lt;li>读取配置 kubelet 配置文件。kubelet 启动时首先会根据 &lt;code>--config=PATH&lt;/code> 参数指定路径(默认 /var/lib/kubelet/config.yaml)读取配置文件，并根据其内配置加载 kubelet 相关参数，
&lt;ol>
&lt;li>根据 ca 配置路径加载 ca.crt 文件，并在 /var/lib/kubelet/pki 目录下生成关于 kubelet 的 10250 私有 api 端口所需的 crt 与 key 文件。&lt;/li>
&lt;li>如果该文件不存在或有问题，则启动失败。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>配置与 apiserver 通信的 kubeconfig 文件。根据 &amp;ndash;bootstrap-kubeconfig=PATH 参数加载 /etc/kubernetes/bootstrap-kubelet.conf 文件(如果不存在则根据 &amp;ndash;kubeconfig=PATH 参数加载 /etc/kubernetes/kubelet.conf 文件)，两个文件都不存在则报错
&lt;ol>
&lt;li>如果当前节点不在集群中，则会执行证书申请操作
&lt;ol>
&lt;li>首先 kubelet 向 bootstrap-kubelet.conf 文件内配置的 apiserver(文件中 server 的配置) 发送加入集群的申请申，同时 kubelet 会根据 bootstrap-kubelet.conf 文件生成 kubelet.conf 文件，将该 kubeconfig 文件中的 user 认证方式改为证书认证方式，并指定证书路径为/var/lib/kubelet/pki/kubelet-client-current.pem。&lt;/li>
&lt;li>在集群 master 上执行 kubectl get csr 命令获取当前申请列表，并使用 kubectl certificate approve XXXX 命令通过该节点的申请&lt;/li>
&lt;li>master 节点的 controller-manager 处理完该请求后，本地 kubelet 将生成 kubelet.conf 文件所需证书，并保存在 /var/lib/kubelet/pki/ 目录下，以 /var/lib/kubelet/pki/kubelet-client-current.pem-TIME 命名，并建立软件链接指向该文件。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>如果当前节点已在集群中，则会根据 bootstrap-kubelet.conf 文件生成 kubelet.conf 文件，且根据 kubelet.conf 文件中的证书信息与 apiserver 进行通信。&lt;/li>
&lt;li>如果当前节点已在集群且存在 kubelet.conf 文件，则使用该 kubeconfig 文件与 apiserver 进行交互后执行后续操作&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>初始化 kubelet 组件内部模块
&lt;ol>
&lt;li>会在 /var/lib/kubelet 目录下添加相关文件，用以驱动 pod 及 kubelet 相关功能。&lt;/li>
&lt;li>检查 cgroup 驱动设置与 CRI 的 cgroup 驱动设置是否一致，如果不一致则启动失败&lt;/li>
&lt;li>等等操作，后续有发现再补充&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>启动 kubelet 内部模块及服务(Note：当 csr 申请发送后，即可执行后续工作，无需等待申请审批通过)&lt;/li>
&lt;/ol>
&lt;h2 id="kubelet-启动后的工作原理">kubelet 启动后的工作原理&lt;a class="td-heading-self-link" href="#kubelet-%e5%90%af%e5%8a%a8%e5%90%8e%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。如下图所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kubernetes/kubelet/1616120074647-12f084c2-e91c-475d-93d4-f4e3fd3e61ef.png" alt="">&lt;/p>
&lt;h3 id="kubelet-创建-pod-流程">kubelet 创建 pod 流程&lt;a class="td-heading-self-link" href="#kubelet-%e5%88%9b%e5%bb%ba-pod-%e6%b5%81%e7%a8%8b" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>参考：&lt;/p>
&lt;p>&lt;a href="https://www.jianshu.com/p/5e0c9d1dbe95">https://www.jianshu.com/p/5e0c9d1dbe95&lt;/a>
&lt;a href="https://www.kubernetes.org.cn/6766.html">https://www.kubernetes.org.cn/6766.html&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kubernetes/kubelet/1616120074625-d25d5fad-d58b-4142-958d-b1aee16d8e71.png" alt="">&lt;/p>
&lt;p>Note：注意 14，,15 步，kubelet 会先将生成配置(volume 挂载、配置主机名等等)，才会去启动 pod，哪怕 pod 启动失败，挂载依然存在。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/kubernetes/kubelet/1616120074636-5f1f460b-6cf2-4190-ab01-6284178805e6.png" alt="">&lt;/p>
&lt;h1 id="kubelet-所管理三大板块">Kubelet 所管理三大板块&lt;a class="td-heading-self-link" href="#kubelet-%e6%89%80%e7%ae%a1%e7%90%86%e4%b8%89%e5%a4%a7%e6%9d%bf%e5%9d%97" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="kubelet-负责所在节点-containercri-的管理">kubelet 负责所在节点 Container(CRI) 的管理&lt;a class="td-heading-self-link" href="#kubelet-%e8%b4%9f%e8%b4%a3%e6%89%80%e5%9c%a8%e8%8a%82%e7%82%b9-containercri-%e7%9a%84%e7%ae%a1%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>CRI 的起源：&lt;/p>
&lt;p>官方介绍：&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&lt;/a>&lt;/p>
&lt;p>Kubernetes 项目之所以要在 kubelet 中引入这样一层单独的抽象，是为了对 Kubernetes 屏蔽下层容器运行时的差异。因为 Kubernets 是一个编排工具，不只可以编排 Docker，还可以编排除 Docker 以外的其余的容器项目。而每种容器项目的实现方式不尽相同，为了解决这个问题，那么可以把 kubelet 对容器的操作，统一抽象成一个借口，这样，kubelet 就只需要跟这个借口打交道，而作为具体的容器项目，它们只需要自己提供一个该接口的实现，然后对 kubelet 暴露出 gRPC 服务即可(docker shim 现在集成在了 kubelet 中，以后会单独拿出来甚至废弃)&lt;/p>
&lt;p>CRI 的运作方式：&lt;/p>
&lt;ul>
&lt;li>当 kubernetes 通过编排能力声明一个 Pod 后，调度器会为这个 pod 选择一个具体的 Node 来运行，这时候，该 Node 上的 kubelet 会通过 SyncLoop 判断需要执行的具体操作，这个时候 kubelet 会调用一个叫做 GenericRuntime 的通用组件来发起创建 Pod 的 CRI 请求。&lt;/li>
&lt;li>CRI shim(CRI 垫片，宿主机与 kubelet 之间的东西)来响应 CRI 请求，然后把请求“翻译”成对后端容器项目的请求或者操作。&lt;/li>
&lt;li>每个容器项目都会自己实现一个 CRI shim，然后 CRI shim 收到的请求会转给对应的容器守护进程(e.g.docker 项目里的 dockerd)，由该守护进程进行容器的创建、更改、删除、exec 等操作&lt;/li>
&lt;/ul>
&lt;p>当前主流的 CRI 有如下几种：&lt;/p>
&lt;ul>
&lt;li>Docker(kubelet 默认的 CRI)
&lt;ul>
&lt;li>Note：kubelet 内置 dockershim，在启动或，会生成 dockersim.sock 文件，kubelet 与 crictl 都会默认与该文件关联&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CRI-O&lt;/li>
&lt;li>Containerd(通常与 docker 同时安装)&lt;/li>
&lt;li>Frakti(kata OCI 的实现)&lt;/li>
&lt;/ul>
&lt;p>kubelet 与 CRI 对接的方式&lt;/p>
&lt;ul>
&lt;li>kubelet 根据参数 &amp;ndash;container-runtime-endpoint 来决定其所绑定的 CRI sock。默认使用 docker 的 sock，路径为：/var/run/dockershim.sock
&lt;ul>
&lt;li>可以通过 crictl 工具来测试目标 sock 是否可用，crictl 用法详见：crictl 命令行工具&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>如果使用不同的 CRI 运行时，则需要为 kubelet 指定不同的 flag。 例如，当使用非 docker CRI 时， 则需要使用 &amp;ndash;container-runtime=remote 与 &amp;ndash;container-runtime-path-endpoint=&amp;lt;PATH&amp;gt; 指定 CRI 端点。endpoint 的值为指定 CRI 的 sock 文件。&lt;/li>
&lt;li>各个 CRI 需要进行配置才可与 kubelet 对接成功，如果不进行初始化配置，则 kubelet 无法获取到该 CRI 对于 k8s 的相关配置参数&lt;/li>
&lt;/ul>
&lt;p>kubelet 通过如下两个命令行标志来指定要使用的 CRI&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>--container-runtime&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>remote
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--container-runtime-endpoint&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>unix:///run/containerd/containerd.sock
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="kubelet-负责所在节点-networkcni-的管理">kubelet 负责所在节点 Network(CNI) 的管理&lt;a class="td-heading-self-link" href="#kubelet-%e8%b4%9f%e8%b4%a3%e6%89%80%e5%9c%a8%e8%8a%82%e7%82%b9-networkcni-%e7%9a%84%e7%ae%a1%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>kubelet 对 cni(Container Network Interface 容器网络接口)的调用详见另一片关于网络介绍的文章：&lt;a href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes%20%E7%BD%91%E7%BB%9C/Kubernetes%20%E7%BD%91%E7%BB%9C.md">Kubernetes 网络&lt;/a>&lt;/p>
&lt;p>kubelet 配置 pod 网络时，首先会读取下 /etc/cni/net.d/&lt;em>目录下的配置，查看当前所使用的 CNI 插件及插件参数，比如现在是 flannel ，那么 flannel 会将 /run/flannel/subnet.env 文件的配置信息传递给 kubelet ，然后 kubelet 使用 /opt/cni/bin/&lt;/em> 目录中的二进制文件，来处理处理 pod 的网络信息。&lt;/p>
&lt;h2 id="kubelet-负责所在节点-volumecvi-的管理">kubelet 负责所在节点 Volume(CVI) 的管理&lt;a class="td-heading-self-link" href="#kubelet-%e8%b4%9f%e8%b4%a3%e6%89%80%e5%9c%a8%e8%8a%82%e7%82%b9-volumecvi-%e7%9a%84%e7%ae%a1%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h2></description></item><item><title>Docs: Kubelet 配置</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Kubelet-%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 04 Nov 2019 09:09:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Kubelet-%E9%85%8D%E7%BD%AE/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">官方文档，入门-生产环境-使用工具安装 Kubernetes-使用 kubeadm 引导集群-使用 kubeadm 配置集群中每个 kubelet&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">官方文档，参考-配置 APIs-Kubelet 配置(v1beta1)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>可以通过两种方式配置 kubelet 运行时行为&lt;/p>
&lt;ol>
&lt;li>&lt;strong>config.yaml 配置文件&lt;/strong> # config.yaml 文件默认路径为 /var/lib/kubelet/config.yaml ，可以通过 &amp;ndash;config &amp;lt;FILE&amp;gt; 来指定其他的文件。
&lt;ol>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">这里&lt;/a>是官方文档对于配置文件的概述。在&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go">章节中间部分&lt;/a>，可以直接看到配置文件对应的代码中结构体，也就是配置文件详细内容&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">这里&lt;/a>是配置文件中每个字段的详解，与代码中的结构体互相对应，只不过是整理后，可以直接在网页上查看，更清晰。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>kubelet 命令行标志&lt;/strong>
&lt;ol>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">这里&lt;/a>是官方文档对命令行标志的详解&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>官方更推荐使用第一种方式，通过 config.yaml 的文件修改，来改变 kubelet 的运行时参数。&lt;/p>
&lt;p>很多配置文件的内容与命令行标志具有一一对应的关系，比如：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>配置文件&lt;/th>
&lt;th>命令行标志&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cgroupDriver: systemd&lt;/td>
&lt;td>&amp;ndash;cgroup-driver=systemd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>clusterDNS: [10.96.0.10,&amp;hellip;]&lt;/td>
&lt;td>&amp;ndash;cluster-dns=10.96.0.10,&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>authentication.x509.clientCAFile: /etc/kubernetes/pki/ca.crt&lt;/td>
&lt;td>&amp;ndash;client-ca-file=/etc/kubernetes/pki/ca.crt&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>等等&lt;/td>
&lt;td>等等&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>但是也有一些是没有对应关系的，只能通过配置文件，或者命令行标志配置。比如命令行标志的 &lt;code>--container-runtime&lt;/code> 就无法在配置文件中配置。在命令行标志官方文档中，凡是标着 &lt;code>DEPRECATED&lt;/code> 的命令行标志，都是可以在配置文件中配置的。&lt;/p>
&lt;h1 id="命令行标志详解">命令行标志详解&lt;a class="td-heading-self-link" href="#%e5%91%bd%e4%bb%a4%e8%a1%8c%e6%a0%87%e5%bf%97%e8%af%a6%e8%a7%a3" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">官方文档,参考-组件工具-kubelet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>&amp;ndash;cni-conf-dir &amp;lt;STRING&amp;gt;&lt;/strong> # Warning：Alpha 功能。指定 STRING 目录中搜索 CNI 配置文件。 &lt;code>默认值：/etc/cni/net.d&lt;/code>&lt;/p>
&lt;ul>
&lt;li>仅当 CRI 为 docker 时此标志才有效&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&amp;ndash;config=&amp;lt;STRING&amp;gt;&lt;/strong> # 加载配置文件的路径。kubelet 将从该标志指定的文件中加载其初始配置。&lt;/p>
&lt;p>&lt;strong>&amp;ndash;pod-infra-container-image &amp;lt;STRINIG&amp;gt;&lt;/strong> # 指定在每个 pod 中将会使用的 network/ipc 名称空间的基础容器。&lt;code>默认值：k8s.gcr.io/pause:3.1&lt;/code>&lt;/p>
&lt;ul>
&lt;li>这个就是用来指定 infra 基础设施容器。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&amp;ndash;container-runtime &amp;lt;STRING&amp;gt;&lt;/strong> # kubelet 要使用的容器运行时，也就是要对接的 CRI。&lt;code>默认值：docker&lt;/code>。&lt;/p>
&lt;ul>
&lt;li>remote # 表示使用其他运行时。需要配合 &lt;code>--container-runtime-endpoint&lt;/code> 标志一起使用。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&amp;ndash;container-runtime-endpoint &amp;lt;STRING&amp;gt;&lt;/strong> # kubelet 要使用的运行时的路径。&lt;code>默认值：unix:///var/run/dockershim.sock&lt;/code>&lt;/p>
&lt;ul>
&lt;li>STRING 是 socket 路径，现阶段只支持 UNIX sock，后面还可以支持远程，比如通过 http 来连接运行时。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>&amp;ndash;image-service-endpoint &amp;lt;STRING&amp;gt;&lt;/strong> # kubelet 处理镜像所用的后端路径。若未指定，则于 &lt;code>--container-runtime-endpoint&lt;/code> 标志的值相同&lt;/p>
&lt;h1 id="配置文件详解">配置文件详解&lt;a class="td-heading-self-link" href="#%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e8%af%a6%e8%a7%a3" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">官方文档,任务-通过配置文件设置 kubelet 参数&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">官方文档,参考-配置 APIs-Kubelet 配置&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go">代码&lt;/a> # 该代码是 Go 结构体与 JSON 格式的解析对应关系。其中的注释就是配置文件各字段的含义&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;strong>apiVersion:&lt;/strong> kubelet.config.k8s.io/v1beta1&lt;/p>
&lt;p>&lt;strong>kind:&lt;/strong> KubeletConfiguration&lt;/p>
&lt;p>&lt;strong>address(STRING)&lt;/strong> # kubelet 服务的 IP。默认为 0.0.0.0&lt;/p>
&lt;p>&lt;strong>cgroupDriver(cgroupfs|systemd)&lt;/strong> # kubelet 用于操纵主机上 cgroup 的驱动程序。&lt;code>默认值：cgroupfs&lt;/code>&lt;/p>
&lt;p>&lt;strong>imageMinimumGCAge(DURATION)&lt;/strong> # 未使用的 image 进行垃圾回收之前的最小期限。&lt;code>默认值：2m&lt;/code>&lt;/p>
&lt;p>&lt;strong>nodeStatusReportFrequency(DURATION)&lt;/strong> # 节点状态报告频率。&lt;code>默认值：10s&lt;/code>&lt;/p>
&lt;p>&lt;strong>nodeStatusUpdateFrequency(DURATION)&lt;/strong> # 节点状态更新频率。&lt;code>默认值：5m&lt;/code>&lt;/p>
&lt;p>&lt;strong>resolvConf(STRING)&lt;/strong> # kubelet 启动的容器所使用的解析器的配置文件。&lt;code>默认值：/etc/resolv.conf&lt;/code>&lt;/p>
&lt;ul>
&lt;li>Ubuntu 中，配置则会被改为 &lt;code>/run/systemd/resolve/resolv.conf&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="配置文件示例">配置文件示例&lt;a class="td-heading-self-link" href="#%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e7%a4%ba%e4%be%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>下面是 kubelet 1.18.8 版本的基本示例，其中通过 kubeadm 传递了 cgroupDriver 的值。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">apiVersion&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">kubelet.config.k8s.io/v1beta1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">kind&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">KubeletConfiguration&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">authentication&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">anonymous&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">false&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">webhook&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">cacheTTL&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">x509&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">clientCAFile&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">/etc/kubernetes/pki/ca.crt&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">authorization&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">mode&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">Webhook&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">webhook&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">cacheAuthorizedTTL&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">cacheUnauthorizedTTL&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">cgroupDriver&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">systemd&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#8f5902;font-style:italic">#kubelet 用于操纵主机上 cgroup 的驱动程序。 (默认为 cgroupfs )&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">clusterDNS&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#0000cf;font-weight:bold">10.96.0.10&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">clusterDomain&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">cluster.local&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">cpuManagerReconcilePeriod&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">evictionPressureTransitionPeriod&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">fileCheckFrequency&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">healthzBindAddress&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">127.0.0.1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">healthzPort&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10248&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">httpCheckFrequency&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">imageMinimumGCAge&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">nodeStatusReportFrequency&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">nodeStatusUpdateFrequency&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">rotateCertificates&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">runtimeRequestTimeout&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">staticPodPath&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">/etc/kubernetes/manifests&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">streamingConnectionIdleTimeout&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">syncFrequency&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumeStatsAggPeriod&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">0s&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Runtime</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Runtime/</link><pubDate>Wed, 13 Nov 2019 22:09:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Runtime/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>原文链接：&lt;a href="https://aleiwu.com/post/cncf-runtime-landscape/">白话 Kubernetes Runtime&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>回想最开始接触 k8s 的时候, 经常搞不懂 CRI 和 OCI 的联系和区别, 也不知道为啥要垫那么多的 “shim”(尤其是 containerd-shim 和 dockershim 这两个完全没啥关联的东西还恰好都叫 shim). 所以嘛, 这篇就写一写 k8s 的 runtime 部分, 争取一篇文章把下面这张 Landscape 里的核心项目给白话明白。&lt;/p>
&lt;p>(以上理由其实都是为了说服自己写写水文也是可以的…)&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553266-dd129ef4-21d8-42bd-b1ed-ecbbdbca2e78.png" alt="">&lt;/p>
&lt;h1 id="典型的-runtime-架构">典型的 Runtime 架构&lt;a class="td-heading-self-link" href="#%e5%85%b8%e5%9e%8b%e7%9a%84-runtime-%e6%9e%b6%e6%9e%84" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>我们从最常见的 runtime 方案 Docker 说起, 现在 Kubelet 和 Docker 的集成还是挺啰嗦的:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553275-0587e792-8d38-4736-a4cb-1a1216dd9617.png" alt="">&lt;/p>
&lt;p>当 Kubelet 想要创建一个容器时, 有这么几步:&lt;/p>
&lt;ol>
&lt;li>Kubelet 通过 CRI 接口(gRPC) 调用 dockershim, 请求创建一个容器. CRI 即容器运行时接口(Container Runtime Interface), 这一步中, Kubelet 可以视作一个简单的 CRI Client, 而 dockershim 就是接收请求的 Server. 目前 dockershim 的代码其实是内嵌在 Kubelet 中的, 所以接收调用的凑巧就是 Kubelet 进程;&lt;/li>
&lt;li>dockershim 收到请求后, 转化成 Docker Daemon 能听懂的请求, 发到 Docker Daemon 上请求创建一个容器;&lt;/li>
&lt;li>Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程: containerd 中了, 因此 Docker Daemon 仍然不能帮我们创建容器, 而是要请求 containerd 创建一个容器;&lt;/li>
&lt;li>containerd 收到请求后, 并不会自己直接去操作容器, 而是创建一个叫做 containerd-shim 的进程, 让 containerd-shim 去操作容器. 这是因为容器进程需要一个父进程来做诸如收集状态, 维持 stdin 等 fd 打开等工作. 而假如这个父进程就是 containerd, 那每次 containerd 挂掉或升级, 整个宿主机上所有的容器都得退出了. 而引入了 containerd-shim 就规避了这个问题(containerd 和 shim 并不需要是父子进程关系, 当 containerd 退出或重启时, shim 会 re-parent 到 systemd 这样的 1 号进程上);&lt;/li>
&lt;li>我们知道创建容器需要做一些设置 namespaces 和 cgroups, 挂载 root filesystem 等等操作, 而这些事该怎么做已经有了公开的规范了, 那就是 OCI(Open Container Initiative, 开放容器标准). 它的一个参考实现叫做 runc. 于是, containerd-shim 在这一步需要调用 runc 这个命令行工具, 来启动容器;&lt;/li>
&lt;li>runc 启动完容器后本身会直接退出, containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程;&lt;/li>
&lt;/ol>
&lt;p>这个过程乍一看像是在搞我们: Docker Daemon 和 dockershim 看上去就是两个不干活躺在中间划水的啊, Kubelet 为啥不直接调用 containerd 呢?&lt;/p>
&lt;p>当然是可以的, 不过咱们先不提那个, 先看看为什么现在的架构如此繁冗.&lt;/p>
&lt;h1 id="小插曲-容器历史小叙不负责任版">小插曲: 容器历史小叙(不负责任版)&lt;a class="td-heading-self-link" href="#%e5%b0%8f%e6%8f%92%e6%9b%b2-%e5%ae%b9%e5%99%a8%e5%8e%86%e5%8f%b2%e5%b0%8f%e5%8f%99%e4%b8%8d%e8%b4%9f%e8%b4%a3%e4%bb%bb%e7%89%88" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>其实 k8s 最开始的 Runtime 架构远没这么复杂: kubelet 想要创建容器直接跟 Docker Daemon 说一声就行, 而那时也不存在 containerd, Docker Daemon 自己调一下 libcontainer 这个库把容器跑起来, 整个过程就搞完了.&lt;/p>
&lt;p>而熟悉容器和容器编排历史的读者老爷应该知道, 这之后就是容器圈的一系列政治斗争, 先是大佬们认为运行时标准不能被 Docker 一家公司控制, 于是就撺掇着搞了开放容器标准 OCI. Docker 则把 libcontainer 封装了一下, 变成 runC 捐献出来作为 OCI 的参考实现.&lt;/p>
&lt;p>再接下来就是 rkt 想从 docker 那边分一杯羹, 希望 k8s 原生支持 rkt 作为运行时, 而且 PR 还真的合进去了. 维护过一块业务同时接两个需求方的读者老爷应该都知道类似的事情有多坑, k8s 中负责维护 kubelet 的小组 sig-node 也是被狠狠坑了一把.&lt;/p>
&lt;p>大家一看这么搞可不行, 今天能有 rkt, 明天就能有更多幺蛾子出来, 这么搞下去我们小组也不用干活了, 整天搞兼容性的 bug 就够呛. 于是乎, k8s 1.5 推出了 CRI 机制, 即容器运行时接口(Container Runtime Interface), k8s 告诉大家, 你们想做 Runtime 可以啊, 我们也资瓷欢迎, 实现这个接口就成, 成功反客为主.&lt;/p>
&lt;p>不过 CRI 本身只是 k8s 推的一个标准, 当时的 k8s 尚未达到如今这般武林盟主的地位, 容器运行时当然不能说我跟 k8s 绑死了只提供 CRI 接口, 于是就有了 shim(垫片) 这个说法, 一个 shim 的职责就是作为 Adapter 将各种容器运行时本身的接口适配到 k8s 的 CRI 接口上.&lt;/p>
&lt;p>接下来就是 Docker 要搞 Swarm 进军 PaaS 市场, 于是做了个架构切分, 把容器操作都移动到一个单独的 Daemon 进程 containerd 中去, 让 Docker Daemon 专门负责上层的封装编排. 可惜 Swarm 在 k8s 面前实在是不够打, 惨败之后 Docker 公司就把 containerd 项目捐给 CNCF 缩回去安心搞 Docker 企业版了.&lt;/p>
&lt;p>最后就是我们在上一张图里看到的这一坨东西了, 尽管现在已经有 CRI-O, containerd-plugin 这样更精简轻量的 Runtime 架构, dockershim 这一套作为经受了最多生产环境考验的方案, 迄今为止仍是 k8s 默认的 runtime 实现.&lt;/p>
&lt;p>了解这些具体的架构有时能在 debug 时候帮我们一些忙, 但更重要的是它们能作为一个例子, 帮助我们更好地理解整个 k8s runtime 背后的设计逻辑, 我们这就言归正传.&lt;/p>
&lt;h1 id="oci-cri-与被滥用的名词-runtime">OCI, CRI 与被滥用的名词 “Runtime”&lt;a class="td-heading-self-link" href="#oci-cri-%e4%b8%8e%e8%a2%ab%e6%bb%a5%e7%94%a8%e7%9a%84%e5%90%8d%e8%af%8d-runtime" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>OCI 是对 Docker 来说的 runtime。i.e.docker 使用什么 oci-runtime 来启动容器&lt;/p>
&lt;p>CRI 是对 kubernetes 来说的。i.e.kubernetes 使用什么 cri-runtime 来与后面操作容器的程序对接&lt;/p>
&lt;p>OCI, 也就是前文提到的”开放容器标准”其实就是一坨文档, 其中主要规定了两点:&lt;/p>
&lt;ol>
&lt;li>容器镜像要长啥样, 即 ImageSpec. 里面的大致规定就是你这个东西需要是一个压缩了的文件夹, 文件夹里以 xxx 结构放 xxx 文件;&lt;/li>
&lt;li>容器要需要能接收哪些指令, 这些指令的行为是什么, 即 RuntimeSpec. 这里面的大致内容就是”容器”要能够执行 “create”, “start”, “stop”, “delete” 这些命令, 并且行为要规范.&lt;/li>
&lt;/ol>
&lt;p>runC 为啥叫参考实现呢, 就是它能按照标准将符合标准的容器镜像运行起来(当然, 这里为了易读性略去了很多细节, 要了解详情建议点前文的链接读文档)&lt;/p>
&lt;p>标准的好处就是方便搞创新, 反正只要我符合标准, 生态圈里的其它工具都能和我一起愉快地工作(…当然 OCI 这个标准本身制订得不怎么样, 真正工程上还是要做一些 adapter 的), 那我的镜像就可以用任意的工具去构建, 我的”容器”就不一定非要用 namespace 和 cgroups 来做隔离. 这就让各种虚拟化容器可以更好地参与到游戏当中, 我们暂且不表.&lt;/p>
&lt;p>CRI 更简单, 单纯是一组 gRPC 接口, 扫一眼 kubelet/apis/cri/services.go 就能归纳出几套核心接口:&lt;/p>
&lt;ul>
&lt;li>一套针对容器操作的接口, 包括创建,启停容器等等;&lt;/li>
&lt;li>一套针对镜像操作的接口, 包括拉取镜像删除镜像等;&lt;/li>
&lt;li>还有一套针对 PodSandbox (容器沙箱环境) 的操作接口, 我们之后再说;&lt;/li>
&lt;/ul>
&lt;p>现在我们可以找到很多符合 OCI 标准或兼容了 CRI 接口的项目, 而这些项目就大体构成了整个 Kuberentes 的 Runtime 生态:&lt;/p>
&lt;ul>
&lt;li>OCI Compatible: runC, Kata(以及它的前身 runV 和 Clear Containers), gVisor. 其它比较偏门的还有 Rust 写的 railcar&lt;/li>
&lt;li>CRI Compatible: Docker(借助 dockershim), containerd(借助 CRI-containerd), CRI-O, frakti, etc.&lt;/li>
&lt;/ul>
&lt;p>最开始 k8s 的时候我经常弄不清 OCI 和 CRI 的区别与联系, 其中一大原因就是社区里糟糕的命名: 这上面的项目统统可以称为容器运行时(Container Runtime), 彼此之间区分的办法就是给”容器运行时”这个词加上各种定语和从句来进行修饰. Dave Cheney 有条推说:&lt;/p>
&lt;p>Good naming is like a good joke. If you have to explain it, it’s not funny.&lt;/p>
&lt;p>显然 Container Runtime 在这里就不是一个好名字了, 我们接下来换成一个在这篇文章的语境中更准确的说法: cri-runtime 和 oci-runtime. 通过这个粗略的分类, 我们其实可以总结出整个 runtime 架构万变不离其宗的三层抽象:&lt;/p>
&lt;pre>&lt;code>Orchestration API -&amp;gt; Container API -&amp;gt; Kernel API
&lt;/code>&lt;/pre>
&lt;p>这其中 k8s 已经是 Orchestration API 的事实标准, 而在 k8s 中, Container API 的接口标准就是 CRI, 由 cri-runtime 实现, Kernel API 的规范是 OCI, 由 oci-runtime 实现.&lt;/p>
&lt;p>根据这个思路, 我们就很容易理解下面这两种东西:&lt;/p>
&lt;ul>
&lt;li>各种更为精简的 cri-runtime (反正就是要干掉 Docker)&lt;/li>
&lt;li>各种”强隔离”容器方案&lt;/li>
&lt;/ul>
&lt;h1 id="containerd-和-cri-o">containerd 和 CRI-O&lt;a class="td-heading-self-link" href="#containerd-%e5%92%8c-cri-o" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>我们在第一节就看到现在的 runtime 实在是有点复杂了, 而复杂是万恶之源(其实本质上就是想干掉 docker), 于是就有了直接拿 containerd 做 oci-runtime 的方案. 当然, 除了 k8s 之外, containerd 还要接诸如 Swarm 等调度系统, 因此它不会去直接实现 CRI, 这个适配工作当然就要交给一个 shim 了.&lt;/p>
&lt;p>containerd 1.0 中, 对 CRI 的适配通过一个单独的进程 CRI-containerd 来完成:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553282-8c3efc09-ee3a-4e6d-9627-5f380c3363da.png" alt="">&lt;/p>
&lt;p>containerd 1.1 中做的又更漂亮一点, 砍掉了 CRI-containerd 这个进程, 直接把适配逻辑作为插件放进了 containerd 主进程中:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553327-f8a19256-d235-45cb-b6d5-9336561882a7.png" alt="">&lt;/p>
&lt;p>但在 containerd 做这些事情之情, 社区就已经有了一个更为专注的 cri-runtime: CRI-O, 它非常纯粹, 就是兼容 CRI 和 OCI, 做一个 k8s 专用的运行时:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553308-ce56e68f-09ab-4e08-bc35-b8b120afbd87.png" alt="">&lt;/p>
&lt;p>其中 conmon 就对应 containerd-shim, 大体意图是一样的.&lt;/p>
&lt;p>CRI-O 和 (直接调用)containerd 的方案比起默认的 dockershim 确实简洁很多, 但没啥生产环境的验证案例, 我所知道的仅仅是 containerd 在 GKE 上是 beta 状态. 因此假如你对 docker 没有特殊的政治恨意, 大可不必把 dockershim 这套换掉.&lt;/p>
&lt;h1 id="强隔离容器-kata-gvisor-firecracker">强隔离容器: Kata, gVisor, firecracker&lt;a class="td-heading-self-link" href="#%e5%bc%ba%e9%9a%94%e7%a6%bb%e5%ae%b9%e5%99%a8-kata-gvisor-firecracker" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>一直以来 k8s 都有一个被诟病的点: 难以实现真正的多租户.&lt;/p>
&lt;p>为什么这么说呢, 我们先考虑一下什么样是理想的多租户状态:&lt;/p>
&lt;p>理想来说, 平台的各个租户(tenant)之间应该无法感受到彼此的存在, 表现得就像每个租户独占这整个平台一样. 具体来说, 我不能看到其它租户的资源, 我的资源跑满了不能影响其它租户的资源使用, 我也无法从网络或内核上攻击其它租户.&lt;/p>
&lt;p>k8s 当然做不到, 其中最大的两个原因是:&lt;/p>
&lt;ul>
&lt;li>kube-apiserver 是整个集群中的单例, 并且没有多租户概念&lt;/li>
&lt;li>默认的 oci-runtime 是 runC, 而 runC 启动的容器是共享内核的&lt;/li>
&lt;/ul>
&lt;p>对于第二个问题, 一个典型的解决方案就是提供一个新的 OCI 实现, 用 VM 来跑容器, 实现内核上的硬隔离. runV 和 Clear Containers 都是这个思路. 因为这两个项目做得事情是很类似, 后来就合并成了一个项目 Kata Container. Kata 的一张图很好地解释了基于虚拟机的容器与基于 namespaces 和 cgroups 的容器间的区别:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553328-07dc12ea-490c-43d3-b7c7-c1de3257b81e.png" alt="">&lt;/p>
&lt;p>当然, 没有系统是完全安全的, 假如 hypervisor 存在漏洞, 那么用户仍有可能攻破隔离. 但所有的事情都要对比而言, 在共享内核的情况下, 暴露的攻击面是非常大的, 做安全隔离的难度就像在美利坚和墨西哥之间修 The Great Wall, 而当内核隔离之后, 只要守住 hypervisor 这道关子就后顾无虞了&lt;/p>
&lt;p>嗯, 一个 VM 里跑一个容器, 听上去隔离性很不错, 但不是说虚拟机又笨重又不好管理才切换到容器的吗, 怎么又要走回去了?&lt;/p>
&lt;p>Kata 告诉你, 虚拟机没那么邪恶, 只是以前没玩好:&lt;/p>
&lt;ul>
&lt;li>不好管理是因为没有遵循”不可变基础设施”, 大家都去虚拟机上这摸摸那碰碰, 这台装 Java 8 那台装 Java 6, Admin 是要 angry 的. Kata 则支持 OCI 镜像, 完全可以用上 Dockerfile + 镜像, 让不好管理成为了过去时;&lt;/li>
&lt;li>笨重是因为之前要虚拟化整个系统, 现在我们只着眼于虚拟化应用, 那就可以裁剪掉很多功能, 把 VM 做得很轻量, 因此即便用虚拟机来做容器, Kata 还是可以将容器启动时间压缩得非常短, 启动后在内存上和 IO 上的 overhead 也尽可能去优化;&lt;/li>
&lt;/ul>
&lt;p>不过话说回来, k8s 上的调度单位是 Pod, 是容器组啊, Kata 这样一个虚拟机里一个容器, 同一个 Pod 间的容器还怎么做 namespace 的共享?&lt;/p>
&lt;p>这就要说回我们前面讲到的 CRI 中针对 PodSandbox (容器沙箱环境) 的操作接口了. 第一节中, 我们刻意简化了场景, 只考虑创建一个容器, 而没有讨论创建一个 Pod. 大家都知道, 真正启动 Pod 里定义的容器之前, kubelet 会先启动一个 infra 容器, 并执行 /pause 让 infra 容器的主进程永远挂起. 这个容器存在的目的就是维持住整个 pod 的各种 namespace, 真正的业务容器只要加入 infra 容器的 network 等 namespace 就能实现对应 namespace 的共享. 而 infra 容器创造的这个共享环境则被抽象为 PodSandbox. 每次 kubelet 在创建 Pod 时, 就会先调用 CRI 的 RunPodSandbox 接口启动一个沙箱环境, 再调用 CreateContainer 在沙箱中创建容器.&lt;/p>
&lt;p>这里就已经说出答案了, 对于 Kata Container 而言, 只要在 RunPodSandbox 调用中创建一个 VM, 之后再往 VM 中添加容器就可以了. 最后运行 Pod 的样子就是这样的:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/bc0a9o/1616120553300-2de1d661-ee54-42f1-9359-4174b5c34b58.png" alt="">&lt;/p>
&lt;p>说完了 Kata, 其实 gVisor 和 firecracker 都不言自明了, 大体上都是类似的, 只是:&lt;/p>
&lt;ul>
&lt;li>gVisor 并不会去创建一个完整的 VM, 而是实现了一个叫 “Sentry” 的用户态进程来处理容器的 syscall, 而拦截 syscall 并重定向到 Sentry 的过程则由 KVM 或 ptrace 实现.&lt;/li>
&lt;li>firecracker 称自己为 microVM, 即轻量级虚拟机, 它本身还是基于 KVM 的, 不过 KVM 通常使用 QEMU 来虚拟化除 CPU 和内存外的资源, 比如 IO 设备,网络设备. firecracker 则使用 rust 实现了最精简的设备虚拟化, 为的就是压榨虚拟化的开销, 越轻量越好.&lt;/li>
&lt;/ul>
&lt;h1 id="安全容器与-serverless">安全容器与 Serverless&lt;a class="td-heading-self-link" href="#%e5%ae%89%e5%85%a8%e5%ae%b9%e5%99%a8%e4%b8%8e-serverless" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>你可能觉得安全容器对自己而言没什么用: 大不了我给每个产品线都部署 k8s, 机器池也都隔离掉, 从基础设施的层面就隔离掉嘛.&lt;/p>
&lt;p>这么做当然可以, 但同时也要知道, 这种做法最终其实是以 IaaS 的方式在卖资源, 是做不了真正的 PaaS 乃至 Serverless 的.&lt;/p>
&lt;p>Serverless 要做到所有的用户容器或函数按需使用计算资源, 那必须满足两点:&lt;/p>
&lt;ul>
&lt;li>多租户强隔离: 用户的容器或函数都是按需启动按秒计费, 我们可不能给每个用户预先分配一坨隔离的资源,因此我们要保证整个 Platform 是多租户强隔离的;&lt;/li>
&lt;li>极度轻量: Serverless 的第一个特点是运行时沙箱会更频繁地创建和销毁, 第二个特点是切分的粒度会非常非常细, 细中细就是 FaaS, 一个函数就要一个沙箱. 因此就要求两点: 1. 沙箱启动删除必须飞快; 2. 沙箱占用的资源越少越好. 这两点在 long-running, 粒度不大的容器运行环境下可能不明显, 但在 Serverless 环境下就会急剧被放大. 这时候去做 MicroVM 的 ROI 就比以前要高很多. 想想, 用传统的 KVM 去跑 FaaS, 那还不得亏到姥姥家了?&lt;/li>
&lt;/ul>
&lt;h1 id="结尾">结尾&lt;a class="td-heading-self-link" href="#%e7%bb%93%e5%b0%be" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>这次的内容是越写越多, 感觉怎么都写不完的样子, rkt, lxd 其实都还没涉及, 这里就提供下类比, 大家可以自行做拓展阅读: rkt 跟 docker 一样是一个容器引擎, 特点是无 daemon, 目前项目基本不活跃了; lxc 是 docker 最早使用的容器工具集, 位置可以类比 runc, 提供跟 kernel 打交道的库&amp;amp;命令行工具, lxd 则是基于 lxc 的一个容器引擎, 只不过大多数容器引擎的目标是容器化应用, lxd 的目标则是容器化操作系统.&lt;/p>
&lt;p>最后, 这篇文章涉及内容较多, 如有纰漏, 敬请指正!&lt;/p></description></item><item><title>Docs: CRI 对比</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/CRI-%E5%AF%B9%E6%AF%94/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/CRI-%E5%AF%B9%E6%AF%94/</guid><description>
&lt;p>原文：&lt;a href="https://mp.weixin.qq.com/s/H3vUUvEiOfLkd_YEoo8sNg">阳明公众号&lt;/a>&lt;/p>
&lt;p>下面是我已经测试的几个 CRI，并进行一些基准测试来对他们进行了简单的对比，希望对你有所帮助：&lt;/p>
&lt;ul>
&lt;li>dockershim&lt;/li>
&lt;li>containerd&lt;/li>
&lt;li>crio&lt;/li>
&lt;/ul>
&lt;p>对于 cri-o，已经测试了 2 个后端：runc 和 crun，以测试对 &lt;code>cgroupsv2&lt;/code> 的影响。&lt;/p>
&lt;h2 id="测试环境">测试环境&lt;a class="td-heading-self-link" href="#%e6%b5%8b%e8%af%95%e7%8e%af%e5%a2%83" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>我这里的测试环境是一个 1.19.4 版本的 kubernetes 集群，使用 ansible 进行创建（&lt;a href="https://gitlab.com/incubateur-pe">https://gitlab.com/incubateur-pe&lt;/a>）。集群运行在 kvm 上，配置如下：&lt;/p>
&lt;ul>
&lt;li>master：Centos/7, 2vcpus/2G 内存。&lt;/li>
&lt;li>crio-crun 节点：Fedora-32, 2vcpus/4G 内存。&lt;/li>
&lt;li>其他节点：Centos/7, 2vcpus/4G 内存.&lt;/li>
&lt;/ul>
&lt;p>底层是 i7-9700K ，64G 的内存和一个 mp510 nvme 硬盘。&lt;/p>
&lt;h2 id="创建集群">创建集群&lt;a class="td-heading-self-link" href="#%e5%88%9b%e5%bb%ba%e9%9b%86%e7%be%a4" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>这里我直接使用 molecule 创建一个集群，并配置了它在每个 worker 节点上使用不同的 cri，对应的 ansible 源码位于：&lt;a href="https://gitlab.com/incubateur-pe/kubernetes-bare-metal/-/tree/dev/molecule/criBench">https://gitlab.com/incubateur-pe/kubernetes-bare-metal/-/tree/dev/molecule/criBench&lt;/a>&lt;/p>
&lt;p>使用上面的脚本，执行 &lt;code>molecule converge&lt;/code> 命令后，大概 10 分钟左右，我们就可以得到一个如下所示的 kubernetes 集群。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034837-61ee3a87-ccca-4c3a-91ff-983ad9808b6d.png" alt="">&lt;/p>
&lt;p>接下来我们就可以进行一些简单的基准测试了。&lt;/p>
&lt;h2 id="测试">测试&lt;a class="td-heading-self-link" href="#%e6%b5%8b%e8%af%95" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="1-bucketbench-测试">1. bucketbench 测试&lt;a class="td-heading-self-link" href="#1-bucketbench-%e6%b5%8b%e8%af%95" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Bucketbench (&lt;a href="https://github.com/estesp/bucketbench">https://github.com/estesp/bucketbench&lt;/a>) 是一个可以对容器引擎执行一系列操作的测试工具，它非常适合于了解之前每个节点的性能。&lt;/p>
&lt;p>这里我们的测试参数很简单：&lt;/p>
&lt;ul>
&lt;li>3 个线程&lt;/li>
&lt;li>15 次循环&lt;/li>
&lt;li>run/stop/delete 操作&lt;/li>
&lt;/ul>
&lt;p>对应的结果如下所示（ms 为单位）：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034822-55f1f7a0-b8cf-422b-9490-f0a7b5f8593d.png" alt="">&lt;/p>
&lt;p>我们可以看到在性能上还是有相当大的差异的。但是需要注意的是我们这里为什么测试了 5 个实例呢？上面不是只有 4 个 worker 节点吗？&lt;/p>
&lt;p>这里其实是因为这里我们使用的 docker 客户端并不是 kubernetes 使用的，事实上 docker 实现了 CRI，并提供了一个 socket，这个 socket 和其他 cri socket 一样可以调用。所以这里的区别是：&lt;/p>
&lt;ul>
&lt;li>docker-shim：是通过 cri 的 socket 来做测试&lt;/li>
&lt;li>docker-cli：是通过 docker 客户端来做测试&lt;/li>
&lt;/ul>
&lt;p>但是实际上 docker 并没有想象中那么差，在这个测试中我们可以看到他比 cri-o 要快点，当然这个测试中很明显 dockerd 是表现最好的。&lt;/p>
&lt;h3 id="2-kubernetes-测试">2. kubernetes 测试&lt;a class="td-heading-self-link" href="#2-kubernetes-%e6%b5%8b%e8%af%95" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>上面的测试并不能完整说明这几个 cri 之间的差距，当它们被 kubernetes 使用的时候，它们表现又如何呢？是否不止 &lt;code>run/stop/delete&lt;/code> 这些操作？性能上的差异在真正的集群上又有什么意义吗?&lt;/p>
&lt;p>下面我们就来深入了解下，这次我们使用集群中的 Prometheus、Grafana 来可视化监控指标，对应的自定义 dashboard 数据可以在 &lt;a href="https://gitlab.com/ulrich.giraud/bench-cri/-/blob/master/dashboard/dashboard_bench.json">https://gitlab.com/ulrich.giraud/bench-cri/-/blob/master/dashboard/dashboard_bench.json&lt;/a> 这里获取。由于只是测试容器运行时，不是工作负载，所以这里我们只是简单的在集群中部署的一个 busybox 镜像并一直 sleep 的 DaemonSet 应用。&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1kind: DaemonSetmetadata: name: benchds-replaceme namespace: benchds labels: k8s-app: benchdsspec: selector: matchLabels: name: benchds template: metadata: labels: name: benchds spec: containers: - name: benchds image: busybox:latest command: - sleep - infinity resources: limits: memory: 20Mi requests: cpu: 10m memory: 20Mi
&lt;/code>&lt;/pre>
&lt;p>该 DamonSet 将用唯一的名称进行部署：&lt;/p>
&lt;ul>
&lt;li>100 次（两次创建之间有一定延迟）&lt;/li>
&lt;li>批量 100 次&lt;/li>
&lt;li>批量 1000 次&lt;/li>
&lt;/ul>
&lt;p>对应的 Grafana 展示图表信息如下所示：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034839-e6353428-f475-4c08-9be7-73a5d7b656c2.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034856-b41bdfbc-a4b4-477a-914c-95f009b88193.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034825-ef0d77c9-202f-45f6-aeb5-6282f10214e7.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034888-e1840303-2907-45d0-a780-05119ec475eb.png" alt="">&lt;/p>
&lt;p>缓慢创建数百个 DaemonSets&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034850-86008be9-f893-47c9-a20b-d18523e12d20.png" alt="">&lt;/p>
&lt;p>快速创建数百个 DaemonSets&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034854-a1a537e5-e71e-4313-aec4-42265497e49d.png" alt="">&lt;/p>
&lt;p>快速创建数千个 DaemonSets&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/czfhxy/1616120034848-339697b7-b147-4af4-aae5-08728a93332c.png" alt="">&lt;/p>
&lt;p>现在我们来分析下上面的测试结果。&lt;/p>
&lt;ul>
&lt;li>Cri-o/runc：令人惊讶的是，在所有 create/delete 中是最慢的，但在其他方面处于中等水平。&lt;/li>
&lt;li>Cri-o/crun：在 create/delete 方面不是很好，但是在其他方面表现是最好的。&lt;/li>
&lt;li>Containerd：表现非常好，几乎在所有情况下都可以快速响应。&lt;/li>
&lt;li>Docker：在 create/delete 方面比 cri-o 快，但在 status/list 请求方面是最慢的。&lt;/li>
&lt;/ul>
&lt;p>status/list 请求是 cri 上最频繁的请求，所以这也是性能最重要的地方，cri-o 在这里似乎是更好的选择，其次就是 containerd。&lt;/p>
&lt;p>containerd 在所有指标上的表现都比较好，应该是最均衡的一个选择了。另外一方面，docker 并没有得到很好的测试结果，但是无论负载情况如何，它的表现基本上都是一致的。&lt;/p>
&lt;h2 id="总结">总结&lt;a class="td-heading-self-link" href="#%e6%80%bb%e7%bb%93" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>从纯性能角度来说，确实有比 docker 更好的替代品，我们的集群也不会替换 docker 产生什么影响。从另外一个角度来看，kubernetes 这次废弃 docker 的事情也算是一件好事，让更多的人意识到 docker 并不是唯一可用的 CRI，甚至不是唯一的构建镜像工具。&lt;/p>
&lt;p>在我看来，docker 仍然是让整个容器化向前发展的一个伟大工具。但是好像我还没有回答我最初的问题，那就是：我应该为我的 k8s 集群使用什么 CRI？&lt;/p>
&lt;p>从我个人角度考虑的话，我个人的选择是：containerd，他速度快，配置方便，相当可靠和安全，不过 cri-o 已经支持 cgroupsv2 了，所以如果我使用 fedora 或者 centos/8 的话我会优先选择 cri-o。&lt;/p>
&lt;blockquote>
&lt;p>原文链接：&lt;a href="https://ulrich-giraud.medium.com/which-cri-should-i-use-to-replace-docker-for-my-kubernetes-cluster-14a45c080004">https://ulrich-giraud.medium.com/which-cri-should-i-use-to-replace-docker-for-my-kubernetes-cluster-14a45c080004&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: crictl 命令行工具</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/crictl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/crictl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cri-tools">项目地址&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/">使用 crictl 对 kubernetes 进行调试&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>crictl 用于为 kubelet CRI 进行调试的 命令行工具&lt;/p>
&lt;p>cri-tools 旨在为 Kubelet CRI 提供一系列调试和验证工具，其中包括：&lt;/p>
&lt;ul>
&lt;li>crictl: kubelet 的 CRI 命令行工具&lt;/li>
&lt;li>critest:kubelet CRI 的验证测试套件&lt;/li>
&lt;/ul>
&lt;p>用白话说就是：kubelet 如果要与 CRI 对接，那么如何检测对接成功呢，就是使用 crictl 工具来测试。还可以对已经与 kubelet 建立连接的 CRI 执行相关操作，比如启停容器等。&lt;/p>
&lt;p>Note：要想使用 crictl 命令行工具，必须要先进行配置，指定好要操作的 CRI 的 endpoint，才可以正常使用&lt;/p>
&lt;h1 id="crictl-配置">crictl 配置&lt;a class="td-heading-self-link" href="#crictl-%e9%85%8d%e7%bd%ae" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;strong>/etc/crictl.yaml&lt;/strong> # crictl 命令行工具运行时配置文件&lt;/p>
&lt;h2 id="基本配置文件示例">基本配置文件示例&lt;a class="td-heading-self-link" href="#%e5%9f%ba%e6%9c%ac%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e7%a4%ba%e4%be%8b" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;pre>&lt;code>runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: true
&lt;/code>&lt;/pre>
&lt;h1 id="crictl-命令行工具">crictl 命令行工具&lt;a class="td-heading-self-link" href="#crictl-%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%b7%a5%e5%85%b7" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;strong>crictl [Global OPTIONS] COMMAND [COMMAND OPETIONS] [ARGUMENTS&amp;hellip;]&lt;/strong>
COMMMAND&lt;/p>
&lt;ul>
&lt;li>attach Attach to a running container&lt;/li>
&lt;li>create Create a new container&lt;/li>
&lt;li>exec Run a command in a running container&lt;/li>
&lt;li>version Display runtime version information&lt;/li>
&lt;li>images List images&lt;/li>
&lt;li>inspect Display the status of one or more containers&lt;/li>
&lt;li>inspecti Return the status of one or more images&lt;/li>
&lt;li>inspectp Display the status of one or more pods&lt;/li>
&lt;li>logs Fetch the logs of a container&lt;/li>
&lt;li>port-forward Forward local port to a pod&lt;/li>
&lt;li>ps List containers&lt;/li>
&lt;li>pull Pull an image from a registry&lt;/li>
&lt;li>runp Run a new pod&lt;/li>
&lt;li>rm Remove one or more containers&lt;/li>
&lt;li>rmi Remove one or more images&lt;/li>
&lt;li>rmp Remove one or more pods&lt;/li>
&lt;li>pods List pods&lt;/li>
&lt;li>start Start one or more created containers&lt;/li>
&lt;li>&lt;strong>info&lt;/strong> # 显示与 crictl 对接的 CRI 信息&lt;/li>
&lt;li>stop Stop one or more running containers&lt;/li>
&lt;li>stopp Stop one or more running pods&lt;/li>
&lt;li>update Update one or more running containers&lt;/li>
&lt;li>config Get and set crictl options&lt;/li>
&lt;li>stats List container(s) resource usage statistics&lt;/li>
&lt;li>completion Output bash shell completion code&lt;/li>
&lt;li>help, h Shows a list of commands or help for one command&lt;/li>
&lt;/ul>
&lt;p>OPTIONS&lt;/p>
&lt;p>EXAMPLE&lt;/p>
&lt;ul>
&lt;li>crictl info # 显示与 crictl 对接的 CRI 信息&lt;/li>
&lt;li>crictl pull docker.io/lchdzh/pause:3.1 # 拉取 dockerhub 上的 lchdzh 中的 pause 镜像&lt;/li>
&lt;/ul></description></item><item><title>Docs: Kubelet 特性</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Kubelet-%E7%89%B9%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubelet/Kubelet-%E7%89%B9%E6%80%A7/</guid><description/></item></channel></rss>