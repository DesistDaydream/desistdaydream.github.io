<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kube-proxy(实现 Service 功能的组件) on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/</link><description>Recent content in kube-proxy(实现 Service 功能的组件) on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/index.xml" rel="self" type="application/rss+xml"/><item><title>kube-proxy</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/kube-proxy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/kube-proxy/</guid><description>概述 参考：
官方文档，概念-概述-Kubernetes 组件-kube-proxy kube-proxy 是实现 Service(服务) 功能的组件，可以转发 Service 的流量到 POD
kube-proxy 有三种模式，userspace、iptables、ipvs。
service 在逻辑上代表了后端的多个 Pod，外界通过 service 访问 Pod。service 接收到的请求是如何转发到 Pod 的呢？这就是 kube-proxy 要完成的工作。接管系统的 iptables，所有到达 Service 的请求，都会根据 proxy 所定义的 iptables 的规则，进行 nat 转发 每个 Node 都会运行 kube-proxy 服务，它负责将访问 service 的 TCP/UPD 数据流转发到后端的容器。如果有多个副本，kube-proxy 会实现负载均衡。 每个 Service 的变动(创建，改动，摧毁)都会通知 proxy，在 proxy 所在的本节点创建响应的 iptables 规则，如果 Service 后端的 Pod 摧毁后重新建立了，那么就是靠 proxy 来把 pod 信息提供给 Service。 Note:
kube-proxy 的 ipvs 模式为 lvs 的 nat 模型 如果想要在 ipvs 模式下从 VIP:nodePort 去访问就请你暴露的服务的话，需要将 VIP 的掩码设置为 /32。 参考 issue：https://github.</description></item><item><title>IPVS 模式原理</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/IPVS-%E6%A8%A1%E5%BC%8F%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/IPVS-%E6%A8%A1%E5%BC%8F%E5%8E%9F%E7%90%86/</guid><description>原文链接：https://mp.weixin.qq.com/s/X6EL8GwWoi9_DyvhHL6Mlw Kubernetes 中的 Service 就是一组同 label 类型 Pod 的服务抽象，为服务提供了负载均衡和反向代理能力，在集群中表示一个微服务的概念。kube-proxy 组件则是 Service 的具体实现，了解了 kube-proxy 的工作原理，才能洞悉服务之间的通信流程，再遇到网络不通时也不会一脸懵逼。 kube-proxy 有三种模式：userspace、iptables 和 IPVS，其中 userspace 模式不太常用。iptables 模式最主要的问题是在服务多的时候产生太多的 iptables 规则，非增量式更新会引入一定的时延，大规模情况下有明显的性能问题。为解决 iptables 模式的性能问题，v1.11 新增了 IPVS 模式（v1.8 开始支持测试版，并在 v1.11 GA），采用增量式更新，并可以保证 service 更新期间连接保持不断开。 目前网络上关于 kube-proxy 工作原理的文档几乎都是以 iptables 模式为例，很少提及 IPVS，本文就来破例解读 kube-proxy IPVS 模式的工作原理。为了理解地更加彻底，本文不会使用 Docker 和 Kubernetes，而是使用更加底层的工具来演示。 我们都知道，Kubernetes 会为每个 Pod 创建一个单独的网络命名空间 (Network Namespace) ，本文将会通过手动创建网络命名空间并启动 HTTP 服务来模拟 Kubernetes 中的 Pod。 本文的目标是通过模拟以下的 Service 来探究 kube-proxy 的 IPVS 和 ipset 的工作原理：
apiVersion: v1 kind: Service metadata: name: app-service spec: clusterIP: 10.</description></item><item><title>基于 IPVS 的集群内负载平衡深入探讨</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/%E5%9F%BA%E4%BA%8E-IPVS-%E7%9A%84%E9%9B%86%E7%BE%A4%E5%86%85%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/%E5%9F%BA%E4%BA%8E-IPVS-%E7%9A%84%E9%9B%86%E7%BE%A4%E5%86%85%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8/</guid><description>参考： 原文链接：https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/ 阳明,kubernetes 中的 ipvs：https://www.qikqiak.com/post/how-to-use-ipvs-in-kubernetes/
Introduction Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.
What Is IPVS? IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel. IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers.</description></item><item><title>深入 kube-proxy ipvs 模式的 conn_reuse_mode 问题</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/%E6%B7%B1%E5%85%A5-kube-proxy-ipvs-%E6%A8%A1%E5%BC%8F%E7%9A%84-conn_reuse_mode-%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%BD%91%E7%BB%9C/kube-proxy/%E6%B7%B1%E5%85%A5-kube-proxy-ipvs-%E6%A8%A1%E5%BC%8F%E7%9A%84-conn_reuse_mode-%E9%97%AE%E9%A2%98/</guid><description>在高并发、短连接的场景下，kube-proxy ipvs 存在 rs 删除失败或是延迟高的问题，社区也有不少 Issue 反馈，比如kube-proxy ipvs conn_reuse_mode setting causes errors with high load from single client。文本对这些问题进行了梳理，试图介绍产生这些问题的内部原因。由于能力有限，其中涉及内核部分，只能浅尝辄止。
背景 端口重用 一切问题来源于端口重用。在 TCP 四次挥手中有个TIME_WAIT的状态，作为先发送FIN包的一端，在接收到对端发送的FIN包后进入TIME_WAIT，在经过2MSL后才会真正关闭连接。TIME_WAIT状态的存在，一来可以避免将之前连接的延迟报文，作为当前连接的报文处理；二是可以处理最后一个 ACK 丢失带来的问题。 而在短连接、高并发的场景下，会出现大量的TIME-WAIT连接，导致资源无法及时释放。Linux 中内核参数net.ipv4.tcp_tw_reuse提供了一种减少TIME-WAIT连接的方式，可以将TIME-WAIT连接的端口分配给新的 TCP 连接，来复用端口。
tcp_tw_reuse - BOOLEAN Allow to reuse TIME-WAIT sockets for new connections when it is safe from protocol viewpoint. Default value is 0. It should not be changed without advice/request of technical experts. ipvs 如何处理端口重用？ ipvs 对端口的复用策略主要由内核参数net.ipv4.vs.conn_reuse_mode决定
conn_reuse_mode - INTEGER 1 - default Controls how ipvs will deal with connections that are detected port reuse.</description></item></channel></rss>