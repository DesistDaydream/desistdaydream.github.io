<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes 管理 on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/</link><description>Recent content in Kubernetes 管理 on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>HPA(Horizontal Pod Autoscaler)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/HPAHorizontal-Pod-Autoscaler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/HPAHorizontal-Pod-Autoscaler/</guid><description>概述 参考：
官方文档：https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ https://www.qikqiak.com/post/k8s-hpa-usage/ Pod 水平自动扩缩（Horizontal Pod Autoscaler） 可以基于 CPU 利用率自动扩缩 ReplicationController、Deployment 和 ReplicaSet 中的 Pod 数量。 除了 CPU 利用率，也可以基于其他应程序提供的自定义度量指标 来执行自动扩缩。 Pod 自动扩缩不适用于无法扩缩的对象，比如 DaemonSet。
Pod 水平自动扩缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的调整副本控制器或 Deployment 中的副本数量，以使得 Pod 的平均 CPU 利用率与用户所设定的目标值匹配。
我们可以简单的通过 kubectl autoscale 命令来创建一个 HPA 资源对象， HPAController 默认 30s 轮询一次（可通过 kube-controller-manager 的 &amp;ndash;horizontal-pod-autoscaler-sync-period 参数进行设置），查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。
Metrics Server 在 HPA 的第一个版本中，我们需要 Heapster 提供 CPU 和内存指标，在 HPA v2 过后就需要安装 Metrcis Server 了，Metrics Server 可以通过标准的 Kubernetes API 把监控数据暴露出来，有了 Metrics Server 之后，我们就完全可以通过标准的 Kubernetes API 来访问我们想要获取的监控数据了：</description></item><item><title>Kubernetes 管理</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E7%AE%A1%E7%90%86/</guid><description>概述 Telepresence 参考：
GitHub 项目，telepresenceio/telepresence 公众号-马哥 Linux 运维，K8S 运维开发调试神器 Telepresence 实践及踩坑记</description></item><item><title>Quota(配额)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Quota%E9%85%8D%E9%A2%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Quota%E9%85%8D%E9%A2%9D/</guid><description>概述 参考：
官方文档，概念-策略-LimitRange 官方文档，概念-策略-ResourceQuotas namespace 中的资源配额 官方文档：https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/
当多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。比如，不同团队使用不同的 namespace，然后给该 namespace 进行资源限制即可
目前有两种 k8s 对象分配管理相关的控制策略
LimitRange(限制范围) 设定 pod 等对象的默认资源消耗以及可以消耗的资源范围
官方文档：
概念：https://kubernetes.io/docs/concepts/policy/limit-range/ 用法： https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/ https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/ https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/ https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/ &amp;hellip;..等等 ResourceQuota(资源配额) 基于 namespace，限制该 namesapce 下的总体资源的创建和消耗
官方文档：
概念：https://kubernetes.io/docs/concepts/policy/resource-quotas/ 用法： https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/ https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/ https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/ # 为指定的 API 对象设置 resourceQuota 资源配额分为三种类型：
计算资源配额 存储资源配额 对象数量配额 总结 仅设置 ResourceQuota 时，如果不再 pod 上设置资源的需求和限制，则无法成功创建 pod，需要配合 LimitRange 设置 pod 的默认需求和限制，才可成功创建 pod 两种控制策略的作用范围都是对于某一 namespace ResourceQuota 用来限制 namespace 中所有的 Pod 占用的总的资源 request 和 limit LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值，还有，Pod 的可用资源的 request 和 limit 值的最大与最小值。 简单的应用示例 Note：polinux/stress 这是一个非常好用的压测容器，可以对容器指定其所使用的内存和 cpu 等资源的大小。当创建完资源配合等资源限制的对象后，可以通过该容器来测试资源限制是否生效。</description></item><item><title>好用的镜像-有特殊功能</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E5%A5%BD%E7%94%A8%E7%9A%84%E9%95%9C%E5%83%8F-%E6%9C%89%E7%89%B9%E6%AE%8A%E5%8A%9F%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E5%A5%BD%E7%94%A8%E7%9A%84%E9%95%9C%E5%83%8F-%E6%9C%89%E7%89%B9%E6%AE%8A%E5%8A%9F%E8%83%BD/</guid><description>node-shell
可以通过 exec 像 ssh 一样控制节点的镜像，好像是 https://github.com/kvaps/kubectl-node-shell 这个？待确认
polinux/stress
一个非常好用的压测容器，可以对容器指定其所使用的内存和 cpu 等资源的大小。当创建完资源配合等资源限制的对象后，可以通过该容器来测试资源限制是否生效。
containous/whoami
一个 go 语言编写的 web 服务器，当请求该容器时，可以输出操作系统信息和 HTTP 请求等，信息如下所示：包括当前容器的 ip 地址，容器的主机名等等
Hostname: whoami-bd6b677dc-7tq7h IP: 127.0.0.1 IP: 10.252.131.122 RemoteAddr: 127.0.0.1:35358 GET /notls HTTP/1.1 Host: 10.10.9.51:30272 User-Agent: curl/7.29.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 10.10.9.51 X-Forwarded-Host: 10.10.9.51:30272 X-Forwarded-Port: 30272 X-Forwarded-Proto: http X-Forwarded-Server: traefik-6fbbb464b5-mcq99 X-Real-Ip: 10.10.9.51 kiwigrid/k8s-sidecar 参考：GitHub 项目
该容器会持续监听指定的 configmap 和 secret 资源，当 configmap 或 secret 对象被创建或更新时，会将该对象内的数据，转换成文件，并保存在容器内指定的路径中。
这个镜像常常作为 sidecar 容器使用，与主容器共享相同目录，这样，主程序就可以实时读取到新创建的 configmap 或 secret</description></item></channel></rss>