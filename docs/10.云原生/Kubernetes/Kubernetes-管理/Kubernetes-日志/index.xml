<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – Kubernetes 日志</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/</link><description>Recent content in Kubernetes 日志 on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Event 资源</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/Event-%E8%B5%84%E6%BA%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/Event-%E8%B5%84%E6%BA%90/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/lOM5BnvYixOnWFR7Axky8Q">公众号，张晋涛-彻底搞懂 Kubernetes 中的 Events&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>之前我写了一篇&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI2ODAwMzUwNA==&amp;amp;mid=2649296276&amp;amp;idx=1&amp;amp;sn=e4832b819db454fb13af5b052a3158d9&amp;amp;chksm=f2eb9c4bc59c155d24a253981db7ff39c50c38ea40f6c87a9f0f04ce5a0f3012627a3d7ccde0&amp;amp;scene=21#wechat_redirect">《更优雅的 Kubernetes 集群事件度量方案》&lt;/a>，利用 Jaeger 利用 tracing 的方式来采集 Kubernetes 集群中的 events 并进行展示。最终效果如下：&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ho6kqg/1641358554427-b25ce8b8-3629-4659-9064-70b089ab1ead.webp" alt="">&lt;/p>
&lt;p>写那篇文章的时候，立了个 flag 要详细介绍下其中的原理，鸽了很久，现在年底了，也该发出来了。&lt;/p>
&lt;h1 id="eents-概览">Eents 概览&lt;a class="td-heading-self-link" href="#eents-%e6%a6%82%e8%a7%88" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>我们先来做个简单的示例，来看看 Kubernetes 集群中的 events 是什么。
创建一个新的名叫 moelove 的 namespace ，然后在其中创建一个叫做 redis 的 deployment。接下来查看这个 namespace 中的所有 events。
(MoeLove) ➜ kubectl create ns moelove
namespace/moelove created
(MoeLove) ➜ kubectl -n moelove create deployment redis &amp;ndash;image=ghcr.io/moelove/redis:alpine 
deployment.apps/redis created
(MoeLove) ➜ kubectl -n moelove get deploy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
redis   1/1     1            1           11s
(MoeLove) ➜ kubectl -n moelove get events
LAST SEEN   TYPE     REASON              OBJECT                        MESSAGE
21s         Normal   Scheduled           pod/redis-687967dbc5-27vmr    Successfully assigned moelove/redis-687967dbc5-27vmr to kind-worker3
21s         Normal   Pulling             pod/redis-687967dbc5-27vmr    Pulling image &amp;ldquo;ghcr.io/moelove/redis:alpine&amp;rdquo;
15s         Normal   Pulled              pod/redis-687967dbc5-27vmr    Successfully pulled image &amp;ldquo;ghcr.io/moelove/redis:alpine&amp;rdquo; in 6.814310968s
14s         Normal   Created             pod/redis-687967dbc5-27vmr    Created container redis
14s         Normal   Started             pod/redis-687967dbc5-27vmr    Started container redis
22s         Normal   SuccessfulCreate    replicaset/redis-687967dbc5   Created pod: redis-687967dbc5-27vmr
22s         Normal   ScalingReplicaSet   deployment/redis              Scaled up replica set redis-687967dbc5 to 1&lt;/p>
&lt;p>但是我们会发现默认情况下 kubectl get events 并没有按照 events 发生的顺序进行排列，所以我们往往需要为其增加 &amp;ndash;sort-by=&amp;rsquo;{.metadata.creationTimestamp}&amp;rsquo; 参数来让其输出可以按时间进行排列。
这也是为何 Kubernetes v1.23 版本中会新增 kubectl alpha events 命令的原因。我在之前的&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI2ODAwMzUwNA==&amp;amp;mid=2649297076&amp;amp;idx=1&amp;amp;sn=4d6a86220535a0aaa9fdbcbaf66625f4&amp;amp;chksm=f2eb816bc59c087db1ff984e94e449cb9bd6ee6efd18cf25cf946f3019532e56463629e2f1e0&amp;amp;scene=21#wechat_redirect">文章《K8S 生态周报| Kubernetes v1.23.0 正式发布，新特性一览》&lt;/a>中已进行了详细的介绍，这里就不展开了。
按时间排序后可以看到如下结果：
(MoeLove) ➜ kubectl -n moelove get events &amp;ndash;sort-by=&amp;rsquo;{.metadata.creationTimestamp}'
LAST SEEN   TYPE     REASON              OBJECT                        MESSAGE
2m12s       Normal   Scheduled           pod/redis-687967dbc5-27vmr    Successfully assigned moelove/redis-687967dbc5-27vmr to kind-worker3
2m13s       Normal   SuccessfulCreate    replicaset/redis-687967dbc5   Created pod: redis-687967dbc5-27vmr
2m13s       Normal   ScalingReplicaSet   deployment/redis              Scaled up replica set redis-687967dbc5 to 1
2m12s       Normal   Pulling             pod/redis-687967dbc5-27vmr    Pulling image &amp;ldquo;ghcr.io/moelove/redis:alpine&amp;rdquo;
2m6s        Normal   Pulled              pod/redis-687967dbc5-27vmr    Successfully pulled image &amp;ldquo;ghcr.io/moelove/redis:alpine&amp;rdquo; in 6.814310968s
2m5s        Normal   Created             pod/redis-687967dbc5-27vmr    Created container redis
2m5s        Normal   Started             pod/redis-687967dbc5-27vmr    Started container redis&lt;/p>
&lt;p>通过以上的操作，我们可以发现 &lt;strong>events 实际上是 Kubernetes 集群中的一种资源。当 Kubernetes 集群中资源状态发生变化时，可以产生新的 events&lt;/strong>。&lt;/p>
&lt;h1 id="深入-events">深入 Events&lt;a class="td-heading-self-link" href="#%e6%b7%b1%e5%85%a5-events" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="单个-event-对象">单个 Event 对象&lt;a class="td-heading-self-link" href="#%e5%8d%95%e4%b8%aa-event-%e5%af%b9%e8%b1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>既然 events 是 Kubernetes 集群中的一种资源，正常情况下它的 metadata.name 中应该包含其名称，用于进行单独操作。所以我们可以使用如下命令输出其 name ：
(MoeLove) ➜ kubectl -n moelove get events &amp;ndash;sort-by=&amp;rsquo;{.metadata.creationTimestamp}&amp;rsquo; -o jsonpath=&amp;rsquo;{range .items[*]}{.metadata.name}{&amp;quot;\n&amp;quot;}{end}'
redis-687967dbc5-27vmr.16c4fb7bde8c69d2
redis-687967dbc5.16c4fb7bde6b54c4
redis.16c4fb7bde1bf769
redis-687967dbc5-27vmr.16c4fb7bf8a0ab35
redis-687967dbc5-27vmr.16c4fb7d8ecaeff8
redis-687967dbc5-27vmr.16c4fb7d99709da9
redis-687967dbc5-27vmr.16c4fb7d9be30c06&lt;/p>
&lt;p>选择其中的任意一条 event 记录，将其输出为 YAML 格式进行查看：
(MoeLove) ➜ kubectl -n moelove get events redis-687967dbc5-27vmr.16c4fb7bde8c69d2 -o yaml
action: Binding
apiVersion: v1
eventTime: &amp;ldquo;2021-12-28T19:31:13.702987Z&amp;rdquo;
firstTimestamp: null
involvedObject:
  apiVersion: v1
  kind: Pod
  name: redis-687967dbc5-27vmr
  namespace: moelove
  resourceVersion: &amp;ldquo;330230&amp;rdquo;
  uid: 71b97182-5593-47b2-88cc-b3f59618c7aa
kind: Event
lastTimestamp: null
message: Successfully assigned moelove/redis-687967dbc5-27vmr to kind-worker3
metadata:
  creationTimestamp: &amp;ldquo;2021-12-28T19:31:13Z&amp;rdquo;
  name: redis-687967dbc5-27vmr.16c4fb7bde8c69d2
  namespace: moelove
  resourceVersion: &amp;ldquo;330235&amp;rdquo;
  uid: e5c03126-33b9-4559-9585-5e82adcd96b0
reason: Scheduled
reportingComponent: default-scheduler
reportingInstance: default-scheduler-kind-control-plane
source: {}
type: Normal&lt;/p>
&lt;p>可以看到其中包含了很多信息, 这里我们先不展开。我们看另一个例子。&lt;/p>
&lt;h2 id="kubectl-describe-中的-events">kubectl describe 中的 Events&lt;a class="td-heading-self-link" href="#kubectl-describe-%e4%b8%ad%e7%9a%84-events" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>我们可以分别对 Deployment 对象和 Pod 对象执行 describe 的操作，可以得到如下结果（省略掉了中间输出）：&lt;/p>
&lt;ul>
&lt;li>对 Deployment 操作&lt;/li>
&lt;/ul>
&lt;p>(MoeLove) ➜ kubectl -n moelove describe deploy/redis                
Name:                   redis
Namespace:              moelove
&amp;hellip;
Events:
  Type    Reason             Age   From                   Message
  &amp;mdash;-    &amp;mdash;&amp;mdash;             &amp;mdash;-  &amp;mdash;-                   &amp;mdash;&amp;mdash;-
  Normal  ScalingReplicaSet  15m   deployment-controller  Scaled up replica set redis-687967dbc5 to 1&lt;/p>
&lt;ul>
&lt;li>对 Pod 操作&lt;/li>
&lt;/ul>
&lt;p>(MoeLove) ➜ kubectl -n moelove describe pods redis-687967dbc5-27vmr
Name:         redis-687967dbc5-27vmr                                                                 
Namespace:    moelove
Priority:     0
Events:
  Type    Reason     Age   From               Message
  &amp;mdash;-    &amp;mdash;&amp;mdash;     &amp;mdash;-  &amp;mdash;-               &amp;mdash;&amp;mdash;-
  Normal  Scheduled  18m   default-scheduler  Successfully assigned moelove/redis-687967dbc5-27vmr to kind-worker3
  Normal  Pulling    18m   kubelet            Pulling image &amp;ldquo;ghcr.io/moelove/redis:alpine&amp;rdquo;
  Normal  Pulled     17m   kubelet            Successfully pulled image &amp;ldquo;ghcr.io/moelove/redis:alpine&amp;rdquo; in 6.814310968s
  Normal  Created    17m   kubelet            Created container redis
  Normal  Started    17m   kubelet            Started container redis&lt;/p>
&lt;p>我们可以发现 &lt;strong>对不同的资源对象进行 describe 的时候，能看到的 events 内容都是与自己有直接关联的&lt;/strong>。在 describe Deployment 的时候，看不到 Pod 相关的 Events 。
这说明， &lt;strong>Event 对象中是包含它所描述的资源对象的信息的&lt;/strong>，它们是有直接联系的。
结合前面我们看到的单个 Event 对象，我们发现 &lt;strong>involvedObject 字段中内容就是与该 Event 相关联的资源对象的信息&lt;/strong>。&lt;/p>
&lt;h1 id="更进一步了解-events">更进一步了解 Events&lt;a class="td-heading-self-link" href="#%e6%9b%b4%e8%bf%9b%e4%b8%80%e6%ad%a5%e4%ba%86%e8%a7%a3-events" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>我们来看看如下的示例，创建一个 Deployment ，但是使用一个不存在的镜像：
(MoeLove) ➜ kubectl -n moelove create deployment non-exist &amp;ndash;image=ghcr.io/moelove/non-exist
deployment.apps/non-exist created
(MoeLove) ➜ kubectl -n moelove get pods
NAME                        READY   STATUS         RESTARTS   AGE
non-exist-d9ddbdd84-tnrhd   0/1     ErrImagePull   0          11s
redis-687967dbc5-27vmr      1/1     Running        0          26m&lt;/p>
&lt;p>我们可以看到当前的 Pod 处于一个 ErrImagePull 的状态。查看当前 namespace 中的 events (我省略掉了之前 deploy/redis 的记录)
(MoeLove) ➜ kubectl -n moelove get events &amp;ndash;sort-by=&amp;rsquo;{.metadata.creationTimestamp}&amp;rsquo;                                                           
LAST SEEN   TYPE      REASON              OBJECT                           MESSAGE
35s         Normal    SuccessfulCreate    replicaset/non-exist-d9ddbdd84   Created pod: non-exist-d9ddbdd84-tnrhd
35s         Normal    ScalingReplicaSet   deployment/non-exist             Scaled up replica set non-exist-d9ddbdd84 to 1
35s         Normal    Scheduled           pod/non-exist-d9ddbdd84-tnrhd    Successfully assigned moelove/non-exist-d9ddbdd84-tnrhd to kind-worker3
17s         Warning   Failed              pod/non-exist-d9ddbdd84-tnrhd    Error: ErrImagePull
17s         Warning   Failed              pod/non-exist-d9ddbdd84-tnrhd    Failed to pull image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;: rpc error: code = Unknown desc = failed to pull and unpack image &amp;ldquo;ghcr.io/moelove/non-exist:latest&amp;rdquo;: failed to resolve reference &amp;ldquo;ghcr.io/moelove/non-exist:latest&amp;rdquo;: failed to authorize: failed to fetch anonymous token: unexpected status: 403 Forbidden
18s         Normal    Pulling             pod/non-exist-d9ddbdd84-tnrhd    Pulling image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;
4s          Warning   Failed              pod/non-exist-d9ddbdd84-tnrhd    Error: ImagePullBackOff
4s          Normal    BackOff             pod/non-exist-d9ddbdd84-tnrhd    Back-off pulling image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;&lt;/p>
&lt;p>对这个 Pod 执行 describe 操作：
(MoeLove) ➜ kubectl -n moelove describe pods non-exist-d9ddbdd84-tnrhd
&amp;hellip;
Events:
  Type     Reason     Age                    From               Message
  &amp;mdash;-     &amp;mdash;&amp;mdash;     &amp;mdash;-                   &amp;mdash;-               &amp;mdash;&amp;mdash;-
  Normal   Scheduled  4m                     default-scheduler  Successfully assigned moelove/non-exist-d9ddbdd84-tnrhd to kind-worker3
  Normal   Pulling    2m22s (x4 over 3m59s)  kubelet            Pulling image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;
  Warning  Failed     2m21s (x4 over 3m59s)  kubelet            Failed to pull image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;: rpc error: code = Unknown desc = failed to pull and unpack image &amp;ldquo;ghcr.io/moelove/non-exist:latest&amp;rdquo;: failed to resolve reference &amp;ldquo;ghcr.io/moelove/non-exist:latest&amp;rdquo;: failed to authorize: failed to fetch anonymous token: unexpected status: 403 Forbidden
  Warning  Failed     2m21s (x4 over 3m59s)  kubelet            Error: ErrImagePull
  Warning  Failed     2m9s (x6 over 3m58s)   kubelet            Error: ImagePullBackOff
  Normal   BackOff    115s (x7 over 3m58s)   kubelet            Back-off pulling image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;&lt;/p>
&lt;p>我们可以发现，这里的输出和之前正确运行 Pod 的不一样。最主要的区别在于 Age 列。这里我们看到了类似 115s (x7 over 3m58s) 这样的输出。
它的含义表示：&lt;strong>该类型的 event 在 3m58s 中已经发生了 7 次，最近的一次发生在 115s 之前&lt;/strong>
但是当我们去直接 kubectl get events 的时候，我们并没有看到有 7 次重复的 event 。这说明 &lt;strong>Kubernetes 会自动将重复的 events 进行合并&lt;/strong>。
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ho6kqg/1641358554448-7ded1c2c-85b2-47ce-9df9-1b1fc531112d.png" alt="">
&lt;strong>MoeLove&lt;/strong>
不只限于 Container, Docker, Kubernetes 等技术，与你分享更多实用且具有前景的技术。欢迎关注
173 篇原创内容
公众号
选择最后一条 Events (方法前面内容已经讲了) 并将其内容使用 YAML 格式进行输出：
(MoeLove) ➜ kubectl -n moelove get events non-exist-d9ddbdd84-tnrhd.16c4fce570cfba46 -o yaml
apiVersion: v1
count: 43
eventTime: null
firstTimestamp: &amp;ldquo;2021-12-28T19:57:06Z&amp;rdquo;
involvedObject:
  apiVersion: v1
  fieldPath: spec.containers{non-exist}
  kind: Pod
  name: non-exist-d9ddbdd84-tnrhd
  namespace: moelove
  resourceVersion: &amp;ldquo;333366&amp;rdquo;
  uid: 33045163-146e-4282-b559-fec19a189a10
kind: Event
lastTimestamp: &amp;ldquo;2021-12-28T18:07:14Z&amp;rdquo;
message: Back-off pulling image &amp;ldquo;ghcr.io/moelove/non-exist&amp;rdquo;
metadata:
  creationTimestamp: &amp;ldquo;2021-12-28T19:57:06Z&amp;rdquo;
  name: non-exist-d9ddbdd84-tnrhd.16c4fce570cfba46
  namespace: moelove
  resourceVersion: &amp;ldquo;334638&amp;rdquo;
  uid: 60708be0-23b9-481b-a290-dd208fed6d47
reason: BackOff
reportingComponent: &amp;quot;&amp;quot;
reportingInstance: &amp;quot;&amp;quot;
source:
  component: kubelet
  host: kind-worker3
type: Normal&lt;/p>
&lt;p>这里我们可以看到其字段中包括一个 count 字段，表示同类 event 发生了多少次。以及 firstTimestamp 和 lastTimestamp 分别表示了这个 event 首次出现了最近一次出现的时间。这样也就解释了前面的输出中 events 持续的周期了。&lt;/p>
&lt;h1 id="彻底搞懂-events">彻底搞懂 Events&lt;a class="td-heading-self-link" href="#%e5%bd%bb%e5%ba%95%e6%90%9e%e6%87%82-events" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>以下内容是从 Events 中随便选择的一条，我们可以看到它包含的一些字段信息：
apiVersion: v1
count: 1
eventTime: null
firstTimestamp: &amp;ldquo;2021-12-28T19:31:13Z&amp;rdquo;
involvedObject:
  apiVersion: apps/v1
  kind: ReplicaSet
  name: redis-687967dbc5
  namespace: moelove
  resourceVersion: &amp;ldquo;330227&amp;rdquo;
  uid: 11e98a9d-9062-4ccb-92cb-f51cc74d4c1d
kind: Event
lastTimestamp: &amp;ldquo;2021-12-28T19:31:13Z&amp;rdquo;
message: &amp;lsquo;Created pod: redis-687967dbc5-27vmr&amp;rsquo;
metadata:
  creationTimestamp: &amp;ldquo;2021-12-28T19:31:13Z&amp;rdquo;
  name: redis-687967dbc5.16c4fb7bde6b54c4
  namespace: moelove
  resourceVersion: &amp;ldquo;330231&amp;rdquo;
  uid: 8e37ec1e-b3a1-420c-96d4-3b3b2995c300
reason: SuccessfulCreate
reportingComponent: &amp;quot;&amp;quot;
reportingInstance: &amp;quot;&amp;quot;
source:
  component: replicaset-controller
type: Normal&lt;/p>
&lt;p>其中主要字段的含义如下：&lt;/p>
&lt;ul>
&lt;li>count: 表示当前同类的事件发生了多少次 （前面已经介绍）&lt;/li>
&lt;li>involvedObject: 与此 event 有直接关联的资源对象 （前面已经介绍） , 结构如下：&lt;/li>
&lt;/ul>
&lt;p>type ObjectReference struct {
 Kind string
 Namespace string
 Name string
 UID types.UID
 APIVersion string
 ResourceVersion string
 FieldPath string
}&lt;/p>
&lt;ul>
&lt;li>source: 直接关联的组件, 结构如下：&lt;/li>
&lt;/ul>
&lt;p>type EventSource struct {
 Component string
 Host string
}&lt;/p>
&lt;ul>
&lt;li>reason: 简单的总结（或者一个固定的代码），比较适合用于做筛选条件，主要是为了让机器可读，当前有超过 50 种这样的代码；&lt;/li>
&lt;li>message: 给一个更易让人读懂的详细说明&lt;/li>
&lt;li>type: 当前只有 Normal 和 Warning 两种类型, 源码中也分别写了其含义：&lt;/li>
&lt;/ul>
&lt;p>&lt;em>// staging/src/k8s.io/api/core/v1/types.go&lt;/em>
const (
 &lt;em>// Information only and will not cause any problems&lt;/em>
 EventTypeNormal string = &amp;ldquo;Normal&amp;rdquo;
 &lt;em>// These events are to warn that something might go wrong&lt;/em>
 EventTypeWarning string = &amp;ldquo;Warning&amp;rdquo;
)&lt;/p>
&lt;p>所以，当我们将这些 Events 都作为 tracing 的 span 采集回来后，就可以按照其 source 进行分类，按 involvedObject 进行关联，按时间进行排序了。&lt;/p>
&lt;h1 id="总结">总结&lt;a class="td-heading-self-link" href="#%e6%80%bb%e7%bb%93" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>在这篇文章中，我主要通过两个示例，一个正确部署的 Deploy，以及一个使用不存在镜像部署的 Deploy，深入的介绍了 Events 对象的实际的作用及其各个字段的含义。
对于 Kubernetes 而言，Events 中包含了很多有用的信息，但是这些信息却并不会对 Kubernetes 造成什么影响，它们也并不是实际的 Kubernetes 的日志。默认情况下 Kubernetes 中的日志在 1 小时后就会被清理掉，以便释放对 etcd 的资源占用。
所以为了能更好的让集群管理员知道发生了什么，在生产环境中，我们通常会把 Kubernetes 集群的 events 也给采集回来。我个人比较推荐的工具是：https://github.com/opsgenie/kubernetes-event-exporter
当然你也可以按照我之前的文章 《更优雅的 Kubernetes 集群事件度量方案》，利用 Jaeger 利用 tracing 的方式来采集 Kubernetes 集群中的 events 并进行展示。&lt;/p></description></item><item><title>Docs: Kubernetes 日志</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/Kubernetes-%E6%97%A5%E5%BF%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/Kubernetes-%E6%97%A5%E5%BF%97/Kubernetes-%E6%97%A5%E5%BF%97/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">官方文档,概念-集群管理-日志架构&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>集群级日志架构需要一个单独的后端来存储、分析和查询日志。Kubernetes 不提供日志数据的原生存储解决方案。相反，有许多与 Kubernetes 集成的日志记录解决方案。&lt;/p>
&lt;h1 id="kubernetes-日志管理机制">Kubernetes 日志管理机制&lt;a class="td-heading-self-link" href="#kubernetes-%e6%97%a5%e5%bf%97%e7%ae%a1%e7%90%86%e6%9c%ba%e5%88%b6" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>在 Kubernetes 中日志也主要有两大类：&lt;/p>
&lt;ul>
&lt;li>应用 Pod 日志；&lt;/li>
&lt;li>Kuberntes 集群组件日志；&lt;/li>
&lt;/ul>
&lt;h2 id="应用-pod-日志">应用 Pod 日志&lt;a class="td-heading-self-link" href="#%e5%ba%94%e7%94%a8-pod-%e6%97%a5%e5%bf%97" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Kubernetes Pod 的日志管理是基于 Docker 引擎的，Kubernetes 并不管理日志的轮转策略，日志的存储都是基于 Docker 的日志管理策略。k8s 集群调度的基本单位就是 Pod，而 Pod 是一组容器，所以 k8s 日志管理基于 Docker 引擎这一说法也就不难理解了，最终日志还是要落到一个个容器上面。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dstb2v/1616116985979-e135fbea-a7b9-4da3-81ab-ae6e56dc742b.png" alt="">&lt;/p>
&lt;p>假设 Docker 日志驱动为 json-file，那么在 k8s 每个节点上，kubelet 会为每个容器的日志创建一个软链接，软连接存储路径为：/var/log/containers/，软连接会链接到 /var/log/pods/ 目录下相应 pod 目录的容器日志，被链接的日志文件也是软链接，最终链接到 Docker 容器引擎的日志存储目录：/var/lib/docker/container 下相应容器的日志。另外这些软链接文件名称含有 k8s 相关信息，比如：Pod id，名字空间，容器 ID 等信息，这就为日志收集提供了很大的便利。&lt;/p>
&lt;p>举例：我们跟踪一个容器日志文件，证明上述的说明，跟踪一个 kong Pod 日志，Pod 副本数为 1&lt;/p>
&lt;p>/var/log/containers/kong-kong-d889cf995-2ntwz_kong_kong-432e47df36d0992a3a8d20ef6912112615ffeb30e6a95c484d15614302f8db03.log&lt;/p>
&lt;p>&amp;mdash;&amp;mdash;-&amp;gt;&lt;/p>
&lt;p>/var/log/pods/kong_kong-kong-d889cf995-2ntwz_a6377053-9ca3-48f9-9f73-49856908b94a/kong/0.log&lt;/p>
&lt;p>&amp;mdash;&amp;mdash;-&amp;gt;&lt;/p>
&lt;p>/var/lib/docker/containers/432e47df36d0992a3a8d20ef6912112615ffeb30e6a95c484d15614302f8db03/432e47df36d0992a3a8d20ef6912112615ffeb30e6a95c484d15614302f8db03-json.log&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dstb2v/1616116985855-728179e1-7d6a-48da-a79e-d0e4c15be782.png" alt="">&lt;/p>
&lt;h2 id="kuberntes-集群组件日志">Kuberntes 集群组件日志&lt;a class="td-heading-self-link" href="#kuberntes-%e9%9b%86%e7%be%a4%e7%bb%84%e4%bb%b6%e6%97%a5%e5%bf%97" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Kuberntes 集群组件日志分为两类：&lt;/p>
&lt;ul>
&lt;li>运行在容器中的 Kubernetes scheduler 和 kube-proxy。&lt;/li>
&lt;li>未运行在容器中的 kubelet 和容器 runtime，比如 Docker。&lt;/li>
&lt;/ul>
&lt;p>在使用 systemd 机制的服务器上，kubelet 和容器 runtime 写入日志到 journald。如果没有 systemd，他们写入日志到 /var/log 目录的 .log 文件。容器中的系统组件通常将日志写到 /var/log 目录，在 kubeadm 安装的集群中它们以静态 Pod 的形式运行在集群中，因此日志一般在 /var/log/pods 目录下。&lt;/p>
&lt;h1 id="kubernetes-集群日志收集方案">Kubernetes 集群日志收集方案&lt;a class="td-heading-self-link" href="#kubernetes-%e9%9b%86%e7%be%a4%e6%97%a5%e5%bf%97%e6%94%b6%e9%9b%86%e6%96%b9%e6%a1%88" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Kubernetes 本身并未提供集群日志收集方案，k8s 官方文档给了三种日志收集的建议方案：&lt;/p>
&lt;ul>
&lt;li>使用运行在每个节点上的节点级的日志代理&lt;/li>
&lt;li>在应用程序的 pod 中包含专门记录日志 sidecar 容器&lt;/li>
&lt;li>应用程序直接将日志传输到日志平台&lt;/li>
&lt;/ul>
&lt;h2 id="节点级日志代理方案">节点级日志代理方案&lt;a class="td-heading-self-link" href="#%e8%8a%82%e7%82%b9%e7%ba%a7%e6%97%a5%e5%bf%97%e4%bb%a3%e7%90%86%e6%96%b9%e6%a1%88" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>从前面的介绍我们已经了解到，k8s 每个节点都将容器日志统一存储到了 /var/log/containers/ 目录下，因此可以在每个节点安装一个日志代理，将该目录下的日志实时传输到日志存储平台。&lt;/p>
&lt;p>由于需要每个节点运行一个日志代理，因此日志代理推荐以 DaemonSet 的方式运行在每个节点。官方推荐的日志代理是 fluentd，当然也可以使用其他日志代理，比如 filebeat，logstash 等。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dstb2v/1616116985903-131ba156-b0a4-448f-a2f0-474461f0d060.png" alt="">&lt;/p>
&lt;h2 id="sidecar-容器方案">sidecar 容器方案&lt;a class="td-heading-self-link" href="#sidecar-%e5%ae%b9%e5%99%a8%e6%96%b9%e6%a1%88" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>有两种使用 sidecar 容器的方式：&lt;/p>
&lt;ul>
&lt;li>sidecar 容器重定向日志流&lt;/li>
&lt;li>sidecar 容器作为日志代理&lt;/li>
&lt;/ul>
&lt;h3 id="sidecar-容器重定向日志流">sidecar 容器重定向日志流&lt;a class="td-heading-self-link" href="#sidecar-%e5%ae%b9%e5%99%a8%e9%87%8d%e5%ae%9a%e5%90%91%e6%97%a5%e5%bf%97%e6%b5%81" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这种方式基于节点级日志代理方案，sidecar 容器和应用容器在同一个 Pod 运行，这个容器的作用就是读取应用容器的日志文件，然后将读取的日志内容重定向到 stdout 和 stderr，然后通过节点级日志代理统一收集。这种方式不推荐使用，缺点就是日志重复存储了，导致磁盘使用会成倍增加。比如应用容器的日志本身打到文件存储了一份，sidecar 容器重定向又存储了一份（存储到了 /var/lib/docker/containers/ 目录下）。这种方式的应用场景是应用本身不支持将日志打到 stdout 和 stderr，所以才需要 sidecar 容器重定向下。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dstb2v/1616116985876-0caf78da-d1b0-481d-b01a-7889a5a39e42.png" alt="">&lt;/p>
&lt;h3 id="sidecar-容器作为日志代理">sidecar 容器作为日志代理&lt;a class="td-heading-self-link" href="#sidecar-%e5%ae%b9%e5%99%a8%e4%bd%9c%e4%b8%ba%e6%97%a5%e5%bf%97%e4%bb%a3%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这种方式不需要节点级日志代理，和应用容器在一起的 sidecar 容器直接作为日志代理方式运行在 Pod 中，sidecar 容器读取应用容器的日志，然后直接实时传输到日志存储平台。很显然这种方式也存在一个缺点，就是每个应用 Pod 都需要有个 sidecar 容器作为日志代理，而日志代理对系统 CPU、和内存都有一定的消耗，在节点 Pod 数很多的时候这个资源消耗其实是不小的。另外还有个问题就是在这种方式下由于应用容器日志不直接打到 stdout 和 stderr，所以是无法使用 kubectl logs 命令查看 Pod 中容器日志。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/dstb2v/1616116985873-cb51bd59-c44d-4460-aa4e-35d6507e4fff.png" alt="">&lt;/p>
&lt;h2 id="应用程序直接将日志传输到日志平台">应用程序直接将日志传输到日志平台&lt;a class="td-heading-self-link" href="#%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f%e7%9b%b4%e6%8e%a5%e5%b0%86%e6%97%a5%e5%bf%97%e4%bc%a0%e8%be%93%e5%88%b0%e6%97%a5%e5%bf%97%e5%b9%b3%e5%8f%b0" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>这种方式就是应用程序本身直接将日志打到统一的日志收集平台，比如 Java 应用可以配置日志的 appender，打到不同的地方，很显然这种方式对应用程序有一定的侵入性，而且还要保证日志系统的健壮性，从这个角度看应用和日志系统还有一定的耦合性，所以个人不是很推荐这种方式。&lt;/p>
&lt;p>总结：综合对比上述三种日志收集方案优缺点，更推荐使用节点级日志代理方案，这种方式对应用没有侵入性，而且对系统资源没有额外的消耗，也不影响 kubelet 工具查看 Pod 容器日志。&lt;/p></description></item></channel></rss>