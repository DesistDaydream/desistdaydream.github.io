<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – 故障处理案例</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</link><description>Recent content in 故障处理案例 on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: kubernetes 环境开启 bridge-nf-call-iptables</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/kubernetes-%E7%8E%AF%E5%A2%83%E5%BC%80%E5%90%AF-bridge-nf-call-iptables/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/kubernetes-%E7%8E%AF%E5%A2%83%E5%BC%80%E5%90%AF-bridge-nf-call-iptables/</guid><description>
&lt;h1 id="概述">概述&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%bf%b0" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/f6M-xjvvgwZ75ufAfHOEbg">原文,公众号,为什么 kubernetes 环境要求开启 bridge-nf-call-iptables ?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="背景">背景&lt;a class="td-heading-self-link" href="#%e8%83%8c%e6%99%af" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Kubernetes 环境中，很多时候都要求节点内核参数开启 &lt;code>bridge-nf-call-iptables&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>sysctl -w net.bridge.bridge-nf-call-iptables=1
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>参考官方文档 Network Plugin Requirements&lt;/p>
&lt;/blockquote>
&lt;p>如果不开启或中途因某些操作导致参数被关闭了，就可能造成一些奇奇怪怪的网络问题，排查起来非常麻烦。&lt;/p>
&lt;p>为什么要开启呢？本文就来跟你详细掰扯下。&lt;/p>
&lt;h2 id="基于网桥的容器网络">基于网桥的容器网络&lt;a class="td-heading-self-link" href="#%e5%9f%ba%e4%ba%8e%e7%bd%91%e6%a1%a5%e7%9a%84%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Kubernetes 集群网络有很多种实现，有很大一部分都用到了 Linux 网桥:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wbh8l2/1622078114992-3789489c-ec6b-4424-80a7-d61eb8f5a0c9.webp" alt="">&lt;/p>
&lt;ul>
&lt;li>每个 Pod 的网卡都是 veth 设备，veth pair 的另一端连上宿主机上的网桥。&lt;/li>
&lt;li>由于网桥是虚拟的二层设备，同节点的 Pod 之间通信直接走二层转发，跨节点通信才会经过宿主机 eth0。&lt;/li>
&lt;/ul>
&lt;h2 id="service-同节点通信问题">Service 同节点通信问题&lt;a class="td-heading-self-link" href="#service-%e5%90%8c%e8%8a%82%e7%82%b9%e9%80%9a%e4%bf%a1%e9%97%ae%e9%a2%98" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>不管是 iptables 还是 ipvs 转发模式，Kubernetes 中访问 Service 都会进行 DNAT，将原本访问 ClusterIP:Port 的数据包 DNAT 成 Service 的某个 Endpoint (PodIP:Port)，然后内核将连接信息插入 conntrack 表以记录连接，目的端回包的时候内核从 conntrack 表匹配连接并反向 NAT，这样原路返回形成一个完整的连接链路:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wbh8l2/1622078114956-2f2ca711-0bda-46ae-a73d-5b4c4fc05ba0.webp" alt="">&lt;/p>
&lt;p>但是 Linux 网桥是一个虚拟的二层转发设备，而 iptables conntrack 是在三层上，所以如果直接访问同一网桥内的地址，就会直接走二层转发，不经过 conntrack:&lt;/p>
&lt;ol>
&lt;li>Pod 访问 Service，目的 IP 是 Cluster IP，不是网桥内的地址，走三层转发，会被 DNAT 成 PodIP:Port。&lt;/li>
&lt;li>如果 DNAT 后是转发到了同节点上的 Pod，目的 Pod 回包时发现目的 IP 在同一网桥上，就直接走二层转发了，没有调用 conntrack，导致回包时没有原路返回 (见下图)。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/wbh8l2/1622078115240-6df97fce-9fc4-4966-abbd-378a3d0c4bb1.webp" alt="">
由于没有原路返回，客户端与服务端的通信就不在一个 &amp;ldquo;频道&amp;rdquo; 上，不认为处在同一个连接，也就无法正常通信。&lt;/p>
&lt;p>常见的问题现象就是偶现 DNS 解析失败，当 coredns 所在节点上的 pod 解析 dns 时，dns 请求落到当前节点的 coredns pod 上时，就可能发生这个问题。&lt;/p>
&lt;h2 id="开启-bridge-nf-call-iptables">开启 bridge-nf-call-iptables&lt;a class="td-heading-self-link" href="#%e5%bc%80%e5%90%af-bridge-nf-call-iptables" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>如果 Kubernetes 环境的网络链路中走了 bridge 就可能遇到上述 Service 同节点通信问题，而 Kubernetes 很多网络实现都用到了 bridge。&lt;/p>
&lt;p>启用 &lt;code>bridge-nf-call-iptables&lt;/code> 这个内核参数 (置为 1)，表示 bridge 设备在二层转发时也去调用 iptables 配置的三层规则 (包含 conntrack)，所以开启这个参数就能够解决上述 Service 同节点通信问题，这也是为什么在 Kubernetes 环境中，大多都要求开启 &lt;code>bridge-nf-call-iptables&lt;/code> 的原因。&lt;/p></description></item><item><title>Docs: BUG</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/BUG/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/BUG/</guid><description>
&lt;h2 id="orphaned-pod-xx-found-but-volume-paths-are-still-present-on-disk">orphaned pod &amp;ldquo;XX&amp;rdquo; found, but volume paths are still present on disk&lt;a class="td-heading-self-link" href="#orphaned-pod-xx-found-but-volume-paths-are-still-present-on-disk" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/60987">issue #60987&lt;/a>&lt;/p>
&lt;p>kubelet 执行逻辑：&lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.19/pkg/kubelet/kubelet_volumes.go#L173">https://github.com/kubernetes/kubernetes/blob/release-1.19/pkg/kubelet/kubelet_volumes.go#L173&lt;/a>&lt;/p>
&lt;p>解决方式：&lt;/p>
&lt;ul>
&lt;li>更新至 1.19.8 版本及以上，&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1198">ChangeLog 中提到，在 #95301 Merged&lt;/a> 中已解决&lt;/li>
&lt;li>未更新的话，通过 &lt;a href="https://raw.githubusercontent.com/AliyunContainerService/kubernetes-issues-solution/master/kubelet/kubelet.sh">ali 提供的脚本&lt;/a>，进行一些修改，该脚本会手动 umount 和 rm 目录&lt;/li>
&lt;/ul>
&lt;h1 id="已修复">已修复&lt;a class="td-heading-self-link" href="#%e5%b7%b2%e4%bf%ae%e5%a4%8d" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="aggregator_unavailable_apiservice">aggregator_unavailable_apiservice&lt;a class="td-heading-self-link" href="#aggregator_unavailable_apiservice" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>问题描述：聚合 API 删除之后，依然存在于 kube-apiserver 的 metrics 中，这会导致频繁告警&lt;/p>
&lt;p>跟踪连接：&lt;a href="https://github.com/kubernetes/kubernetes/issues/92671">https://github.com/kubernetes/kubernetes/issues/92671&lt;/a>&lt;/p>
&lt;p>解决方式：&lt;a href="https://github.com/kubernetes/kubernetes/pull/96421">https://github.com/kubernetes/kubernetes/pull/96421&lt;/a>&lt;/p>
&lt;p>&lt;strong>将在 1.20 版本解决&lt;/strong>&lt;/p>
&lt;h2 id="scope-libcontainer-21733-systemd-test-default-dependenciesscope-has-no-pids-refusing">Scope libcontainer-21733-systemd-test-default-dependencies.scope has no PIDs. Refusing&lt;a class="td-heading-self-link" href="#scope-libcontainer-21733-systemd-test-default-dependenciesscope-has-no-pids-refusing" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/71887">https://github.com/kubernetes/kubernetes/issues/71887&lt;/a>&lt;/p>
&lt;p>解决方式：（1.16 及以后的版本中，无该问题。主要是 18+版本 docker 无该问题）&lt;/p>
&lt;p>忽略该告警：&lt;a href="https://www-01.ibm.com/support">https://www-01.ibm.com/support&lt;/a> &amp;hellip; .wss?uid=ibm10883724&lt;/p>
&lt;h2 id="error-while-processing-event-sysfscgroupdeviceslibcontainer_2434_systemd_test_defaultslice-0x40000100--in_createin_isdir-open-sysfscgroupdeviceslibcontainer_2434_systemd_test_defaultslice-no-such-file-or-directory">Error while processing event (&amp;quot;/sys/fs/cgroup/devices/libcontainer_2434_systemd_test_default.slice&amp;quot;: 0x40000100 == IN_CREATE|IN_ISDIR): open /sys/fs/cgroup/devices/libcontainer_2434_systemd_test_default.slice: no such file or directory&lt;a class="td-heading-self-link" href="#error-while-processing-event-sysfscgroupdeviceslibcontainer_2434_systemd_test_defaultslice-0x40000100--in_createin_isdir-open-sysfscgroupdeviceslibcontainer_2434_systemd_test_defaultslice-no-such-file-or-directory" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>解决方式：1.16 及以后版本解决该问题&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/issues/76531#issuecomment-548230839">https://github.com/kubernetes/kubernetes/issues/76531#issuecomment-548230839&lt;/a>&lt;/p>
&lt;h2 id="setting-volume-ownership-for-xxx-and-fsgroup-set-if-the-volume-has-a-lot-of-files-then-setting-volume-ownership-could-be-slow">Setting volume ownership for XXX and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow&lt;a class="td-heading-self-link" href="#setting-volume-ownership-for-xxx-and-fsgroup-set-if-the-volume-has-a-lot-of-files-then-setting-volume-ownership-could-be-slow" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> Apr &lt;span style="color:#0000cf;font-weight:bold">20&lt;/span> 11:03:37 lxkubenode01 kubelet&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>9103&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: W0420 11:03:37.275020 &lt;span style="color:#0000cf;font-weight:bold">9103&lt;/span> volume_linux.go:49&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> Setting volume ownership &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> /var/lib/kubelet/pods/63c4a49f-bf23-4b87-989e-102f5fcdb315/volumes/kubernetes.io~secret/seq-token-whpfk and fsGroup set. If the volume has a lot of files &lt;span style="color:#204a87;font-weight:bold">then&lt;/span> setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Apr &lt;span style="color:#0000cf;font-weight:bold">20&lt;/span> 11:03:46 lxkubenode01 kubelet&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>9103&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: W0420 11:03:46.198559 &lt;span style="color:#0000cf;font-weight:bold">9103&lt;/span> volume_linux.go:49&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> Setting volume ownership &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> /var/lib/kubelet/pods/cdb79fa0-4942-4211-9261-a4928f872bd6/volumes/kubernetes.io~secret/prometheus-operator-prometheus-node-exporter-token-snbtf and fsGroup set. If the volume has a lot of files &lt;span style="color:#204a87;font-weight:bold">then&lt;/span> setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Apr &lt;span style="color:#0000cf;font-weight:bold">20&lt;/span> 11:03:47 lxkubenode01 kubelet&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>9103&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: W0420 11:03:47.201212 &lt;span style="color:#0000cf;font-weight:bold">9103&lt;/span> volume_linux.go:49&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> Setting volume ownership &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> /var/lib/kubelet/pods/8cafe45f-2d14-4eb1-8c38-71a54b34f83c/volumes/kubernetes.io~secret/default-token-9hq84 and fsGroup set. If the volume has a lot of files &lt;span style="color:#204a87;font-weight:bold">then&lt;/span> setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>该报警发生于 1.17 及其以后的版本，由于 kubelet 代码更改后，导致频繁得大量刷新类似的日志信息&lt;/p>
&lt;p>问题跟踪：&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/issues/90293">https://github.com/kubernetes/kubernetes/issues/90293&lt;/a>&lt;/p>
&lt;p>解决方式：&lt;/p>
&lt;p>治标不治本方法：配置 rayslog 忽略&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &amp;gt; /etc/rsyslog.d/ignore-kubelet-volume.conf &lt;span style="color:#4e9a06">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">if (\$programname == &amp;#34;kubelet&amp;#34;) and (\$msg contains &amp;#34;Setting volume ownership&amp;#34;) then {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> stop
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>根治方法：升级集群
&lt;a href="https://github.com/kubernetes/kubernetes/pull/92878">https://github.com/kubernetes/kubernetes/pull/92878&lt;/a>，pr 已被 merged，在 1.18.8 及 1.19.0 之后得版本已修复，参见：
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#other-cleanup-or-flake">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#other-cleanup-or-flake&lt;/a>
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#other-cleanup-or-flake-1">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#other-cleanup-or-flake-1&lt;/a>&lt;/p>
&lt;h2 id="aggregatedapidown-报警">AggregatedAPIDown 报警&lt;a class="td-heading-self-link" href="#aggregatedapidown-%e6%8a%a5%e8%ad%a6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>问题描述：添加聚合 API 再删除后，kube-apiserver 中的 metircs 并不会删除，导致一致产生报警。报警规则：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>aggregator_unavailable_apiservice
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>问题跟踪：&lt;a href="https://github.com/kubernetes/kubernetes/issues/92671">https://github.com/kubernetes/kubernetes/issues/92671&lt;/a>&lt;/p>
&lt;p>解决方式：v1.20 有解决 PR&lt;/p>
&lt;h2 id="kubectl-get-cs-获取信息-unknown">kubectl get cs 获取信息 unknown&lt;a class="td-heading-self-link" href="#kubectl-get-cs-%e8%8e%b7%e5%8f%96%e4%bf%a1%e6%81%af-unknown" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>相关报警信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>watch chan error: etcdserver: mvcc: required revision has been compacted
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># kubectl get cs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>controller-manager &amp;lt;unknown&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>scheduler &amp;lt;unknown&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>etcd-0 &amp;lt;unknown&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>问题连接：&lt;a href="https://github.com/kubernetes/kubernetes/issues/83024#issuecomment-559538245">https://github.com/kubernetes/kubernetes/issues/83024#issuecomment-559538245&lt;/a>&lt;/p>
&lt;p>大意是在 1.17 中得到解决，1.19 及其以后版本不再支持 cs&lt;/p>
&lt;h2 id="kube-apiserver-响应超时的问题记录">kube-apiserver 响应超时的问题记录&lt;a class="td-heading-self-link" href="#kube-apiserver-%e5%93%8d%e5%ba%94%e8%b6%85%e6%97%b6%e7%9a%84%e9%97%ae%e9%a2%98%e8%ae%b0%e5%bd%95" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>首先是收到 apiserver 响应时间过长的告警，查看日志，发现频繁出现如下内容&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> I0331 01:40:30.953289 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> trace.go:116&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> Trace&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>2133477734&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;Get&amp;#34;&lt;/span> url:/api/v1/namespaces/kube-system &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>started: 2020-03-31 01:40:21.623714299 +0000 UTC &lt;span style="color:#000">m&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>+338766.344413381&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>total time: 9.329480544s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trace&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>2133477734&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>9.329404093s&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>9.329362028s&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> About to write a response
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> I0331 01:40:36.528652 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> trace.go:116&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> Trace&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>1431450424&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;Get&amp;#34;&lt;/span> url:/api/v1/namespaces/default &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>started: 2020-03-31 01:40:28.063278623 +0000 UTC &lt;span style="color:#000">m&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>+338772.783977705&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>total time: 8.465254793s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trace&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>1431450424&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>8.465073901s&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>8.465027207s&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> About to write a response
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> I0331 01:40:37.333718 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> trace.go:116&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> Trace&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>1319947973&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: &lt;span style="color:#4e9a06">&amp;#34;Get&amp;#34;&lt;/span> url:/api/v1/namespaces/kube-public &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>started: 2020-03-31 01:40:30.954280125 +0000 UTC &lt;span style="color:#000">m&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>+338775.674979196&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>total time: 6.379382999s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trace&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>1319947973&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>: &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>6.37929667s&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>6.379253238s&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> About to write a response
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>查看 etcd 日志，发现很多 took too long 的信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>etcdserver: read-only range request ..... took too long to execute
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>怀疑有可能是磁盘性能问题，使用 dd 测试，发现只有 10+M/s，查看 Raid 卡发现模式是直写，更换 Raid 卡，修改模式为强制回写。问题解决&lt;/p></description></item><item><title>Docs: kubelet 相关</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/kubelet-%E7%9B%B8%E5%85%B3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/kubelet-%E7%9B%B8%E5%85%B3/</guid><description/></item><item><title>Docs: Kubernetes 网络疑难杂症排查分享</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/Kubernetes-%E7%BD%91%E7%BB%9C%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87%E6%8E%92%E6%9F%A5%E5%88%86%E4%BA%AB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/Kubernetes-%E7%BD%91%E7%BB%9C%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87%E6%8E%92%E6%9F%A5%E5%88%86%E4%BA%AB/</guid><description>
&lt;p>原文链接：&lt;a href="https://zhuanlan.zhihu.com/p/77808615">https://zhuanlan.zhihu.com/p/77808615&lt;/a>&lt;/p>
&lt;p>大家好，我是 roc，来自腾讯云容器服务(TKE)团队，经常帮助用户解决各种 K8S 的疑难杂症，积累了比较丰富的经验，本文分享几个比较复杂的网络方面的问题排查和解决思路，深入分析并展开相关知识，信息量巨大，相关经验不足的同学可能需要细细品味才能消化，我建议收藏本文反复研读，当完全看懂后我相信你的功底会更加扎实，解决问题的能力会大大提升。
本文发现的问题是在使用 TKE 时遇到的，不同厂商的网络环境可能不一样，文中会对不同的问题的网络环境进行说明&lt;/p>
&lt;h2 id="跨-vpc-访问-nodeport-经常超时">跨 VPC 访问 NodePort 经常超时&lt;a class="td-heading-self-link" href="#%e8%b7%a8-vpc-%e8%ae%bf%e9%97%ae-nodeport-%e7%bb%8f%e5%b8%b8%e8%b6%85%e6%97%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现象: 从 VPC a 访问 VPC b 的 TKE 集群的某个节点的 NodePort，有时候正常，有时候会卡住直到超时。
原因怎么查？&lt;/p>
&lt;p>当然是先抓包看看啦，抓 server 端 NodePort 的包，发现异常时 server 能收到 SYN，但没响应 ACK:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321043666-3f97a2ad-dba6-44b2-a3b8-e57899a2295e.png" alt="image.png">
反复执行 netstat -s | grep LISTEN 发现 SYN 被丢弃数量不断增加:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321043214-2e62d692-f891-4b70-a25e-aeb224938825.png" alt="image.png">
分析：&lt;/p>
&lt;ul>
&lt;li>两个 VPC 之间使用对等连接打通的，CVM 之间通信应该就跟在一个内网一样可以互通。&lt;/li>
&lt;li>为什么同一 VPC 下访问没问题，跨 VPC 有问题? 两者访问的区别是什么?&lt;/li>
&lt;/ul>
&lt;p>再仔细看下 client 所在环境，发现 client 是 VPC a 的 TKE 集群节点，捋一下:&lt;/p>
&lt;ul>
&lt;li>client 在 VPC a 的 TKE 集群的节点&lt;/li>
&lt;li>server 在 VPC b 的 TKE 集群的节点&lt;/li>
&lt;/ul>
&lt;p>因为 TKE 集群中有个叫 ip-masq-agent 的 daemonset，它会给 node 写 iptables 规则，默认 SNAT 目的 IP 是 VPC 之外的报文，所以 client 访问 server 会做 SNAT，也就是这里跨 VPC 相比同 VPC 访问 NodePort 多了一次 SNAT，如果是因为多了一次 SNAT 导致的这个问题，直觉告诉我这个应该跟内核参数有关，因为是 server 收到包没回包，所以应该是 server 所在 node 的内核参数问题，对比这个 node 和 普通 TKE node 的默认内核参数，发现这个 node net.ipv4.tcp_tw_recycle = 1，这个参数默认是关闭的，跟用户沟通后发现这个内核参数确实在做压测的时候调整过。&lt;/p>
&lt;p>解释一下，TCP 主动关闭连接的一方在发送最后一个 ACK 会进入 TIME_AWAIT 状态，再等待 2 个 MSL 时间后才会关闭(因为如果 server 没收到 client 第四次挥手确认报文，server 会重发第三次挥手 FIN 报文，所以 client 需要停留 2 MSL 的时长来处理可能会重复收到的报文段；同时等待 2 MSL 也可以让由于网络不通畅产生的滞留报文失效，避免新建立的连接收到之前旧连接的报文)，了解更详细的过程请参考 TCP 四次挥手。
参数 tcp_tw_recycle 用于快速回收 TIME_AWAIT 连接，通常在增加连接并发能力的场景会开启，比如发起大量短连接，快速回收可避免 tw_buckets 资源耗尽导致无法建立新连接 (time wait bucket table overflow)
查得 tcp_tw_recycle 有个坑，在 RFC1323 有段描述:
An additional mechanism could be added to the TCP, a per-host cache of the last timestamp received from any connection. This value could then be used in the PAWS mechanism to reject old duplicate segments from earlier incarnations of the connection, if the timestamp clock can be guaranteed to have ticked at least once since the old connection was open. This would require that the TIME-WAIT delay plus the RTT together must be at least one tick of the sender’s timestamp clock. Such an extension is not part of the proposal of this RFC.
大概意思是说 TCP 有一种行为，可以缓存每个连接最新的时间戳，后续请求中如果时间戳小于缓存的时间戳，即视为无效，相应的数据包会被丢弃。
Linux 是否启用这种行为取决于 tcp_timestamps 和 tcp_tw_recycle，因为 tcp_timestamps 缺省开启，所以当 tcp_tw_recycle 被开启后，实际上这种行为就被激活了，当客户端或服务端以 NAT 方式构建的时候就可能出现问题。
当多个客户端通过 NAT 方式联网并与服务端交互时，服务端看到的是同一个 IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。如果发生了此类问题，具体的表现通常是是客户端明明发送的 SYN，但服务端就是不响应 ACK。
回到我们的问题上，client 所在节点上可能也会有其它 pod 访问到 server 所在节点，而它们都被 SNAT 成了 client 所在节点的 NODE IP，但时间戳存在差异，server 就会看到时间戳错乱，因为开启了 tcp_tw_recycle 和 tcp_timestamps 激活了上述行为，就丢掉了比缓存时间戳小的报文，导致部分 SYN 被丢弃，这也解释了为什么之前我们抓包发现异常时 server 收到了 SYN，但没有响应 ACK，进而说明为什么 client 的请求部分会卡住直到超时。
由于 tcp_tw_recycle 坑太多，在内核 4.12 之后已移除: &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/torvalds/linux/commit/4396e46187ca5070219b81773c4e65088dac50cc">remove tcp_tw_recycle&lt;/a>&lt;/p>
&lt;h2 id="lb-压测-cps-低">LB 压测 CPS 低&lt;a class="td-heading-self-link" href="#lb-%e5%8e%8b%e6%b5%8b-cps-%e4%bd%8e" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现象: LoadBalancer 类型的 Service，直接压测 NodePort CPS 比较高，但如果压测 LB CPS 就很低。
环境说明: 用户使用的黑石 TKE，不是公有云 TKE，黑石的机器是物理机，LB 的实现也跟公有云不一样，但 LoadBalancer 类型的 Service 的实现同样也是 LB 绑定各节点的 NodePort，报文发到 LB 后转到节点的 NodePort， 然后再路由到对应 pod，而测试在公有云 TKE 环境下没有这个问题。
client 抓包: 大量 SYN 重传。
server 抓包: 抓 NodePort 的包，发现当 client SYN 重传时 server 能收到 SYN 包但没有响应。&lt;/p>
&lt;p>又是 SYN 收到但没响应，难道又是开启 tcp_tw_recycle 导致的？检查节点的内核参数发现并没有开启，除了这个原因，还会有什么情况能导致被丢弃？
conntrack -S 看到 insert_failed 数量在不断增加，也就是 conntrack 在插入很多新连接的时候失败了，为什么会插入失败？什么情况下会插入失败？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321043606-5bb9ba47-ac62-4390-a027-a8bdd9a16751.png" alt="image.png">&lt;/p>
&lt;p>挖内核源码: netfilter conntrack 模块为每个连接创建 conntrack 表项时，表项的创建和最终插入之间还有一段逻辑，没有加锁，是一种乐观锁的过程。conntrack 表项并发刚创建时五元组不冲突的话可以创建成功，但中间经过 NAT 转换之后五元组就可能变成相同，第一个可以插入成功，后面的就会插入失败，因为已经有相同的表项存在。比如一个 SYN 已经做了 NAT 但是还没到最终插入的时候，另一个 SYN 也在做 NAT，因为之前那个 SYN 还没插入，这个 SYN 做 NAT 的时候就认为这个五元组没有被占用，那么它 NAT 之后的五元组就可能跟那个还没插入的包相同。
在我们这个问题里实际就是 netfilter 做 SNAT 时源端口选举冲突了，黑石 LB 会做 SNAT，SNAT 时使用了 16 个不同 IP 做源，但是短时间内源 Port 却是集中一致的，并发两个 SYN a 和 SYN b，被 LB SNAT 后源 IP 不同但源 Port 很可能相同，这里就假设两个报文被 LB SNAT 之后它们源 IP 不同源 Port 相同，报文同时到了节点的 NodePort 会再次做 SNAT 再转发到对应的 Pod，当报文到了 NodePort 时，这时它们五元组不冲突，netfilter 为它们分别创建了 conntrack 表项，SYN a 被节点 SNAT 时默认行为是 从 port_range 范围的当前源 Port 作为起始位置开始循环遍历，选举出没有被占用的作为源 Port，因为这两个 SYN 源 Port 相同，所以它们源 Port 选举的起始位置相同，当 SYN a 选出源 Port 但还没将 conntrack 表项插入时，netfilter 认为这个 Port 没被占用就很可能给 SYN b 也选了相同的源 Port，这时他们五元组就相同了，当 SYN a 的 conntrack 表项插入后再插入 SYN b 的 conntrack 表项时，发现已经有相同的记录就将 SYN b 的 conntrack 表项丢弃了。
解决方法探索: 不使用源端口选举，在 iptables 的 MASQUERADE 规则如果加 &amp;ndash;random-fully 这个 flag 可以让端口选举完全随机，基本上能避免绝大多数的冲突，但也无法完全杜绝。最终决定开发 LB 直接绑 Pod IP，不基于 NodePort，从而避免 netfilter 的 SNAT 源端口冲突问题。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321043854-95ba17f1-7a8e-4d2a-9985-966c083483f5.png" alt="image.png">&lt;/p>
&lt;h2 id="dns-解析偶尔-5s-延时">DNS 解析偶尔 5S 延时&lt;a class="td-heading-self-link" href="#dns-%e8%a7%a3%e6%9e%90%e5%81%b6%e5%b0%94-5s-%e5%bb%b6%e6%97%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>网上一搜，是已知问题，仔细分析，实际跟之前黑石 TKE 压测 LB CPS 低的根因是同一个，都是因为 netfilter conntrack 模块的设计问题，只不过之前发生在 SNAT，这个发生在 DNAT，这里用我的语言来总结下原因:&lt;/p>
&lt;p>DNS client (glibc 或 musl libc) 会并发请求 A 和 AAAA 记录，跟 DNS Server 通信自然会先 connect (建立 fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议， connect 时并不会创建 conntrack 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，这时它们源 Port 相同，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们分别创建 conntrack 表项，而集群内请求 kube-dns 或 coredns 都是访问的 CLUSTER-IP，报文最终会被 DNAT 成一个 endpoint 的 POD IP，当两个包被 DNAT 成同一个 IP，最终它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 dns 的 pod 副本只有一个实例的情况就很容易发生，现象就是 dns 请求超时，client 默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 dns 请求有 5s 的延时。&lt;/p>
&lt;p>参考 weave works 工程师总结的文章: &lt;a href="https://link.zhihu.com/?target=https%3A//www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts">Racy conntrack and DNS lookup timeouts&lt;/a>&lt;/p>
&lt;p>解决方案一: 使用 TCP 发送 DNS 请求&lt;/p>
&lt;p>如果使用 TCP 发 DNS 请求，connect 时就会插入 conntrack 表项，而并发的 A 和 AAAA 请求使用同一个 fd，所以只会有一次 connect，也就只会尝试创建一个 conntrack 表项，也就避免插入时冲突。&lt;/p>
&lt;p>resolv.conf 可以加 options use-vc 强制 glibc 使用 TCP 协议发送 DNS query。下面是这个 man resolv.conf 中关于这个选项的说明:&lt;/p>
&lt;p>use-vc &lt;strong>(&lt;strong>since glibc 2.14&lt;/strong>)&lt;/strong> Sets RES_USEVC in _res.options. This option forces the use of TCP &lt;strong>for&lt;/strong> DNS resolutions.&lt;/p>
&lt;p>解决方案二: 避免相同五元组 DNS 请求的并发&lt;/p>
&lt;p>resolv.conf 还有另外两个相关的参数：&lt;/p>
&lt;ul>
&lt;li>single-request-reopen (since glibc 2.9): A 和 AAAA 请求使用不同的 socket 来发送，这样它们的源 Port 就不同，五元组也就不同，避免了使用同一个 conntrack 表项。&lt;/li>
&lt;li>single-request (since glibc 2.10): A 和 AAAA 请求改成串行，没有并发，从而也避免了冲突。&lt;/li>
&lt;/ul>
&lt;p>man resolv.conf 中解释如下:&lt;/p>
&lt;pre>&lt;code>single-request-reopen (since glibc 2.9)
Sets RES_SNGLKUPREOP in _res.options. The resolver
uses the same socket for the A and AAAA requests. Some
hardware mistakenly sends back only one reply. When
that happens the client system will sit and wait for
the second reply. Turning this option on changes this
behavior so that if two requests from the same port are
not handled correctly it will close the socket and open
a new one before sending the second request.
single-request (since glibc 2.10)
Sets RES_SNGLKUP in _res.options. By default, glibc
performs IPv4 and IPv6 lookups in parallel since
version 2.9. Some appliance DNS servers cannot handle
these queries properly and make the requests time out.
This option disables the behavior and makes glibc
perform the IPv6 and IPv4 requests sequentially (at the
cost of some slowdown of the resolving process).
&lt;/code>&lt;/pre>
&lt;p>要给容器的 resolv.conf 加上 options 参数，最方便的是直接在 Pod Spec 里面的 dnsConfig 加 (k8s v1.9 及以上才支持)&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321043750-825f29ce-ad7d-4d3c-885c-b911b8f5616c.png" alt="image.png">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">spec&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">dnsConfig&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">options&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">single-request-reopen&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>加 options 还有其它一些方法:&lt;/p>
&lt;ul>
&lt;li>在容器的 ENTRYPOINT 或者 CMD 脚本中，执行 /bin/echo &amp;lsquo;options single-request-reopen&amp;rsquo; &amp;raquo; /etc/resolv.conf&lt;/li>
&lt;li>在 postStart hook 里加:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">lifecycle&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">postStart&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">exec&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">command&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/bin/sh&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- -&lt;span style="color:#000">c&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#4e9a06">&amp;#34;/bin/echo &amp;#39;options single-request-reopen&amp;#39; &amp;gt;&amp;gt; /etc/resolv.conf&amp;#34;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>使用 &lt;a href="https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/reference/access-authn-authz/admission-controllers/%23mutatingadmissionwebhook-beta-in-1-9">MutatingAdmissionWebhook&lt;/a>，这是 1.9 引入的 Controller，用于对一个指定的资源的操作之前，对这个资源进行变更。 istio 的自动 sidecar 注入就是用这个功能来实现的，我们也可以通过 MutatingAdmissionWebhook 来自动给所有 Pod 注入 resolv.conf 文件，不过需要一定的开发量。&lt;/li>
&lt;/ul>
&lt;p>解决方案三: 使用本地 DNS 缓存&lt;/p>
&lt;p>仔细观察可以看到前面两种方案是 glibc 支持的，而基于 alpine 的镜像底层库是 musl libc 不是 glibc，所以即使加了这些 options 也没用，这种情况可以考虑使用本地 DNS 缓存来解决，容器的 DNS 请求都发往本地的 DNS 缓存服务(dnsmasq, nscd 等)，不需要走 DNAT，也不会发生 conntrack 冲突。另外还有个好处，就是避免 DNS 服务成为性能瓶颈。&lt;/p>
&lt;p>使用本地 DNS 缓存有两种方式：&lt;/p>
&lt;ul>
&lt;li>每个容器自带一个 DNS 缓存服务&lt;/li>
&lt;li>每个节点运行一个 DNS 缓存服务，所有容器都把本节点的 DNS 缓存作为自己的 nameserver&lt;/li>
&lt;/ul>
&lt;p>从资源效率的角度来考虑的话，推荐后一种方式。&lt;/p>
&lt;h2 id="pod-访问另一个集群的-apiserver-有延时">Pod 访问另一个集群的 apiserver 有延时&lt;a class="td-heading-self-link" href="#pod-%e8%ae%bf%e9%97%ae%e5%8f%a6%e4%b8%80%e4%b8%aa%e9%9b%86%e7%be%a4%e7%9a%84-apiserver-%e6%9c%89%e5%bb%b6%e6%97%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现象：集群 a 的 Pod 内通过 kubectl 访问集群 b 的内网地址，偶尔出现延时的情况，但直接在宿主机上用同样的方法却没有这个问题。
提炼环境和现象精髓:&lt;/p>
&lt;ol>
&lt;li>在 pod 内将另一个集群 apiserver 的 ip 写到了 hosts，因为 TKE apiserver 开启内网集群外内网访问创建的内网 LB 暂时没有支持自动绑内网 DNS 域名解析，所以集群外的内网访问 apiserver 需要加 hosts&lt;/li>
&lt;li>pod 内执行 kubectl 访问另一个集群偶尔延迟 5s，有时甚至 10s&lt;/li>
&lt;/ol>
&lt;p>观察到 5s 延时，感觉跟之前 conntrack 的丢包导致 dns 解析 5s 延时有关，但是加了 hosts 呀，怎么还去解析域名？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321044470-414c4a58-f758-4a7e-839d-589a7f95b4bf.png" alt="image.png">&lt;/p>
&lt;p>进入 pod netns 抓包: 执行 kubectl 时确实有 dns 解析，并且发生延时的时候 dns 请求没有响应然后做了重试。&lt;/p>
&lt;p>看起来延时应该就是之前已知 conntrack 丢包导致 dns 5s 超时重试导致的。但是为什么会去解析域名? 明明配了 hosts 啊，正常情况应该是优先查找 hosts，没找到才去请求 dns 呀，有什么配置可以控制查找顺序?&lt;/p>
&lt;p>搜了一下发现: /etc/nsswitch.conf 可以控制，但看有问题的 pod 里没有这个文件。然后观察到有问题的 pod 用的 alpine 镜像，试试其它镜像后发现只有基于 alpine 的镜像才会有这个问题。&lt;/p>
&lt;p>再一搜发现: musl libc 并不会使用 /etc/nsswitch.conf ，也就是说 alpine 镜像并没有实现用这个文件控制域名查找优先顺序，瞥了一眼 musl libc 的 gethostbyname 和 getaddrinfo 的实现，看起来也没有读这个文件来控制查找顺序，写死了先查 hosts，没找到再查 dns。&lt;/p>
&lt;p>这么说，那还是该先查 hosts 再查 dns 呀，为什么这里抓包看到是先查的 dns? (如果是先查 hosts 就能命中查询，不会再发起 dns 请求)&lt;/p>
&lt;p>访问 apiserver 的 client 是 kubectl，用 go 写的，会不会是 go 程序解析域名时压根没调底层 c 库的 gethostbyname 或 getaddrinfo?&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321044460-39624243-9c14-4afa-b67b-e1e5f6f16d9e.png" alt="image.png">&lt;/p>
&lt;p>搜一下发现果然是这样: go runtime 用 go 实现了 glibc 的 getaddrinfo 的行为来解析域名，减少了 c 库调用 (应该是考虑到减少 cgo 调用带来的的性能损耗)&lt;/p>
&lt;p>issue: &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/golang/go/issues/18518">net: replicate DNS resolution behaviour of getaddrinfo(glibc) in the go dns resolver&lt;/a>&lt;/p>
&lt;p>翻源码验证下:&lt;/p>
&lt;p>Unix 系的 OS 下，除了 openbsd， go runtime 会读取 /etc/nsswitch.conf (net/conf.go):&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321044701-c2416183-d8fb-491e-a107-a29d9807f817.png" alt="image.png">&lt;/p>
&lt;p>hostLookupOrder 函数决定域名解析顺序的策略，Linux 下，如果没有 nsswitch.conf 文件就 dns 比 hosts 文件优先 (net/conf.go):&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321045092-7e5684b8-b12c-4edf-bd36-646478cfca03.png" alt="image.png">&lt;/p>
&lt;p>可以看到 hostLookupDNSFiles 的意思是 dns first (net/dnsclient_unix.go):&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321045155-044354f3-7ca9-4bbf-9a07-1a710f9ab352.png" alt="image.png">&lt;/p>
&lt;p>所以虽然 alpine 用的 musl libc 不是 glibc，但 go 程序解析域名还是一样走的 glibc 的逻辑，而 alpine 没有 /etc/nsswitch.conf 文件，也就解释了为什么 kubectl 访问 apiserver 先做 dns 解析，没解析到再查的 hosts，导致每次访问都去请求 dns，恰好又碰到 conntrack 那个丢包问题导致 dns 5s 延时，在用户这里表现就是 pod 内用 kubectl 访问 apiserver 偶尔出现 5s 延时，有时出现 10s 是因为重试的那次 dns 请求刚好也遇到 conntrack 丢包导致延时又叠加了 5s 。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321045410-394de883-24aa-4663-b744-68902b0c4a43.png" alt="image.png">&lt;/p>
&lt;p>解决方案:&lt;/p>
&lt;ol>
&lt;li>换基础镜像，不用 alpine&lt;/li>
&lt;li>挂载 nsswitch.conf 文件 (可以用 hostPath)&lt;/li>
&lt;/ol>
&lt;h2 id="dns-解析异常">DNS 解析异常&lt;a class="td-heading-self-link" href="#dns-%e8%a7%a3%e6%9e%90%e5%bc%82%e5%b8%b8" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现象: 有个用户反馈域名解析有时有问题，看报错是解析超时。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321045606-9ec23bb0-a6a5-4b83-8c8e-b2ee954a23f3.png" alt="image.png">&lt;/p>
&lt;p>第一反应当然是看 coredns 的 log:&lt;/p>
&lt;pre>&lt;code>[ERROR] 2 loginspub.gaeamobile-inc.net.
A: unreachable backend: read udp 172.16.0.230:43742-&amp;gt;10.225.30.181:53: i/o timeout
&lt;/code>&lt;/pre>
&lt;p>这是上游 DNS 解析异常了，因为解析外部域名 coredns 默认会请求上游 DNS 来查询，这里的上游 DNS 默认是 coredns pod 所在宿主机的 resolv.conf 里面的 nameserver (coredns pod 的 dnsPolicy 为 &amp;ldquo;Default&amp;rdquo;，也就是会将宿主机里的 resolv.conf 里的 nameserver 加到容器里的 resolv.conf, coredns 默认配置 proxy . /etc/resolv.conf, 意思是非 service 域名会使用 coredns 容器中 resolv.conf 文件里的 nameserver 来解析)
确认了下，超时的上游 DNS 10.225.30.181 并不是期望的 nameserver，VPC 默认 DNS 应该是 180 开头的。看了 coredns 所在节点的 resolv.conf，发现确实多出了这个非期望的 nameserver，跟用户确认了下，这个 DNS 不是用户自己加上去的，添加节点时这个 nameserver 本身就在 resolv.conf 中。
根据内部同学反馈， 10.225.30.181 是广州一台年久失修将被撤裁的 DNS，物理网络，没有 VIP，撤掉就没有了，所以如果 coredns 用到了这台 DNS 解析时就可能 timeout。后面我们自己测试，某些 VPC 的集群确实会有这个 nameserver，奇了怪了，哪里冒出来的？&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321045721-41dbc1cf-65c1-4f8d-acae-7f187be1d58d.png" alt="image.png">&lt;/p>
&lt;p>又试了下直接创建 CVM，不加进 TKE 节点发现没有这个 nameserver，只要一加进 TKE 节点就有了 !!!
看起来是 TKE 的问题，将 CVM 添加到 TKE 集群会自动重装系统，初始化并加进集群成为 K8S 的 node，确认了初始化过程并不会写 resolv.conf，会不会是 TKE 的 OS 镜像问题？尝试搜一下除了 /etc/resolv.conf 之外哪里还有这个 nameserver 的 IP，最后发现 /etc/resolvconf/resolv.conf.d/base 这里面有。
看下 /etc/resolvconf/resolv.conf.d/base 的作用：Ubuntu 的 /etc/resolv.conf 是动态生成的，每次重启都会将 /etc/resolvconf/resolv.conf.d/base 里面的内容加到 /etc/resolv.conf 里。
经确认: 这个文件确实是 TKE 的 Ubuntu OS 镜像里自带的，可能发布 OS 镜像时不小心加进去的。
那为什么有些 VPC 的集群的节点 /etc/resolv.conf 里面没那个 IP 呢？它们的 OS 镜像里也都有那个文件那个 IP 呀。
请教其它部门同学发现:&lt;/p>
&lt;ul>
&lt;li>非 dhcp 子机，cvm 的 cloud-init 会覆盖 /etc/resolv.conf 来设置 dns&lt;/li>
&lt;li>dhcp 子机，cloud-init 不会设置，而是通过 dhcp 动态下发&lt;/li>
&lt;li>2018 年 4 月 之后创建的 VPC 就都是 dhcp 类型了的，比较新的 VPC 都是 dhcp 类型的&lt;/li>
&lt;/ul>
&lt;p>真相大白：/etc/resolv.conf 一开始内容都包含 /etc/resolvconf/resolv.conf.d/base 的内容，也就是都有那个不期望的 nameserver，但老的 VPC 由于不是 dhcp 类型，所以 cloud-init 会覆盖 /etc/resolv.conf，抹掉了不被期望的 nameserver，而新创建的 VPC 都是 dhcp 类型，cloud-init 不会覆盖 /etc/resolv.conf，导致不被期望的 nameserver 残留在了 /etc/resolv.conf，而 coredns pod 的 dnsPolicy 为 “Default”，也就是会将宿主机的 /etc/resolv.conf 中的 nameserver 加到容器里，coredns 解析集群外的域名默认使用这些 nameserver 来解析，当用到那个将被撤裁的 nameserver 就可能 timeout。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321047012-a469c265-666b-4a7c-8772-2ca29d31fb38.png" alt="image.png">&lt;/p>
&lt;p>临时解决: 删掉 /etc/resolvconf/resolv.conf.d/base 重启
长期解决: 我们重新制作 TKE Ubuntu OS 镜像然后发布更新
这下应该没问题了吧，But, 用户反馈还是会偶尔解析有问题，但现象不一样了，这次并不是 dns timeout。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321046862-da081636-c09a-41dd-bc9c-81a198596688.png" alt="image.png">&lt;/p>
&lt;p>用脚本跑测试仔细分析现象:&lt;/p>
&lt;ul>
&lt;li>请求 loginspub.gaeamobile-inc.net 时，偶尔提示域名无法解析&lt;/li>
&lt;li>请求 accounts.google.com 时，偶尔提示连接失败&lt;/li>
&lt;/ul>
&lt;p>进入 dns 解析偶尔异常的容器的 netns 抓包:&lt;/p>
&lt;ul>
&lt;li>dns 请求会并发请求 A 和 AAAA 记录&lt;/li>
&lt;li>测试脚本发请求打印序号，抓包然后 wireshark 分析对比异常时请求序号偏移量，找到异常时的 dns 请求报文，发现异常时 A 和 AAAA 记录的请求 id 冲突，并且 AAAA 响应先返回&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321046872-917a5fcb-e2aa-4f34-af44-61c8889b2d41.png" alt="image.png">&lt;/p>
&lt;p>正常情况下 id 不会冲突，这里冲突了也就能解释这个 dns 解析异常的现象了:&lt;/p>
&lt;ul>
&lt;li>loginspub.gaeamobile-inc.net 没有 AAAA (ipv6) 记录，它的响应先返回告知 client 不存在此记录，由于请求 id 跟 A 记录请求冲突，后面 A 记录响应返回了 client 发现 id 重复就忽略了，然后认为这个域名无法解析&lt;/li>
&lt;li>accounts.google.com 有 AAAA 记录，响应先返回了，client 就拿这个记录去尝试请求，但当前容器环境不支持 ipv6，所以会连接失败&lt;/li>
&lt;/ul>
&lt;p>那为什么 dns 请求 id 会冲突?&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321046898-b53265be-d847-4900-99c4-ba76dd5885aa.png" alt="image.png">&lt;/p>
&lt;p>继续观察发现: 其它节点上的 pod 不会复现这个问题，有问题这个节点上也不是所有 pod 都有这个问题，只有基于 alpine 镜像的容器才有这个问题，在此节点新起一个测试的 alpine:latest 的容器也一样有这个问题。
为什么 alpine 镜像的容器在这个节点上有问题在其它节点上没问题？ 为什么其他镜像的容器都没问题？它们跟 alpine 的区别是什么？
发现一点区别: alpine 使用的底层 c 库是 musl libc，其它镜像基本都是 glibc
翻 musl libc 源码, 构造 dns 请求时，请求 id 的生成没加锁，而且跟当前时间戳有关:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321046965-02bd884e-0de2-447c-8613-b60b646ed17b.png" alt="image.png">&lt;/p>
&lt;p>看注释，作者应该认为这样 id 基本不会冲突，事实证明，绝大多数情况确实不会冲突，我在网上搜了很久没有搜到任何关于 musl libc 的 dns 请求 id 冲突的情况。这个看起来取决于硬件，可能在某种类型硬件的机器上运行，短时间内生成的 id 就可能冲突。我尝试跟用户在相同地域的集群，添加相同配置相同机型的节点，也复现了这个问题，但后来删除再添加时又不能复现了，看起来后面新建的 cvm 又跑在了另一种硬件的母机上了。
OK，能解释通了，再底层的细节就不清楚了，我们来看下解决方案:&lt;/p>
&lt;ul>
&lt;li>换基础镜像 (不用 alpine)&lt;/li>
&lt;li>完全静态编译业务程序(不依赖底层 c 库)，比如 go 语言程序编译时可以关闭 cgo (CGO_ENABLED=0)，并告诉链接器要静态链接 (go build 后面加 -ldflags &amp;lsquo;-d&amp;rsquo;)，但这需要语言和编译工具支持才可以&lt;/li>
&lt;/ul>
&lt;p>最终建议用户基础镜像换成另一个比较小的镜像: debian:stretch-slim。
问题解决，但用户后面觉得 debian:stretch-slim 做出来的镜像太大了，有 6MB 多，而之前基于 alpine 做出来只有 1MB 多，最后使用了一个非官方的修改过 musl libc 的 alpine 镜像作为基础镜像，里面禁止了 AAAA 请求从而避免这个问题。&lt;/p>
&lt;h2 id="pod-偶尔存活检查失败">Pod 偶尔存活检查失败&lt;a class="td-heading-self-link" href="#pod-%e5%81%b6%e5%b0%94%e5%ad%98%e6%b4%bb%e6%a3%80%e6%9f%a5%e5%a4%b1%e8%b4%a5" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现象: Pod 偶尔会存活检查失败，导致 Pod 重启，业务偶尔连接异常。
之前从未遇到这种情况，在自己测试环境尝试复现也没有成功，只有在用户这个环境才可以复现。这个用户环境流量较大，感觉跟连接数或并发量有关。
用户反馈说在友商的环境里没这个问题。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321047595-763d2583-dd02-4da2-bc96-2d83b3189e3b.png" alt="image.png">&lt;/p>
&lt;p>对比友商的内核参数发现有些区别，尝试将节点内核参数改成跟友商的一样，发现问题没有复现了。
再对比分析下内核参数差异，最后发现是 backlog 太小导致的，节点的 net.ipv4.tcp_max_syn_backlog 默认是 1024，如果短时间内并发新建 TCP 连接太多，SYN 队列就可能溢出，导致部分新连接无法建立。
解释一下:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321048182-d3b4a0f8-bdcc-4874-8e35-ca6bb0ff5393.png" alt="image.png">&lt;/p>
&lt;p>TCP 连接建立会经过三次握手，server 收到 SYN 后会将连接加入 SYN 队列，当收到最后一个 ACK 后连接建立，这时会将连接从 SYN 队列中移动到 ACCEPT 队列。在 SYN 队列中的连接都是没有建立完全的连接，处于半连接状态。如果 SYN 队列比较小，而短时间内并发新建的连接比较多，同时处于半连接状态的连接就多，SYN 队列就可能溢出，tcp_max_syn_backlog 可以控制 SYN 队列大小，用户节点的 backlog 大小默认是 1024，改成 8096 后就可以解决问题。&lt;/p>
&lt;h2 id="访问-externaltrafficpolicy-为-local-的-service-对应-lb-有时超时">访问 externalTrafficPolicy 为 Local 的 Service 对应 LB 有时超时&lt;a class="td-heading-self-link" href="#%e8%ae%bf%e9%97%ae-externaltrafficpolicy-%e4%b8%ba-local-%e7%9a%84-service-%e5%af%b9%e5%ba%94-lb-%e6%9c%89%e6%97%b6%e8%b6%85%e6%97%b6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>现象：用户在 TKE 创建了公网 LoadBalancer 类型的 Service，externalTrafficPolicy 设为了 Local，访问这个 Service 对应的公网 LB 有时会超时。
externalTrafficPolicy 为 Local 的 Service 用于在四层获取客户端真实源 IP，官方参考文档：&lt;a href="https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/tutorials/services/source-ip/%23source-ip-for-services-with-type-loadbalancer">Source IP for Services with Type=LoadBalancer&lt;/a>
TKE 的 LoadBalancer 类型 Service 实现是使用 CLB 绑定所有节点对应 Service 的 NodePort，CLB 不做 SNAT，报文转发到 NodePort 时源 IP 还是真实的客户端 IP，如果 NodePort 对应 Service 的 externalTrafficPolicy 不是 Local 的就会做 SNAT，到 pod 时就看不到客户端真实源 IP 了，但如果是 Local 的话就不做 SNAT，如果本机 node 有这个 Service 的 endpoint 就转到对应 pod，如果没有就直接丢掉，因为如果转到其它 node 上的 pod 就必须要做 SNAT，不然无法回包，而 SNAT 之后就无法获取真实源 IP 了。
LB 会对绑定节点的 NodePort 做健康检查探测，检查 LB 的健康检查状态: 发现这个 NodePort 的所有节点都不健康 !!!&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321048129-4b563755-681b-4722-89de-fbf1dc02edf3.png" alt="image.png">&lt;/p>
&lt;p>那么问题来了:&lt;/p>
&lt;ol>
&lt;li>为什么会全不健康，这个 Service 有对应的 pod 实例，有些节点上是有 endpoint 的，为什么它们也不健康?&lt;/li>
&lt;li>LB 健康检查全不健康，但是为什么有时还是可以访问后端服务?&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321048164-ea436898-4080-4978-af6e-b9f67b8d2d0b.png" alt="image.png">&lt;/p>
&lt;p>跟 LB 的同学确认: 如果后端 rs 全不健康会激活 LB 的全死全活逻辑，也就是所有后端 rs 都可以转发。
那么有 endpoint 的 node 也是不健康这个怎么解释?
在有 endpoint 的 node 上抓 NodePort 的包: 发现很多来自 LB 的 SYN，但是没有响应 ACK。
看起来报文在哪被丢了，继续抓下 cbr0 看下: 发现没有来自 LB 的包，说明报文在 cbr0 之前被丢了。
再观察用户集群环境信息:&lt;/p>
&lt;ol>
&lt;li>k8s 版本 1.12&lt;/li>
&lt;li>启用了 ipvs&lt;/li>
&lt;li>只有 local 的 service 才有异常&lt;/li>
&lt;/ol>
&lt;p>尝试新建一个 1.12 启用 ipvs 和一个没启用 ipvs 的测试集群。也都创建 Local 的 LoadBalancer Service，发现启用 ipvs 的测试集群复现了那个问题，没启用 ipvs 的集群没这个问题。
再尝试创建 1.10 的集群，也启用 ipvs，发现没这个问题。
看起来跟集群版本和是否启用 ipvs 有关。
1.12 对比 1.10 启用 ipvs 的集群: 1.12 的会将 LB 的 EXTERNAL-IP 绑到 kube-ipvs0 上，而 1.10 的不会:&lt;/p>
&lt;pre>&lt;code>$ ip a show kube-ipvs0 | grep -A2 170.106.134.124
inet 170.106.134.124/32 brd 170.106.134.124 scope global kube-ipvs0
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>170.106.134.124 是 LB 的公网 IP&lt;/li>
&lt;li>1.12 启用 ipvs 的集群将 LB 的公网 IP 绑到了 kube-ipvs0 网卡上&lt;/li>
&lt;/ul>
&lt;p>kube-ipvs0 是一个 dummy interface，实际不会接收报文，可以看到它的网卡状态是 DOWN，主要用于绑 ipvs 规则的 VIP，因为 ipvs 主要工作在 netfilter 的 INPUT 链，报文通过 PREROUTING 链之后需要决定下一步该进入 INPUT 还是 FORWARD 链，如果是本机 IP 就会进入 INPUT，如果不是就会进入 FORWARD 转发到其它机器。所以 k8s 利用 kube-ipvs0 这个网卡将 service 相关的 VIP 绑在上面以便让报文进入 INPUT 进而被 ipvs 转发。
当 IP 被绑到 kube-ipvs0 上，内核会自动将上面的 IP 写入 local 路由:&lt;/p>
&lt;pre>&lt;code>$ ip route show table local | grep 170.106.134.124
local 170.106.134.124 dev kube-ipvs0 proto kernel scope host src 170.106.134.124
&lt;/code>&lt;/pre>
&lt;p>内核认为在 local 路由里的 IP 是本机 IP，而 linux 默认有个行为: 忽略任何来自非回环网卡并且源 IP 是本机 IP 的报文。而 LB 的探测报文源 IP 就是 LB IP，也就是 Service 的 EXTERNAL-IP 猜想就是因为这个 IP 被绑到 kube-ipvs0，自动加进 local 路由导致内核直接忽略了 LB 的探测报文。
带着猜想做实现， 试一下将 LB IP 从 local 路由中删除:&lt;/p>
&lt;pre>&lt;code>ip route del table local local 170.106.134.124 dev kube-ipvs0 proto kernel scope host src 170.106.134.124
&lt;/code>&lt;/pre>
&lt;p>发现这个 node 的在 LB 的健康检查的状态变成健康了! 看来就是因为这个 LB IP 被绑到 kube-ipvs0 导致内核忽略了来自 LB 的探测报文，然后 LB 收不到回包认为不健康。
那为什么其它厂商没反馈这个问题？应该是 LB 的实现问题，腾讯云的公网 CLB 的健康探测报文源 IP 就是 LB 的公网 IP，而大多数厂商的 LB 探测报文源 IP 是保留 IP 并非 LB 自身的 VIP。
如何解决呢? 发现一个内核参数: &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/torvalds/linux/commit/8153a10c08f1312af563bb92532002e46d3f504a">accept_local&lt;/a> 可以让 linux 接收源 IP 是本机 IP 的报文。
试了开启这个参数，确实在 cbr0 收到来自 LB 的探测报文了，说明报文能被 pod 收到，但抓 eth0 还是没有给 LB 回包。&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321048476-f4d6996a-24b1-453e-83db-157e9edd8284.png" alt="image.png">&lt;/p>
&lt;p>为什么没有回包? 分析下五元组，要给 LB 回包，那么 目的 IP:目的 Port 必须是探测报文的 源 IP:源 Port，所以目的 IP 就是 LB IP，由于容器不在主 netns，发包经过 veth pair 到 cbr0 之后需要再经过 netfilter 处理，报文进入 PREROUTING 链然后发现目的 IP 是本机 IP，进入 INPUT 链，所以报文就出不去了。再分析下进入 INPUT 后会怎样，因为目的 Port 跟 LB 探测报文源 Port 相同，是一个随机端口，不在 Service 的端口列表，所以没有对应的 IPVS 规则，IPVS 也就不会转发它，而 kube-ipvs0 上虽然绑了这个 IP，但它是一个 dummy interface，不会收包，所以报文最后又被忽略了。
再看看为什么 1.12 启用 ipvs 会绑 EXTERNAL-IP 到 kube-ipvs0，翻翻 k8s 的 kube-proxy 支持 ipvs 的 &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/enhancements/blob/baca87088480254b26d0fdeb26303d7c51a20fbd/keps/sig-network/0011-ipvs-proxier.md%23support-loadbalancer-service">proposal&lt;/a>，发现有个地方说法有点漏洞:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321048780-c1ea012a-6743-4f8d-b65d-5e9414c6fae3.png" alt="image.png">&lt;/p>
&lt;p>LB 类型 Service 的 status 里有 ingress IP，实际就是 kubectl get service 看到的 EXTERNAL-IP，这里说不会绑定这个 IP 到 kube-ipvs0，但后面又说会给它创建 ipvs 规则，既然没有绑到 kube-ipvs0，那么这个 IP 的报文根本不会进入 INPUT 被 ipvs 模块转发，创建的 ipvs 规则也是没用的。
后来找到作者私聊，思考了下，发现设计上确实有这个问题。
看了下 1.10 确实也是这么实现的，但是为什么 1.12 又绑了这个 IP 呢? 调研后发现是因为 &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/issues/59976">#59976&lt;/a> 这个 issue 发现一个问题，后来引入 &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/pull/63066">#63066&lt;/a> 这个 PR 修复的，而这个 PR 的行为就是让 LB IP 绑到 kube-ipvs0，这个提交影响 1.11 及其之后的版本。
&lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/issues/59976">#59976&lt;/a> 的问题是因为没绑 LB IP 到 kube-ipvs0 上，在自建集群使用 MetalLB 来实现 LoadBalancer 类型的 Service，而有些网络环境下，pod 是无法直接访问 LB 的，导致 pod 访问 LB IP 时访问不了，而如果将 LB IP 绑到 kube-ipvs0 上就可以通过 ipvs 转发到 LB 类型 Service 对应的 pod 去， 而不需要真正经过 LB，所以引入了 &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/pull/63066">#63066&lt;/a> 这个 PR。
临时方案: 将 &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/pull/63066">#63066&lt;/a> 这个 PR 的更改回滚下，重新编译 kube-proxy，提供升级脚本升级存量 kube-proxy。
如果是让 LB 健康检查探测支持用保留 IP 而不是自身的公网 IP ，也是可以解决，但需要跨团队合作，而且如果多个厂商都遇到这个问题，每家都需要为解决这个问题而做开发调整，代价较高，所以长期方案需要跟社区沟通一起推进，所以我提了 issue，将问题描述的很清楚: &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/issues/79783">#79783&lt;/a>
小思考: 为什么 CLB 可以不做 SNAT ? 回包目的 IP 就是真实客户端 IP，但客户端是直接跟 LB IP 建立的连接，如果回包不经过 LB 是不可能发送成功的呀。
是因为 CLB 的实现是在母机上通过隧道跟 CVM 互联的，多了一层封装，回包始终会经过 LB。
就是因为 CLB 不做 SNAT，正常来自客户端的报文是可以发送到 nodeport，但健康检查探测报文由于源 IP 是 LB IP 被绑到 kube-ipvs0 导致被忽略，也就解释了为什么健康检查失败，但通过 LB 能访问后端服务，只是有时会超时。那么如果要做 SNAT 的 LB 岂不是更糟糕，所有报文都变成 LB IP，所有报文都会被忽略?
我提的 issue 有回复指出，AWS 的 LB 会做 SNAT，但它们不将 LB 的 IP 写到 Service 的 Status 里，只写了 hostname，所以也不会绑 LB IP 到 kube-ipvs0:&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321048653-3d139244-a1e8-4f3b-bdfb-347a696e2ba8.png" alt="image.png">&lt;/p>
&lt;p>但是只写 hostname 也得 LB 支持自动绑域名解析，并且个人觉得只写 hostname 很别扭，通过 kubectl get svc 或者其它 k8s 管理系统无法直接获取 LB IP，这不是一个好的解决方法。
我提了 &lt;a href="https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/pull/79976">#79976&lt;/a> 这个 PR 可以解决问题: 给 kube-proxy 加 &amp;ndash;exclude-external-ip 这个 flag 控制是否为 LB IP 创建 ipvs 规则和绑定 kube-ipvs0。
但有人担心增加 kube-proxy flag 会增加 kube-proxy 的调试复杂度，看能否在 iptables 层面解决:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321049321-245b0b45-c999-41bf-9a4b-bc8ae92b1640.png" alt="image.png">&lt;/p>
&lt;p>仔细一想，确实可行，打算有空实现下，重新提个 PR:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/incvdy/1626321049812-36974aec-fcdb-4635-ac2c-8918d38005c3.png" alt="image.png">&lt;/p>
&lt;h2 id="结语">结语&lt;a class="td-heading-self-link" href="#%e7%bb%93%e8%af%ad" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>至此，我们一起完成了一段奇妙的问题排查之旅，信息量很大并且比较复杂，有些没看懂很正常，但我希望你可以收藏起来反复阅读，一起在技术的道路上打怪升级。
发布于 2019-08-12&lt;/p></description></item><item><title>Docs: 常见问题排查与解决方案</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</guid><description>
&lt;p>原文链接：&lt;a href="https://mp.weixin.qq.com/s/URU5jz8oFi-jQhWR4snGWQ">https://mp.weixin.qq.com/s/URU5jz8oFi-jQhWR4snGWQ&lt;/a>
&lt;a href="https://kubesphere.com.cn/forum/d/5152-kubernetes">https://kubesphere.com.cn/forum/d/5152-kubernetes&lt;/a>&lt;/p>
&lt;h3 id="xxx10250-connect-no-route-to-host">XXX:10250 connect: no route to host&lt;a class="td-heading-self-link" href="#xxx10250-connect-no-route-to-host" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>网络问题，API Server 与 Kubelet 通信失败，将会导致很多很多问题&lt;/p>
&lt;h3 id="crd-specversions-invalid-value">CRD spec.versions: Invalid value&lt;a class="td-heading-self-link" href="#crd-specversions-invalid-value" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795842662-52020db8-cb01-4164-a488-25273674f882.png" alt="image.png">
&lt;strong>原因:&lt;/strong> CRD yaml 文件中 apiVersion 与 versions 中的版本不对应
参考: &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/&lt;/a>&lt;/p>
&lt;h3 id="删除-namespaces-时-terminating无法强制删除且无法在该-ns-下创建对象">删除 namespaces 时 Terminating，无法强制删除且无法在该 ns 下创建对象&lt;a class="td-heading-self-link" href="#%e5%88%a0%e9%99%a4-namespaces-%e6%97%b6-terminating%e6%97%a0%e6%b3%95%e5%bc%ba%e5%88%b6%e5%88%a0%e9%99%a4%e4%b8%94%e6%97%a0%e6%b3%95%e5%9c%a8%e8%af%a5-ns-%e4%b8%8b%e5%88%9b%e5%bb%ba%e5%af%b9%e8%b1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795842467-c0df8305-29f6-4095-a3f7-cc12c7f2d3ad.png" alt="image.png">
&lt;strong>原因:&lt;/strong> ns 处于 terminating 时 hang 住了，使用 &amp;ndash;grace-period=0 &amp;ndash;force 强制删除也无效
&lt;strong>解决:&lt;/strong>
&lt;em>#  导出 K8s 访问密钥&lt;/em>
echo $(kubectl config view &amp;ndash;raw -oyaml | grep client-cert  |cut -d &amp;rsquo; &amp;rsquo; -f 6) |base64 -d &amp;gt; /tmp/client.pem
echo $(kubectl config view &amp;ndash;raw -oyaml | grep client-key-data  |cut -d &amp;rsquo; &amp;rsquo; -f 6 ) |base64 -d &amp;gt; /tmp/client-key.pem
echo $(kubectl config view &amp;ndash;raw -oyaml | grep certificate-authority-data  |cut -d &amp;rsquo; &amp;rsquo; -f 6  ) |base64 -d &amp;gt; /tmp/ca.pem
&lt;em>#  解决 namespace Terminating，根据实际情况修改&amp;lt;namespaces&amp;gt;&lt;/em>
curl &amp;ndash;cert /tmp/client.pem &amp;ndash;key /tmp/client-key.pem &amp;ndash;cacert /tmp/ca.pem -H &amp;ldquo;Content-Type: application/json&amp;rdquo; -X PUT &amp;ndash;data-binary @/tmp/temp.json https://xxx.xxx.xxx.xxx:6443/api/v1/namespaces/&amp;lt;namespaces&amp;gt;/finalize&lt;/p>
&lt;h3 id="docker-启动时提示-no-sockets-found-via-socket-activation">Docker 启动时提示 no sockets found via socket activation&lt;a class="td-heading-self-link" href="#docker-%e5%90%af%e5%8a%a8%e6%97%b6%e6%8f%90%e7%a4%ba-no-sockets-found-via-socket-activation" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795842598-50fc963e-e60e-47ed-ad40-b7bdb78e0396.png" alt="image.png">
&lt;strong>解决:&lt;/strong> 在启动 Docker 前先执行 systemctl unmask Docker.socket 即可&lt;/p>
&lt;h3 id="prometheus-opening-storage-failed-invalid-block-sequence">Prometheus opening storage failed: invalid block sequence&lt;a class="td-heading-self-link" href="#prometheus-opening-storage-failed-invalid-block-sequence" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795842458-c0924cb6-06dd-4052-a378-31d068f6494b.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 这个需要排查 Prometheus 持久化目录中是否存在时间超出设置阈值的时间段的文件，删掉后重启即可&lt;/p>
&lt;h3 id="kubelet-提示-the-node-was-low-on-resource-ephemeral-storage">Kubelet 提示: The node was low on resource: ephemeral-storage&lt;a class="td-heading-self-link" href="#kubelet-%e6%8f%90%e7%a4%ba-the-node-was-low-on-resource-ephemeral-storage" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>原因:&lt;/strong> 节点上 Kubelet 的配置路径超过阈值会触发驱逐，默认情况下阈值是 85%
&lt;strong>解决:&lt;/strong> 或者清理磁盘释放资源，或者通过可修改 Kubelet 的配置参数 imagefs.available 来提高阈值,然后重启 Kubelet.
参考: &lt;a href="https://cloud.tencent.com/developer/article/1456389">https://cloud.tencent.com/developer/article/1456389&lt;/a>&lt;/p>
&lt;h3 id="kubectl-查看日志时提示-error-from-server-get-httpsxxx10250containerlogsspring-prodxxx-0xxx-dial-tcp-xxx10250-io-timeout">kubectl 查看日志时提示: Error from server: Get https://xxx:10250/containerLogs/spring-prod/xxx-0/xxx: dial tcp xxx:10250: i/o timeout&lt;a class="td-heading-self-link" href="#kubectl-%e6%9f%a5%e7%9c%8b%e6%97%a5%e5%bf%97%e6%97%b6%e6%8f%90%e7%a4%ba-error-from-server-get-httpsxxx10250containerlogsspring-prodxxx-0xxx-dial-tcp-xxx10250-io-timeout" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>原因:&lt;/strong> 目地机器的 iptables 对 10250 这个端口进行了 drop，如下图
iptables-save -L INPUT –-line-numbers&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795842260-c7c3b3c6-8e6e-46c0-9cd1-1f059d858cf9.png" alt="image.png">
&lt;strong>解决:&lt;/strong> 删除对应的规则
iptables -D INPUT 10&lt;/p>
&lt;h3 id="service-解析提示-temporary-failure-in-name-resolution">Service 解析提示 Temporary failure in name resolution&lt;a class="td-heading-self-link" href="#service-%e8%a7%a3%e6%9e%90%e6%8f%90%e7%a4%ba-temporary-failure-in-name-resolution" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795843427-7456fcdd-df06-406b-826a-e1745b6e3eab.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 出现这种情况很奇怪，现象显示就是域名无法解析，全格式的域名能够解析是因为在 pod 的/etc/hosts 中有全域名的记录,那么问题就出在于 CoreDNS 解析上，CoreDNS 从日志来看，没有任何报错，但是从 pod 的状态来看，虽然处于 Running 状态，但是 0/1 可以看出 CoreDNS 并未处于 ready 状态.
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795843336-22cc40fb-29a4-4a46-b135-17acd860101a.png" alt="image.png">
可以查看 ep 记录，会发现 Endpoint 那一栏是空的，这也就证实了 K8s 把 CoreDNS 的状态分为了 notready 状态，所以 ep 才没有记录，经过与其它环境比较后发现跟配置有关，最终定位在 CoreDNS 的配置文件上,在插件上需要加上 ready
&lt;strong>解决:&lt;/strong> 在 cm 的配置上添加 read 插件，如下图
&lt;em># &amp;hellip;  省略&lt;/em>
data:
  Corefile: |
    .:53 {
        errors
        health
        ready  &lt;em>#  加上该行后问题解决&lt;/em>
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream /etc/resolv.conf
          fallthrough in-addr.arpa ip6.arpa
        }
       &lt;em># &amp;hellip;  省略&lt;/em>&lt;/p>
&lt;p>关于 CoreDNS 的 ready 插件的使用,可以参考 👉 这里
总结起来就是使用 ready 来表明当前已准备好可以接收请求，从 codedns 的 yaml 文件也可以看到有 livenessProbe&lt;/p>
&lt;h3 id="使用-kubectl-命令行时提示-unable-to-connect-to-the-server-x509-certificate-relies-on-legacy-common-name-field-use-sans-or-temporarily-enable-common-name-matching-with-godebugx509ignorecn0">使用 Kubectl 命令行时提示: Unable to connect to the server: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0&lt;a class="td-heading-self-link" href="#%e4%bd%bf%e7%94%a8-kubectl-%e5%91%bd%e4%bb%a4%e8%a1%8c%e6%97%b6%e6%8f%90%e7%a4%ba-unable-to-connect-to-the-server-x509-certificate-relies-on-legacy-common-name-field-use-sans-or-temporarily-enable-common-name-matching-with-godebugx509ignorecn0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>原因:&lt;/strong> 这个跟本地的 go 环境有关
&lt;strong>解决:&lt;/strong> 在使用 kubectl 前使用命令 export GODEBUG=x509ignoreCN=0 即可&lt;/p>
&lt;h3 id="namespaces-kube-system-is-forbidden-this-namespace-may-not-be-deleted">namespaces &amp;ldquo;kube-system&amp;rdquo; is forbidden: this namespace may not be deleted&lt;a class="td-heading-self-link" href="#namespaces-kube-system-is-forbidden-this-namespace-may-not-be-deleted" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>原因:&lt;/strong> kube-system 是集群中受保护的 ns, 被禁止删除，主要是防止误操作，如果需要删除的话，可以使用 &amp;ndash;force
参考: &lt;a href="https://github.com/kubernetes/kubernetes/pull/62167/files">https://github.com/kubernetes/kubernetes/pull/62167/files&lt;/a>&lt;/p>
&lt;h3 id="unknown-field-volumeclaimtemplates">unknown field volumeClaimTemplates&lt;a class="td-heading-self-link" href="#unknown-field-volumeclaimtemplates" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795843447-8705e384-6c44-4fd7-a7a8-c1fcf995505e.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 提示这个错误的原因是资源对象是 Deployment, 而 Deployment 本就是无状态的， 所以也就没有使用 pv 这一说法了，可以参考 api
参考: 👉Deploymentspec-v1-apps&lt;/p>
&lt;h3 id="coredns-提示-loop-12700138827---53-detected-for-zone-">CoreDNS 提示 Loop (127.0.0.1:38827 -&amp;gt; :53) detected for zone &amp;ldquo;.&amp;rdquo;&lt;a class="td-heading-self-link" href="#coredns-%e6%8f%90%e7%a4%ba-loop-12700138827---53-detected-for-zone-" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795843702-35c559c4-d985-49a6-8bff-ba6598670ac9.png" alt="image.png">
&lt;strong>原因:&lt;/strong> CoreDNS 所在的宿主机上 /etc/resolv.conf 中存在有 127.0.xx 的 nameserver，这样会造成解析死循环。
&lt;strong>解决:&lt;/strong> 修改宿主机 /etc/resolv.conf 或者将 CoreDNS 的 ConfigMap 中的 forward 修改为一个可用的地址, 如 8.8.8.8。&lt;/p>
&lt;h3 id="hostpath-volumes-are-not-allowed-to-be-used">hostPath volumes are not allowed to be used&lt;a class="td-heading-self-link" href="#hostpath-volumes-are-not-allowed-to-be-used" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795843658-b6818606-bcc9-4a9a-8e0f-670242f281ee.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 集群中存在 psp 禁止 pod 直接挂载 hostpath.
&lt;strong>解决:&lt;/strong> 通过添加以下的 psp 规则来允许或者删除存在的 psp 都可
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: auth-privilege-psp
spec:
  allowPrivilegeEscalation: true
  allowedHostPaths:
  - pathPrefix: /
  fsGroup:
    ranges:
    - max: 65535
      min: 1
    rule: RunAsAny
  hostNetwork: true
  hostPID: true
  hostPorts:
  - max: 9796
    min: 9796
  privileged: true
  requiredDropCapabilities:
  - ALL
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    ranges:
    - max: 65535
      min: 1
    rule: RunAsAny
  volumes:
  - configMap
  - emptyDir
  - projected
  - secret
  - downwardAPI
  - persistentVolumeClaim
  - hostPath&lt;/p>
&lt;h3 id="container-has-runasnonroot-and-image-has-non-numeric-user-grafana-cannot-verify-user-is-non-root">container has runAsNonRoot and image has non-numeric user (grafana), cannot verify user is non-root&lt;a class="td-heading-self-link" href="#container-has-runasnonroot-and-image-has-non-numeric-user-grafana-cannot-verify-user-is-non-root" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795843986-21fcb1fe-752b-4829-a3a8-e300254fc295.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 这是由于在 deploy 中设置了 securityContext: runAsNonRoot: true，在这种情况下，当 pod 启动时，使用的默认用户。比如上面的 grafana，K8s 无法确定他是不是 root 用户
&lt;strong>解决:&lt;/strong> 指定 securityContext:runAsUser: 1000，随便一个 id 号即可，只要不是 0(0 代表 root)。
参考: &lt;a href="https://stackoverflow.com/questions/51544003/using-runasnonroot-in-kubernetes">https://stackoverflow.com/questions/51544003/using-runasnonroot-in-kubernetes&lt;/a>&lt;/p>
&lt;h3 id="oci-runtime-create-failed-no-such-file-or-directory">OCI runtime create failed: no such file or directory&lt;a class="td-heading-self-link" href="#oci-runtime-create-failed-no-such-file-or-directory" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795844250-2ddf9955-a0cd-4f0a-8569-e44454ef9cbf.png" alt="image.png">
&lt;strong>原因:&lt;/strong> /var/lib/Kubelet/pod 下的数据目录已经损坏.
&lt;strong>解决:&lt;/strong> 删除对应的目录即可&lt;/p>
&lt;h3 id="镜像拉取时出现-imageinspecterror">镜像拉取时出现 ImageInspectError&lt;a class="td-heading-self-link" href="#%e9%95%9c%e5%83%8f%e6%8b%89%e5%8f%96%e6%97%b6%e5%87%ba%e7%8e%b0-imageinspecterror" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795844359-d580b01d-02a1-456f-9303-475ddeeb8ca7.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 这种情况下一般都是镜像损坏了
&lt;strong>解决:&lt;/strong> 把相关的镜像删除后重新拉取&lt;/p>
&lt;h3 id="kubelet-日志提示-node-not-found">Kubelet 日志提示: node not found&lt;a class="td-heading-self-link" href="#kubelet-%e6%97%a5%e5%bf%97%e6%8f%90%e7%a4%ba-node-not-found" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795844738-03cb47b6-c4ea-435a-942a-f01aa23b25f1.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 这个报错只是中间过程，真正的原因在于 apiserver 没有启动成功，导致会一直出现这个错误
&lt;strong>解决:&lt;/strong> 排查 Kubelet 与 apiserver 的连通是否正常&lt;/p>
&lt;h3 id="oci-runtime-create-failed-executable-file-not-found-in-path">OCI runtime create failed: executable file not found in PATH&lt;a class="td-heading-self-link" href="#oci-runtime-create-failed-executable-file-not-found-in-path" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795844911-0f208c9e-c77f-43a7-abdf-3df68963489d.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 在 path 中没有 nvidia-container-runtime-hook 这个二进制文件，可能跟本人删除 nvidia 显卡驱动有关.
&lt;strong>解决:&lt;/strong> nvidia-container-runtime-hook 是 Docker nvidia 的 runtime 文件，重新安装即可.&lt;/p>
&lt;h3 id="nginx-ingress-empty-address">Nginx Ingress Empty address&lt;a class="td-heading-self-link" href="#nginx-ingress-empty-address" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;em># kubectl get ingress&lt;/em>
NAME         HOSTS                                       ADDRESS   PORTS   AGE
Prometheus   Prometheus.1box.com                                   80      31d&lt;/p>
&lt;p>会发现 address 中的 ip 是空的，而查看生产环境时却是有 ip 列表的.
&lt;strong>原因:&lt;/strong> 这个其实不是一个错误，也不影响使用，原因在于测试环境中是不存在 LoadBalancer 类型的 svc, 如果需要 address 中显示 ip 的话需要做些额外的设置
&lt;strong>解决:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>在 nginx controller 的容器中指定启动参数-report-ingress-status&lt;/li>
&lt;li>在 nginx controller 引用的 ConfigMap 中添加 external-status-address: &amp;ldquo;10.164.15.220&amp;rdquo;&lt;/li>
&lt;/ol>
&lt;p>这样的话,在 address 中变会显示 10.164.15.220 了
参考:
&lt;a href="https://github.com/nginxinc/kubernetes-ingress/issues/587">https://github.com/nginxinc/kubernetes-ingress/issues/587&lt;/a>
&lt;a href="https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/reporting-resources-status/">https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/reporting-resources-status/&lt;/a>&lt;/p>
&lt;h3 id="kubelet-but-volume-paths-are-still-present-on-disk">Kubelet: but volume paths are still present on disk&lt;a class="td-heading-self-link" href="#kubelet-but-volume-paths-are-still-present-on-disk" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795844960-bf400d2f-82f2-4005-b134-596eb9a33011.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 这种 pod 已经被删除了，但是 volume 还存在于 disk 中
&lt;strong>解决:&lt;/strong> 删除对应的目录/var/lib/Kubelet/pods/3cd73&amp;hellip;
参考: &lt;a href="https://github.com/longhorn/longhorn/issues/485">https://github.com/longhorn/longhorn/issues/485&lt;/a>&lt;/p>
&lt;h3 id="pleg-is-not-healthy">PLEG is not healthy&lt;a class="td-heading-self-link" href="#pleg-is-not-healthy" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795845375-52becf2c-1526-4044-92b2-6358b9779f97.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 宿主机上面跑的容器太多，导致 pod 无法在 3m 钟内完成生命周期检查
&lt;strong>解决:&lt;/strong> PLEG(Pod Lifecycle Event Generator) 用于 kublet 同步 pod 生命周期，本想着如果是因为时间短导致的超时，那是不是可以直接调整这个时间呢? 查看 Kubelet 的源码发现不太行，3m 时间是写在代码里的因此无法修改，当然修改再编译肯定没问题，但成本太大，所以只得优化容器的调度情况.
参考: &lt;a href="https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/">https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/&lt;/a>&lt;/p>
&lt;h3 id="metrics-server-10255-connection-refused">metrics-server: 10255 connection refused&lt;a class="td-heading-self-link" href="#metrics-server-10255-connection-refused" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>unable to fully collect metrics: [unable to fully scrape metrics from source Kubelet_summary:K8s-node-49: unable to fetch metrics from Kubelet K8s-node-49 (xxx.xxx.xxx.49): Get http://xxx.xxx.xxx.49:10255/stats/summary?only_cpu_and_memory=true: dial tcp xxx.xxx.xxx.49:10255: connect: connection refused&lt;/p>
&lt;p>&lt;strong>原因:&lt;/strong> 现在的 K8s 都默认禁用了 Kubelet 的 10255 端口，出现这个错误是因此在 Kubelet 启动命令中启用了该端口
&lt;strong>解决:&lt;/strong> 将 - &amp;ndash;Kubelet-port=10255 注释&lt;/p>
&lt;h3 id="metrics-server-no-such-host">metrics-server: no such host&lt;a class="td-heading-self-link" href="#metrics-server-no-such-host" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>unable to fetch metrics from Kubelet K8s-node-234 (K8s-node-234): Get https://K8s-node-234:10250/stats/summary?only_cpu_and_memory=true: dial tcp: lookup K8s-node-234 on 10.96.0.10:53: no such host&lt;/p>
&lt;p>&lt;strong>解决:&lt;/strong> 使用 Kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP 参数
参考: &lt;a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md">https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md&lt;/a>&lt;/p>
&lt;h3 id="pod-无法解析域名">pod 无法解析域名&lt;a class="td-heading-self-link" href="#pod-%e6%97%a0%e6%b3%95%e8%a7%a3%e6%9e%90%e5%9f%9f%e5%90%8d" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>集群中新增了几台机器用于部署 clickhouse 用于做大数据分析，为了不让这类占用大量资源的 Pod 影响其它 Pod，因此选择给机器打 taint 的形式控制该类 Pod 的调度, 创建 Pod 后发现这些 Pod 都会出现 DNS 解析异常,
原因；要注意容器网络，比如这里使用的是 flannel 是否容忍了这些机器的 taint，不然的话，flannel 是无法被调度到这些机器的,因此容器间的通信会出现问题，&lt;strong>可以将类似 flannel 这些的基础 POD 容忍所有的 NoScheule 与 NoExecute&lt;/strong>
&lt;strong>解决:&lt;/strong> flannel 的 ds yaml 中添加以下 toleration，这样适用任何的场景
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists&lt;/p>
&lt;h3 id="are-you-tring-to-mount-a-directory-on-to-a-file">Are you tring to mount a directory on to a file&lt;a class="td-heading-self-link" href="#are-you-tring-to-mount-a-directory-on-to-a-file" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795845346-b553ddc2-30c1-42db-8e6a-2ea5c4433bc7.png" alt="image.png">
&lt;strong>原因:&lt;/strong> Yaml 文件中使用了 subPath, 但是 mountPath 指向了一个目录
&lt;strong>解决:&lt;/strong> mountPath 需要加上文件名
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795845724-89cd98d0-1c7a-4701-9a6e-1c4584dd5619.png" alt="image.png">&lt;/p>
&lt;h3 id="kubernetes-启动后提示-slice-no-such-file-ro-directory">Kubernetes 启动后提示 slice: no such file ro directory&lt;a class="td-heading-self-link" href="#kubernetes-%e5%90%af%e5%8a%a8%e5%90%8e%e6%8f%90%e7%a4%ba-slice-no-such-file-ro-directory" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795845678-9dc9dc51-8c3e-4461-a002-a4ffb9dc3723.png" alt="image.png">
&lt;strong>原因:&lt;/strong> yum 安装的 Kubelet 默认的是 cgroupfs，而 Docker 一般默认的是 systemd。但是 kubernetes 安装的时候建议使用 systemd, Kubelet 跟 Docker 的不一致, 要么修改 Kubelet 的启动参数 , 要么修改 dokcer 启动参数
&lt;strong>解决:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Docker 的启动参数文件为: /etc/Docker/daemon.json: &amp;ldquo;exec-opts&amp;rdquo;: [&amp;ldquo;native.cgroupdriver=systemd”]&lt;/li>
&lt;li>Kubelet 的启动参数文件为: /var/lib/Kubelet/config.yaml: cgroupDriver: systemd&lt;/li>
&lt;/ul>
&lt;h3 id="cni0-already-has-an-ip-address-different-from-xxxxxxxxxxxxx">&amp;ldquo;cni0&amp;rdquo; already has an IP address different from xxx.xxxx.xxx.xxx&lt;a class="td-heading-self-link" href="#cni0-already-has-an-ip-address-different-from-xxxxxxxxxxxxx" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795845816-7daf652c-acde-4e70-8dc4-4d889b6df5aa.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 使用 kubeadm reset 重复操作过, reset 之后，之前 flannel 创建的 bridge device cni0 和网口设备 flannel.1 依然健在
&lt;strong>解决:&lt;/strong> 添加之前需要清除下网络
kubeadm reset
systemctl stop Kubelet
systemctl stop Docker
rm -rf /var/lib/cni/
rm -rf /var/lib/Kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig Docker0 down
ip link delete cni0
ip link delete flannel.1
systemctl start Docker
systemctl start Kubelet&lt;/p>
&lt;h3 id="kubeadm-初始化时提示-cpu-小于-2">kubeadm 初始化时提示 CPU 小于 2&lt;a class="td-heading-self-link" href="#kubeadm-%e5%88%9d%e5%a7%8b%e5%8c%96%e6%97%b6%e6%8f%90%e7%a4%ba-cpu-%e5%b0%8f%e4%ba%8e-2" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
    [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2
[preflight] If you know what you are doing, you can make a check non-fatal with &lt;code>--ignore-preflight-errors=...&lt;/code>&lt;/p>
&lt;p>&lt;strong>原因:&lt;/strong> kubeadm 对资源一定的要求，如果是测试环境无所谓的话,可忽略
&lt;strong>解决:&lt;/strong>
使用  &amp;ndash;ignore-preflight-errors  忽略&lt;/p>
&lt;h3 id="unable-to-update-cni-config-no-network-found">Unable to update cni config: no network found&lt;a class="td-heading-self-link" href="#unable-to-update-cni-config-no-network-found" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795846340-4412cff2-a01b-4ab1-8716-7ee4cd3a0409.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 还未部署网络插件容器，导致在 /etc/cni 下还没有文件
&lt;strong>解决:&lt;/strong> 根据实际情况部署网络插件&lt;/p>
&lt;h3 id="while-reading-google-dockercfg-metadata">while reading &amp;lsquo;google-Dockercfg&amp;rsquo; metadata&lt;a class="td-heading-self-link" href="#while-reading-google-dockercfg-metadata" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795846370-2c2f039d-127a-4b47-895e-c265511d3ce9.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 从其它机器访问上述这些 url 确实出现 404
&lt;strong>解决:&lt;/strong> 由于是在 RKE 上部署 K8s, 所以可能会去访问 google 相关的 url, 不影响业务,可以忽略&lt;/p>
&lt;h3 id="no-providers-available-to-validate-pod-request">no providers available to validate pod request&lt;a class="td-heading-self-link" href="#no-providers-available-to-validate-pod-request" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795846584-1bddd303-39e9-402b-bdde-6b7c7ac69ac2.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 在 api-server 的启动参数 enable-admission 中设置了 PodSecrityPolicy, 但是集群中又没有任何的 podsecritypolicy，因此导致整个集群都无法新建出 pod
&lt;strong>解决:&lt;/strong> 删除相应的 podsecritypolicy 即可&lt;/p>
&lt;h3 id="unable-to-upgrade-connection-unauthorized">unable to upgrade connection: Unauthorized&lt;a class="td-heading-self-link" href="#unable-to-upgrade-connection-unauthorized" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795846680-575f17cb-69b1-4774-8760-311b14dbb792.png" alt="image.png">
&lt;strong>原因:&lt;/strong> Kubelet 的启动参数少了 x509 认证方式
&lt;strong>解决:&lt;/strong> 配置证书的路径, 加上重启 Kubelet 即可
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795846898-75454713-4d2f-4691-8a30-fa92f04df863.png" alt="image.png">&lt;/p>
&lt;h3 id="kubectl-get-cs-提示unknown">kubectl get cs 提示&amp;lt;unknown&amp;gt;&lt;a class="td-heading-self-link" href="#kubectl-get-cs-%e6%8f%90%e7%a4%baunknown" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795847105-09736a43-5687-4c97-b73a-f8e22a440f64.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 这是个 kubectl 的 bug, 跟版本相关，kubernetes 有意废除 get cs 命令
&lt;strong>解决:&lt;/strong> 目前对集群的运行无影响, 可通过加 -o yaml 查看状态&lt;/p>
&lt;h3 id="安装-kubeadm-时提示-depends-错误">安装 kubeadm 时提示 Depends 错误&lt;a class="td-heading-self-link" href="#%e5%ae%89%e8%a3%85-kubeadm-%e6%97%b6%e6%8f%90%e7%a4%ba-depends-%e9%94%99%e8%af%af" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795847695-8edead12-7272-461d-9b74-ce74bc525af4.png" alt="image.png">
&lt;strong>原因:&lt;/strong> 跟 kubeadm 没多大关系, 系统安装的有问题
&lt;strong>解决:&lt;/strong> 执行以下命令修复
apt &amp;ndash;fix-broken install
apt-get update&lt;/p>
&lt;h3 id="访问-service-时提示-connection-refused">访问 service 时提示 Connection refused&lt;a class="td-heading-self-link" href="#%e8%ae%bf%e9%97%ae-service-%e6%97%b6%e6%8f%90%e7%a4%ba-connection-refused" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>现象: 从另一环境中把 yaml 文件导入到新环境后有些 service 访问不通
telnet mongodb-mst.external 27017
Trying 10.97.135.242&amp;hellip;
telnet: Unable to connect to remote host: Connection refused&lt;/p>
&lt;p>首先排除了域名、端口的配置问题。
会发现提示连接拒绝.可以确定的是集群内的 DNS 是正常的.
那么就是通过 clusterIP 无法到达 realserver. 查看 iptables 规则
发现提示 default has no Endpoints &amp;ndash;reject-with icmp-port-unreachable
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795847625-b50a453d-74ba-4a1f-96fb-b6e9416385e5.png" alt="image.png">
很奇怪, 提示没有 Endpoints, 但是使用 kubectl get ep 又能看到 ep 存在且配置没有问题
而且这个 default 是怎么来的.
为了方便部署, 很多配置是从别的环境导出的配置, 有些 service 访问是没问题的, 只有少部分 connection refused。
结比一下发现一个很有趣的问题，先来看下不正常的 yaml 文件:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795847852-63786a28-76ca-4716-94ff-dbcb16defcb1.png" alt="image.png">
由于服务在集群外部署的, 因此这里使用了 subset 方式, 开始怀疑问题在这里, 但是后来知道这个不是重点
乍一看这个配置没什么问题, 部署也很正常, 但是对比正常的 yaml 文件，发现一个区别：
如果在 services 中的端口指定了名字, 那么在 subsets 中的端口也要带名字, 没有带名字的就会出现 connection refused，这个确实之前从来没有关注过, 一个端口的情况下也不会指定名字
而且这面 iptalbes 中提示的 default 刚好就是这里的 port name,虽然不敢相信，但是也只能试一试这个方法: 在 subsets 中也加了 port name
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795848243-58d550ed-6077-4d55-8b93-e49c42860058.png" alt="image.png">
重新部署一个，再次查看 iptalbes 规则
iptables-save|grep mongodb-mst
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795848135-b82e144a-718a-47e0-a923-d4dbef27e87a.png" alt="image.png">
OMG, 居然可行, 再看下 telnet 的结果:
Trying 10.105.116.92&amp;hellip;
Connected to mongodb-mst.external.svc.cluster.local.
Escape character is &amp;lsquo;^]&amp;rsquo;.&lt;/p>
&lt;p>访问也是没问题, 那么原因就在于:
&lt;strong>在 service 中指定了 port name 时, 也需要在 ep 中指定 port name&lt;/strong>&lt;/p>
&lt;h3 id="error-converting-fieldpath-field-label-not-supported">error converting fieldPath: field label not supported&lt;a class="td-heading-self-link" href="#error-converting-fieldpath-field-label-not-supported" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>今天遇到一个部署 Deployment 出错的问题, yaml 文件如下:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">apiVersion: apps/v1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#000">kind: Deployment&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">metadata&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">  &lt;/span>&lt;span style="color:#000">name: demo-Deployment&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">  &lt;/span>&lt;span style="color:#000">namespace: 4test&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">  &lt;/span>&lt;span style="color:#204a87;font-weight:bold">labels&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">    &lt;/span>&lt;span style="color:#000">app: config-demo-app&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">spec&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">  &lt;/span>&lt;span style="color:#000">replicas: 1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">  &lt;/span>&lt;span style="color:#204a87;font-weight:bold">selector&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">    &lt;/span>&lt;span style="color:#204a87;font-weight:bold">matchLabels&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">      &lt;/span>&lt;span style="color:#000">app: config-demo-app&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">  &lt;/span>&lt;span style="color:#204a87;font-weight:bold">template&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">    &lt;/span>&lt;span style="color:#204a87;font-weight:bold">metadata&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">      &lt;/span>&lt;span style="color:#204a87;font-weight:bold">labels&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#000">app: config-demo-app&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">      &lt;/span>&lt;span style="color:#204a87;font-weight:bold">annotations&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#8f5902;font-style:italic">*#&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">The field we&amp;#39;ll use to couple our ConfigMap and Deployment*&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#000">configHash: 4431f6d28fdf60c8140d28c42cde331a76269ac7a0e6af01d0de0fa8392c1145&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">    &lt;/span>&lt;span style="color:#204a87;font-weight:bold">spec&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">      &lt;/span>&lt;span style="color:#204a87;font-weight:bold">containers&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">      &lt;/span>-&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">name: config-demo-app&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#000">image: gcr.io/optimum-rock-145719/config-demo-app&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#204a87;font-weight:bold">ports&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>-&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">containerPort: 80&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#204a87;font-weight:bold">envFrom&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#8f5902;font-style:italic">*#&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">The ConfigMap we want to use*&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#204a87;font-weight:bold">- configMapRef&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">            &lt;/span>&lt;span style="color:#000">name: demo-config&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#8f5902;font-style:italic">*#&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">Extra-curricular: We can make the hash of our ConfigMap available at a*&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#8f5902;font-style:italic">*#&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">(e.g.) debug Endpoint via a fieldRef*&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>&lt;span style="color:#204a87;font-weight:bold">env&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">        &lt;/span>-&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">name: CONFIG_HASH&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">          &lt;/span>&lt;span style="color:#8f5902;font-style:italic">*#value:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;4431f6d28fdf60c8140d28c42cde331a76269ac7a0e6af01d0de0fa8392c1145&amp;#34;&lt;/span>*&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">          &lt;/span>&lt;span style="color:#204a87;font-weight:bold">valueFrom&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">            &lt;/span>&lt;span style="color:#204a87;font-weight:bold">fieldRef&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">              &lt;/span>&lt;span style="color:#000">fieldPath: spec.template.metadata.annotations.configHash&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>提示以下错误:
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/oreyv8/1627795856579-787ee326-6723-4a6b-96ac-9983b2c819b2.png" alt="image.png">
会提示 Unsupported value:spec.template.metadata.annotations.configHash。
目的很简单: container 中的环境变量中引用 configHash 变量, 这个值是当 ConfigMap 变更时比对两个不同的 sha 值以此达到重启 pod 的目的, 但 fieldPath 显然不支持 spec.template.metadata.annotations.configHash。
从报错提示来看, 支持列表有 metadata.name, metadata.namespace, metadata.uid, spec.nodeName,spec.serviceAccountName, status.hostIp, status.PodIP, status.PodIPs。
这些值用于容器中需要以下信息时可以不从 K8s 的 apiserver 中获取而是可以很方便地从这些变量直接获得。
参考:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern">https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="参考文章">&lt;strong>参考文章:&lt;/strong>&lt;a class="td-heading-self-link" href="#%e5%8f%82%e8%80%83%e6%96%87%e7%ab%a0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state">https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/19317">https://github.com/kubernetes/kubernetes/issues/19317&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.xuyasong.com/?p=1725">http://www.xuyasong.com/?p=1725&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/">https://kubernetes.io/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://fuckcloudnative.io/">https://fuckcloudnative.io/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.cnblogs.com/breezey/p/8810039.html">https://www.cnblogs.com/breezey/p/8810039.html&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ieevee.com/tech/2018/04/25/downwardapi.html">https://ieevee.com/tech/2018/04/25/downwardapi.html&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern">https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#Deploymentspec-v1-apps">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#Deploymentspec-v1-apps&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/62167/files">https://github.com/kubernetes/kubernetes/pull/62167/files&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md">https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: 当master丢失一个节点后，如何恢复</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E5%BD%93master%E4%B8%A2%E5%A4%B1%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E5%BD%93master%E4%B8%A2%E5%A4%B1%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/</guid><description>
&lt;h1 id="故障现象">故障现象&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e7%8e%b0%e8%b1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>在日常维护中，如果三台 master 节点的其中一个节点故障，并不可恢复，我们如何重新建建立一个 master 节点并重新加入进去呢？&lt;/p>
&lt;p>假设曾经有三个节点&lt;/p>
&lt;ol>
&lt;li>master-1.tj-test&lt;/li>
&lt;li>master-2.tj-test&lt;/li>
&lt;li>master-3.tj-test&lt;/li>
&lt;/ol>
&lt;p>其中一个节点丢失后，想要新建一个节点并重新加入集群，但是失败了&lt;/p>
&lt;h1 id="故障排查">故障排查&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e6%8e%92%e6%9f%a5" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>当 master-3 挂掉并不可恢复时，首先需要通过 kubectl delete node master-3.tj-test 命令来删除该节点。然后使用一台新的设备初始化环境，并通过 kubeadm join 命令来加入集群，但是这时候，加入集群是失败的。&lt;/p>
&lt;p>因为虽然使用命令删除了 master-3 节点，但是 etcd 集群的 master-3 这个 member 还存在&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># etcdv3 member list&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>13b7460f0eebe6ea, started, master-1.tj-test, https://172.38.40.212:2380, https://172.38.40.212:2379
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>fdddf32d7b4d4498, started, master-3.tj-test, https://172.38.40.214:2380, https://172.38.40.214:2379
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>fed9f57af62ba6a0, started, master-2.tj-test, https://172.38.40.213:2380, https://172.38.40.213:2379
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="故障处理">故障处理&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e5%a4%84%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>这时候需要通过 etcdctl 命令 &lt;code>etcdv3 member remove fdddf32d7b4d4498&lt;/code> 将该 member 移除，再重新让 master-3 加入集群，就可以了。&lt;/p></description></item><item><title>Docs: 故障处理案例</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/</guid><description>
&lt;h1 id="案例列表">案例列表&lt;a class="td-heading-self-link" href="#%e6%a1%88%e4%be%8b%e5%88%97%e8%a1%a8" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/m4x_o0WC26oivNAPwGflVg">公众号-云原生实验室，JVM 内存与 K8s 容器内存不一致引发的 OOMKilled 总结&lt;/a>&lt;/p>
&lt;h1 id="nfs-相关">nfs 相关&lt;a class="td-heading-self-link" href="#nfs-%e7%9b%b8%e5%85%b3" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>K8S 中与 NFS 相关的故障通常为 Node 没有安装 nfs 客户端。还有不太常见的版本问题（在 storageclass 中添加 mountOptions 字段指定 nfs 版本即可）。&lt;/p>
&lt;p>&lt;a href="https://zhangguanzhang.github.io/2023/08/18/kubernetes-nfs-waiting-condition/">张馆长，k8s 使用 nfs 下 pod 无法创建的解决思路&lt;/a>&lt;/p>
&lt;h1 id="kube-proxy-无法绑定-nodeport-端口">kube-proxy 无法绑定 NodePort 端口&lt;a class="td-heading-self-link" href="#kube-proxy-%e6%97%a0%e6%b3%95%e7%bb%91%e5%ae%9a-nodeport-%e7%ab%af%e5%8f%a3" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="故障现象">故障现象&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e7%8e%b0%e8%b1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>其他有相同现象的人：
&lt;ul>
&lt;li>&lt;a href="https://zhangguanzhang.github.io/2019/07/08/nodeport-err/">馆长&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ieevee.com/tech/2019/07/20/svc-nodeport.html">ieevee&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>kube-proxy 日志报错：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>root@desistdaydream:~# kubectl logs -n kube-system kube-proxy-4thfl &lt;span style="color:#000;font-weight:bold">|&lt;/span> more
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>E0507 06:05:09.433545 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> proxier.go:1445&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> can&lt;span style="color:#4e9a06">&amp;#39;t open &amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34; (:33306/tcp), skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">E0507 06:05:09.602044 1 proxier.go:1445] can&amp;#39;&lt;/span>t open &lt;span style="color:#4e9a06">&amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>:33306/tcp&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>E0507 06:05:39.333119 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> proxier.go:1445&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> can&lt;span style="color:#4e9a06">&amp;#39;t open &amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34; (:33306/tcp), skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">E0507 06:06:09.494578 1 proxier.go:1445] can&amp;#39;&lt;/span>t open &lt;span style="color:#4e9a06">&amp;#34;nodePort for mysql/mysql-bj-net:mysql&amp;#34;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>:33306/tcp&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, skipping this nodePort: listen tcp4 :33306: bind: address already in use
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="故障排查">故障排查&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e6%8e%92%e6%9f%a5" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>这个 kube-proxy 在 master-2 上，去 master-2 上看，发现根本没有人占用 33306。反倒是 kube-apiserver 作为客户端，使用 33306 端口，与 etcd 的 2379 进行互联&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>root@master-2 ~&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span>&lt;span style="color:#8f5902;font-style:italic"># ss -ntap | grep 33306&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ESTAB &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 127.0.0.1:33306 127.0.0.1:2379 users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;kube-apiserver&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>2746,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>77&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ESTAB &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 127.0.0.1:2379 127.0.0.1:33306 users:&lt;span style="color:#ce5c00;font-weight:bold">((&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;etcd&amp;#34;&lt;/span>,pid&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>2768,fd&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>100&lt;span style="color:#ce5c00;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="故障处理">故障处理&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e5%a4%84%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>将 kube-apiserver 的 manifest 从 /etc/kubernetes/manifests 目录中移出。待 kube-proxy 创建完端口后，再移回 manifest&lt;/p>
&lt;h2 id="故障分析">故障分析&lt;a class="td-heading-self-link" href="#%e6%95%85%e9%9a%9c%e5%88%86%e6%9e%90" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>kubernetes 这样设计，这不是给 Local Process 挖坑吗，内核在随机选择本地端口的时候，很可能会命中 kubernetes svc 的端口号呀。
其实 Kubernetes 已经尽力了。
当创建 nodePort 类型的 svc 时，kube-proxy 除了会下发 iptables 规则，还会创建一个监听 Socket，该 Socket 监听的端口号就是 nodePort，因此：&lt;/p>
&lt;ul>
&lt;li>当内核指定 bind 该端口号时，会返回端口已使用&lt;/li>
&lt;li>当内核随机选择本地端口号时，不会命中该端口&lt;/li>
&lt;/ul>
&lt;p>因此，正常情况下，Local Process 不会进坑。
但是，如果 Local Process 先启动，kube-proxy 后启动，则会出现上文描述的情况。
此时，kube-proxy 仍然会下发 iptables 规则，并且尝试 bind 该端口号，但会不成功，因为已经被 Local Process 占用了。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> E0729 01:48:43.034098 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> proxier.go:1072&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> can&lt;span style="color:#4e9a06">&amp;#39;t open &amp;#34;nodePort for default/nginx:&amp;#34; (:31325/tcp), skipping this nodePort: listen tcp :31325: bind: address already in use
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> E0729 01:49:13.064492 1 proxier.go:1072] can&amp;#39;&lt;/span>t open &lt;span style="color:#4e9a06">&amp;#34;nodePort for default/nginx:&amp;#34;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>:31325/tcp&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, skipping this nodePort: listen tcp :31325: bind: address already in use
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> E0729 01:49:43.094846 &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> proxier.go:1072&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> can&lt;span style="color:#a40000">&amp;#39;&lt;/span>t open &lt;span style="color:#4e9a06">&amp;#34;nodePort for default/nginx:&amp;#34;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>:31325/tcp&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>, skipping this nodePort: listen tcp :31325: bind: address already in use
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>但由于 iptables 已经下发，因此 Local Process 只能空守着端口号流眼泪，眼睁睁的看着报文被劫走。&lt;/p>
&lt;h1 id="深信服-与-flannel-vxlan-8472-端口冲突">深信服 与 Flannel VxLan 8472 端口冲突&lt;a class="td-heading-self-link" href="#%e6%b7%b1%e4%bf%a1%e6%9c%8d-%e4%b8%8e-flannel-vxlan-8472-%e7%ab%af%e5%8f%a3%e5%86%b2%e7%aa%81" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ufbiwm/1625537665271-2ae56e76-3de0-4598-ac39-d3dd0a165198.png" alt="image.png">&lt;/p>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/ufbiwm/1625537959161-bafbdab6-d95c-46b2-b4b7-1440ce01be81.png" alt="image.png">&lt;/p>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/article/1746944">https://cloud.tencent.com/developer/article/1746944&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://segmentfault.com/a/1190000037782599">https://segmentfault.com/a/1190000037782599&lt;/a>&lt;/p></description></item><item><title>Docs: 记一次 K8S 内部服务调用域名解析超时排坑经历</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E8%AE%B0%E4%B8%80%E6%AC%A1-K8S-%E5%86%85%E9%83%A8%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E6%8E%92%E5%9D%91%E7%BB%8F%E5%8E%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Kubernetes-%E7%AE%A1%E7%90%86/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B/%E8%AE%B0%E4%B8%80%E6%AC%A1-K8S-%E5%86%85%E9%83%A8%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E6%8E%92%E5%9D%91%E7%BB%8F%E5%8E%86/</guid><description>
&lt;p>原文连接：&lt;a href="https://juejin.im/post/6844904178582552590">https://juejin.im/post/6844904178582552590&lt;/a>&lt;/p>
&lt;p>记一次 K8S 内部服务调用域名解析超时排坑经历&lt;/p>
&lt;h2 id="前言">前言&lt;a class="td-heading-self-link" href="#%e5%89%8d%e8%a8%80" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>近期线上 k8s 时不时就会出现一些内部服务间的调用超时问题，通过日志可以得知超时的原因都是出现在&lt;code>域名解析&lt;/code>上，并且都是 k8s 内部的域名解析超时，于是直接先将内部域名替换成 k8s service 的 IP，观察一段时间发现没有超时的情况发生了，但是由于使用 service IP 不是长久之计，所以还要去找解决办法。&lt;/p>
&lt;h2 id="复现">复现&lt;a class="td-heading-self-link" href="#%e5%a4%8d%e7%8e%b0" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>一开始运维同事在调用方 pod 中使用&lt;code>ab&lt;/code>工具对目标服务进行了多次压测，并没有发现有超时的请求，我介入之后分析&lt;code>ab&lt;/code>这类 http 压测工具应该都会有 dns 缓存，而我们主要是要测试 dns 服务的性能，于是直接动手撸了一个压测工具只做域名解析，代码如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">package&lt;/span> &lt;span style="color:#000">main&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;context&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;flag&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;fmt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;net&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;sync/atomic&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#4e9a06">&amp;#34;time&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">host&lt;/span> &lt;span style="color:#204a87;font-weight:bold">string&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">connections&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">duration&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">limit&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">timeoutCount&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">func&lt;/span> &lt;span style="color:#000">main&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic">// os.Args = append(os.Args, &amp;#34;-host&amp;#34;, &amp;#34;www.baidu.com&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;200&amp;#34;, &amp;#34;-d&amp;#34;, &amp;#34;30&amp;#34;, &amp;#34;-l&amp;#34;, &amp;#34;5000&amp;#34;)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">&lt;/span> &lt;span style="color:#000">flag&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">StringVar&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">host&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;host&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;Resolve host&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">flag&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">IntVar&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">connections&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;c&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">100&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;Connections&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">flag&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Int64Var&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">duration&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;d&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;Duration(s)&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">flag&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Int64Var&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">limit&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;l&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;Limit(ms)&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">flag&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Parse&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">count&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000">errCount&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">pool&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#204a87">make&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#204a87;font-weight:bold">chan&lt;/span> &lt;span style="color:#204a87;font-weight:bold">interface&lt;/span>&lt;span style="color:#000;font-weight:bold">{},&lt;/span> &lt;span style="color:#000">connections&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">exit&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#204a87">make&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#204a87;font-weight:bold">chan&lt;/span> &lt;span style="color:#204a87;font-weight:bold">bool&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">var&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">min&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">max&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">sum&lt;/span> &lt;span style="color:#204a87;font-weight:bold">int64&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">go&lt;/span> &lt;span style="color:#204a87;font-weight:bold">func&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Sleep&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Second&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span> &lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Duration&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">duration&lt;/span>&lt;span style="color:#000;font-weight:bold">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">exit&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;-&lt;/span> &lt;span style="color:#204a87;font-weight:bold">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">endD&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">select&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">case&lt;/span> &lt;span style="color:#000">pool&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;-&lt;/span> &lt;span style="color:#204a87;font-weight:bold">nil&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">go&lt;/span> &lt;span style="color:#204a87;font-weight:bold">func&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">defer&lt;/span> &lt;span style="color:#204a87;font-weight:bold">func&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;-&lt;/span>&lt;span style="color:#000">pool&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">resolver&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">net&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Resolver&lt;/span>&lt;span style="color:#000;font-weight:bold">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">now&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Now&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">_&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">err&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000">resolver&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">LookupIPAddr&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">context&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Background&lt;/span>&lt;span style="color:#000;font-weight:bold">(),&lt;/span> &lt;span style="color:#000">host&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">use&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Since&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">now&lt;/span>&lt;span style="color:#000;font-weight:bold">).&lt;/span>&lt;span style="color:#000">Nanoseconds&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">/&lt;/span> &lt;span style="color:#204a87">int64&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">time&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Millisecond&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">min&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">==&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">||&lt;/span> &lt;span style="color:#000">use&lt;/span> &lt;span style="color:#000;font-weight:bold">&amp;lt;&lt;/span> &lt;span style="color:#000">min&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">min&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#000">use&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">use&lt;/span> &lt;span style="color:#000;font-weight:bold">&amp;gt;&lt;/span> &lt;span style="color:#000">max&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">max&lt;/span> &lt;span style="color:#000;font-weight:bold">=&lt;/span> &lt;span style="color:#000">use&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">sum&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+=&lt;/span> &lt;span style="color:#000">use&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">limit&lt;/span> &lt;span style="color:#000;font-weight:bold">&amp;gt;&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#000">use&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;gt;=&lt;/span> &lt;span style="color:#000">limit&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">timeoutCount&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">++&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">atomic&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">AddInt64&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">count&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">if&lt;/span> &lt;span style="color:#000">err&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">!=&lt;/span> &lt;span style="color:#204a87;font-weight:bold">nil&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">fmt&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Println&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">err&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Error&lt;/span>&lt;span style="color:#000;font-weight:bold">())&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">atomic&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">AddInt64&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">errCount&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">case&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;-&lt;/span>&lt;span style="color:#000">exit&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">break&lt;/span> &lt;span style="color:#000">endD&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">fmt&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Printf&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;request count：%d\nerror count：%d\n&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">count&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">errCount&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">fmt&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Printf&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;request time：min(%dms) max(%dms) avg(%dms) timeout(%dn)\n&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">min&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">max&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">sum&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">/&lt;/span>&lt;span style="color:#000">count&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">timeoutCount&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>编译好二进制程序直接丢到对应的 pod 容器中进行压测：&lt;/p>
&lt;pre>&lt;code># 200个并发,持续30秒
./dns -host {service}.{namespace} -c 200 -d 30
&lt;/code>&lt;/pre>
&lt;p>这次可以发现最大耗时有&lt;code>5s&lt;/code>多，多次测试结果都是类似：
&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vhcqem/1616118022002-f6487df3-068d-4151-9ba9-3681c697bda2.jpeg" alt="">
而我们内部服务间 HTTP 调用的超时一般都是设置在&lt;code>3s&lt;/code>左右，以此推断出与线上的超时情况应该是同一种情况，在并发高的情况下会出现部分域名解析超时而导致 HTTP 请求失败。&lt;/p>
&lt;p>原因&lt;/p>
&lt;p>起初一直以为是&lt;code>coredns&lt;/code>的问题，于是找运维升级了下&lt;code>coredns&lt;/code>版本再进行压测，发现问题还是存在，说明不是版本的问题，难道是&lt;code>coredns&lt;/code>本身的性能就差导致的？想想也不太可能啊，才 200 的并发就顶不住了那性能也未免太弱了吧，结合之前的压测数据，平均响应都挺正常的(82ms)，但是就有个别请求会延迟，而且都是 5 秒左右，所以就又带着&lt;code>k8s dns 5s&lt;/code>的关键字去 google 搜了一下，这不搜不知道一搜吓一跳啊，原来是 k8s 里的一个大坑啊(其实和 k8s 没有太大的关系，只是 k8s 层面没有提供解决方案)。&lt;/p>
&lt;h3 id="5s-超时原因">5s 超时原因&lt;a class="td-heading-self-link" href="#5s-%e8%b6%85%e6%97%b6%e5%8e%9f%e5%9b%a0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>linux 中 &lt;code>glibc&lt;/code> 的 resolver 的缺省超时时间是 5s，而导致超时的原因是内核 &lt;code>conntrack&lt;/code> 模块的 bug。&lt;/p>
&lt;blockquote>
&lt;p>Weave works 的工程师 Martynas Pumputis 对这个问题做了很详细的分析：&lt;/p>
&lt;/blockquote>
&lt;p>这里再引用下 imroc.io/posts/kuber…文章中的解释：&lt;/p>
&lt;blockquote>
&lt;p>DNS client (glibc 或 musl libc) 会并发请求 A 和 AAAA 记录，跟 DNS Server 通信自然会先 connect (建立 fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议， connect 时并不会发包，也就不会创建 conntrack 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，send 时各自发的包它们源 Port 相同(因为用的同一个 socket 发送)，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们分别创建 conntrack 表项，而集群内请求 kube-dns 或 coredns 都是访问的 CLUSTER-IP，报文最终会被 DNAT 成一个 endpoint 的 POD IP，当两个包恰好又被 DNAT 成同一个 POD IP 时，它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 dns 的 pod 副本只有一个实例的情况就很容易发生(始终被 DNAT 成同一个 POD IP)，现象就是 dns 请求超时，client 默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 dns 请求有 5s 的延时。&lt;/p>
&lt;/blockquote>
&lt;h2 id="解决方案">解决方案&lt;a class="td-heading-self-link" href="#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="方案一使用-tcp-协议发送-dns-请求">方案（一）：使用 TCP 协议发送 DNS 请求&lt;a class="td-heading-self-link" href="#%e6%96%b9%e6%a1%88%e4%b8%80%e4%bd%bf%e7%94%a8-tcp-%e5%8d%8f%e8%ae%ae%e5%8f%91%e9%80%81-dns-%e8%af%b7%e6%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>通过&lt;code>resolv.conf&lt;/code>的&lt;code>use-vc&lt;/code>选项来开启 TCP 协议&lt;/p>
&lt;p>测试&lt;/p>
&lt;ol>
&lt;li>
&lt;p>修改&lt;code>/etc/resolv.conf&lt;/code>文件，在最后加入一行文本：options use-vc&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进行压测：# 200 个并发,持续 30 秒,记录超过 5s 的请求个数./dns -host {service}.{namespace} -c 200 -d 30 -l 5000 复制代码结果如下：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vhcqem/1616118021995-d7ec02ba-a66f-4045-a6e1-9a19ac683c5d.png" alt="">&lt;/p>
&lt;p>结论&lt;/p>
&lt;p>确实没有出现&lt;code>5s&lt;/code>的超时问题了，但是部分请求耗时还是比较高，在&lt;code>4s&lt;/code>左右，而且平均耗时比 UPD 协议的还高，效果并不好。&lt;/p>
&lt;h3 id="方案二避免相同五元组-dns-请求的并发">方案（二）：避免相同五元组 DNS 请求的并发&lt;a class="td-heading-self-link" href="#%e6%96%b9%e6%a1%88%e4%ba%8c%e9%81%bf%e5%85%8d%e7%9b%b8%e5%90%8c%e4%ba%94%e5%85%83%e7%bb%84-dns-%e8%af%b7%e6%b1%82%e7%9a%84%e5%b9%b6%e5%8f%91" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>通过 &lt;code>resolv.conf&lt;/code> 的 &lt;code>single-request-reopen&lt;/code> 和 &lt;code>single-request&lt;/code> 选项来避免：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>single-request-reopen (glibc&amp;gt;=2.9) 发送 A 类型请求和 AAAA 类型请求使用不同的源端口。这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>single-request (glibc&amp;gt;=2.10) 避免并发，改为串行发送 A 类型和 AAAA 类型请求，没有了并发，从而也避免了冲突。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>测试 single-request-reopen&lt;/p>
&lt;ul>
&lt;li>
&lt;p>修改 &lt;code>/etc/resolv.conf&lt;/code> 文件，在最后加入一行文本：options single-request-reopen&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进行压测：# 200 个并发,持续 30 秒,记录超过 5s 的请求个数./dns -host {service}.{namespace} -c 200 -d 30 -l 5000 复制代码结果如下：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://notes-learning.oss-cn-beijing.aliyuncs.com/vhcqem/1616118022005-4c81f4a7-7082-41e7-9132-bc9a994cbe04.jpeg" alt="">&lt;/p>
&lt;p>测试 single-request&lt;/p>
&lt;ol>
&lt;li>
&lt;p>修改&lt;code>/etc/resolv.conf&lt;/code>文件，在最后加入一行文本：options single-request 复制代码&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进行压测：# 200 个并发,持续 30 秒,记录超过 5s 的请求个数./dns -host {service}.{namespace} -c 200 -d 30 -l 5000 复制代码结果如下：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>结论&lt;/p>
&lt;p>通过压测结果可以看到&lt;code>single-request-reopen&lt;/code>和&lt;code>single-request&lt;/code>选项确实可以显著的降低域名解析耗时。&lt;/p>
&lt;h3 id="关于方案一和方案二的实施步骤和缺点">关于方案（一）和方案（二）的实施步骤和缺点&lt;a class="td-heading-self-link" href="#%e5%85%b3%e4%ba%8e%e6%96%b9%e6%a1%88%e4%b8%80%e5%92%8c%e6%96%b9%e6%a1%88%e4%ba%8c%e7%9a%84%e5%ae%9e%e6%96%bd%e6%ad%a5%e9%aa%a4%e5%92%8c%e7%bc%ba%e7%82%b9" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>实施步骤&lt;/p>
&lt;p>其实就是要给容器的&lt;code>/etc/resolv.conf&lt;/code>文件添加选项，目前有两个方案比较合适：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>通过修改 pod 的 postStart hook 来设置&lt;/p>
&lt;p>lifecycle:
postStart:
exec:
command:
- /bin/sh
- -c
- &amp;ldquo;/bin/echo &amp;lsquo;options single-request-reopen&amp;rsquo; &amp;raquo; /etc/resolv.conf&amp;rdquo;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过修改 pod 的 template.spec.dnsConfig 来设置&lt;/p>
&lt;p>template:
spec:
dnsConfig:
options:
- name: single-request-reopen&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>缺点&lt;/p>
&lt;p>不支持&lt;code>alpine&lt;/code>基础镜像的容器，因为&lt;code>apline&lt;/code>底层使用的&lt;code>musl libc&lt;/code>库并不支持这些 resolv.conf 选项，所以如果使用&lt;code>alpine&lt;/code>基础镜像构建的应用，还是无法规避超时的问题。&lt;/p>
&lt;h3 id="方案三本地-dns-缓存">方案（三）：本地 DNS 缓存&lt;a class="td-heading-self-link" href="#%e6%96%b9%e6%a1%88%e4%b8%89%e6%9c%ac%e5%9c%b0-dns-%e7%bc%93%e5%ad%98" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>其实 k8s 官方也意识到了这个问题比较常见，给出了 coredns 以 cache 模式作为 daemonset 部署的解决方案: github.com/kubernetes/…&lt;/p>
&lt;p>大概原理就是：&lt;/p>
&lt;blockquote>
&lt;p>本地 DNS 缓存以 DaemonSet 方式在每个节点部署一个使用 hostNetwork 的 Pod，创建一个网卡绑上本地 DNS 的 IP，本机的 Pod 的 DNS 请求路由到本地 DNS，然后取缓存或者继续使用 TCP 请求上游集群 DNS 解析 (由于使用 TCP，同一个 socket 只会做一遍三次握手，不存在并发创建 conntrack 表项，也就不会有 conntrack 冲突)&lt;/p>
&lt;/blockquote>
&lt;p>部署&lt;/p>
&lt;ol>
&lt;li>获取当前&lt;code>kube-dns service&lt;/code>的 clusterIP&lt;/li>
&lt;/ol>
&lt;h1 id="kubectl--n-kube-system-get-svc-kube-dns--o-jsonpathspecclusterip">kubectl -n kube-system get svc kube-dns -o jsonpath=&amp;quot;{.spec.clusterIP}&amp;quot;&lt;a class="td-heading-self-link" href="#kubectl--n-kube-system-get-svc-kube-dns--o-jsonpathspecclusterip" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;pre>&lt;code>10.96.0.10
复制代码
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>
&lt;p>下载官方提供的 yaml 模板进行关键字替换&lt;/p>
&lt;p>wget -O nodelocaldns.yaml &amp;ldquo;&lt;a href="https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml%22">https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml&amp;quot;&lt;/a> &amp;amp;&amp;amp; &lt;br>
sed -i &amp;rsquo;s/&lt;strong>PILLAR__DNS__SERVER&lt;/strong>/10.96.0.10/g&amp;rsquo; nodelocaldns.yaml &amp;amp;&amp;amp; &lt;br>
sed -i &amp;rsquo;s/&lt;strong>PILLAR__LOCAL__DNS&lt;/strong>/169.254.20.10/g&amp;rsquo; nodelocaldns.yaml &amp;amp;&amp;amp; &lt;br>
sed -i &amp;rsquo;s/&lt;strong>PILLAR__DNS__DOMAIN&lt;/strong>/cluster.local/g&amp;rsquo; nodelocaldns.yaml &amp;amp;&amp;amp; &lt;br>
sed -i &amp;rsquo;s/&lt;strong>PILLAR__CLUSTER__DNS&lt;/strong>/10.96.0.10/g&amp;rsquo; nodelocaldns.yaml &amp;amp;&amp;amp; &lt;br>
sed -i &amp;rsquo;s/&lt;strong>PILLAR__UPSTREAM__SERVERS&lt;/strong>//etc/resolv.conf/g&amp;rsquo; nodelocaldns.yaml&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最终 yaml 文件如下：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="copyright-2018-the-kubernetes-authors">Copyright 2018 The Kubernetes Authors&lt;a class="td-heading-self-link" href="#copyright-2018-the-kubernetes-authors" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="heading">&lt;a class="td-heading-self-link" href="#heading" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="licensed-under-the-apache-license-version-20-the-license">Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;)&lt;a class="td-heading-self-link" href="#licensed-under-the-apache-license-version-20-the-license" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="you-may-not-use-this-file-except-in-compliance-with-the-license">you may not use this file except in compliance with the License&lt;a class="td-heading-self-link" href="#you-may-not-use-this-file-except-in-compliance-with-the-license" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="you-may-obtain-a-copy-of-the-license-at">You may obtain a copy of the License at&lt;a class="td-heading-self-link" href="#you-may-obtain-a-copy-of-the-license-at" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="heading-1">&lt;a class="td-heading-self-link" href="#heading-1" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="httpwwwapacheorglicenseslicense-20">&lt;a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0&lt;/a>&lt;a class="td-heading-self-link" href="#httpwwwapacheorglicenseslicense-20" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="heading-2">&lt;a class="td-heading-self-link" href="#heading-2" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="unless-required-by-applicable-law-or-agreed-to-in-writing-software">Unless required by applicable law or agreed to in writing, software&lt;a class="td-heading-self-link" href="#unless-required-by-applicable-law-or-agreed-to-in-writing-software" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="distributed-under-the-license-is-distributed-on-an-as-is-basis">distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS&lt;a class="td-heading-self-link" href="#distributed-under-the-license-is-distributed-on-an-as-is-basis" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="without-warranties-or-conditions-of-any-kind-either-express-or-implied">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied&lt;a class="td-heading-self-link" href="#without-warranties-or-conditions-of-any-kind-either-express-or-implied" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="see-the-license-for-the-specific-language-governing-permissions-and">See the License for the specific language governing permissions and&lt;a class="td-heading-self-link" href="#see-the-license-for-the-specific-language-governing-permissions-and" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="limitations-under-the-license">limitations under the License&lt;a class="td-heading-self-link" href="#limitations-under-the-license" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h1 id="heading-3">&lt;a class="td-heading-self-link" href="#heading-3" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>apiVersion: v1
kind: ServiceAccount
metadata:
name: node-local-dns
namespace: kube-system
labels:
kubernetes.io/cluster-service: &amp;ldquo;true&amp;rdquo;
addonmanager.kubernetes.io/mode: Reconcile
&amp;mdash;&lt;/p>
&lt;p>apiVersion: v1
kind: Service
metadata:
name: kube-dns-upstream
namespace: kube-system
labels:
k8s-app: kube-dns
kubernetes.io/cluster-service: &amp;ldquo;true&amp;rdquo;
addonmanager.kubernetes.io/mode: Reconcile
kubernetes.io/name: &amp;ldquo;KubeDNSUpstream&amp;rdquo;
spec:
ports:
- name: dns
port: 53
protocol: UDP
targetPort: 53
- name: dns-tcp
port: 53
protocol: TCP
targetPort: 53
selector:
k8s-app: kube-dns
&amp;mdash;&lt;/p>
&lt;p>apiVersion: v1
kind: ConfigMap
metadata:
name: node-local-dns
namespace: kube-system
labels:
addonmanager.kubernetes.io/mode: Reconcile
data:
Corefile: |
cluster.local:53 {
errors
cache {
success 9984 30
denial 9984 5
}
reload
loop
bind 169.254.20.10 10.96.0.10
forward . 10.96.0.10 {
force_tcp
}
prometheus :9253
health 169.254.20.10:8080
}
in-addr.arpa:53 {
errors
cache 30
reload
loop
bind 169.254.20.10 10.96.0.10
forward . 10.96.0.10 {
force_tcp
}
prometheus :9253
}
ip6.arpa:53 {
errors
cache 30
reload
loop
bind 169.254.20.10 10.96.0.10
forward . 10.96.0.10 {
force_tcp
}
prometheus :9253
}
.:53 {
errors
cache 30
reload
loop
bind 169.254.20.10 10.96.0.10
forward . /etc/resolv.conf {
force_tcp
}
prometheus :9253
}
&amp;mdash;&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: DaemonSet
metadata:
name: node-local-dns
namespace: kube-system
labels:
k8s-app: node-local-dns
kubernetes.io/cluster-service: &amp;quot;true&amp;quot;
addonmanager.kubernetes.io/mode: Reconcile
spec:
updateStrategy:
rollingUpdate:
maxUnavailable: 10%
selector:
matchLabels:
k8s-app: node-local-dns
template:
metadata:
labels:
k8s-app: node-local-dns
spec:
priorityClassName: system-node-critical
serviceAccountName: node-local-dns
hostNetwork: true
dnsPolicy: Default # Don't use cluster DNS.
tolerations:
- key: &amp;quot;CriticalAddonsOnly&amp;quot;
operator: &amp;quot;Exists&amp;quot;
containers:
- name: node-cache
image: k8s.gcr.io/k8s-dns-node-cache:1.15.7
resources:
requests:
cpu: 25m
memory: 5Mi
args:
[
&amp;quot;-localip&amp;quot;,
&amp;quot;169.254.20.10,10.96.0.10&amp;quot;,
&amp;quot;-conf&amp;quot;,
&amp;quot;/etc/Corefile&amp;quot;,
&amp;quot;-upstreamsvc&amp;quot;,
&amp;quot;kube-dns-upstream&amp;quot;,
]
securityContext:
privileged: true
ports:
- containerPort: 53
name: dns
protocol: UDP
- containerPort: 53
name: dns-tcp
protocol: TCP
- containerPort: 9253
name: metrics
protocol: TCP
livenessProbe:
httpGet:
host: 169.254.20.10
path: /health
port: 8080
initialDelaySeconds: 60
timeoutSeconds: 5
volumeMounts:
- mountPath: /run/xtables.lock
name: xtables-lock
readOnly: false
- name: config-volume
mountPath: /etc/coredns
- name: kube-dns-config
mountPath: /etc/kube-dns
volumes:
- name: xtables-lock
hostPath:
path: /run/xtables.lock
type: FileOrCreate
- name: kube-dns-config
configMap:
name: kube-dns
optional: true
- name: config-volume
configMap:
name: node-local-dns
items:
- key: Corefile
path: Corefile.base
&lt;/code>&lt;/pre>
&lt;p>通过 yaml 可以看到几个细节：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>部署类型是使用的&lt;code>DaemonSet&lt;/code>，即在每个 k8s node 节点上运行一个 dns 服务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hostNetwork&lt;/code>属性为&lt;code>true&lt;/code>，即直接使用 node 物理机的网卡进行端口绑定，这样在此 node 节点中的 pod 可以直接访问 dns 服务，不通过 service 进行转发，也就不会有 DNAT&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>dnsPolicy&lt;/code>属性为&lt;code>Default&lt;/code>，不使用 cluster DNS，在解析外网域名时直接使用本地的 DNS 设置&lt;/p>
&lt;/li>
&lt;li>
&lt;p>绑定在 node 节点&lt;code>169.254.20.10&lt;/code>和&lt;code>10.96.0.10&lt;/code>IP 上，这样节点下面的 pod 只需要将 dns 设置为&lt;code>169.254.20.10&lt;/code>即可直接访问宿主机上的 dns 服务。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>测试&lt;/p>
&lt;ol>
&lt;li>
&lt;p>修改&lt;code>/etc/resolv.conf&lt;/code>文件中的 nameserver：nameserver 169.254.20.10 复制代码&lt;/p>
&lt;/li>
&lt;li>
&lt;p>进行压测：# 200 个并发,持续 30 秒,记录超过 5s 的请求个数./dns -host {service}.{namespace} -c 200 -d 30 -l 5000 复制代码结果如下：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>结论&lt;/p>
&lt;p>通过压测发现并没有解决超时的问题，按理说没有&lt;code>conntrack&lt;/code>冲突应该表现出的情况与方案(二)类似才对，也可能是我使用的姿势不对，不过虽然这个问题还存在，但是通过&lt;code>DaemonSet&lt;/code>将 dns 请求压力分散到各个 node 节点，也可以有效的缓解域名解析超时问题。&lt;/p>
&lt;p>实施&lt;/p>
&lt;ul>
&lt;li>
&lt;p>方案（一）：通过修改 pod 的 template.spec.dnsConfig 来设置，并将&lt;code>dnsPolicy&lt;/code>设置为&lt;code>None&lt;/code>&lt;/p>
&lt;p>template:
spec:
dnsConfig:
nameservers:
- 169.254.20.10
searches:
- public.svc.cluster.local
- svc.cluster.local
- cluster.local
options:
- name: ndots
value: &amp;ldquo;5&amp;rdquo;
dnsPolicy: None&lt;/p>
&lt;/li>
&lt;li>
&lt;p>方案（二）：修改默认的&lt;code>cluster-dns&lt;/code>，在 node 节点上将&lt;code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf&lt;/code>文件中的&lt;code>--cluster-dns&lt;/code>参数值修改为&lt;code>169.254.20.10&lt;/code>，然后重启&lt;code>kubelet&lt;/code>&lt;/p>
&lt;p>systemctl restart kubelet&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="最终解决方案">最终解决方案&lt;a class="td-heading-self-link" href="#%e6%9c%80%e7%bb%88%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>最后还是决定使用&lt;code>方案(二)+方案(三)&lt;/code>配合使用，来最大程度的优化此问题，并且将线上所有的基础镜像都替换为非&lt;code>apline&lt;/code>的镜像版本，至此问题基本解决，也希望 K8S 官方能早日将此功能直接集成进去。&lt;/p></description></item></channel></rss>