<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scheduling on 断念梦的站点</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/</link><description>Recent content in Scheduling on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/index.xml" rel="self" type="application/rss+xml"/><item><title>Scheduling</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/Scheduling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/Scheduling/</guid><description>概述 参考：
官方文档，概念 - 调度、抢占与驱逐 Scheduling(调度) 是一个行为，用来让 Pod 匹配到 Node，以便 Node 上的 Kubelet 可以运行这些 Pod。如果没有调度系统，Kubernetes 集群就不知道 Pod 应该运行在哪里。这种调度的概念，与 Linux 中调度任务来使用 CPU 是一个意思。可以看看 Scheduler 相关文章，调度是在 IT 行业中，很多程序都很重要的概念。
与 Scheduling(调度) 伴生的，还有 Preemption(抢占) 与 Eviction(驱逐) 两个概念。顾名思义：
Preemption(抢占) 是指终止优先级较低的 Pod 的行为，以便优先级较高的 Pod 可以在节点上调度。 抢占行为通常发生在资源不足时，当一个新 Pod 需要调度，但是资源不足，那么就可能需要抢占优先级低的 Pod，这个低优先级的 Pod 将会被驱逐，以便让优先级高的 Pod 运行在节点上。 Eviction(驱逐) 是指终止节点上一个或多个 Pod 的行为。 由 抢占 与 驱逐 两个行为，还引申出了 Pod Disruption(中断) 的概念。Pod Disruption(中断) 是指节点上的 Pod 自愿或者非资源终止运行的行为。
自愿中断是由应用程序所有者或者集群管理故意启动的(比如.维护节点前手动驱逐 Pod) 非自愿中断是无意的，可能由不可避免的问题触发(比如.节点资源耗尽或意外删除)</description></item><item><title>kube-scheduler 实现调度器的程序</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/kube-scheduler-%E5%AE%9E%E7%8E%B0%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E7%A8%8B%E5%BA%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/kube-scheduler-%E5%AE%9E%E7%8E%B0%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E7%A8%8B%E5%BA%8F/</guid><description>概述 参考：
kube-scheduler 是实现 kuberntes Scheduler 的应用程序
kube-scheduler 启动后监听两个端口：
10251 端口为无需身份验证和授权即可不安全地为 HTTP 服务的端口。(1.18 版本后将要弃用) 10259 端口为需要身份验证和授权为 HTTPS 服务的端口。 kube-scheduler 高科用 与 [kube-controller-manager 高可用](/docs/10.云原生/2.3.Kubernetes%20 容器编排系统/4.Controller(控制器)/kube-controller-manager%20 实现控制器的程序.md 实现控制器的程序.md) 原理相同。
kube-scheduler 监控指标 详见：[kubernetes 监控](/docs/10.云原生/2.3.Kubernetes%20 容器编排系统/Kubernetes%20 管理/Kubernetes%20 监控/Kubernetes%20 系统组件指标.md 管理/Kubernetes 监控/Kubernetes 系统组件指标.md)
Kube-scheduler 参数详解 参考：
官方文档，参考-组件工具-kube-scheduler 默认的 manifest 示例 apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-scheduler tier: control-plane name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.</description></item><item><title>Scheduler(调度器)</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/Scheduler%E8%B0%83%E5%BA%A6%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/Scheduler%E8%B0%83%E5%BA%A6%E5%99%A8/</guid><description>概述 参考：
官方文档, 概念 - 调度与驱逐 公众号 - 云原生社区动态，6 张图带你深入了解 kube-scheduler Scheduler(调度器) 负责决定 Pod 与 Node 的匹配关系，并将 Pod 调度到匹配到的 Node 上，以便 Kubelet 可以运行这些 Pod。Scheduler 在调度时会充分考虑 Cluster 的拓扑结构，当前各个节点的负载，以及应用对高可用、性能、数据亲和性的需求。
Scheduler 通过 Kubernetes 的 watch 机制来发现集群中新创建且尚未被调度到 Node 的 Pod。Scheduler 会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上去运行。
调度器的实现 kube-scheduler 是实现 Kubernetes 调度器功能的程序。
Scheduler 调度策略 参考：
官方文档, 参考 - 调度 - 调度策略 Scheduler 调度的时候，通过以下步骤来完成调度
Predicate(预选) # 排除那些不满足 Pod 运行环境的 Node Priorities(优选) # 通过算法，剩余可运行 Pod 的 Node 进行计算后排序，选择结果最高的 Node Select(选定) # 若优选后有多个 Node 得分相同，则随机挑选，将选择的结果告诉 APIServer 用哪个 Node 部署 Pod 调度倾向性：亲合性 Affinity，反亲合性 AntiAffinity，污点 Taints，容忍度 Tolerations</description></item><item><title>让 Pod 运行在指定 Node 上</title><link>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/%E8%AE%A9-Pod-%E8%BF%90%E8%A1%8C%E5%9C%A8%E6%8C%87%E5%AE%9A-Node-%E4%B8%8A/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/docs/10.%E4%BA%91%E5%8E%9F%E7%94%9F/Kubernetes/Scheduling/%E8%AE%A9-Pod-%E8%BF%90%E8%A1%8C%E5%9C%A8%E6%8C%87%E5%AE%9A-Node-%E4%B8%8A/</guid><description>概述 参考：
官方文档, 概念 - 调度、抢占、驱逐 - 让 Pod 运行在指定节点上 官方文档, 概念 - 调度、抢占、驱逐 - 污点与容忍度 Kubernetes 内置的 标签、注释、污点 CSDN, k8s 之 pod 亲和与反亲和的 topologyKey 通常情况下 Scheduler(调度器) 将自动进行合理的分配(例如，将 Pods 分散到所有节点上，以防止单独节点上资源使用率远高于其他节点)。但是在某些情况下我们需要更多控制 Pods 落在某个指定的节点上，例如确保一个 Pod 部署在装有 SSD 的机器上，或者将两个不同服务中的 Pods 共同定位到同一可用区域。
所以我们需要 constrain(约束) Pod 运行在指定的节点。可以实现该效果的方式有以下几种：
nodeName(节点名称) # nodeSelector(节点选择器) # 根据节点的标签，选择 pod 要运行在哪个节点上 这种行为定义 Pod 必须在特定节点上运行。 Affinity(亲和) 与 Anti-Affinity(反亲和) # 根据亲和原则，让 pod 更趋向于与哪些 XXX 运行在同一个节点 这种行为定义 Pod 更倾向于在特定节点上运行。 Taint(污点) 与 Toleration(容忍度) # 根据节点上的污点，以及 pod 是否可以容忍该污点来决定 pod 是否可以运行在哪些节点上 其中 nodeSelector 和 Affinity 与 Anti-Affinity 是通过 Label Selectors(标签选择器) 来实现的。而 Taint 与 Toleration 是另一套类似于标签选择器的机制。</description></item></channel></rss>