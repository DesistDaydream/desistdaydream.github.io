<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>云原生 on 断念梦的站点</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/</link><description>Recent content in 云原生 on 断念梦的站点</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/index.xml" rel="self" type="application/rss+xml"/><item><title>ChatGPT 团队是如何使用Kubernetes的</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/ChatGPT-%E5%9B%A2%E9%98%9F%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Kubernetes%E7%9A%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/ChatGPT-%E5%9B%A2%E9%98%9F%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Kubernetes%E7%9A%84/</guid><description>原文链接
https://openai.com/research/scaling-kubernetes-to-7500-nodes
在本文中，OpenAI 的工程师团队分享了他们在 Kubernetes 集群扩展过程中遇到的各种挑战和解决方案，以及他们取得的性能和效果。
我们已经将 Kubernetes 集群扩展到 7500 个节点，为大型模型（如 GPT-3、 CLIP 和 DALL·E）创建了可扩展的基础设施，同时也为快速小规模迭代研究（如 神经语言模型的缩放定律）创建了可扩展的基础设施。
将单个 Kubernetes 集群扩展到这种规模很少见，但好处是能够提供一个简单的基础架构，使我们的机器学习研究团队能够更快地推进并扩展，而无需更改代码。
自上次发布关于扩展到 2500 个节点的帖子以来，我们继续扩大基础设施以满足研究人员的需求，在此过程中学到了许多的经验教训。本文总结了这些经验教训，以便 Kubernetes 社区里的其他人也能从中受益，并最后会介绍下我们仍然面临的问题，我们也将继续解决这些问题。
我们的工作负载
在深入探讨之前，我们着重描述一下我们的工作负载。我们在 Kubernetes 上运行的应用程序和硬件与大家在普通公司遇到的可能相当不同。因此，我们的问题及解决方案可能与你自己的设置匹配，也可能不匹配！
一个大型的机器学习作业跨越许多节点，当它可以访问每个节点上的所有硬件资源时，运行效率最高。这允许 GPU 直接使用 NVLink 进行交叉通信，或者 GPU 使用 GPUDirect 直接与 NIC 进行通信。因此，对于我们的许多工作负载，单个 Pod 占用整个节点。任何 NUMA、CPU 或 PCIE 资源争用都不是调度的因素。装箱或碎片化不是常见的问题。我们目前的集群具有完全的二分带宽，因此我们也不考虑机架或网络拓扑。所有这些都意味着，虽然我们有许多节点，但调度程序的负载相对较低。
话虽如此，kube-scheduler 的负载是有波动的。一个新的作业可能由许多数百个 Pod 同时创建组成，然后返回到相对较低的流失率。
我们最大的作业运行 MPI，作业中的所有 Pod 都参与一个单一的 MPI 通信器。如果任何一个参与的 Pod 挂掉，整个作业就会停止，需要重新启动。作业会定期进行检查点，当重新启动时，它会从上一个检查点恢复。因此，我们认为 Pod 是半有状态的——被删掉的 Pod 可以被替换并且工作可以继续，但这样做会造成干扰，应该尽量减少发生。
我们并不太依赖 Kubernetes 的负载均衡。我们的 HTTPS 流量非常少，不需要进行 A/B 测试、蓝 / 绿或金丝雀部署。Pod 使用 SSH 直接通过 Pod IP 地址与 MPI 进行通信，而不是通过服务端点。服务“发现”是有限的；我们只在作业启动时进行一次查找，查找哪些 Pod 参与 MPI。</description></item><item><title>灵魂拷问，上 Kubernetes 有什么业务价值？</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%81%B5%E9%AD%82%E6%8B%B7%E9%97%AE%E4%B8%8A-Kubernetes-%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%9A%E5%8A%A1%E4%BB%B7%E5%80%BC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%81%B5%E9%AD%82%E6%8B%B7%E9%97%AE%E4%B8%8A-Kubernetes-%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%9A%E5%8A%A1%E4%BB%B7%E5%80%BC/</guid><description>原文链接
本文整理自 2020 年 7 月 22 日《基于 Kubernetes 与 OAM 构建统一、标准化的应用管理平台》主题线上网络研讨会。文章共分为上下两篇，本文为上篇，主要和大家介绍上Kubernetes有什么业务价值，以及什么是“以应用为中心”的 Kubernetes。下篇将跟大家具体分享如何构建“以应用为中心”的 Kubernetes。
关注公众号，回复“0722”即可下载 PPT
非常感谢大家来到 CNCF 的直播，我是张磊，阿里云的高级技术专家，Kubernetes 项目资深维护者。同时也是 CNCF 应用交付领域 co-chair。我今天给大家带来的分享主题是《基于 Kubernetes 与 OAM 构建统一、标准化的应用管理平台》。在封面上有个钉钉群组二维码。大家可以通过这个二维码进入线上交流群。
上 Kubernetes 有什么业务价值？
今天要演讲的主题是跟应用管理或者说是云原生应用交付是相关的。首先我们想要先回答这么一个问题：为什么我们要基于 Kubernetes 去构建一个应用管理平台？
上图是一个本质的问题，我们在落地 K8s 经常遇到的一个问题。尤其是我们的业务方会问到这么一个问题，我们上 Kubernetes 有什么业务价值？这时候作为我们 K8s 工程师往往是很难回答的。原因在哪里呢？实际上这跟 K8s 的定位是相关的。K8s 这个项目呢，如果去做一个分析的话，我们会发现 K8s 不是一个 PaaS 或者应用管理的平台。实际上它是一个标准化的能力接入层。什么是能力接入层呢？大家可以看一下下图。
实际上通过 Kubernetes 对用户暴露出来的是一组声明式 API，这些声明式 API 无论是 Pod 还是 Service 都是对底层基础设施的一个抽象。比如 Pod 是对一组容器的抽象，而 Deployment 是对一组 pod 的抽象。而 Service 作为 Pod 的访问入口，实际上是对集群基础设施：网络、网关、iptables 的一个抽象。Node 是对宿主机的抽象。Kubernetes 还提供了我们叫做 CRD（也就是 Custom Resource）的自定义对象。让你自己能够自定义底层基础设施的一个抽象。</description></item><item><title>如何构建以应用为中心的Kubernetes</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%BA%94%E7%94%A8%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84Kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%BA%94%E7%94%A8%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84Kubernetes/</guid><description>原文链接
本文整理自 2020 年 7 月 22 日《基于 Kubernetes 与 OAM 构建统一、标准化的应用管理平台》主题线上网络研讨会。
关注公众号，回复“0722”即可下载 PPT
文章共分为上下两篇。上篇文章《灵魂拷问，上 Kubernetes 有什么业务价值？》，主要和大家介绍了上 Kubernetes 有什么业务价值，以及什么是“以应用为中心”的 Kubernetes。本文为下篇，将跟大家具体分享如何构建“以应用为中心”的 Kubernetes。
如何构建“以应用为中心”的 Kubernetes？
构建这么一个以用户为中心的 Kubernetes，需要做几个层级的事情。
1. 应用层驱动 首先来看最核心的部分，上图中蓝色部分，也就是 Kubernetes。可以在 Kubernetes 之上定义一组 CRD 和 Controller。可以在 CRD 来做用户这一侧的 API，比如说 pipeline 就是一个 API，应用也是一个 API。像运维侧的扩容策略这些都是可以通过 CRD 的方式安装起来。
2. 应用层抽象 所以我们的需要解决第一个问题是应用抽象。如果在 Kubernetes 去做应用层抽象，就等同于定义 CRD 和 Controller，所以 Controller 可以叫做应用层的抽象。本身可以是社区里的，比如 Tekton，istio 这些，可以作为你的应用驱动层。这是第一个问题，解决的是抽象的问题。不是特别难。
3. 插件能力管理 很多功能不是 K8s 提供的，内置的 Controller 还是有限的，大部分能力来自于社区或者是自己开发的 Controller。这时我的集群里面就会安装好多好多插件。如果要构建以应用为中心的 Kubernetes，那我必须能够管理起来这些能力，否则整个集群就会脱管了。用户想要这么一个能力，我需要告诉他有或者是没有。需要暴露出一个 API 来告诉他，集群是否有他需要的能力。假设需要 istio 的流量切分，需要有个接口告诉用户这个能力存不存在。不能指望用户去 get 一下 crd 合不合适，检查 Controller 是否运行。这不叫以应用为中心的 K8s，这叫裸 K8s。</description></item></channel></rss>