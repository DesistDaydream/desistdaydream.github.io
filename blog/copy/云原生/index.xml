<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>断念梦的站点 – 云原生</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/</link><description>Recent content in 云原生 on 断念梦的站点</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: ChatGPT 团队是如何使用Kubernetes的</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/ChatGPT-%E5%9B%A2%E9%98%9F%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Kubernetes%E7%9A%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/ChatGPT-%E5%9B%A2%E9%98%9F%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Kubernetes%E7%9A%84/</guid><description>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/u7zibC7UmMSAYotWZ81eYg">原文链接&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://openai.com/research/scaling-kubernetes-to-7500-nodes">https://openai.com/research/scaling-kubernetes-to-7500-nodes&lt;/a>&lt;/p>
&lt;p>在本文中，OpenAI 的工程师团队分享了他们在 Kubernetes 集群扩展过程中遇到的各种挑战和解决方案，以及他们取得的性能和效果。&lt;/p>
&lt;p>我们已经将 Kubernetes 集群扩展到 7500 个节点，为大型模型（如 GPT-3、 CLIP 和 DALL·E）创建了可扩展的基础设施，同时也为快速小规模迭代研究（如 神经语言模型的缩放定律）创建了可扩展的基础设施。&lt;/p>
&lt;p>将单个 Kubernetes 集群扩展到这种规模很少见，但好处是能够提供一个简单的基础架构，使我们的机器学习研究团队能够更快地推进并扩展，而无需更改代码。&lt;/p>
&lt;p>自上次发布关于扩展到 2500 个节点的帖子以来，我们继续扩大基础设施以满足研究人员的需求，在此过程中学到了许多的经验教训。本文总结了这些经验教训，以便 Kubernetes 社区里的其他人也能从中受益，并最后会介绍下我们仍然面临的问题，我们也将继续解决这些问题。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKdUh15yLPEl9eIzUgtNqcpB4tp85WFRKYQGYlv5dlPjib6OTbqCqeUdg/640?wx_fmt=png&amp;amp;wxfrom=13&amp;amp;tp=wxpic" alt="">&lt;/p>
&lt;p>我们的工作负载&lt;/p>
&lt;p>在深入探讨之前，我们着重描述一下我们的工作负载。我们在 Kubernetes 上运行的应用程序和硬件与大家在普通公司遇到的可能相当不同。因此，我们的问题及解决方案可能与你自己的设置匹配，也可能不匹配！&lt;/p>
&lt;p>一个大型的机器学习作业跨越许多节点，当它可以访问每个节点上的所有硬件资源时，运行效率最高。这允许 GPU 直接使用 NVLink 进行交叉通信，或者 GPU 使用 GPUDirect 直接与 NIC 进行通信。因此，对于我们的许多工作负载，单个 Pod 占用整个节点。任何 NUMA、CPU 或 PCIE 资源争用都不是调度的因素。装箱或碎片化不是常见的问题。我们目前的集群具有完全的二分带宽，因此我们也不考虑机架或网络拓扑。所有这些都意味着，虽然我们有许多节点，但调度程序的负载相对较低。&lt;/p>
&lt;p>话虽如此，kube-scheduler 的负载是有波动的。一个新的作业可能由许多数百个 Pod 同时创建组成，然后返回到相对较低的流失率。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKLpfylgOVT7BzEulB0dicicTPY64JIp4CzqozxGqiaibbxiawSQliaFeicVhWA/640?wx_fmt=png&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1&amp;amp;tp=wxpic" alt="">&lt;/p>
&lt;p>我们最大的作业运行 MPI，作业中的所有 Pod 都参与一个单一的 MPI 通信器。如果任何一个参与的 Pod 挂掉，整个作业就会停止，需要重新启动。作业会定期进行检查点，当重新启动时，它会从上一个检查点恢复。因此，我们认为 Pod 是半有状态的——被删掉的 Pod 可以被替换并且工作可以继续，但这样做会造成干扰，应该尽量减少发生。&lt;/p>
&lt;p>我们并不太依赖 Kubernetes 的负载均衡。我们的 HTTPS 流量非常少，不需要进行 A/B 测试、蓝 / 绿或金丝雀部署。Pod 使用 SSH 直接通过 Pod IP 地址与 MPI 进行通信，而不是通过服务端点。服务“发现”是有限的；我们只在作业启动时进行一次查找，查找哪些 Pod 参与 MPI。&lt;/p>
&lt;p>大多数作业与某种形式的 Blob 存储进行交互。它们通常会直接从 Blob 存储流式传输一些数据集的分片或检查点，或将其缓存到快速的本地临时磁盘中。我们有一些 PersistentVolumes，用于那些需要 POSIX 语义的情况，但 Blob 存储更具可扩展性，而且不需要缓慢的分离 / 附加操作。&lt;/p>
&lt;p>最后，我们的工作性质本质上是研究，这意味着工作负载本身是不断变化的。虽然超级计算团队努力提供我们认为达到“生产”质量水平的计算基础架构，但在该集群上运行的应用程序寿命很短，它们的开发人员会快速迭代。因此，随时可能出现新的使用模式，这些模式会挑战我们对趋势和适当权衡的设定。我们需要一个可持续的系统，同时也能让我们在事情发生变化时快速做出响应。&lt;/p>
&lt;p>网   络&lt;/p>
&lt;p>随着集群内节点和 Pod 数量的增加，我们发现 Flannel 难以满足所需的吞吐量。因此，我们转而使用 Azure VMSS 的本地 Pod 网络技术和相关 CNI 插件来配置 IP。这使我们的 Pod 能够获得主机级别的网络吞吐量。&lt;/p>
&lt;p>我们转而使用别名 IP 地址的另一个原因是，在我们最大的集群中，可能会同时使用约 20 万个 IP 地址。在测试了基于路由的 Pod 网络后，我们发现能够使用的路由数明显存在限制。&lt;/p>
&lt;p>避免封装会增加底层 SDN 或路由引擎的需求，虽然这使我们的网络设置变得简单。添加 VPN 或隧道可以在不需要任何其他适配器的情况下完成。我们不需要担心由于某部分网络具有较低的 MTU 而导致的分组分段。网络策略和流量监控很简单；没有关于数据包源和目的地的歧义。&lt;/p>
&lt;p>我们使用主机上的 iptables 标记来跟踪每个 Namespace 和 Pod 的网络资源使用情况，这使研究人员可以可视化他们的网络使用模式。特别是，由于我们的许多实验具有不同的 Internet 和 Pod 内通信模式，因此能够调查任何瓶颈发生的位置通常是非常有意义的。&lt;/p>
&lt;p>可以使用 iptables &lt;code>mangle&lt;/code> 规则任意标记符合特定条件的数据包。以下是我们用来检测流量是内部流量还是 Internet 流量的规则。&lt;code>FORWARD&lt;/code> 规则涵盖了来自 Pod 的流量，而 &lt;code>INPUT&lt;/code> 和 &lt;code>OUTPUT&lt;/code> 规则涵盖了主机上的流量：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-nginx" data-lang="nginx">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">iptables&lt;/span> &lt;span style="color:#4e9a06">-t&lt;/span> &lt;span style="color:#4e9a06">mangle&lt;/span> &lt;span style="color:#4e9a06">-A&lt;/span> &lt;span style="color:#4e9a06">INPUT&lt;/span> &lt;span style="color:#4e9a06">!&lt;/span> &lt;span style="color:#4e9a06">-s&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#4e9a06">.0.0.0/8&lt;/span> &lt;span style="color:#4e9a06">-m&lt;/span> &lt;span style="color:#4e9a06">comment&lt;/span> &lt;span style="color:#4e9a06">--comment&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;iptables-exporter&lt;/span> &lt;span style="color:#4e9a06">openai&lt;/span> &lt;span style="color:#4e9a06">traffic=internet-in&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">iptables&lt;/span> &lt;span style="color:#4e9a06">-t&lt;/span> &lt;span style="color:#4e9a06">mangle&lt;/span> &lt;span style="color:#4e9a06">-A&lt;/span> &lt;span style="color:#4e9a06">FORWARD&lt;/span> &lt;span style="color:#4e9a06">!&lt;/span> &lt;span style="color:#4e9a06">-s&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#4e9a06">.0.0.0/8&lt;/span> &lt;span style="color:#4e9a06">-m&lt;/span> &lt;span style="color:#4e9a06">comment&lt;/span> &lt;span style="color:#4e9a06">--comment&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;iptables-exporter&lt;/span> &lt;span style="color:#4e9a06">openai&lt;/span> &lt;span style="color:#4e9a06">traffic=internet-in&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">iptables&lt;/span> &lt;span style="color:#4e9a06">-t&lt;/span> &lt;span style="color:#4e9a06">mangle&lt;/span> &lt;span style="color:#4e9a06">-A&lt;/span> &lt;span style="color:#4e9a06">OUTPUT&lt;/span> &lt;span style="color:#4e9a06">!&lt;/span> &lt;span style="color:#4e9a06">-d&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#4e9a06">.0.0.0/8&lt;/span> &lt;span style="color:#4e9a06">-m&lt;/span> &lt;span style="color:#4e9a06">comment&lt;/span> &lt;span style="color:#4e9a06">--comment&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;iptables-exporter&lt;/span> &lt;span style="color:#4e9a06">openai&lt;/span> &lt;span style="color:#4e9a06">traffic=internet-out&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">iptables&lt;/span> &lt;span style="color:#4e9a06">-t&lt;/span> &lt;span style="color:#4e9a06">mangle&lt;/span> &lt;span style="color:#4e9a06">-A&lt;/span> &lt;span style="color:#4e9a06">FORWARD&lt;/span> &lt;span style="color:#4e9a06">!&lt;/span> &lt;span style="color:#4e9a06">-d&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#4e9a06">.0.0.0/8&lt;/span> &lt;span style="color:#4e9a06">-m&lt;/span> &lt;span style="color:#4e9a06">comment&lt;/span> &lt;span style="color:#4e9a06">--comment&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;iptables-exporter&lt;/span> &lt;span style="color:#4e9a06">openai&lt;/span> &lt;span style="color:#4e9a06">traffic=internet-out&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>一旦标记，iptables 将开始计数以跟踪匹配该规则的字节数和数据包数。你可以使用 &lt;code>iptables&lt;/code> 本身来查看这些计数器：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-typescript" data-lang="typescript">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ce5c00;font-weight:bold">%&lt;/span> &lt;span style="color:#000">iptables&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#000">t&lt;/span> &lt;span style="color:#000">mangle&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#000">L&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#000">v&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">Chain&lt;/span> &lt;span style="color:#000">FORWARD&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">policy&lt;/span> &lt;span style="color:#000">ACCEPT&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">50&lt;/span>&lt;span style="color:#000">M&lt;/span> &lt;span style="color:#000">packets&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">334&lt;/span>&lt;span style="color:#000">G&lt;/span> &lt;span style="color:#000">bytes&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">pkts&lt;/span> &lt;span style="color:#000">bytes&lt;/span> &lt;span style="color:#000">target&lt;/span> &lt;span style="color:#000">prot&lt;/span> &lt;span style="color:#000">opt&lt;/span> &lt;span style="color:#204a87;font-weight:bold">in&lt;/span> &lt;span style="color:#000">out&lt;/span> &lt;span style="color:#000">source&lt;/span> &lt;span style="color:#000">destination&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">....&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">1253&lt;/span>&lt;span style="color:#000">K&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">555&lt;/span>&lt;span style="color:#000">M&lt;/span> &lt;span style="color:#000">all&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">--&lt;/span> &lt;span style="color:#204a87;font-weight:bold">any&lt;/span> &lt;span style="color:#204a87;font-weight:bold">any&lt;/span> &lt;span style="color:#000">anywhere&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">!&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10.0&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.0&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">/&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> &lt;span style="color:#8f5902;font-style:italic">/* iptables-exporter openai traffic=internet-out */&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">1161&lt;/span>&lt;span style="color:#000">K&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">7937&lt;/span>&lt;span style="color:#000">M&lt;/span> &lt;span style="color:#000">all&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">--&lt;/span> &lt;span style="color:#204a87;font-weight:bold">any&lt;/span> &lt;span style="color:#204a87;font-weight:bold">any&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">!&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">10.0&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.0&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">/&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">8&lt;/span> &lt;span style="color:#000">anywhere&lt;/span> &lt;span style="color:#8f5902;font-style:italic">/* iptables-exporter openai traffic=internet-in */&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>我们使用名为 iptables-exporter 的开源 Prometheus 导出器将这些数据追踪到我们的监控系统中。这是一种简单的方法，可以跟踪与各种不同类型的条件匹配的数据包。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKd8NicaEsHsoGOvcBJMt435PARbypib0ENCBwpeXTibnrp9mkYENFG0Hfg/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKQOOXeibMYDia4Eoic4E7iaNI2uPCa1HOVeY75vj4D2SfSnGVm5wVmDx9Ug/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>我们网络模型中比较独特的一点是，我们完全向研究人员公开节点、Pod 和 Service 网络 CIDR 范围。我们采用集线器和分支的网络模型，并使用本机节点和 Pod CIDR 范围路由该流量。研究人员连接到中心枢纽，然后可以访问任何一个单独的集群（分支）。但是这些集群本身无法相互通信。这确保了集群保持隔离、没有跨集群依赖，可以防止故障隔离中的故障传播。&lt;/p>
&lt;p>我们使用一个“NAT”主机来翻译从集群外部传入的服务网络 CIDR 范围的流量。这种设置为我们的研究人员提供了很大的灵活性，他们可以选择各种不同类型的网络配置进行实验。&lt;/p>
&lt;p>API 服务器&lt;/p>
&lt;p>Kubernetes 的 API Server 和 etcd 是保持集群健康运行的关键组件，因此我们特别关注这些系统的压力。我们使用 kube-prometheus 提供的 Grafana 仪表板以及额外的内部仪表板。我们发现，将 HTTP 状态码 429（请求太多）和 5xx（服务器错误）的速率作为高级信号警报是有用的。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKDtNcvSjnOqmZ6iccOSmwmricspB55500xvHpBMAI6HiaGfcCtSjqq3ljA/640?wx_fmt=png&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1&amp;amp;tp=wxpic" alt="">&lt;/p>
&lt;p>虽然有些人在 kube 内部运行 API 服务器，但我们一直在集群外运行它们。etcd 和 API 服务器都在它们自己的专用节点上运行。我们的最大集群运行 5 个 API 服务器和 5 个 etcd 节点，以分散负载并尽可能减少发生故障后带来的影响。自从我们在 上一篇博文 中提到的将 Kubernetes 事件拆分到它们自己的 etcd 集群中以来，我们没有遇到 etcd 的任何值得注意的问题。API 服务器是无状态的，通常很容易在自我修复的实例组或扩展集中运行。我们尚未尝试构建任何自我修复 etcd 集群的自动化，因为发生事故非常罕见。&lt;/p>
&lt;p>API 服务器可能会占用相当多的内存，并且往往会与集群中的节点数量成线性比例。对于我们有 7500 个节点的集群，我们观察到每个 API 服务器使用高达 70GB 的堆内存，因此幸运地是，未来这应该仍然在硬件能力范围之内。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKh2rebxQzCsojXTUN9nDTDZbYWPMpq9ORPlVQfzHwh4sbM4VCztKibWQ/640?wx_fmt=png&amp;amp;tp=wxpic&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>API Servers 受到压力的主要来源之一就是对 Endpoints 的 WATCH。在整个集群中有一些服务，如“kubelet”和“node-exporter”，其中每个节点都是成员。当一个节点被添加或从集群中删除时，这个 WATCH 将被触发。通常，由于每个节点本身都通过 kube-proxy 监视 &lt;code>kubelet&lt;/code> 服务，因此这些响应中所需的数量和带宽将是 N2 非常大，有时会达到 1GB/s 或更高。Kubernetes 1.17 中推出的 EndpointSlices 大大降低了这种负载，减少达 1000 倍。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/YriaiaJPb26VOiciaV2ibeVb4gXQtYocv76XKwwo1NJytItYTcNF6oeOvs5TeLaBNqZuJa7LKFysickwuoBcHEiacZltQ/640?wx_fmt=png&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1&amp;amp;tp=wxpic" alt="">&lt;/p>
&lt;p>总的来说，我们会非常注意随着集群规模增大而增加的 API Server 请求。我们尽量避免任何 DaemonSets 与 API Server 交互。在需要每个节点监视更改的情况下，引入缓存服务（例如 Datadog Cluster Agent）作为中介，似乎是避免集群范围瓶颈的良好模式。&lt;/p>
&lt;p>随着集群的增长，我们对集群的实际自动伸缩越来越少。但当一次性自动扩展太多时，我们偶尔会遇到问题。当新节点加入集群时会生成大量请求，一次性添加数百个节点可能会超过 API 服务器容量的负荷。稍微平滑一下这个过程，即使只有几秒钟也有助于避免宕机。&lt;/p>
&lt;p>时间序列度量与 Prometheus 和 Grafana&lt;/p>
&lt;p>我们使用 Prometheus 收集时间序列度量数据，并使用 Grafana 进行图形、仪表板和警报。我们从 kube-prometheus 部署开始收集了各种各样的度量数据，并使用了一些良好的仪表板进行可视化。随着节点数量的不断增加，我们开始遇到 Prometheus 收集的度量数据数量过多的问题。尽管 kube-prometheus 公开了许多有用的数据，但我们实际上并没有查看所有的度量数据，一些数据也过于细化，无法有效地进行收集、存储和查询。因此，我们使用 Prometheus 规则从被摄入的度量数据中“删掉”一些数据。&lt;/p>
&lt;p>有一段时间，我们遇到了 Prometheus 消耗越来越多的内存问题，最终导致容器崩溃并出现 Out-Of-Memory 错误（OOM）。即使为应用程序分配了大量的内存容量，这种情况似乎仍然会发生。更糟糕的是，它在崩溃时会花费很多时间在启动时回放预写日志文件，直到它再次可用。最终，我们发现了这些 OOM 的来源是 Grafana 和 Prometheus 之间的交互，其中 Grafana 使用 &lt;code>/api/v1/series&lt;/code> API 查询 &lt;code>{le!=&amp;quot;&amp;quot;}&lt;/code>（基本上是“给我所有的直方图度量”）。&lt;code>/api/v1/series&lt;/code> 的实现在时间和空间上没有限制，对于具有大量结果的查询，这将不断消耗更多的内存和时间。即使请求者已经放弃并关闭了连接，它也会继续增长。对于我们来说，内存永远不够，而 Prometheus 最终会崩溃。因此，我们修补了 Prometheus，将此 API 包含在上下文中以强制执行超时，从而完全解决了问题。&lt;/p>
&lt;p>虽然 Prometheus 崩溃的次数大大减少，但在我们需要重新启动它的时候，WAL 回放仍然是一个问题。通常需要多个小时来回放所有 WAL 日志，直到 Prometheus 开始收集新的度量数据并提供服务。在 Robust Perception 的帮助下，我们发现将 &lt;code>GOMAXPROCS=24&lt;/code> 应用于服务器可以显著提高性能。在 WAL 回放期间，Prometheus 尝试使用所有核心，并且对于具有大量核心的服务器，争用会降低所有性能。&lt;/p>
&lt;p>我们正在探索新的选项来增加我们的监控能力，下面“未解决的问题”部分将对此进行描述。&lt;/p>
&lt;p>健康检查&lt;/p>
&lt;p>对于如此庞大的集群，我们当然依赖自动化来检测并从集群中移除行为不当的节点。随着时间的推移，我们建立了许多健康检查系统。&lt;/p>
&lt;p>被动健康检查&lt;/p>
&lt;p>某些健康检查是被动的，总是在所有节点上运行。这些检查监视基本的系统资源，例如网络可达性、坏盘或满盘，或者 GPU 错误。GPU 以许多不同的方式出现问题，但一个容易出现的常见问题是“不可纠正的 ECC 错误”。Nvidia 的数据中心 GPU 管理器（DCGM）工具使查询这个问题和许多其他“Xid”错误变得容易。我们跟踪这些错误的一种方式是通过 dcgm-exporter 将指标收集到我们的监控系统 Prometheus 中。这将出现为 &lt;code>DCGM_FI_DEV_XID_ERRORS&lt;/code> 指标，并设置为最近发生的错误代码。此外，NVML 设备查询 API 公开了有关 GPU 的健康和操作的更详细信息。&lt;/p>
&lt;p>一旦检测到错误，它们通常可以通过重置 GPU 或系统来修复，但在某些情况下确实需要更换基础 GPU。&lt;/p>
&lt;p>另一种健康检查是跟踪来自上游云提供商的维护事件。每个主要的云提供商都公开了一种方式来了解当前 VM 是否需要进行会最终导致中断的、即将发生的维护事件。VM 可能需要重新启动以应用底层的超级管理程序补丁，或者将物理节点替换为其他硬件。&lt;/p>
&lt;p>这些被动健康检查在所有节点上不断运行。如果健康检查开始失败，节点将自动划分，因此不会在节点上安排新的 Pod。对于更严重的健康检查失败，我们还将尝试 Pod 驱逐，以要求当前运行的所有 Pod 立即退出。这仍然取决于 Pod 本身，可通过 Pod 故障预算进行配置来决定是否允许此驱逐发生。最终，无论是在所有 Pod 终止之后，还是在 7 天过去之后（我们的服务级别协议的一部分），我们都将强制终止 VM。&lt;/p>
&lt;p>活动 GPU 测试&lt;/p>
&lt;p>不幸的是，并非所有 GPU 问题都会通过 DCGM 可见的错误代码表现出来。我们建立了自己的测试库，通过对 GPU 进行测试来捕捉其他问题，并确保硬件和驱动程序的行为符合预期。这些测试无法在后台运行 - 它们需要独占 GPU 运行数秒钟或数分钟。&lt;/p>
&lt;p>我们首先在节点启动时运行这些测试，使用我们称之为“预检（preflight）”的系统。所有节点都会附带一个“预检”污点和标签加入集群。这个污点会阻止普通 Pod 被调度到节点上。我们配置了一个 DaemonSet，在所有带有此标签的节点上运行预检测试 Pod。测试成功完成后，测试本身将删除污点和标签，然后该节点就可供一般使用。&lt;/p>
&lt;p>我们还定期在节点的生命周期中运行这些测试。我们将其作为 CronJob 运行，允许它着陆在集群中的任何可用节点上。哪些节点会被测试到可能有些随机和不受控制，但我们发现随着时间的推移，它提供了足够的覆盖率，并且最小化了协调或干扰。&lt;/p>
&lt;p>配额和资源使用&lt;/p>
&lt;p>随着集群规模的扩大，研究人员开始发现他们难以获取分配给他们的全部容量。传统的作业调度系统有许多不同的功能，可以公平地在竞争团队之间运行工作，而 Kubernetes 没有这些功能。随着时间的推移，我们从这些作业调度系统中汲取灵感，并以 Kubernetes 原生的方式构建了几个功能。&lt;/p>
&lt;p>团队污点&lt;/p>
&lt;p>我们在每个集群中都有一个服务，称为“team-resource-manager”，具有多个功能。它的数据源是一个 ConfigMap，为在给定集群中具有容量的所有研究团队指定了 （节点选择器、应用的团队标签、分配数量） 元组。它会将当前节点与这些元组进行对比，并使用 &lt;code>openai.com/team=teamname:NoSchedule&lt;/code> 的污点对适当数量的节点进行标记。&lt;/p>
&lt;p>“team-resource-manager”还有一个入站的 webhook 服务，因此在提交每个作业时会根据提交者的团队成员身份应用相应的容忍度。使用污点使我们能够灵活地限制 Kubernetes Pod 调度程序，例如允许较低优先级的 Pod 具有 &amp;ldquo;any&amp;rdquo; 容忍度，这样团队可以借用彼此的容量，而无需进行大量协调。&lt;/p>
&lt;p>CPU 和 GPU  balloons&lt;/p>
&lt;p>除了使用集群自动缩放器动态扩展我们基于虚拟机的集群之外，我们还使用它来纠正（删除和重新添加）集群中的不健康成员。我们通过将集群的 &amp;ldquo;最小值&amp;rdquo; 设置为零、&amp;ldquo;最大值&amp;rdquo; 设置为可用容量来实现这一点。然而，如果 cluster-autoscaler 发现有空闲节点，它将尝试缩小到只需要的容量。由于多种原因（VM 启动延迟、预分配成本、上面提到的 API 服务器影响），这种空闲缩放并不理想。&lt;/p>
&lt;p>因此，我们为 CPU 和 GPU 主机都引入了“球形”部署。这个部署包含一个具有 &amp;ldquo;最大值&amp;rdquo; 数量的低优先级 Pod 副本集。这些 Pod 占用节点内的资源，因此自动缩放器不会将它们视为空闲。但由于它们是低优先级的，调度程序可以立即将它们驱逐出去，以腾出空间进行实际工作。（我们选择使用 Deployment 而不是 DaemonSet，以避免将 DaemonSet 视为节点上的空闲工作负载。）&lt;/p>
&lt;p>需要注意的是，我们使用 pod 反亲和性（anti-affinity）来确保 pod 在节点之间均匀分布。Kubernetes 调度器的早期版本存在一个 O(N^2) 的性能问题，与 pod 反亲和性有关。自 Kubernetes 1.18 版本以后，这个问题已经得到了纠正。&lt;/p>
&lt;p>Gang 调度&lt;/p>
&lt;p>我们的实验通常涉及一个或多个 StatefulSets，每个 StatefulSet 操作不同部分的训练任务。对于优化器，研究人员需要在进行任何训练之前调度 StatefulSet 的所有成员（因为我们通常使用 MPI 在优化器成员之间协调，而 MPI 对组成员变化很敏感）。&lt;/p>
&lt;p>然而再默认情况下，Kubernetes 不一定会优先满足某个 StatefulSet 的所有请求。例如，如果两个实验都请求 100％的集群容量，那么 Kubernetes 可能只会调度给每个实验需要的一半 Pod，这会导致死锁，使两个实验都无法进行。&lt;/p>
&lt;p>我们尝试了一些需要自定义调度程序的方法，但遇到了一些与正常 Pod 调度方式冲突的边缘情况。Kubernetes 1.18 引入了核心 Kubernetes 调度程序的插件体系结构，使本地添加此类功能变得更加容易。我们最近选择了 Coscheduling 插件作为解决此问题的方法。&lt;/p>
&lt;p>未解决的问题&lt;/p>
&lt;p>随着 Kubernetes 集群规模的扩大，我们仍有许多问题需要解决。其中一些问题包括：&lt;/p>
&lt;p>指标&lt;/p>
&lt;p>在如今的规模下，Prometheus 内置的 TSDB 存储引擎很难压缩，并且每次重新启动时需要长时间回放 WAL（预写式日志）。查询还往往会导致“查询处理会加载过多样本”的错误。我们正在迁移到不同的、与 Prometheus 兼容的存储和查询引擎。大家可以期待下我们未来的博客文章，看看它的表现如何！&lt;/p>
&lt;p>Pod 网络流量整形&lt;/p>
&lt;p>随着集群规模的扩大，每个 Pod 的互联网带宽量被计算了出来。每个人的聚合互联网带宽需求变得非常大，我们的研究人员现在有能力会意外地对互联网上的其他位置施加重大资源压力，例如要下载的数据集和要安装的软件包。&lt;/p>
&lt;p>结   论&lt;/p>
&lt;p>Kubernetes 是一个非常灵活的平台，可以满足我们的研究需求。它具有满足我们所面临的最苛刻工作负载的能力。尽管它仍有许多需要改进的地方，但 OpenAI 的超级计算团队将继续探索 Kubernetes 的可扩展性。&lt;/p></description></item><item><title>Blog: 灵魂拷问，上 Kubernetes 有什么业务价值？</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%81%B5%E9%AD%82%E6%8B%B7%E9%97%AE%E4%B8%8A-Kubernetes-%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%9A%E5%8A%A1%E4%BB%B7%E5%80%BC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E7%81%B5%E9%AD%82%E6%8B%B7%E9%97%AE%E4%B8%8A-Kubernetes-%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%9A%E5%8A%A1%E4%BB%B7%E5%80%BC/</guid><description>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/a3NE5fSpZIM9qlOofGTMWQ">原文链接&lt;/a>&lt;/p>
&lt;p>本文整理自 2020 年 7 月 22 日《基于 Kubernetes 与 OAM 构建统一、标准化的应用管理平台》主题线上网络研讨会。文章共分为上下两篇，本文为上篇，主要和大家介绍上Kubernetes有什么业务价值，以及什么是“以应用为中心”的 Kubernetes。下篇将跟大家具体分享如何构建“以应用为中心”的 Kubernetes。&lt;/p>
&lt;p>&lt;strong>&lt;strong>关注公众号，回复&lt;/strong>&lt;/strong>“0722”&lt;strong>&lt;strong>即可下载 PPT&lt;/strong>&lt;/strong>&lt;/p>
&lt;p>非常感谢大家来到 CNCF 的直播，我是张磊，阿里云的高级技术专家，Kubernetes 项目资深维护者。同时也是 CNCF 应用交付领域 co-chair。我今天给大家带来的分享主题是《基于 Kubernetes 与 OAM 构建统一、标准化的应用管理平台》。在封面上有个钉钉群组二维码。大家可以通过这个二维码进入线上交流群。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QooolHAJELazlEgJaCtxTJSXlz0ze2ryrlyNs08awKug6GMpxVDqYDg/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>&lt;strong>上 Kubernetes 有什么业务价值？&lt;/strong>&lt;/p>
&lt;p>今天要演讲的主题是跟应用管理或者说是云原生应用交付是相关的。首先我们想要先回答这么一个问题：为什么我们要基于 Kubernetes 去构建一个应用管理平台？&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QjhcMMlwCRicqK1hhjiaYqVdUD9gSEhsLSDZvdlich8ZFgOWpzic3fomQsg/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>上图是一个本质的问题，我们在落地 K8s 经常遇到的一个问题。尤其是我们的业务方会问到这么一个问题，我们上 Kubernetes 有什么业务价值？这时候作为我们 K8s 工程师往往是很难回答的。原因在哪里呢？实际上这跟 K8s 的定位是相关的。K8s 这个项目呢，如果去做一个分析的话，我们会发现 K8s 不是一个 PaaS 或者应用管理的平台。实际上它是一个标准化的能力接入层。什么是能力接入层呢？大家可以看一下下图。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8Qibw7FRA1G9hpUenLEOUI0PKbdfr1OReJhtM36pjzQAswxYvM9ib5PdsA/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>实际上通过 Kubernetes 对用户暴露出来的是一组声明式 API，这些声明式 API 无论是 Pod 还是 Service 都是对底层基础设施的一个抽象。比如 Pod 是对一组容器的抽象，而 Deployment 是对一组 pod 的抽象。而 Service 作为 Pod 的访问入口，实际上是对集群基础设施：网络、网关、iptables 的一个抽象。Node 是对宿主机的抽象。Kubernetes 还提供了我们叫做 CRD（也就是 Custom Resource）的自定义对象。让你自己能够自定义底层基础设施的一个抽象。&lt;/p>
&lt;p>而这些抽象本身或者是 API 本身，是通过另外一个模式叫做控制器(Controller)去实现的。通过控制器去驱动我们的底层基础设施向我的抽象逼近，或者是满足我抽象定义的一个终态。&lt;/p>
&lt;p>所以本质来讲，Kubernetes 他的专注点是“如何标准化的接入来自于底层，无论是容器、虚机、负载均衡各种各样的一个能力，然后通过声明式 API 的方式去暴露给用户”。这就意味着 Kubernetes 实际用户不是业务研发，也不是业务运维。那是谁呢？是我们的平台开发者。希望平台开发者能够基于 Kubernetes 再去做上层的框架或者是平台。那就导致了今天我们的业务研发和业务运维对 Kubernetes 直接暴露出来的这一层抽象，感觉并不是很友好。&lt;/p>
&lt;p>这里的关键点在于，Kubernetes 对这些基础设施的抽象，跟业务研发和业务运维看待系统的角度是完全不同的。这个抽象程度跟业务研发和业务运维希望的抽象程度也是不一样的。语义完全对不上，使用习惯也是有很大的鸿沟。所以说为了解决这样一个问题，都在思考一些解决方法。怎么能让我 Kubernetes 提供的基础设施的抽象能够满足我业务研发和业务运维的一个诉求呢？怎么能让 Kubernetes 能够成为业务研发和业务运维喜欢的一个平台呢？&lt;/p>
&lt;h3 id="方法一把所有人都变成-kubernetes-专家">&lt;strong>方法一：把所有人都变成 Kubernetes 专家&lt;/strong>&lt;a class="td-heading-self-link" href="#%e6%96%b9%e6%b3%95%e4%b8%80%e6%8a%8a%e6%89%80%e6%9c%89%e4%ba%ba%e9%83%bd%e5%8f%98%e6%88%90-kubernetes-%e4%b8%93%e5%ae%b6" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QicqQTtcXytBOyTdQQe7zLsWCVoicvvVMlZYVzm6Pz1XQ0mc9LD0siavcA/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>假如我们所有人都是 Kubernetes 专家，那当然会喜欢 Kubernetes 对我提供的服务，这里给他发个 Kubernetes 的 PhD 博士。这里我强烈推荐阿里云和 CNCF 主办的云原生技术公开课。大家试试学完这门课程后，能不能变成 Kubernetes 专家。&lt;/p>
&lt;p>这个方法门槛比较高，因为每个人对于这个系统本身感兴趣程度不太一样，学习能力也不太一样。&lt;/p>
&lt;h3 id="方法二构建一个面向用户的应用管理平台">&lt;strong>方法二：构建一个面向用户的应用管理平台&lt;/strong>&lt;a class="td-heading-self-link" href="#%e6%96%b9%e6%b3%95%e4%ba%8c%e6%9e%84%e5%bb%ba%e4%b8%80%e4%b8%aa%e9%9d%a2%e5%90%91%e7%94%a8%e6%88%b7%e7%9a%84%e5%ba%94%e7%94%a8%e7%ae%a1%e7%90%86%e5%b9%b3%e5%8f%b0" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>业界常见的方法，大家会基于 Kubernetes 构建一个面向用户的应用管理平台，或者说是一个 PaaS，有人直接做成一个 Serverless。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QYKexjNJtDyz9V5AMQ1xAh470gONwXHlgQ60WRyt6N5TIWPibNXWdRuQ/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>那这个具体是怎么做呢？还是在 Kubernetes 之上，会搭建一个东西叫做上层应用管理平台，这个上层应用平台对业务研发和业务运维暴露出来一个上层的 API。比如说业务研发这一侧，他不太会暴露 Pod，Deployment 这样的抽象。只会暴露出来 CI/CD 流水线。或者说一个应用，WordPress，一个外部网站，暴露出这样一个上层的概念，这是第一个部分。&lt;/p>
&lt;p>第二部分，它也会给业务运维暴露出一组运维的 API。比如说：水平扩容，发布策略，分批策略，访问控制，流量配置。这样的话有一个好处，业务研发和业务运维面对的 API 不是 Kubernetes 底层的 API，不是 Node，不是 Service，不是 Deployment，不是我们的 CRD。是这样一组经过抽象经过封装后的 API。这样的业务研发和业务运维用起来会跟他所期望的 Ops 流水线，它所熟悉的使用体检有个天然的结合点。&lt;/p>
&lt;p>所以说只有这么做了之后，我们才能够跟我们的业务老大说，Kubernetes 的业务价值来了。实际上业务价值不是在 Kubernetes 这一层，而是在 Kubernetes 往上的这一层&amp;ndash;&amp;quot;&lt;strong>你的解决方案&lt;/strong>&amp;quot;。所以说这样的一个系统构建出来之后呢，实际上是对 Kubernetes 又做了一层封装。变成了很多公司都有的，比如说 Kubernetes 应用平台。这是一个非常常见的做法。相比于我们让研发运维变成 Kubernetes 专家来说会更加实际一点。&lt;/p>
&lt;p>但是我们在阿里也好，在很多社区的实际场景也好，它往往会伴随着这么一个问题。这个问题是：今天 Kubernetes 的生态是非常非常繁荣的，下图是我在 CNCF 截的图，好几百个项目，几千个可以让我们 Kubernetes 即插即用的能力。比如 istio，KEDA，Promethues 等等都是 Kubernetes 的插件。正是基于这么一个扩展性非常高的声明式 API 体系才会有了这么繁荣的 Kubernetes 生态。所以可以认为 Kubernetes 能力是无限的，非常强大。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QLHL0ibjiauseNYYibibyBwA6lazv8vlz5k9X1Fx9MWCG8FBz3aj2NqpsXQ/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>可是这么一个无限能力，如果对接到一个非常传统的，非常经典的一个应用管理平台。比如说我们的 PaaS 上，如 Cloud Foundry。立刻就会发现一个问题，PaaS 虽然对用户提供的是很友好的 API，但是这个 API 本身是有限的，是难以扩展的。比如说 Cloud Foundry 要给用户使用，就有 Buildpack 这么一个概念，而不是 Kubernetes 所有的能力都能给用户去使用。其实几乎所有的 PaaS 都会存在这么一个问题。它往上暴露的是一个用户的API，是不可扩展的，是个有限集。&lt;/p>
&lt;p>下面一个非常庞大繁荣的 Kubernetes 生态，没办法直接给用户暴露出去。可能每使用一个插件就要重新迭代开发你的 PaaS，重新交付你的 PaaS。这个是很难接受的。&lt;/p>
&lt;h3 id="传统-paas-的能力困境">&lt;strong>传统 PaaS 的“能力困境”&lt;/strong>&lt;a class="td-heading-self-link" href="#%e4%bc%a0%e7%bb%9f-paas-%e7%9a%84%e8%83%bd%e5%8a%9b%e5%9b%b0%e5%a2%83" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这问题是一个普遍存在的问题，我们叫做传统 PaaS 的“能力困境”。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QoDERgH8CalNEQRVsZ3UdnxkcLXerLqIUdB52bzKpxSVzPEXvaxDFaQ/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>本质上来说这个困境是什么意思呢？K8s 生态繁荣多样的应用基础设施能力，与业务开发人员日益增长的应用管理诉求，中间存在一个传统的 PaaS，他就会变成一个瓶颈。K8s 无限的能力无法让你的研发与运维立刻用到。所以传统 PaaS 就会成为一个显而易见的瓶颈。&lt;/p>
&lt;p>这样给我带来一个思考：我们能不能抛弃传统 PaaS 的一个做法，基于 K8s 打造高可扩展的应用管理平台。我们想办法能把 K8s 能力无缝的透给用户，同时又能提供传统 PaaS 比较友好的面向研发运维的使用体验呢？&lt;/p>
&lt;p>其实可以从另外一个角度思考这个问题：如何基于 K8s 打造高可扩展的应用管理平台，实际上等同于 如何打造一个“以应用为中心的”的 Kubernetes。或者说能不能基于 Kubernetes 去封装下，让它能够像 PaaS 一样，去面向我的实际用户去使用呢？这个就是我们要聊的关键点。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_gif/US10Gcd0tQGY9ddd5GpbmVRuaRfuaESAUBGE7uHX5G0nxxLSub2QTKZdu538V7GaHXS5jsTCebYCUibaHsjg0ow/640?wx_fmt=gif" alt="">&lt;/p>
&lt;p>&lt;strong>什么是“以应用为中心”的 Kubernetes&lt;/strong>&lt;/p>
&lt;h3 id="特征一通过原生的声明式-api-和插件体系暴露面向最终用户的上层语义和抽象">&lt;strong>特征一：通过原生的声明式 API 和插件体系，暴露面向最终用户的上层语义和抽象&lt;/strong>&lt;a class="td-heading-self-link" href="#%e7%89%b9%e5%be%81%e4%b8%80%e9%80%9a%e8%bf%87%e5%8e%9f%e7%94%9f%e7%9a%84%e5%a3%b0%e6%98%8e%e5%bc%8f-api-%e5%92%8c%e6%8f%92%e4%bb%b6%e4%bd%93%e7%b3%bb%e6%9a%b4%e9%9c%b2%e9%9d%a2%e5%90%91%e6%9c%80%e7%bb%88%e7%94%a8%e6%88%b7%e7%9a%84%e4%b8%8a%e5%b1%82%e8%af%ad%e4%b9%89%e5%92%8c%e6%8a%bd%e8%b1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8QhBSxNlRvpGWfJDkyx8ftxpO0rswAky1rVmVWho2Ey6RWqE7Dia6LBIw/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>我们不是说要在 Kubernetes 上盖一个 PaaS，或者说是盖一个大帽子，不干这件事情。因为 K8s 本身可以扩展，可以写一组 CRD，把我们要的 API 给装上去。比如 CI/CD 流水线，就可以像 Tektong 系统直接使用 pipeline。应用也可以通过某些项目直接暴露出来。运维这一侧的发布扩容等，都可以通过安装一个 Operator 去解决问题。当然也需要一些技术将这些运维策略绑定到应用或者流水线中。&lt;/p>
&lt;p>这就是我们第一个点，以应用为中心的 K8s 首先是暴露给用户的语义和 API，而不是非常底层的，比如 Service、Node 或者是 Ingress。可能用户都不知道什么意思，也不知道怎么写的。&lt;/p>
&lt;h3 id="特征二上层语义和抽象可插拔可扩展没有抽象程度锁定和任何能力限制">&lt;strong>特征二：上层语义和抽象可插拔，可扩展，没有抽象程度锁定和任何能力限制&lt;/strong>&lt;a class="td-heading-self-link" href="#%e7%89%b9%e5%be%81%e4%ba%8c%e4%b8%8a%e5%b1%82%e8%af%ad%e4%b9%89%e5%92%8c%e6%8a%bd%e8%b1%a1%e5%8f%af%e6%8f%92%e6%8b%94%e5%8f%af%e6%89%a9%e5%b1%95%e6%b2%a1%e6%9c%89%e6%8a%bd%e8%b1%a1%e7%a8%8b%e5%ba%a6%e9%94%81%e5%ae%9a%e5%92%8c%e4%bb%bb%e4%bd%95%e8%83%bd%e5%8a%9b%e9%99%90%e5%88%b6" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvk7Pu7LACjibshibapmweer8Q0Z4N3HbqpJnstTLAEicg9IR7QVTMWGzN6toXCticVeGHtCxLXUI2kBpQ/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>第二个点很重要，上层语义和抽象必须是可插拔的，必须是可扩展的，是无缝兼容利用 K8s 的可扩展能力的。并且也不应该有对抽象程度的锁定。&lt;/p>
&lt;p>举个例子：比如一个应用本身既可以是 Deployment，这是一个比较低程度的抽象。也可以是 Knative Service，这是一个相对来说高程度的抽象，相对于 deployment 来说比较简单，只有一个 PodTemplate。甚至可以更简单，可以是一个 Service，或者是个 Function。这个时候抽象程度就很高。如果基于 K8s 做一个以应用为中心的框架的话，它应该是能够暴露工作负载的多种抽象程度的。而不是说单独去使用 Knative，只能暴露出 Knative Service。假如我想使用 Knative 部署一个 Statefulset，这当然是不可以的。抽象程度是完全不一致的。所以我希望这个以应用为中心的 K8s 是没有抽象程度的锁定的。&lt;/p>
&lt;p>同时也不应该有能力的限制，什么叫没有能力的限制呢？比如从运维侧举个例子，运维侧有很多很多扩容策略、发布策略等等。如果我想新加一个策略能力，它应该是非常简单的，就像在 K8s 安装一个 Operator 一样非常简单，能 helm insatll 就能搞定，答案是必须的。假如需要添加一个水平扩容，直接 helm install vpa 就能解决。通过这种方式才能做一个以应用为中心的 Kubernetes。&lt;/p>
&lt;p>可以看到它跟我们的传统 PaaS 还是有很大区别的，它的可扩展能力非常非常强。它本质上就是一个 K8s，但是它跟专有的 Service，Knative，OpenFaaS 也不一样。它不会把抽象程度锁定到某一种 Workload 上，你的 Workload 是可以随意去定义。运维侧的能力也可以随意可插拔的去定义。这才是我们叫做一个以应用为中心的 Kubernetes。那么这么一个 Kubernetes 怎么做呢？&lt;/p>
&lt;p>后续我们将会在下篇文章中详细为大家解读如何构建“以应用为中心”的 Kubernetes？以及构建这么一个以用户为中心的 Kubernetes，需要做几个层级的事情。&lt;/p></description></item><item><title>Blog: 如何构建以应用为中心的Kubernetes</title><link>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%BA%94%E7%94%A8%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84Kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://desistdaydream.github.io/blog/copy/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%BA%94%E7%94%A8%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84Kubernetes/</guid><description>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/ql_AIFc0s5HwZgsML63zQA">原文链接&lt;/a>&lt;/p>
&lt;p>本文整理自 2020 年 7 月 22 日《基于 Kubernetes 与 OAM 构建统一、标准化的应用管理平台》主题线上网络研讨会。&lt;/p>
&lt;p>&lt;strong>&lt;strong>关注公众号，回复&lt;/strong>&lt;/strong>“0722”&lt;strong>&lt;strong>即可下载 PPT&lt;/strong>&lt;/strong>&lt;/p>
&lt;p>文章共分为上下两篇。上篇文章《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzUzNzYxNjAzMg==&amp;amp;mid=2247492713&amp;amp;idx=1&amp;amp;sn=63d26542a935a6b3d1cfd7a72f71425b&amp;amp;chksm=fae6efa6cd9166b0c66e73ad47be04d029d40066b7697f2f4c7cd7a53d08ba6e019419166bb8&amp;amp;scene=21#wechat_redirect">&lt;strong>灵魂拷问，上 Kubernetes 有什么业务价值？&lt;/strong>&lt;/a>》，主要和大家介绍了上 Kubernetes 有什么业务价值，以及什么是“以应用为中心”的 Kubernetes。本文为下篇，将跟大家具体分享如何构建“以应用为中心”的 Kubernetes。&lt;/p>
&lt;p>&lt;strong>如何构建“以应用为中心”的 Kubernetes？&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZwcdrrveib3FgS5lfWSW255sBic8ichpP7AYUlokiaq0vwibQ6tx6xBFiayjQ/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>构建这么一个以用户为中心的 Kubernetes，需要做几个层级的事情。&lt;/p>
&lt;h3 id="1-应用层驱动">&lt;strong>1. 应用层驱动&lt;/strong>&lt;a class="td-heading-self-link" href="#1-%e5%ba%94%e7%94%a8%e5%b1%82%e9%a9%b1%e5%8a%a8" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>首先来看最核心的部分，上图中蓝色部分，也就是 Kubernetes。可以在 Kubernetes 之上定义一组 CRD 和 Controller。可以在 CRD 来做用户这一侧的 API，比如说 pipeline 就是一个 API，应用也是一个 API。像运维侧的扩容策略这些都是可以通过 CRD 的方式安装起来。&lt;/p>
&lt;h3 id="2-应用层抽象">&lt;strong>2. 应用层抽象&lt;/strong>&lt;a class="td-heading-self-link" href="#2-%e5%ba%94%e7%94%a8%e5%b1%82%e6%8a%bd%e8%b1%a1" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>所以我们的需要解决第一个问题是应用抽象。如果在 Kubernetes 去做应用层抽象，就等同于定义 CRD 和 Controller，所以 Controller 可以叫做应用层的抽象。本身可以是社区里的，比如 Tekton，istio 这些，可以作为你的应用驱动层。这是第一个问题，解决的是抽象的问题。不是特别难。&lt;/p>
&lt;h3 id="3-插件能力管理">&lt;strong>3. 插件能力管理&lt;/strong>&lt;a class="td-heading-self-link" href="#3-%e6%8f%92%e4%bb%b6%e8%83%bd%e5%8a%9b%e7%ae%a1%e7%90%86" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>很多功能不是 K8s 提供的，内置的 Controller 还是有限的，大部分能力来自于社区或者是自己开发的 Controller。这时我的集群里面就会安装好多好多插件。如果要构建以应用为中心的 Kubernetes，那我必须能够管理起来这些能力，否则整个集群就会脱管了。用户想要这么一个能力，我需要告诉他有或者是没有。需要暴露出一个 API 来告诉他，集群是否有他需要的能力。假设需要 istio 的流量切分，需要有个接口告诉用户这个能力存不存在。不能指望用户去 get 一下 crd 合不合适，检查 Controller 是否运行。这不叫以应用为中心的 K8s，这叫裸 K8s。&lt;/p>
&lt;p>所以必须有个能力，叫做插件能力管理。如果我装了 Tekton，kEDA，istio 这些组件，我必须将这些组件注册到能力注册中心，让用户能够发现这些能力，查询这些能力。这叫做：插件能力管理。&lt;/p>
&lt;h3 id="4-用户体验层">&lt;strong>4. 用户体验层&lt;/strong>&lt;a class="td-heading-self-link" href="#4-%e7%94%a8%e6%88%b7%e4%bd%93%e9%aa%8c%e5%b1%82" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>有了应用层驱动，应用层抽象，插件能力管理，我们才能更好地去考虑，如何给用户暴露一个友好的 API 或者是界面出来。有这么几种方式，比如 CLI 客户端命令行工具，或者是一个 Dashboard，又或者是研发侧的 Docker Compose。或者可以让用户写代码，用 python 或者 go 等实现 DSL，这都是可以的。&lt;/p>
&lt;p>用户体验层怎么做，完全取决于用户接受什么样的方式。关键点在于以应用为中心的 Kubernetes，UI 层就可以非常方便的基于应用层抽象去做。比如 CLI 就可以直接创建一个流水线和应用，而不是兜兜转转去创建 Deployment 和 Pod，这两个的衔接方式是完全不一样的。pipeline 只需要生成一下就结束了。然后去把 Pod 和 Deployment 组成一个 Pipeline，那这个工作就非常繁琐了。这是非常重要的一点，当你有了应用层驱动，应用层抽象，插件能力管理，再去构建用户体验层就会非常非常简单。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_gif/US10Gcd0tQGY9ddd5GpbmVRuaRfuaESAUBGE7uHX5G0nxxLSub2QTKZdu538V7GaHXS5jsTCebYCUibaHsjg0ow/640?wx_fmt=gif" alt="">&lt;/p>
&lt;p>&lt;strong>Open Application Model(OAM)&lt;/strong>&lt;/p>
&lt;p>如果想构建一个应用为中心的 Kubernetes，有没有一个标准化的、简单的方案呢？&lt;/p>
&lt;p>&lt;strong>下面就要为大家介绍：&lt;/strong> &lt;strong>Open Application Model(OAM)。&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZ6yAC7L8wPYdL8WnH4Tr8wlvibeicaQDALk3FpYb6jPIJdgicmmaxkK1yw/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>OAM 的本质是帮助你构建一个“以应用为中心“的 Kubernetes 标准规范和框架，相比较前面的方案，OAM 专注于做这三个层次。&lt;/p>
&lt;h3 id="1-应用组件-components">&lt;strong>1. 应用组件 Components&lt;/strong>&lt;a class="td-heading-self-link" href="#1-%e5%ba%94%e7%94%a8%e7%bb%84%e4%bb%b6-components" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>第一个叫做应用层抽象，OAM 对用户暴露出自己定义的应用层抽象，第一个抽象叫做 Components。Components 实际上是帮助我们定义 Deployment、StatefulSet 这样的 Workload 的。暴露给用户，让他去定义这些应用的语义。&lt;/p>
&lt;h3 id="2-应用特征-traits">&lt;strong>2. 应用特征 Traits&lt;/strong>&lt;a class="td-heading-self-link" href="#2-%e5%ba%94%e7%94%a8%e7%89%b9%e5%be%81-traits" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>第二个叫做应用特征，叫做 Traits。运维侧的概念，比如扩容策略，发布策略，这些策略通过一个叫做 Traits 的 API 暴露给用户。首先 OAM 给你做了一个应用层定义抽象的方式，分别叫做 Components 和 Traits。由于你需要将 Traits 应用特征关联给应用组件 Components，例如 Deployment 需要某种扩容策略或者是发布策略，怎么把他们关联在一起呢？&lt;/p>
&lt;h3 id="3-应用配置-application-configuration">&lt;strong>3. 应用配置 Application Configuration&lt;/strong>&lt;a class="td-heading-self-link" href="#3-%e5%ba%94%e7%94%a8%e9%85%8d%e7%bd%ae-application-configuration" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这个就需要第三种配置叫做 Application Configuration 应用配置。最终这些概念和配置都会变成 CRD，如果你的 K8s 里面安装了 OAM 的 Kubernetes Runtime 组件，那么那就能解析你 CRD 定义的策略和 Workload，最终去交给 K8s 去执行运行起来。就这么一个组件帮助你更好地去定义抽象应用层，提供了几个标准化的方法。&lt;/p>
&lt;h3 id="4-能力定义对象-definitions">&lt;strong>4. 能力定义对象 Definitions&lt;/strong>&lt;a class="td-heading-self-link" href="#4-%e8%83%bd%e5%8a%9b%e5%ae%9a%e4%b9%89%e5%af%b9%e8%b1%a1-definitions" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>这些抽象和能力交给 K8s 去处理之后，我这些能力需要的 Controller 插件在哪？有没有 Ready？这些版本是不是已经有了，能不能自动去安装。这是第四个能力了：能力定义对象。这是 OAM 提供的最后一个 API，通过这个 API 可以自己去注册 K8s 所有插件，比如 Tekton、KEDA、istio 等。&lt;/p>
&lt;p>把它注册为组件的一个能力，或者是某一个特征。比如说 Flager，可以把它注册为金丝雀发布的能力，用户只要发现这个发布策略存在，说明这个集群支持 Flager，那么他就可以去使用。这就是一个以应用为中心的一个玩法。以用户侧为出发点，而不是以集群侧为出发点，用户侧通过一个上层的 api，特征和组件来去了解他的系统，去操作他的系统。以上就是 OAM 提供的策略和方法。&lt;/p>
&lt;p>总结下来就是 OAM 可以通过标准化的方式帮助平台构建者或者开发者去定义用户侧，应用侧的抽象。第二点是提供了插件化能力注册于管理机制。并且有了这些抽象和机制之后，可以非常方便的构建可扩展的 UI 层。这就是 OAM 最核心的功能和价值。&lt;/p>
&lt;h3 id="5-oam-会怎样给用户提供一个-api-呢">&lt;strong>5. OAM 会怎样给用户提供一个 API 呢？&lt;/strong>&lt;a class="td-heading-self-link" href="#5-oam-%e4%bc%9a%e6%80%8e%e6%a0%b7%e7%bb%99%e7%94%a8%e6%88%b7%e6%8f%90%e4%be%9b%e4%b8%80%e4%b8%aa-api-%e5%91%a2" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h3 id="1components">&lt;strong>1）Components&lt;/strong>&lt;a class="td-heading-self-link" href="#1components" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZicrjzfqhULmjvxiciaXF9tyibpvKn9priauiaUEiaEvJRLtNia1Saib4TLsggjA/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>Component 是工作负载的版本化定义，例如上图中创建一个 Component，实际上就是创建一个 Deployment。这样一个 Component 交给 K8s 之后，首先会创建一个 Component 来管理这个 Workload，当你修改 Component 之后就会生成一个对应版本的 deployment。这个 Component 实际上是 Deployment 的一个模板。比如我把 image 的版本修改一下，这个操作就会触发 OAM 插件，生成一个新的版本的 Deployment，这是第一个点。其实就版本化管理机制去管理 Component。&lt;/p>
&lt;p>第二点是 Workload 部分完全是自定义的，或者是是可插拔的。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZQDmYSgw2tyF9jpq4ibd0tPkpLG6V18L6VSOFpwPOYXK5NOosH3icCCYQ/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>今天可以定义为 Deployment，明天可以定义为一个非常简单的版本。也就是说我 Components 的抽象程度完全取决于用户自己决定的。后期也可以改成 Knative Service，甚至改成一个 Open PaaS。所以说在 Components 的 Workload 部分你可以自由的去定义自己的抽象。只要你提前安装了对应 CRD 即可，这是一个非常高级的玩法。&lt;/p>
&lt;p>此外在 OAM 中，”云服务“也是一种 Workload， 只要你能用 CRD 定义你的云服务，就可以直接在 OAM 中定义为一个应用所依赖的组件。比如上图中的redis实际上是阿里云的 Redis 服务，大概是这么一个玩法。&lt;/p>
&lt;h3 id="2trait-和-application-configuration">&lt;strong>2）Trait 和 Application Configuration&lt;/strong>&lt;a class="td-heading-self-link" href="#2trait-%e5%92%8c-application-configuration" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZV3ozMFKxZb0rx5OjNF4JK3iacnBZavqticlsEhib8ETcvlUIw6doE5dibw/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>首先 Trait 是声明式运维能力的描述，其实就是 Kubernetes API 对象。任何管理和运维 Workload 的组件和能力，都可以以这种 CER 的方式定义为一个 Trait。所以像 HPA，API gateway，istio 里面的 Virtual Services 都是 Trait。&lt;/p>
&lt;p>Application Configuration 就像是一个信封，将 Traits 绑定给 Component，这个是显式绑定的。OAM 里面不建议去使用 Label 这样的松耦合的方式去关联你的工作负载。建议通过这种结构化的方式，通过 CRD 去显式的绑定你的特征和工作负载。这样的好处是我的绑定关系是可管理的。可以通过 kubectl get 看到这个绑定关系。作为管理员或者用户，就非常容易的看到某一个组件绑定的所有运维能力有哪些，这是可以直接展示出来的，如果通过 label 是很难做到的。同时 Label 本身有个问题是，本身不是版本化的，不是结构体，很难去升级，很难去扩展。通过这么结构化定义，后面的升级扩展将会变得非常简单。 &lt;/p>
&lt;p>在一个用户配置里面，可以关联多个 Components。它认为一个应用运行所需要的所有组件和所依赖的运维能力，都应该定义为一个文件叫做 ApplicationConfiguration。所以在任何环境，只要拥有这个文件，提交之后，这个应用就会生效了。OAM 是希望能够提供一个自包含的应用声明方式。&lt;/p>
&lt;h3 id="3definition-object">&lt;strong>3）Definition Object&lt;/strong>&lt;a class="td-heading-self-link" href="#3definition-object" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZhAQ2EeRQcL8DztTJdZLrSbV4AuicsTFnmpXYokfia6jA2Fjjj7usHpAw/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>除此之外，还提到了对应管理员提供了 Definition Object，这是用来注册和发现插件化能力的 API 对象。&lt;/p>
&lt;p>比如我想讲 Knative Service 定义为平台支持的一种工作负载，如上图只需要简单的写一个文件即可。其中在 definitionRef 中引用 service.serving.knative.dev 即可。这样的好处就是可以直接用 kubectl get Workload 查看 Knative Service 的 Workload。所以这是一个用来注册和发现插件化能力的机制，使得用户非常简单的看到系统中当前有没有一个工作负载叫做 Knative Service。而不是让用户去看 CRD，看插件是否安装，看 Controller 是否 running，这是非常麻烦的一件事情。所以必须有这么一个插件注册和发现机制。&lt;/p>
&lt;p>这一部分还有其他额外的能力，可以注册 Trait，并且允许注册的 Trait-A 和 Trait-B 是冲突的。这个信息也能带进去，这样部署的时候检查到 A 和 B 是冲突的，会产生报错信息。否则部署下去结果什么都不知道，两个能力是冲突的，赶紧删了回滚重新创建。OAM 在注册的时候就会暴露出来运维能力的冲突，这也是靠 Definition 去做的。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZcuSZtGhUKRMs2jjmLD5PiaO72IKtaxbfibNcWGsliawhtqUKWXSrS3Vpg/640?wx_fmt=jpeg" alt="">&lt;/p>
&lt;p>除此之外，OAM 的 model 这层其他的一些附加能力，能够让你定义更为复杂的应用。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_gif/US10Gcd0tQGY9ddd5GpbmVRuaRfuaESAUBGE7uHX5G0nxxLSub2QTKZdu538V7GaHXS5jsTCebYCUibaHsjg0ow/640?wx_fmt=gif" alt="">&lt;/p>
&lt;p>&lt;strong>总结&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZQ4fuTxibpUFIWWVFhEoPu3GhZJT3JCk3PICeTxuk7m0xlgFOnMnJ3Lw/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>前面我们提到很多企业等等都在基于 Kubernetes 去构建一个上层应用管理平台。Kubernetes 实际上是面向平台开发者，而不是面向研发和应用运维的这么一个项目。它天生就是这么设计的，所以说需要基于 Kubernetes 去构建应用管理平台。去更好的服务研发和运维，这也是一个非常自然的选择。不是说必须使用 K8s 去服务你的用户。如果你的用户都是 K8s 专家，这是没问题的。如果不是的话，你去做这样一个应用平台是非常自然的事情。&lt;/p>
&lt;p>但是我们不想在 K8s 之前架一个像 Cloud Foundry 传统的 PaaS。因为它会把 K8s 的能力完全遮住。它有自己的一套 API，自己的理念，自己的模型，自己的使用方式。跟 Kubernetes 都是不太一样的，很难把 Kubernetes 的能力给暴露出去。这是经典 PaaS 的一个用法，但是我们不想要这么一个理念。我们的目标是既能给用户提供一个使用体验，同时又能把 Kubernetes 的能力全部发挥出来。并且使用体验跟 Kubernetes 是完全一致的。OAM 本质上要做的是面向开发和运维的，或者说是面向以应用为中心的Kubernetes。&lt;/p>
&lt;p>所以今天所介绍的 OAM 是一个统一、标准、高可扩展的应用管理平台，能够以应用为中心的全新的 Kubernetes，这是今天讨论的一个重点。OAM 这个项目就是支撑这种理念的核心依赖和机制。简单地来说 OAM 能够让你以统一的，标准化的方式去做这件事情。比如标准化定义应用层抽象，标准化编写底层应用驱动，标准化管理 K8s 插件能力。&lt;/p>
&lt;p>对于平台工程师来说，日常的工作能不能以一个标准化的框架或者依赖让平台工程师更简单更快的做这件事情。这就是 OAM 给平台工程师带来的价值。当然它也有些额外的好处，基于 OAM 暴露出来的新的 API 之后，你上层的UI构建起来会非常简单。&lt;/p>
&lt;p>你的 OAM 天然分为两类，一类叫做工作负载，一类叫做运维特征。所以你的 UI 这层可以直接去对接了，会减少很多前端的工作。如果基于 CI/CD 做 GitOps / 持续集成发现也会变得非常简单。因为它把一个应用通过自包含的方式给定义出来了，而不是说写很多个 yaml 文件。并且这个文件不仅自包含了工作负载，也包括了运维特征。所以创建好了这个文件往 Kubernetes 中提交，这个应用要做金丝雀发布或者是蓝绿发布，流量控制，全部是清清楚楚的定义在这个应用配置文件里面的。因为 GitOps 也好，持续集成也好，是不想管你的 pod 或者是 Deployment 怎么生成的，这个应用怎么运维，怎么 run 起来，还是要靠 Kubernetes 插件或者内置能力去做的。这些能力都被定义到一个自包含的文件，适用于所有集群。所以这就会使得你的 GitOps 和持续集成变得简单。&lt;/p>
&lt;p>以上就是 OAM 给平台工程师带来的一些特有的价值。简单来说是统一、标准的 API，区分研发和运维策略，让你的 UI 和 GitOps 特别容易去构建。另一点是向下提供了高可扩展的管理 K8s 插件能力。这样的系统真正做到了标准，自运维，一个以应用为中心和用户为中心的 Kubernetes 平台。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_gif/US10Gcd0tQGY9ddd5GpbmVRuaRfuaESAUBGE7uHX5G0nxxLSub2QTKZdu538V7GaHXS5jsTCebYCUibaHsjg0ow/640?wx_fmt=gif" alt="">&lt;/p>
&lt;p>&lt;strong>OAM 社区&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/yvBJb5Iiafvlb7BGwRI3ap7CLbL5GOIyZJ2k4MkCDLm5ggusQFO3JsudicVZT9OUe43OHSsLD8TRQYEVSDh4blBg/640?wx_fmt=png" alt="">&lt;/p>
&lt;p>上面最后希望大家踊跃加入 OAM 社区，参与讨论。上图中有钉钉群二维码，目前人数有几千人，讨论非常激烈，我们会在里面讨论 GitOps，CI/CD，构建 OAM 平台等等。OAM 也有亚太地区的周会，大家可以去参加。上面的链接是开源项目地址，将这个安装到 Kubernetes 中就可以使用上面我们说的这些能力了。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_gif/US10Gcd0tQGY9ddd5GpbmVRuaRfuaESAUBGE7uHX5G0nxxLSub2QTKZdu538V7GaHXS5jsTCebYCUibaHsjg0ow/640?wx_fmt=gif" alt="">&lt;/p>
&lt;p>&lt;strong>QA 环节&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Q1：&lt;/strong> 例子中提问到了 Function 的例子，是否可以理解为 Serverless 或者是 PaaS？&lt;/p>
&lt;p>&lt;strong>A1&lt;/strong>**：** 这样理解是没错的，可以理解为阿里云的一个 Function，或者是 Knative Service。&lt;/p>
&lt;p>&lt;strong>Q2：&lt;/strong> 有没有可以让我自由定义出相应的规则这种规范？&lt;/p>
&lt;p>&lt;strong>A2：&lt;/strong> 有的，在 OAM 里面有个规范，叫做 spec。spec 里面有提交容器化的规范。后面会增加更多抽象的规范。当然也分类别，有一些是非常标准化的，需要严格遵守。有一些是比较松的，可以不用严格遵守。&lt;/p>
&lt;p>&lt;strong>Q3：&lt;/strong> docker-compose 的例子可否再谈谈？&lt;/p>
&lt;p>&lt;strong>A3：&lt;/strong> 本次 ppt 中没有 docker-compose 的例子，但是这个其实很容易去理解，因为 OAM 将 Kubernetes API 分为两类，一个叫做 Components，一个叫T raits。有这么一个 Componets 文件，就可以直接映射 OAM 的概念，docker-compose 中有个概念叫做 Service，其实就是对应了 OAM 中的 Component。这完全是一对一对应关系。Service 下面有个 Deployment，有个部署策略，其实对应的就是 OAM 的 Trait。&lt;/p>
&lt;p>&lt;strong>Q4：&lt;/strong> 定义阿里云的 redis 是否已经实现了？&lt;/p>
&lt;p>&lt;strong>A4：&lt;/strong> 已经实现了，但是功能有限。内部已经实现了一个更强大的功能，通过 OAM 将阿里云的所有资源给创建起来。目前这个是在 Crossplane 去做的。但是内部更完整的实现还没有完全的放出去。我们还在规划中，希望通过一个叫做 Alibaba Opreator 的方式暴露出去。&lt;/p>
&lt;p>&lt;strong>Q5：&lt;/strong> 是否可以理解 OAM 通过管理元数据通过编写 CRD 来打包 Components 和 Traits。&lt;/p>
&lt;p>&lt;strong>A5：&lt;/strong> 可以说是对的。你把自己的 CRD 也好，社区里面的 CRD 也好，稍微做个分类或者封装，暴露给用户。所以对于用户来说只要理解两个概念——Components 和 Traits。Components 里面的内容是靠你的 CRD 来决定的，所以说这是一个比较轻量级的抽象。&lt;/p>
&lt;p>&lt;strong>Q6：&lt;/strong> 假设 Components 有四个，Traits 有五个，是否可以理解为可封装能力有 20 项。&lt;/p>
&lt;p>&lt;strong>A6：&lt;/strong> 这个不是这么算的，不管有多少 Components 和 Trait，最终有几个能力取决于你注册的实际 CRD。Components 和 Traits 与背后的能力是解耦开的。&lt;/p>
&lt;p>&lt;strong>Q7：&lt;/strong> OAM 能使用 Kustomize 生成么？&lt;/p>
&lt;p>&lt;strong>A7：&lt;/strong> 当然可以了，Kustomize 使一个 yaml 文件操作工具。你可以用这个工具生成任何你想要的 yaml 文件，你也可以用其他的，比如 google 的另一个项目叫 kpt，比如你用 DSL，json。所有可以操作 yaml 文件的工具都可以操作 OAM 文件，OAM 的 yaml 文件跟正常的 K8s 中的 yaml 没有任何区别。在 K8s 看来 OAM 无非就是一个 CRD。&lt;/p>
&lt;p>&lt;strong>Q8：&lt;/strong> OAM 是否可以生产可用？&lt;/p>
&lt;p>&lt;strong>A8：&lt;/strong> 这里面分几个点，OAM 本身分两个部分。第一部分是规范，是处于 alpha 版本，计划在 2020 年内发布 beta 版本。beta 就是一个稳定版本，这是一个比较明确的计划。现在的 spec 是有可能会变的，但是有另外一个版本叫做 oam-kubernetes-runtime 插件，这是作为独立项目去运营的，计划在 Q3 发布稳定版本。即使我的 spec 发生的改变，但是插件会做向下兼容，保证 spec 变化不会影响你的系统，我们的 runtime 会提前发布稳定版本，应该是比较快的。如果构建平台化建议优先使用 runtime。&lt;/p>
&lt;p>&lt;strong>Q9：&lt;/strong> OAM 有没有稳定性考虑？比如说高可用。&lt;/p>
&lt;p>&lt;strong>A9：&lt;/strong> 这个是有的，目前 runtime 这个项目就在做很多稳定性的东西，这是阿里内部和微软内部的一个诉求。这块都是在做，肯定是有这方面考虑的，包括边界条件的一个覆盖。&lt;/p>
&lt;p>&lt;strong>Q10：&lt;/strong> 可不可介绍下双十一的状态下，有多少个 Pod 在支持？&lt;/p>
&lt;p>&lt;strong>A10：&lt;/strong> 这个数量会比较大，大概在十几万这样一个规模，应用容器数也是很多的。这个对大家的参考价值不是很大，因为阿里的架构和应用跟大多数同学看到的是不太一样的，大多数是个单元化的框架，每个应用拆分的微服务非常非常细。pod 数和容器数都是非常多的。&lt;/p>
&lt;p>&lt;strong>Q11：&lt;/strong> 目前 OAM 只有阿里和微软，以后像 google 这些大厂会加入么？&lt;/p>
&lt;p>&lt;strong>A11：&lt;/strong> 一定会的，接下来的计划会引入新的合作方。目前 google 和 aws 都对 OAM 有一些社区的支持。本身作为云原生的一个规范，也是有一些想法的。在初期的时候，大厂加入的速度会比较慢，更希望的是用户使用起来。大厂并不一定是 OAM 的主要用户，他们更多的是商业考虑。&lt;/p>
&lt;p>&lt;strong>Q12：&lt;/strong> OAM 是否会关联 Mesh？&lt;/p>
&lt;p>&lt;strong>A12：&lt;/strong> 一定会的，但是并不是说直接 Mesh 一个核心能力，更多的说作为 OAM trait 使用,比如描述一个流量的拓扑关系。&lt;/p>
&lt;p>&lt;strong>Q13：&lt;/strong> OAM 的高可用方案？&lt;/p>
&lt;p>&lt;strong>A13：&lt;/strong> OAM 本身就是个无状态服务，本身的高可用方案不是很复杂。&lt;/p>
&lt;p>&lt;strong>Q14：&lt;/strong> OAM 考虑是单集群还是多集群？&lt;/p>
&lt;p>&lt;strong>A14：&lt;/strong> 目前是单集群，但是我们马上也会发布多集群的模型，在阿里内部已经是多集群模型。简单来说多集群是两层模型。多集群的概念是定义在 Scope 里面的，通过 Scope 来决定 Workload 或者是 Components 放到哪个集群里面。我们会在社区尽快放出来。&lt;/p></description></item></channel></rss>